question_id,question_text,topic,answer_item_1,answer_item_2,answer_item_3,answer_item_4,answer_item_5,answer_item_6,answer_item_7,answer_item_8,answer_item_9,answer_item_1_score,answer_item_2_score,answer_item_3_score,answer_item_4_score,answer_item_5_score,answer_item_6_score,answer_item_7_score,answer_item_8_score,answer_item_9_score
131582,Can I switch from Sales to Data Science/Analysis with no degree in a year?,career,"In my opinion, your career switch would be much more likely to work out if you enrol in some school to get an official diploma: 
 
 The contents of the teaching would be more structured, you're less likely to have gaps in your knowledge. 
 You're not alone to learn, so more motivation, possible discussion with teachers and students. Self-study is harder. 
 But more importantly, at the end you get a diploma that is much more likely to get you a job: without it, it's going to be much hard to convince companies that you can do the job. 
 
 I agree that it's possible to learn on your own (assuming a strong motivation), and you might even be able to get a job eventually, but you'd need luck and it would likely take much longer... I think it would be a bit like gambling, whereas getting a diploma is a sound investment.","Be very careful:  You are going to compete with a lot of people who have more than one year self education in a field that is saturated at the moment. 
 And don't underestimate maths. Watching a YouTube video is an appetizer. Not more, not less. The deep knowledge you need to truly understand what's going on is a lot, if you don't have the basics of calculus, statistics and linear algebra. 
 Programming and it stuff is also necessary, for example setting up a database.
Hard to learn, in my opinion only by doing and time. 
 The ""light"" data science stuff will be more and more done with ai tools. The Bootcamps etc. certificates probably don't land jobs anymore. Toy projects also don't really help anymore to convince someone to pay you. The millionth MINST classification does only demonstrate, that you are able to use a search engine. 
 I don't say that it is impossible. It will just be very hard and one year is a very short time. And then it is even harder to find a job.","I wanna be completely sure before making such an important decision, since I'd be spending a lot of time learning every day for at least a year and don't wanna risk wasting it 
 
 It's a gamble. 
 Here are a few things that could go wrong: 
 
 You might not cover relevant topics, or learn them in enough detail 
 You might find it hard to cover the topics as quickly as you'd like (everyone is different) 
 Employers will find it hard to tell if you have covered relevant topics in enough detail (a degree is an easy tick box, whereas understanding the scope of a DIY tuition is not) 
 South America might not be a good place to get that kind of job (I have no idea!) 
 
 Learning a sufficient amount within a year of self study and convincing someone to give you a job is.... possible. Most people would take longer than that and plenty would never get there. 
 A degree is less of a gamble, but still a gamble. 
 Here are some things which you should do regardless of the route you decide to take. 
 
 Find some local people that do the kind of job you want to do. They can offer direction, give you an idea what the job market looks like, and might be able to open some doors for you further down the line. 
 Learn some programming or some theory in your spare time. This can be tough but if you can't bring yourself to do it in your spare time are you sure you want to make a career out of it? 
 Build programs. Solve exercises on paper. The biggest mistake you can make is to ""learn"" theory by reading and not putting it into practice. Solving problems is  hard  and is required if you want to genuinely understand the theory. 
 Build a portfolio. Make it visual. Showing someone that you built something is much more impressive than telling someone that in theory you could maybe build something. 
 Try to leverage the skills you've built in your current career. They shouldn't be front and center of a job application but you should point out all the skills and experience you can bring to a job that other candidates might not have.","You already have pretty good answers to your question but 5 years ago I was in a similar situation as yours so maybe what I learnt and noticed can be of interest to you and some other people.
My situation was similar but different, compared to you I had some advantages (it was several years ago! + background in engineering and long-time self-taught programming skills) and some disadvantages (age, ...) so take what I write with a grain of salt, as just one example that may not apply to your personal situation. 
 5 years ago, I made a lot of research about Data Science and I arrived to the conclusion that the market will be soon saturated and that because of my unusual profile it would be extremly difficult to find a job as a junior Data Scientist. Nonetheless I decided to take the risk and I am very glad that I made the switch but the journey was much more difficult than I had imagined. 
 In 2025, it doesn't seem to be an easy time to look for a junior job in Data. In 2015 it was, according to what I have been told but for the past few years Data Science and GenAI became so popular that many ""schools"" and ""training institutes"" appeared (good business!) and trained  a lot of students. Now there are many very smart people from all over the world in the field, which also explains all these amazing advances that appear so frequently in AI. 
 But as another consequence, the market is saturated with junior profiles, at least in my geographic zone. It may not be the case where you live so I suggest that you try the free 1 month premium offer at LinkedIn. Like that you will be able to see how many persons answer the job offers that you are interested in. For example, where I live, after one week companies looking for a Data Scientist receive often around 400 CV ! Last month I read a post on LinkedIn from a junior Data Analyst who had sent 800+ CVs and was still unemployed. 
 Also with LinkedIn, you will be able to ""network"" with Data professionals in your country who will be able to give you useful advice and market knowledge.
You wrote that you want to work remotely but many people from all around the world want to so I don't think that it is very realistic for now. Moreover it doesn't seem that there are many remote positions for a junior profile. 
 That's why I asked you for your motivation in the comments. You will need passion and hard work to go over all the challenges that will appear on the way. Talent will help obviously. On the other side, money and wanting to work in a ""cool"" industry would not be enough motivation. Then luck is also a factor but one has to help one's luck and by definition it is not very reliable! 
 Maybe you could try to study while you keep your job. For me, discovering Deep Learning was so fascinating that for more than 1 year I kept working at my job, but I would get up 1 hour earlier everyday to read ML/DL books/websites. Often a good part of my week-ends and sometimes holidays was dedicated to study and practice Data Science. You could also enter some online competitions like Kaggle to learn faster and test yourself but it is not what will land you a job (though again I heard that some people got lucky). 
 Meanwhile if I were you, I would look now for a job in Sales (as it is your expertise today) in an AI company or a big company with Data professionnals working there (could be in any field). And in your country/city would be much better (even if the pay is less). Like that you will be able to meet local people working in Data and they will give you realistic advice tailored to your personal situation. 
 About education 
Of course, don't believe all the ads implying that after a 3 months bootcamp (or even one year) you will find a job in Data starting from scratch. It is not going to happen for 99.9% of people. There is way too much competition in the field now and the subjects to master are quite numerous and often not so easy to grasp. 
Clearly a ""real"" education in a university/school would be much better on your CV than an online ""school"" but it will take more time. Moreover, again think networking (in real life). 
This said, professional experience is much more important to recruiters than where you got your degree from. So if you take this path, whatever you choose, make sure that you will be able to work many months (hopefully a total of 1+ year) and preferably in different companies thanks to internships or apprenticeships. 
Personal projects are good but everybody have several of them. It will probably be a subject of discussion during a technical interview but it is very difficult to get one as a junior and the 1st screen will be on your CV and specially on your professional experience, even for a junior position! It is much easier for the ATS (automatic screening) and the recruiters (who naturally most of the time are not trained in Data) to count the number of years of professional experience than to guess how good is an applicant for a given job. Even more so, when they have hundreds of CV to go through. 
 Data Scientist or Data Analyst ? 
You don't seem sure so first, research precisely the differences between the different jobs in Data. What would you prefer ? Unfortunately, the job market doesn't care much about what we prefer ;-) As you don't have an engineering/math/programming background, my guess is that your best chance (though not easy) would be to target a Data Analyst job. If you really want to become a Data Scientist, after a couple of years as a Data Analyst, the switch could be possible I guess (easier in the same company) even by studying Data Science by yourself on your free time. 
Also in my country, most companies will ask for a 5 years degree for a Data Scientist position and it seems to be sometimes less for Data Analysts. Nonetheless the market for junior Data Analysts looks (I may be wrong) even more crowded than the market for junior Data Scientists. 
 Another option ? 
If you are happy with your current job, life and ""forecasted"" future, another option could be to keep your job and learn Data on your free time as a hobby. And as I wrote above, you have the advantage to probably be able to find a job in many industries so why not do Sales for a company which has a Data department, network there, keep learning and practicing, ... who knows what could happen after a few years ?","Yes, switching in a year is totally possible, but it really depends on how much time you can dedicate and how you approach your learning. The good news is that you already have experience in sales, and that’s actually a big advantage. Understanding the business side and being able to communicate insights is something that many highly technical data scientists struggle with. 
 Now, on the technical side, you’ll definitely need to learn things like Python, SQL, and statistics. But don’t stress about learning everything at once—focus on solving real problems. Get into Kaggle, play around with datasets that interest you, build projects, and upload them to GitHub. Watching courses is fine, but actually practicing with real-world data is what will make the difference. 
 As for getting a job without a degree, the reality is that some companies require one, but many others care more about what you can actually do. If you build a solid portfolio and can prove your skills, you’ll have good chances. You might not land a Data Scientist role right away, but starting as a Data Analyst can be a great way in. 
 The most important thing is that if you genuinely enjoy it, you’re not wasting your time. Just be ready to feel lost at times—that’s normal. The key is to keep making progress and not compare yourself to people who have been doing this for years. If you stick with it, you could be job-ready in a year. Go for it",,,,,52.15092707,55.10739288,52.85635666,61.30128221,59.33664327,,,,
128274,"Why does prompt engineering work, since prompt engineering questions don't appear as training data?",llm,"LLMs are usually fine-tuned on instruction-following datasets so that they can answer user requests. For instance, ChatGPT comes from  InstructGPT , which was trained from GPT-3 with reinforcement learning with human feedback (RLHF) on an instructions dataset (you can learn more about the evolution of ChatGPT/GPT-4 from the original GPT in  this answer ). 
 As for the prompts that request the LLM to impersonate a role giving better results, you can check the article  In-Context Impersonation Reveals Large Language Models' Strengths and Biases , which verifies the claim. The reason for that is difficult to figure out, given the  black-box  nature of LLMs and deep learning in general.","I believe you misunderstand how LLMs work. They are not search engines that look through a data set trying to find a best match. 
 An LLM essentially (very simplified, and I'm not an expert) generates the most probable answer to a given question, using a generalized large data set of input data. 
 The ""you are"" and other context-giving information helps the LLM to narrow down where the most probably answer should come from. As a silly example, if I were to ask an LLM without any prompts ""what is the best lunch?"" it has a huge amount of data that matches ""lunch"" and no guidance how to rate ""best"", so it will mix and match everything from nutritional value to taste to ease of preparation. If I give it context information such as ""as an expert on nutrition"" it can weigh the nutritional value interpretation of ""best"" higher than, say, the taste. 
 On SO, context information can inform the LLM which community is more relevant to an answer, or which nearby concepts to include. While, say, ""pedagogical skills"" may not appear verbatim in a possible answer, the  context  of pedagogy is more likely to be present in a relevant community than in, say, answers about a computer game that by pure word-matching might fit.","There are 3 layers to the answer why: 
 
 ChatGPT is fine tuned with around 11,000 examples of humans asking questions, and receiving helpful and intelligent responses. 
 On the surface, LLMs have only one job. To look at as much of the preceding text as possible, and give the most likely (or one of the top few most likely) next words. By taking the lead, you can say things that would be statistically more likely to be followed by intelligent responses. 
 The transformer architecture encourages the model to look beneath the surface of language, and discover deep abstractions, and layer these abstractions into complex combinations. By triggering keywords with strong connections to your desired content, the model will be more likely to respond with the vibe you are hoping for. 
 
 And yes “vibe” is probably the most accurate way to describe this!","assume an LLM is trained only on stackoverflow questions and answers. 
 
 I think that it is where you are getting things wrong. LMM are trained on pile of texts (one such dataset is even known as "" the pile ""). Adding a prompt will help the model focus on text of better quality... but we can't really get further enough in that explanation as it is very difficult to tell exactly what is happening inside a LLM. Some of the effects are measurable (on a benchmark) but that's it.","I think this article is one of the best I've seen which describes LLMs in an accessible way without dumbing it down to the point where it's unhelpful:  ""Large language models, explained with a minimum of math and jargon"" 
 One thing that might help you is to understand that LLMs store their 'knowledge' as vectors in a 'high-dimensional' space.  As an example, here's a vector representation of 'cat': 
 [0.007398007903248072, 0.0029612560756504536, -0.010482859797775745, 0.0741681158542633, 0.07646718621253967, -0.0011427050922065973, 0.026497453451156616, 0.010595359839498997, 0.0190864410251379, 0.0038335588760674, -0.0468108132481575, -0.021150866523385048, 0.009098375216126442, 0.0030140099115669727, -0.05626726150512695, -0.039609555155038834, -0.09978967905044556, -0.07956799119710922, 0.057768501341342926, -0.017375102266669273, 0.015590683557093143, -0.022376490756869316, 0.10152265429496765, -0.05138462409377098, 0.025884613394737244, 0.07069036364555359, 0.0009145145886577666, -0.06275367736816406, 0.03610750287771225, 0.050807688385248184, -0.06453944742679596, -0.0434986837208271, -0.1264101266860962, -0.0003191891883034259, 0.04311852902173996, -0.14792846143245697, -0.019480768591165543, 0.01992032676935196, 0.011479354463517666, 0.02979433164000511, 0.06154156103730202, -0.04609882831573486, -0.053286727517843246, -0.016268745064735413, 0.03660176321864128, -0.07168425619602203, 0.05497466400265694, -0.1446477174758911, 0.09316877275705338, -0.1279120296239853, 0.030971739441156387, 0.03677519038319588, 0.13407474756240845, -0.028527621179819107, -0.10431249439716339, 0.03328850120306015, 0.1295083463191986, 0.0412190817296505, 0.03605308011174202, 0.0599723681807518, 0.025970442220568657, -0.03521350771188736, -0.015058198012411594, 0.005818498786538839, 0.013812823221087456, 0.015064566396176815, 0.022925062105059624, 0.039051759988069534, 0.007009583059698343, -0.02910810336470604, 0.1011449322104454, 0.13727356493473053, 0.022466043010354042, -0.07582768052816391, -0.04469817131757736, -0.06026916950941086, 0.04192522168159485, 0.1612275242805481, 0.014356226660311222, -0.0647699236869812, -0.14182332158088684, 0.07568981498479843, 0.002798931673169136, 0.012406392954289913, -0.09695082157850266, -0.0014245212078094482, -0.018527435138821602, 0.009911706671118736, 0.013058848679065704, 0.048697732388973236, 0.017661960795521736, 0.036917395889759064, 0.005680330563336611, 0.024947546422481537, 8.419259393122047e-05, -0.002204198157414794, -0.007295176852494478, 0.008355203084647655, -0.015072236768901348, -0.0032011312432587147, 0.05527794361114502, 0.020942343398928642, -0.019445667043328285, -0.15129604935646057, 0.0337672121822834, 0.0019582323729991913, -0.0014046517899259925, -0.05954226478934288, -0.08176489174365997, 0.024112699553370476, -0.1015794649720192, 0.05419696122407913, 0.13000570237636566, -0.05808615684509277, 0.004180640447884798, 0.01880498044192791, 0.01923936977982521, -0.041859131306409836, 0.010098426602780819, 0.025394367054104805, -0.03678150847554207, 0.03255629166960716, -0.008087233640253544, -0.07101460546255112, 0.024909185245633125, -0.0369131900370121, 0.035895638167858124, 0.0047763800248503685, -0.01754925213754177, -0.0029735821299254894, 0.030521586537361145, 0.04243304952979088, 0.05969628319144249, -0.07855783402919769, -0.07639002054929733, -0.004820443224161863, 0.0651308000087738, 0.13445857167243958, -0.06609761714935303, 0.01714201085269451, 0.019574925303459167, -0.00021718056814279407, 0.07559319585561752, 0.05964002385735512, -0.0715465098619461, 0.04068697988986969, -0.09640928357839584, -0.07235930114984512, -0.05935797095298767, 0.009602724574506283, -0.05649569258093834, 0.0025645969435572624, -0.05413592606782913, -0.017797887325286865, 0.05755465477705002, 0.08609342575073242, 0.050908517092466354, -0.05604008585214615, -0.005856652744114399, 0.02329830639064312, 0.08168350160121918, -0.0718611553311348, -0.027544423937797546, -0.08970167487859726, 0.024058541283011436, -0.02770240046083927, -0.025339743122458458, 0.010991393588483334, 0.02215300314128399, -0.02829679660499096, -0.07363404333591461, 0.0556303896009922, 0.0002929845068138093, -0.059732820838689804, -0.04813411086797714, -0.0021529451478272676, 0.004276854917407036, 0.04970701038837433, 0.02516869269311428, -0.05129590258002281, 0.0767771303653717, -0.08236679434776306, 0.019983036443591118, -0.05183032900094986, 0.05824366584420204, 0.047829821705818176, -0.13605566322803497, 0.02234281599521637, -0.03254450857639313, 0.011368651874363422, -0.05135396867990494, -0.00048283161595463753, -0.06719424575567245, -0.018972834572196007, 0.025254448875784874, -0.03858991339802742, 0.036364443600177765, -0.025158191099762917, 0.030907975509762764, -0.08114158362150192, 0.09369450062513351, 0.09405472874641418, 0.012534121051430702, -0.01041880901902914, 0.0552687831223011, 0.07056140154600143, 0.06628888100385666, 0.06548195332288742, 0.01580229587852955, -0.038310837000608444, -0.0032484608236700296, -0.010157674551010132, 0.085805244743824, 0.010575438849627972, 0.06210837885737419, -0.0071502267383039, -0.02955375239253044, 0.0289775263518095, 0.002539787907153368, -0.07370137423276901, 0.026873936876654625, 0.02770836278796196, 0.02373671904206276, 0.04336617887020111, 0.037974126636981964, 0.061377692967653275, 0.05020896717905998, -0.1109858900308609, -0.02423020824790001, 0.03785136342048645, 0.18769624829292297, 0.10594339668750763, -0.05118405446410179, 0.06405289471149445, -0.047474540770053864, 0.04021701216697693, -0.048911526799201965, 0.041514985263347626, -0.005742703098803759, 0.0034058222081512213, 0.01214022096246481, -0.037784647196531296, 0.008946173824369907, -0.030592333525419235, 0.039058126509189606, 0.02660788968205452, 0.05596623942255974, -0.03365514427423477, 0.09071480482816696, 0.034562114626169205, 0.08310434222221375, 0.03441822528839111, 0.003703191876411438, 0.002236866159364581, -0.06042943149805069, 0.06852643936872482, 0.09876436740159988, 0.01411499921232462, -0.07860662043094635, 0.06403335183858871, -0.1592547744512558, -0.01012679934501648, -0.10094276070594788, 0.01604175567626953, 0.006357499398291111, 0.02171235904097557, 0.01998433656990528, -0.029795801267027855, 0.020991159602999687, 0.027527112513780594, 0.07752928882837296, -0.01912834122776985, -0.10472745448350906, -0.0327356792986393, -0.11220412701368332, 0.03347017243504524, -0.04368103668093681, -0.00044717983109876513, -0.029803894460201263, 0.06123579293489456, 0.039308369159698486, -0.055449601262807846, 0.07417158037424088, -0.022331053391098976, -0.11767527461051941, -0.04385286569595337, -0.019754905253648758, 0.031432103365659714, 0.03378641605377197, 0.07572634518146515, -0.04749307036399841, -0.005324371624737978, -0.08255213499069214, -0.010222465731203556, 0.021690042689442635, -0.1339070200920105, 0.007615163456648588, -0.0929502621293068, 0.05977592244744301, 0.00015643733786419034]
 
 The distance and mathematical relationships between vectors is an essential part of how an LLM's abilities emerge. 
 This is likely an over-simplificiation but the way I think about it (I may have read this) is that prompts help orient the LLM context within its vector space.  Prompt engineering is essentially finding ways to shift that context to a place in that vector space where it can produce better (or worse, depending on your goal) results.","The short answer: engineered prompts are deliberately similar to fragments from training data, and this similarity is sufficient to elicit desired responses. Prompts do not need to literally appear in the training data. 
 Suppose we have a series of answers to some question, and we are training a question-answering language model. The model learns that facets of the answers are related to the question, and it also learns that facets of the answers are internally related to other facets of themselves. This latter learned behavior allows the start of an answer to causally influence the rest of that answer. 
 Now, suppose that some answers are written with qualifications at the start. For example, answers might start with ""in my experience doing computer science,"" or ""as a computer scientist,"" or ""I am a computer scientist, and"". These starts are similar to each other, and we can make a probabilistic generalization about their continuations: answers which start this way are likely to continue a certain way. 
 For the prompt engineer, this is usually enough to be able to infer that a prompted answer could start with something like, ""I am a computer scientist."" in order to get answers which are continued as if they were originally written with that starting prompt. 
 Note that this engineered prompt may not yet return decent answers. It could be the case that anybody who self-identifies as a computer scientist also writes poor answers, in which case this prompt would decrease the quality of completions. Prompt engineers have to consider  word choice , just like writers in other disciplines. 
 Also note that part of your question is really about the usage of commands to the second-person ""you"". This is specific to models which have post-training (usually RLHF); they are trained to have an additional conversational behavior which doesn't necessarily exist in their training data. If I am writing on my own, then I can use a first-person perspective; I only need to talk to you in a conversational context. This is a non-trivial insight required to get conversations out of models which weren't trained for it; the model needs to be told who is ""I"" and ""you"", requiring additional prompting, framing, and parsing.",,,,52.94789353,54.14963582,50.41485275,54.86781719,59.47947969,69.9556662,,,
123229,Understanding alpha parameter tuning in LORA paper,transformer,"I had the same question. I still haven't got a convincing answer, but while searching I found these that might be helpful: 
 In  this blog , they say: 
 
 Alpha scales the learned weights. Existing  literature , including the original  LoRA paper , generally advises fixing Alpha—often at 16—rather than treating it as a tunable hyperparameter 
 
 In  literature , they say: 
 
 [...] and LoRA alpha is the scaling factor for the weight matrices. The weight matrix is scaled by  $\frac{lora\_alpha}{lora\_rank}$ , and a higher alpha value assigns more weight to the LoRA activations. We chose 16 since this was common practice in training scripts we reviewed and chose a 1:1 ratio so as not to overpower the base model. 
 
 While these two passages were clear, I still don't understand why one should scale the update weights. Also, I wouldn't have expected a ratio  $\frac{lora\_alpha}{lora\_rank} > 1$ , but in the  tutorial  I am following for applying LoRA to Whisper (an ASR model), the ratio is equal to  $2$ , with   $lora\_alpha=64$  and  $lora\_rank=32$ .","I think lora_alpha/lora_rank is a way of tuning how influential you want finetuning to be. Below is an excerpt from paper which will help explain this better 
 
 h = W0x + ∆W x = W0x + BAx 
 
 
 We use a random Gaussian initialization for A and zero for B, so ∆W =
BA is zero at the beginning of training.  We then scale ∆W x by α/r, 
 where α is a constant in r. 
 
 Pay attention the the equation and the last line. 
 So now if for a certain lora_rank(r) we choose a high lora_alpha(α), the new addition from finetuning is gonna be more influential in comparison to the previous weights as our hidden representation is effectively now
h = W0x + ∆W x = W0x + BAxα/r. But if we were to choose a comparatively lower alpha we would be more aligned towards the original pre-trained weights.","When implementing LoRA, I encountered the same issue, but after carefully reading, I realized that the purpose of alpha is to keep the value of alpha/rank constant. This means that when adjusting the rank, I can adjust the alpha value to ensure that the model's rate of change (i.e., alpha/rank) remains fixed at the original value.","During the forward pass, the output, 
 h, of a LoRA block is: 
 h=W0(x)+(α/r)BAx 
 where
W0 is the frozen original matrix, r is the rank of A and B,x is the input to the block, and α/r is a scaling factor that controls the degree to which
A and B affect the output. 
 The above data is taken from this website : https://www.determined.ai/blog/lora-parameters . 
 By this we can understand that in llm while inferencing we we add BA into W0 but instead of direct addition we assign a specific weight to BA. This weight says that how much effect of the training(i.e multiplication of B and A) we have to consider. in original paper we set this ratio as 2 that indicates we highly rely on new learning compare to original W0. While finetuning llama3.1 for learning and testing I set this α/r ratio as 1 that gives me better result. So this ratio is one of the most important hyper parameter! 
 Also from the provided Website we can see that we get the better result for higher value of α like α=512. Also from that website we can also see that there is no significant impact of r. 
 So as per my knowledge conclusion is we should keep higher value of α like 256, 512 and lower value of r like 8,16.","I also wanted to understand why is A initialized randomly and B to zero. Would it make a difference if it would be the other way around (A zero, B random?). 
 
 I think they just want initial step of fine tuning to be like we fine tune with only pretrained weight (no additional weight affect the result). One of A or B have to be all zero, so (A zero, B random?) should work too. 
 
 Also, what would go wrong if both would be set to zero? 
 
 Initializing both of them to zero might cause some issue, I think it is the same reason why we don't initialize deep learning model with zero. The gradient signal sent from A to B will be the same (all zero) and each node in A will look like the same to B. 
 Related link
 Initialize perceptron weights with zero",,,,,71.34402545,66.99988415,64.19206467,54.440789,52.91017185,,,,
120737,Is using GPT-4 to label data advisable?,machine-learning,"I agree with Jonathan Oren - in general, GPT-4 works fairly well for straightforward sentiment analysis, e.g. product reviews. One caveat though is that there are most certainly biases inherent in the dataset which was used to train GPT-4. The GPT-4 tech report here:  https://cdn.openai.com/papers/gpt-4.pdf  has much more information. 
 
 GPT-4 has various biases in its outputs that we have taken efforts to correct but which will take some time to fully characterize and manage. 
 It can represent various societal biases and worldviews that may not be representative of the users intent or of widely shared values.","GPT-4 Can understand textual context and respond accordingly. For basic labeling of sentiment I think it would work pretty well, if given the correct prompt.","Do you mean that you need to  predict  the labels for some data, or that you need to obtain the gold-standard  labels  for the data? 
 
 predicting  means that the labels are inferred by the ML system, it's a statistical process so it's likely to produce some errors. For most applications, it is important to know the level of errors and/or performance of the system used to predict. 
 obtaining gold-standard labels  usually means that the labels have been manually annotated or manually verified. Typically data annotated this way is considered very high quality, it can be used to train a ML system. A ML system cannot be used to predict gold-standard labels, unless it is proved to perform 100% correctly (and this requires evaluating the system on some labelled data, of course).","Is using GPT-4 to label data advisable? 
 
 Yes. E.g., with GPT-2: 
 
 Veyseh, Amir Pouran Ben, Franck Dernoncourt, Bonan Min, and Thien Huu Nguyen. "" Generating Complement Data for Aspect Term Extraction with GPT-2. "" In Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing, pp. 203-213. 2022.: 
 
 [Task: Aspect Term Extraction (ATE)] We fine-tune the generative language model GPT-2 to allow complement sentence generation at test data. The REINFORCE algorithm is employed to incorporate different expected properties into the reward function to perform the fine-tuning. We perform extensive experiments on the benchmark datasets to demonstrate the benefits of the proposed method that achieve the state-of-the-art performance on different datasets. 
 
 
 Veyseh, Amir Pouran Ben, Viet Lai, Franck Dernoncourt, and Thien Huu Nguyen. "" Unleash GPT-2 power for event detection. "" In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6271-6282. 2021: 
 
 We propose to exploit the powerful pre-trained language model GPT-2 to generate training samples for Event Detection (ED). [...] We evaluate the proposed model on multiple ED benchmark datasets, gaining consistent improvement and establishing state-of-the-art results for ED","The recent study  ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks  suggests that ChatGPT is more than suitable for data annotation: 
 
 Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.",,,,,60.52275213,56.9925309,64.36946137,64.88124627,50.66393306,,,,
120426,How can I ensure anonymity with queries to small datasets?,descriptive-statistics,"This is a difficult problem, regardless of how you look at it. But some solutions might be good enough. Here, you have the advantage that you are returning  aggregate  statistics, and not individual records. 
 Enforcing a minimum size for result sets is a good idea, and should provide privacy guarantees similar to k-anonymity. If a query result would be too small, you could provide a minimum count (e.g. returning  users: 25  even if the true value is  users: 4 ). 
 But if larger counts are accurate and if all possible categories are known, this may still enable recovery of the true value via multiple queries. E.g. if we know that there are 1000 records in total, that there are 599 Android users, 399 iOS users, and that the only other possible OS is Windows Mobile, we can infer that exactly 2 users fall into that category. Similarly, multiple requests with overlapping date ranges could make it possible to make inferences about small result sets. 
 Differential Privacy makes such inferences unreliable. This works by adding noise to the result, where the appropriate level of noise depends on the specific query. If all queries merely count the number of matching records, this is fairly feasible (in differential-privacy lingo, the L1-sensitivity of such queries is 1). On average, the noisy results will still be accurate (and the bit of noise won't matter on larger counts), but adversaries cannot make accurate inferences about small samples. 
 In practice, this can still be difficult to implement appropriately.
For example, an attacker might try to recover the true result of a query by issuing it multiple times.
If each response samples a different noisy result, the true value will be around the average of the responses.
E.g. Reddit fuzzes vote counts, but rolls a different number each time a vote count is requested. Request the count three times, and you'll have a good idea of the true vote count. 
 You can defend against this by using the same RNG seed for each query-response, for example by hashing the query + true result. 
 But then this makes it possible for an attacker to detect when the true value changes: as long as they get the same result they can be somewhat sure that the true value has changed. If the rate of changes is lower than the rate of queries, this could enable an attacker to estimate the true rate of changes, which may allow inferences about the true result of the query (especially since you allow queries for specific time ranges). 
 There is still the issue that an attacker can issue multiple distinct queries with overlapping result sets in order to make inferences. E.g. if an attacker issues a query for A=""Monday"", B=""Tuesday"", and C=""Monday to Tuesday"", they obtain three random variables, but they know that their expected value is related via E[A] + E[B] = E[C]. They can use this to defeat some of the added noise. This means that your Differential Privacy Budget (the standard deviation of the noise distribution) must be large enough to obfuscate the true result of all queries combined, not just the result of a single query. 
 Depending on the sensitivity of the data, I would combine a number of strategies to reduce possible inferences by an attacker: 
 
 do not provide a complete list of facets/categories/values to users 
 reject requests that go over some rate limit 
 reject requests that are very specific (e.g. too many facets, too small date ranges) 
 count the true number of matching records for the query 
 increment that count to some minimum value 
 derive an RNG seed, for example by hashing the count, current date, and normalized query. Use a cryptographically secure hash function, e.g. SHA-2 
 add random noise to the count. The standard deviation of the noise distribution should match your privacy requirements. Larger SD → more privacy, but less accurate results. Use a cryptographically secure RNG, or just use the hash as a source of random bits if it is long enough. 
 if the query could cover recent/live data: cache the noisy result for a reasonable time in order to mask changes to the true value 
 
 If you have very stringent privacy requirements, do not run live queries. Instead, determine all possible queries up front, derive a combined privacy budget from their combined query complexity, then evaluate them together and store the results. Alternatively, determine some privacy budget up front and run live queries, but stop serving responses once the privacy budget has been exhausted.","As the other answer points out, total anonymity is difficult to accomplish especially when considering an adversary who is trying to break your data privacy scheme. But, that might not be what you're really trying to do. 
 Ask yourself exactly what you are trying to accomplish. Do you believe your data is valuable and someone is going to try to steal it? Are you trying to comply with a specific data privacy law? Are you trying to satisfy ethical requirements set by an internal or external review board? Are you just trying to do a good job and sleep well at night? 
 You can only provide ""reasonable anonymity"" after you commit to a specific threat model or objective. What is reasonable for protecting data in a lawyer's office or health data under HIPAA is different from what is reasonable to protect someone's operating system and hardware revision number. 
 One point in particular to make: most data privacy laws require some kind of adherence to best practices. This is challenging because best practices change over time. If someone comes up with a new differential privacy attack tomorrow, and data handling standards change in response to that, then a company might find themselves in violation of the law simply by not keeping up with recent developments. Complying with such laws really require someone in the organization to own the data handling process and ensure that the company is staying current. Best practice compliance can also mean regular outside audits or other forms of external verification. There's no simple technical solution here like  k-anonymity: this is really an organizational and process problem more than a technical problem.","Depending on your privacy requirements, another option could be obfuscating small occurrences by pruning.  I.e. when the statistics for a particular filter are too prone to identification, just regroup them under ""Other"" or something. 
 E.g. say the results for querying about the preferred fruit for people aged between 25 and 35 owning a sidecar are  { ""apples"": 512, ""bananas"": 317, ""litchees"": 3, ""ananas"": 1 } , then return  { ""apples"": 512, ""bananas"": 317, ""other"": 4 }  instead. 
 That’s certainly not bullet proof, but probably good enough in many cases.","If you want to absolutely ensure anonymity, you'll have to do aggregation before any queries are run. Any anonymity policy applied to individual queries is vulnerable to data leakage. While it's theoretically possible to design a per-query system that prevent data leakage, you can't know for sure that there's a vulnerability you've missed. This also means that you can't add data one record at a time; new data will have to be added in batches, with the batches large enough to allow aggregation. The advantage, however, is that there is less concern about securely storing the data. If you store personal information in the database, and try to anonymize after a query has been run, then depending on the jurisdiction, you may have serious legal issues, and those issues will likely be lessened if the database stores only aggregated data.","One thing that might help would be to deterministically round the value to a multiple of e.g. 25. An attacker might be able to infer information from how the results of different queries add up, though...",,,,,60.14020728,55.74608291,53.33045998,67.47707573,53.37750444,,,,
119914,How to handle missing value if imputation doesnt make sense,data-science-model,"I think this is a good solution. You could also try to set a unique negative value for non-married people, especially if you are using a tree-based model.","You could consider setting  years_married  to -1, then it is different from columns for the ones that are just married and could thus be understood by a decision tree. 
 But remember that this might not be intuitive for users other than yourself and that some conditional statements etc. behave differently when turning NaNs into -1 values.","Your approach of a binary categorical feature, is_married definitely sounds good. 
 In some of my projects, I have checked for the percentage of missing values in a column. For instance, if a certain column has more than 40% of missing values then imputation is obviously out of the picture. It is either a replacement by -1 or dropping that column if it is not important.","If the variable is categorical and not ordered, it may make sense to create a new category 'not_married' to represent the missing values. This would allow you to keep the information about marital status and avoid imputing values that don't make sense (e.g., 0 for a newly married person). 
 
 If the variable is categorical and ordered (e.g., 0 = not married, 1 = newly married, 2 = married for 1 year, etc.), imputing a value of -1 for the missing values could make sense. This acknowledges that the missing values represent a previous state (not married) and allows you to preserve the ordering of the categories. However, this approach does sacrifice some granularity in the data. 
 
 If the variable is continuous, imputing a value of 0 may not be appropriate since it assumes that all missing values correspond to newly married individuals. One idea is to use a flag variable to indicate whether someone is married or not, and let the model learn the relationship between this variable and 'years_married'. Another option (that doesn´t reflect reality but may make this variable value less relevant to the model) is to impute values using a more complex approach, such as a regression model that predicts 'years_married' based on other variables in the dataset. 
 
 It's important to consider the context of the dataset and the modeling approach when deciding how to handle missing values. You could try different methods (e.g., imputing -1 vs. creating a 'not_married' category) and compare their performance in terms of how well they explain the variability in the outcome variable.","I'm not sure of the performance, but if it was me I would also explore categorical variables (One-hot encoding). You could make the features: 
 not_married.
newly_married (0-5 years)
a_while_ago_married (5-15 years)
long_term_married (15+ years)
 
 dumb names, but you could try it out. Also with different bin and bin sizes.",,,,,51.0394575,50.99106787,68.12589182,66.52764769,50.51254644,,,,
118260,ChatGPT's Architecture - Decoder Only? Or Encoder-Decoder?,nlp,"Summary 
 ChatGPT  is the fine-tuning of GPT-3.5, which is a language model based on a Transformer decoder with some modifications with respect to the original Transformer architecture. Therefore it is a decoder-only model. 
 Complete information with references 
 The origin of ChatGPT was GPT (Generative pre-Trained Transformer). The evolution from GPT to ChatGPT was as follows: 
 
 GPT  (see the  OpenAI announcement ) was a normal Transformer decoder. From the  GPT paper : 
 
 In our experiments, we use a multi-layer Transformer decoder [34] for the language model [...] 
 
 
 GPT-2  (see the  OpenAI announcement  and the  source code ) is also a Transformer decoder, but with some modifications. It is also bigger and trained on more data. From the  GPT-2 paper : 
 
 We use a Transformer (Vaswani et al., 2017) based architecture for our LMs. The model largely follows the details of the OpenAI GPT model (Radford et al., 2018) with a few modifications. Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the final self-attention block. A modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of  $1/\sqrt{N}$  where  $N$  is the number of residual layers. The vocabulary is expanded to 50,257. We also increase the context size from 512 to 1024 tokens and a larger batch size of 512 is used. 
 
 
 GPT-3  is GPT-2 scaled up and with some modifications. From the  GPT-3 paper  published at NeurIPS'20: 
 
 We use the same model and architecture as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. 
 
 
 InstructGPT  (see the  paper ) is a fine-tuned version of GPT-3. From the paper 
 
 [...] we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning 
 
 
 GPT-3.5  (see  OpenAI announcement ) ( text-davinci-003 ) is a fine-tuned version of InstructGPT. From the announcement: 
 
 code-davinci-002 is a base model, so good for pure code-completion tasks 
 text-davinci-002 is an InstructGPT model based on code-davinci-002 
 text-davinci-003 is an improvement on text-davinci-002 
 
 
 ChatGPT  ( gpt-3.5-turbo* ) is a GPT-3.5 fine-tuned on human instructions by Reinforcement Learning with Human Feedback (RLHF).
From the  OpenAI website : 
 
 gpt-3.5-turbo-0301 is an improvement on text-davinci-003, optimized for chat 
 
 From the ChatGPT  presentation page : 
 
 We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format. 
 To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers had with the chatbot. We randomly selected a model-written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can fine-tune the model using Proximal Policy Optimization. We performed several iterations of this process. 
 
 
 
 To follow in detail the GPT evolution, I recommend the article
 How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources .","TLDR (simplified): 
 
 encoder sees into future, decoder predicts 
 transformer sees into future and then predicts, encodes, then decodes 
 gpt doesnt see into future, it only predicts - thats why it's decoder-only 
 
 you can check karpathy's explanation:  https://youtu.be/kCc8FmEb1nY?t=6159  (~2m short, at the linked time, or for full understanding watch the whole 2h video, or even whole ~10h series) 
 basically, gpt decoder doesnt look into future, it is only predicting, so this is the decoding part.. and it is specific in that we dont know the full context of sentence.. 
 however, in case of translating from one language to another, we know the full context from the original sentence, so basically before running the decoder to predict the next part, we first encode the context from the original language 
 to simplify, you could say, in this usecase, that encoding is more about encoding what the data means into symbolic representation, while decoding is more about predicting the output - the original transformer did both of these things, while gpt only does the second part 
 (to be clear, there is always some ""encoding"" in any code, but we are talking about transformer-style encoding when deciding whether to call gpt an encoder, and such encoding is not being done. but there are word/token embeddings being done in decoder, which you could call encoding, but that is just simple data processing. but in case of transformer, there is huge neural network that does encoding)","GPT stands for Generative Pretrained Transformer. 
 ChatGPT is a fine tuning of GPT. 
 Therefore, it is a Decoder.","Amazing how much dissonance exists on this very basic question-- are GPT2 based LLM's (GPT3 and ChatGPT) encoder-decoder or decoder only models?  Googled ranked answers are all over the map with most converging on the responses here that they are ""decoder only"" models. 
 Well...... you can't decode data without first starting with encoded data.  So the answer is.... OpenAI's GPT2 and subsequent models are encoder-decoder, but are commonly referred to as decoder models because the decoder part (right side of diagram) makes them distinct from the seq2seq -> BERT model evolutionary path from which they descended.  But that does not mean they are ""decoder only"" or lack an encoder.  It means that in contrast to BERT models which are ""encoder only,"" they also invoke decoding. 
 
 All the above hypothesis only.","This is not a complete answer, but I think ChatGPT's response to the same question as the OP was asking is interesting, (1) to highlight that it can change it's answer and even be inconsistent within the same answer, and (2) the text at the end of the following answer seems like a reasonable explanation for why it said what it did before: 
 
 ChatGPT is a type of language model that uses a transformer architecture, which includes both an encoder and a decoder. Specifically, GPT-3, the model on which ChatGPT is based, uses a transformer decoder architecture without an explicit encoder component. However, the transformer decoder can be thought of as both an encoder and a decoder, since it receives input and generates output, similar to the way that an encoder-decoder architecture works. Therefore, even though ChatGPT does not have an explicit encoder component, it can be considered to have both encoding and decoding capabilities within its transformer architecture. 
 
 I appreciate the debate going on in the other answers.  I think we need to get into semantics. 
 There are three generally-accepted classes of transformer LLM architectures: encoder, decoder, and encoder-decoder.  Within this overly-simplistic taxonomy, the OpenAI GPT family of LLMs seem to best fit in the decoder branch because they do not have a full encoder like BERT, so they can't do things like extract structured data, e.g. named entity recognition, directly. 
 On the other hand, no language model can understand language without some form of encoding.  At a minimum, the LM needs to convert words into word embeddings.  This could be considered a form of encoding, though a very minimal one.  I suspect ChatGPT does much more than word embeddings, so it would have an even less-than-minimal encoder. 
 But I'm not an expert, so I can't give details here ... yet.","The  T5 paper  (section 3.2.1) has a very readable discussion of various Transformer model structures. It says 
 
 As such, a Transformer decoder (without an encoder) can be used as a
language model (LM), i.e. a model trained solely for next-step prediction
(Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). 
 
 where  Radford et al., 2018  is the GPT paper. 
 As the T5 paper explains, ""decoder"" refers to a stack of Transformer layers with masked attention that cannot look ahead to future tokens, like the decoder in the original  Transformer paper . It does  not  imply the presence of a side input from an encoder, because there is no encoder. 
 According to noe's answer, this answer for GPT should carry over to the original question on ChatGPT.","I asked my instance of ChatGPT 3.5 today (2023.12.18, state of training: January 2022) about encoder/decoder etc. and got this answer: 
 The GPT-3.5 architecture is a sophisticated model with 96 transformer layers, each containing 3072 nodes, resulting in a massive neural network. It follows an autoregressive transformer model, deviating from the traditional Encoder-Decoder structure and relying on a decoder-only setup. 
 With a vast number of transformer blocks, GPT-3.5 has an impressive scale, boasting 175 billion parameters. These parameters empower the model to comprehend and generate text across a diverse range of contexts, showcasing its versatility and depth of knowledge. 
 Trained on extensive datasets covering various topics, languages, and communication styles, the model can generate contextually relevant responses in numerous scenarios, from answering factual questions to engaging in creative and imaginative conversations. 
 Answering a further ""layman"" question it explained: 
 'In the context of transformer architectures, a ""transformer layer"" typically refers to a single self-attention mechanism layer, while a ""transformer block"" consists of multiple layers stacked together, often including not only self-attention layers but also feedforward layers and normalization layers.' 
 A further question about self-attention layers got me an example explained (abbrev.): 
 'how attention weights might be assigned to different parts of the input sequence ""why is snow white?"" 
 1.  Initialization : each word (""why,"" ""is,"" ""snow,"" ""white"") is associated with a query, key, and value vector. 
2.  Self-Attention Calculation  [..] The attention scores are determined by the compatibility (dot product) between the query vector of the current word and the key vectors of all other words. 
3.  Weight Calculation  [After using the softmax function to normalize] high attention weights indicate that the current word should focus more on certain words in the sequence. 
4. The final representation of each word is obtained by taking a  weighted sum  of the value vectors, where the weights are the attention weights. 
[..] 
 
 If the model is focusing on ""snow,"" it might assign higher attention weights to ""white"" to capture the relationship between the two. 
 The attention weights for each word are dynamically determined based on the context and the model's learned patterns during training.' 
 
 Next I actually asked ""Why is snow white?"" and got: 
 'Snow appears white because it reflects and scatters all the colors of light in the visible spectrum, creating the perception of a white color.' 
 Asking slightly different ""Who is snow white?"" I got: 
 'Snow White is a fictional character and the protagonist of a German fairy tale, famously known from Disney's adaptation.'",,,56.85953091,71.6316337,62.90139194,79.2014465,83.54263215,65.2800827,57.82452181,,
112957,Predicting deterioration of equipment on a production line,machine-learning,"From how you stated the problem it looks safe to assume that the rate of faults (# faults in instance/instance length) in a tool is a function of how long it has been operating since last fixed, as well as possibly differences in the quality of the tool itself. 
 A straightforward way to encode those assumptions would be to assume that for every instance that the tool is working, the rate of faults  $Y$  is distributed as Poisson with the time the tool was operated so far  $t$ : 
 $$\mathbb{E}(Y)=e^{\beta_i + \beta_t}$$ 
 (where  $\beta_i$  is the fault rate at time 0 for tool  $i$ ) 
 The above is also called ""Poisson regression"" (there's lot's of implementations, for example the  glm  function in r) 
 Depending on how much data you have for every tool you could let  $\beta_t$  also vary by tool. 
 
 I believe a good way to do this is to create a multiclass classifier
where per given tool, it can be classified as 'good' , 'OK' or 'bad'
and the bad needs to be fixed. Are there better ways of approaching
this? 
 
 Depending on how you run maintenance it may be a better idea to rank tools according to  $\beta_i + \beta_t \cdot t_i$  where  $t_i$  is the time that tool  $i$  has been operating so far. Then, instead of deciding on an arbitrary threshold for ""good"" vs ""bad"" you could let them fix how many tools they can manage and your model would serve to prioritize. 
 A small simulation study 
 This should make the above suggestion more concrete. 
 library(dplyr)
library(ggplot2)
# you can play around with the parameters below to better match your data
# If you have a very high percentage of 0 faults than you'd probably need to 
# use negative binomial regression instead. But this could be a good start.

N <- 100
beta <- data.frame(i = factor(1:N), beta_i = rgamma(n = N, shape=4, scale = 0.05))
summary(beta$beta_i)
beta_t <- 0.002

session_dat <- data.frame(i = factor(unlist(sapply(1:N, function(i) rep(i, sample(100:150, size = 1)))))) %>% 
  mutate(instance_length = sample(2:8, size = nrow(.), replace = T)) %>% 
  group_by(i) %>% 
  mutate(time_operated_till_instance = lag(cumsum(instance_length)), 
         instance_id = 1:n()) %>% 
  ungroup() %>% 
  replace(is.na(.),0) %>% 
  left_join(beta, by = ""i"") %>% 
  mutate(lambda = exp(log(instance_length) + beta_i + time_operated_till_instance*beta_t), 
         faults = sapply(lambda, function(lam) rpois(1,lam))) %>% 
  select(i, instance_id, instance_length, faults, time_operated_till_instance)

session_dat %>% 
  ggplot(aes(time_operated_till_instance, log(faults))) + 
  geom_point(alpha = 0.05) + stat_smooth()
 
 
 session_dat %>% filter(i %in% sample(i, 10)) %>% 
  ggplot(aes(time_operated_till_instance, log(faults), color = i)) + 
  geom_point(alpha = 0.1) + stat_smooth(se = F)
 
 
 poiss_reg <- glm(faults ~ offset(log(instance_length)) + i + time_operated_till_instance - 1, 
                 data = session_dat, family=poisson(link=log))
poiss_reg

# If you look at the coefficient for `time_operated_till_instance`
# you'll see it's pretty close to `beta_t`. The other betas are wobly, 
# but could be used as a somewhat noisy estimate of the tools quality.","It could be regarded as a predictive maintenance problem: The aim is to detect when the tool has to be repaired or replaced before it makes too many faults. 
 That's why you can detect the intervals between faults: the closer they are, the more the tool is damaged. Thanks to this information, you could alert to repair or change the tool. Algorithms like Random Forest or XGBoost can easily detect when the situation is critical if you have enough data with the whole tool service time (= until there are too many faults). 
 Like any predictive maintenance project, it is possible to detect the main root causes and see if an action could be taken before the situation worsens. This is possible if you classify faults into categories. 
 In terms of algorithms, multi-class predictors are a good option, but they work better if there are several correlated features. 
 Random Forest: 
 https://www.kaggle.com/code/irajahangari/random-forest-for-predictive-maintenance 
 https://github.com/Yi-Chen-Lin2019/Predictive-maintenance-with-machine-learning/blob/master/supervised_learning_failure_prediction.ipynb 
 XGBoost: 
 https://github.com/aws-samples/amazon-sagemaker-predictive-maintenance-deployed-at-edge/blob/master/predictive-maintenance-xgboost.ipynb 
 https://github.com/iameminmammadov/dash-predictive-maintenance 
 https://medium.com/swlh/machine-learning-for-equipment-failure-prediction-and-predictive-maintenance-pm-e72b1ce42da1","infact doesn't really fit any distribution 
 
 Have you considered Poisson distribution? The problem sounds like it's general use case.  An example of the task using R glm and more 
 
 I assume you have more data that just dates of each accidents for each tool id. Otherwise, I'm not sure I'll be able to predict a lot. Under the assumption, you may convert the task from regression to classification trying to predict ""the tool will have more than 20 faults or not"". 
 
 It can be a computer vision problem. The system that tracks the degree of wear visually if the tool guts are visible. In some situations you may use a microphone and detect a bad system state auditory. I have a lot of friends who may detect problems with a car engine this way so I'm sure this information exists in some mechanical systems. There are some other interesting examples 
 
 Imagine a case: you have a saw with a small crack. If you have this information in your data you'll be able to predict the saw is worn out and is going to break down soon. This information ""there is a crack"" is essential for a good prediction, you'll barely be able to extract it's analogue from other saw characteristics like color or length. Maybe you're able to colllect more  relevant  data","There are many ways to frame your problem. 
 One way is as a beta regression, a beta regression predicts values between 0 and 1. The target variable would be the probability of failure. The features would be all the properties of the tools (e.g., time since the last failure, total number of failures, product components, who used a tool, …). After fitting a beta regression model, the trained model would predict the probability of failure given the current values of the features. Then the tools could be rank ordered for maintenance. 
 Another issue is the level of granularity at which to fit the model. You can fit a global model for all tools, a couple of different models based on tool similarity, or an individual model per tool. If you have enough data, one model per tool would be the most precise. If not tools could be grouped together. You mention there are ""good"" and ""problematic"" tools. Often there is not enough data for individual or segmented models, so a single model is fit to all the data.","As pointed out above there is many ways to model your problem. 
 Consider survival regression  https://lifelines.readthedocs.io/en/latest/Survival%20Regression.html#model-probability-calibration  and  https://scikit-survival.readthedocs.io/en/stable/api/generated/sksurv.tree.SurvivalTree.html  for predicting the failure probability over time. 
 You could also try to predict if the machine falls into the group having less than 20 failures per year or if it falls into the other group.",,,,,50,51.68304348,53.30691003,51.99866333,53.13255548,,,,
106895,Machine Learning resources,machine-learning,"How to Learn Data Science For Free 
 Python
Corey Schafer
 https://www.youtube.com/user/schafer5 
Sentdex
 https://www.youtube.com/user/sentdex 
Machine Learning with Maths, Statistics and Linear Algebra
Andrew NG applied AI
 https://www.youtube.com/watch?v=PPLop4L2eGk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN 
Krish Naik
 https://www.youtube.com/watch?v=EqRsD3gqeCo&list=PLZoTAELRMXVOnN_g96ayzXX5i7RRO0QhL 
 Sentdex
 https://www.youtube.com/user/sentdex 
 Statquest with Josh Starmer
 https://www.youtube.com/user/joshstarmer 
 Natural Language Processing
Krish
 https://www.youtube.com/watch?v=6ZVf1jnEKGI&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm 
Sentdex
 https://www.youtube.com/user/sentdex 
 Deep Learning
Andrew Ng
 https://www.youtube.com/watch?v=CS4cs9xVecg&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0 
Krish Naik
 https://www.youtube.com/watch?v=DKSZHN7jftI&list=PLZoTAELRMXVPGU70ZGsckrMdr0FteeRUi 
 Data Science Projects
 https://www.youtube.com/watch?v=5Txi0nHIe0o&list=PLZoTAELRMXVNUcr7osiU7CCm8hcaqSzGw 
 Blogs that are freely Available
 https://towardsdatascience.com/ 
 https://medium.com/topic/machine-learning 
 Feature Engineering Playlist
 https://github.com/aikho/awesome-feature-engineering 
 Feature Selection Playlist
 https://github.com/anujdutt9/Feature-Selection-for-Machine-Learning 
 Krish Naik Featured Engineering:
 https://github.com/krishnaik06/Complete-Feature-Engineering 
 https://github.com/krishnaik06/Feature-Engineering 
 Book (Python For Finance)
 https://github.com/PacktPublishing/Hands-on-Python-for-Finance 
 Kaggle Solution
 http://ndres.me/kaggle-past-solutions/ 
 How to Learn Data Science For Free.docx
Ref :  https://www.kaggle.com/getting-started/113420 
 For more advanced resources visit  this link",Check out  scikit-learn . It is a python library which implements many ML algorithms and describes how they work to some extent.,"This is excellent, for beginners, intermediate level users, and experts too! 
 https://scikit-learn.org/stable/user_guide.html","The following are 2 handy reference books on ML and Deep Learning. There should be some free pdf versions that you can download on the web. 
 
 Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido
 https://pdfroom.com/books/introduction-to-machine-learning-with-python-a-guide-for-data-scientists/qjb5q6ykdxQ 
 
 Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow by Aurélien Géron","Have a look at „ Introduction to Statistical Learning “. This book is for beginners but written by top academics. It comes with R „labs“ (also Python version available), so that you can try things out and learn R if you don‘t do it already. 
 In case you want to dig deeper into one or another topic, you may refer to „Elements of Statistical Learning“ which is like the „grown-up“ counterpart to ISL.","If you are looking for practical python examples, check out this  website with multiple programming examples for beginners .",,,,56.94280625,52.81594327,50,67.5876599,54.25204947,50,,,
97717,What is the best way to train a model?,time-series,"Considering a team like Chelsea has played FA Cup, Champions League, Premier League and other competitions. We need to keep in mind that, other teams would also participate in the same competitions. Sports data from all teams in the competitions would help to identify Chelsea's best win against their toughest competitors that they have faced in FA Cup, Champions League, Premier League. 
 
 So to answer your  first question , you need to take training data for all competitions Chelsea's faced. 
 Coming to  second question , what is noise in this model.  
To identify noise in a signal we can use statistics, noise would be something that's happened by fluke, in other words Randomness. For better understanding, do have a look at  Signal Vs Noise in statistics you tube video . 
 
 
 The  signal  is the meaningful information that you’re actually trying to detect.  The  noise  is the random, unwanted variation or fluctuation that interferes with the signal. 
 
 
 Coming to your  final question   What is the most useful approach data science wise?   Most useful approaches are neural networks which outperformed all ML algorithms or building a hyrbid model.
   Sports Analytics for Football League Table and Player Performance Prediction : Sports analytics were
studied especially team performance prediction and player performance. This paper also provides information on comparative studies of all methods, some of the most promising methods I found are mentioned below. 
 
 Method 1 : Dixon, M.J. and Coles, S.G. Modelling Association Football Scores and
Inefficiencies in the Football Betting Market 
 
 
 predicting the result of a match was
created in 1997 by Dixon and Coles. The model is considered a classic
and was able to extract probabilities for the goals scored in a match,
following Poisson distribution 
 
 
 Method 2 :  Neural Networks   Hucaljuk and Rakipovic utilised Multilayer Perceptron, trained with Back
Propagation, equipped with conjugative–gradient algorithms. They concluded
that NNs performed better than any other ML technique they used. 
 Method 3 : Hybrid model 
Goddard, in 2005, compared the two methods, i.e. modeling the goals
scored vs modeling win–draw–lose match result and concluded that a
hybrid model achieves the best prediction performance 
 Method 4 : Rating concept widely utilised by researchers, most popular ELO Rating 
 Method 5 :Multiple regression model developed by Oberstone.  He also used F distribution to compare means of
multiple samples (i.e. one–way analysis of variance) to investigate
which pitch actions differentiate the four best teams from all the
others in the league. He managed to achieve outstanding results 
 
 
 
 Research Papers proof 
 
 Predicting sport outcomes by mere recognition 
 This paper presents power of recognition in forecasting soccer games. The studies are done on World Cup 2006 and UEFA Euro 2008. Performance measures utilized are ranking rule and odds rule 
 
 Dolores: a model that predicts football match outcomes from all over the world   
 
 Dolores provides empirical proof that a model can make a good prediction for a match outcome between teams x and y even when the prediction is derived from historical match data that neither x nor y participated in. 
While this agrees with past studies in football and other sports, this paper extends the empirical evidence to historical training data that does not just include match results from a single competition but contains results spanning different leagues and divisions from 35 different countries. 
 
 
 Predicting The Dutch Football Competition Using Public Data A Machine Learning Approach   
 
 Principal Component
Analysis (PCA), along with ML algorithms (Naive Bayes and
Multilayer Perceptron) to predict the Dutch football championship.
They achieved an accuracy of almost 55% in their predictions and
proved that a hybrid model, combining public data and betting odds
could improve accuracy","You need to include all competitions for a simple reason: you'll not have enough data if you do not. (Keep in mind that ML models generelly need large datasets while you only have a couple of matches for a given team in a given year in a given competition if it is not the national league) 
 In their paper  Learning to predict soccer results from relational data with gradient boosted trees  the authors found that even when predicting national football leagues (that is, there's no overlap in terms of teams across leagues) it worked better to train their models on the whole dataset instead of training a model per league. Even though this is task and dataset specific, it showcases that in your case restricting the model to a single competition is likely to perform worse. 
 However, it is then important to include information on the competitions in the dataset because teams might perform differently depending on the type of competition. The trivial approach being to simply include a feature indicating the type of competition but I recommend to provide your model with better engineered features. 
 For example, you could use competition specific win rates per team, average goals scored per competition or manually assigned importance ratings of each competition. This goes back to the analysis of match importance which has been discussed in football match prediction across different papers. In the paper I linked above you'll find more details on it and different approaches to tackle it. (See section 4.5) 
 Model-wise the best performance I've seen across a range papers was usually with gradient boosted trees or, in some cases, neural nets. Therefore, I'd suggest to start with boosting (for example use XGBoost) since it's more robust, easier to tune and off-the-shelf solutions are more easily available than in the case of neural nets. 
 Keep in mind to benchmark your model against a range of baseline models incl. predicting the home team as a winner and predictions based on odds (the latter will be very hard to beat).","Elo rating system  is a very useful way to model sport matches by calculating the relative skill levels of different competitors. The difference in Elo ratings between two competitors serves as a predictor of the outcome of a match. 
 One formula for soccer Elo  is:
 $$Rn = Ro + K × (W - We)$$ 
 
 Rn is the new rating 
 Ro is the old (pre-match) rating; 
 K is the weight constant. K is then adjusted for the goal difference in the game. It is increased by half if a game is won by two goals, by ¾ if a game is won by three goals, and by ¾ + (N-3)/8 if the game is won by four or more goals, where N is the goal difference; 
 W is the result of the game (1 for a win, 0.5 for a draw, and 0 for a loss); 
 We is the expected result from this formula. We = 1/(10(-dr/400)+ 1) in which dr equals the difference in ratings plus 100 points for a team playing at home. 
 
 One option is write a simulator that takes the historical data line-by-line and calculates the Elo scores for each match. After training on all historical data, the resulting Elo scores have predictive value for future games.","Lets take an example. The league is Premier League and the teams playing are Chelsea, X, Y and Z (sorry I don't follow football!). So now you have data for all 4 teams for Premier League. Now comes Champions League and the teams playing are Chelsea, Y and Z (X did not get selected for some reason). 
 Now ask yourself if you should consider data only for Champions League or other league's as well. The answer is yes you should consider data for each and every league, because what you want is predicting win or lose scenario for Chelsea, and Chelsea played against Y and Z in Premier League and both Y and Z are playing in Champions League too! So considering the performance of Chelsea (which is a measure of weather Chelsea will win or lose) you should be wise to include data from other league's as well. In fact you should take all the data of Chelsea in each and every League and against every team you can find and add it to your database. 
 Now one would be thinking the match against X in Premier League should be considered as noise because X did not qualify for Champions League.  I would say no, as predicting if Chelsea will win or not is essentially predicting Chelsea's performance (against any team!). Performance of a team does not change drastically based on a League. If a team is good, it will perform good more or less in every League and the same goes for a bad team. So if Chelsea did good in Premier against X, Y and Z, it will ""most probably"" do good in Champions against Y and Z too! 
 So, if Chelsea played against X in Premier League and we want to predict performance of Chelsea in Champions, we should include data of it's match against X. It is valuable info as far as performance of Chelsea is considered. 
 As far as models are considered, your best bets would be Random Forest, XGBoost and LGBM models (you should also play around with all the various hyperparameters...optuna! wink wink!)","Keep the data as is and then predict since the data outside the competitions does not make any difference to the performance of the player. 
 Try using Random forest since multiple variables like home team, away team, league, home score, and away score are involved, and since it uses ensemble techniques thus provides a more accurate result as compared to other algorithms.",,,,,54.01218239,55.08878633,51.32986159,50.61811748,50,,,,
88572,Why do we need data scientists if there are websites like this?,visualization,"Without having complete knowledge of the features on that website, I would say: 
 
 Data visualisation is only one part of data the scientist (ds) pipeline from data understanding thought model validation and model production as per  CRISP-DM . That site seems to be focused only on that visualisation part. 
 
 Introductory courses on data science, most of the time, work with ""ready to use"" data frames (Iris, Titanic) that do not reflect the natural way in which data must be preprocessed and aggregated to the correct level (ex., you may have accounts level data that needs to be aggregated to user's level to predict user's default) also, in many cases you have multiple sources of data stored in a variety of places like relation tables or non-structured information that will need to be queried and joined beforehand. 
 
 
 The latter role is commonly attributed as a ""data engineer""; however, the line between the two parts is less discernible in small to medium projects. However, it is essential to note that this line should be fuzzy, as more often than not, choices made by data scientists require work from a data engineering perspective - as other answers below have touched more upon. 
 And this gives a wrong impression of the simplicity of the task. 
 Besides, the above step would need for you to have at least a basic understanding of the data you are working on to create ""meaningful"" features that otherwise will end just by aggregating by min, max and average (sometimes helpful but not sufficient) 
 
 Finally, a trained model in a notebook would hardly be helpful if you cannot use that model in an ultimately deployed way (having an AWS lambda, for example)","It is a fair question, and now more than ever before with tools like Datarobot and alike...
In addition to the answer by Julio Jesus, being the point about dataset building step one of the most important ones and time consuming, some other relevant points are: 
 
 the selection of right evaluation metrics for your models is crucial for a right interpretation of your models, and that also depends on your specific use case 
 
 not only selecting a right predefined metric, but also sometimes you even have to create a custom metric to be used for your model training 
 
 labelling process: it is very important to carry out a confident labelling of each sample (when doing supervised learning) and is, usually, not straightforward 
 
 specific business case data cleaning and processing, taking into account that not all types of basic filtering methods can be applied  automatically and basic imputation strategies of missing values might be not enough (from a data quality perspective) to get a real use case sense. A nice reference for this point is a recent  tweet  by Andrew NG mentioning another tweet on this topic by François Chollet. 
 
 also about data visualization, these tools usually offer a limited predefined list of options, which you can extend more flexibly if you, as a data analyst/scientst/whatever expertise, know how to code it, interpret it and explain it","To add a few points to the good answers already proposed: 
 In general this kind of site can only propose standard methods applied to standard data. This can be pretty useful, but it covers only a small proportion of the vast diversity of problems addressed by data science. 
 In a broad sense, data science has applications in virtually every imaginable domain (e.g. medicine, astronomy, self-driving cars, machine translation...) and with every possible kind of digital data (e.g. text, speech, images, video...).
The scope of this kind of website is limited to one domain, typically solving standard business problems using standard business data. It's easy to see that the world is full of non-standard problems just by browsing a few questions here on DataScienceSE.","I work in forecasting retail sales, e.g. predicting next week's sales of a particular stock-keeping unit in the presence of promotions, price changes, day of week, calendar events, seasonality and tons of other drivers. (I used to tell my kids that daddy makes sure there is always enough ice cream at the supermarket.) 
 Often, the retailer will wonder just why a particular forecast was off. Why was a promotion underforecast so badly, or the Christmas sales overforecast? To answer that, and to improve forecasts going forward (so we don't have too much product clogging the shelves, or spoiling in the case of perishables, or too little product and unhappy customers), you need to  understand  the data, and the model, and understand if the model could be improved. Or you may need to help the customer understand that the model did the best it could, and that there is simply a lot of residual variation. In which case the question becomes one of how best to deal with this residual variation, by using higher safety stocks, or consciously allowing for stockouts. At this point, business logic enters. 
 Also, when we have a new retailer ramping up with our forecasting product, we need to map their promotion landscape to our model. Retailers can have  quite  complex promotions (buy  $n$  units of product  $X$  and  $m$  of  $Y$ , then you get  $p$  units of  $Z$  at  $x\%$  off, and  $y$  bonus points on your shopper card, and  $z$  airline miles...). Again, you need  understanding  here. AI is not quite there yet. 
 Related, though closed:  Data science without knowledge of a specific topic, is it worth pursuing as a career?  My answer there focused on the necessity of communication and business understanding to a data scientist, both of which a website won't provide.","In my opinion, you need to understand the models, plots, calculation routines to be able to draw the right conclusions out of it. 
There is absolutely no use to have, let's say a Q-Q-Plot, and lack the statistic knowledge to interpret it.
A few other examples: 
 
 ""cleaning"" data for nice and round-up plotting requires either good knowledge of the data or else, if done automatically, assumes statistical properties such as normality, non-collinearity etc. An automatic algorithm either introduces a large bias by transforming the data or will give you a false impression or relations/correlations etc. 
 F.i. when checking for (multi-)collinearity of data, you can use the correlation coefficient. This is typically the Pearson-correlation. But Pearson is only for linearly related data. Now you most often have non-linearly related data, so Pearson coeff. is low and you say: Ok, no collinearity. Instead, with proper statistic knowledge, you can select the correct coefficient and then extend the analysis with other measures such as VIF, condition index, partial pairwise correlation etc... 
 also to know what you can get out of data visualization, you need to know at least the basic set of data visualization methods. F.i. when you want to analyse  sales  dependig on the  size of your stores . But now all your large stores are in low income regions. Plotting a scatter plot of it you see no relation/correlation. So your assumption is: There is no use to build larger stores. But in fact, the interaction between store size and average regional income is concealing the ""real"" influence. You need to know  which  methods are good to find this ""hidden"" information and  how  to apply them. And, in my opinion, finding this ""hidden"" information is what discerns a real data scientist from the nowadays everyone-is-a-datascientist-hipster 
 predictions are highly biased towards preprocessing of input data, the type of the model and the model parameters. You almost never find the best model on the first try. 
 last but not least: If something is  ""super robust""  you can be 99% certain, that some assumptions have been made, introducing bias, cutting outliers which are not outliers, applying overly strong regularizations/transformations etc... If something is  super robust : Either know  why  it is super robust and have the comparison to non-""super""-robust, or be  overly skeptical  about the resulting data quality. Robustness is  never for free , so you should better know the backgrounds of it. 
 
 In my opinion, the problem is that everyone wants to be a data scientist, but no one wants to learn statistics. So we've got tons of data scientist university courses and degrees of some sub-prime universities doing data science name-dropping, but now covering the statistic background. So everyone is applying something but barely anyone knows  what  he/she is doing. 
 It has come so far, that I've seen two ""real"" statistic oriented data scientists with a master in computer science or engineering, both having a  quite solid  statistical background and working with data science on a day-to-day basis (but have no degree explicitly stating ""Data Scientist""), being told when applying for a data scientist job: ""You've got a too low experience with data science, we've hired someone with a bachelor degree in data science."" From which you know exactly: Those hiring guys have no idea about data science and they just want to get  ""something""  done... Well... This is Germany and we are known for having an extremely unflexible job market... ;) 
 So: Learn your statistics or stop ""being"" a data scientist. :)","The exact role of a data scientist varies incredibly from organisation to organisation; o the validity of this answer varies on exactly how you define data scientists. 
 But I would say that  the output of a data scientist is not a visualization;
It is the answer to a question. 
 A data scientist is asked a question like:  ""why were online sales down in March?"" 
They dive into the data and try and answer it.
So a possible answer is:  ""Our web site was really unresponsive in March, and pages kept timing out"" 
And they will support that, and discour that by looking at and showing visualiations.
E.g. that historically when sales have been down it also has corresponded with increased latency, and that this was occuring also in March.",,,,61.29071021,51.88750326,54.78479332,56.0821445,61.96498008,64.79404394,,,
87114,How do I encode the categorical columns if there are more than 15 unique values?,regression,"If you have high cardinality categorical data(+10 distinct values) you can do Target Encoding. 
 One hot Encoding in high cardinality scenarios has the following drawbacks: 
 
 The input data for the model becomes very wide, and neither an optimal nor an efficient approach is guaranteed. 
 
 The created features become sparse(most of the levels hardly appear in the data) 
 
 One Hot Encoding does not handle new or unseen categories 
 
 
 One option here is to do target encoding. 
 Here is an  intro . Find  here  a python package with several implementations. 
 Target Encoding benefits: 
 
 High cardinality problem is handled 
 Categories are ordered allowing for easy extraction of the information and model simplification 
 
 Drawbacks 
 
 Overfitting 
 
 For a more comprehensive literature review: 
 
 Quantile Encoder: Tackling High Cardinality Categorical Features in Regression Problems 
 Fairness implications of encoding protected categorical attributes","While most answers here suggest to use various encoding schemes, I would like to propose a different approach: collapsing categories. The idea is that if there are two (or more) similar categories, you can unite the, into a single category, thus reducing the dimensionality of the feature/variable. Also, if there are some categories with expected low frequencies, you may collapse them into as ""other"" category.","[edit] See also Carlos' answer, I think it's better than mine. 
 You should use one hot encoding for the categorical features. Replacing categorical values with numerical ones would be a bad idea, because it introduces order between the values and the model would try to find patterns based on this order (e.g. 'x < 4'). 
 If there were really too many different values then it's often a good idea to remove or replace the least common ones, but it doesn't seem to be an issue with this data. 
 For the record, more than 15 different values is nothing to be afraid of. For example when working with text data it's common to work with thousands of values for a word.","You are right that most of the algorithms can digest only numerical data, i.e. the categorical features need to be converted to the numerical ones before running the regression. 
 Besides straightforward one-hot encoding and already mentioned target encoding you could try the following approaches 
 
 Collapse the less frequent categories into one bucket (e.g. ""other"") and then apply the one-hot encoding. In particular, this approach might work fine for the variable ""country"" in your dataset: 7 most frequent categories / countries cover more than 95% of observations. 
 
 Use some proxy features, i.e. instead of using the country name directly take some relevant statistics for this country. These would be problem specific, but you could start with something like population, area, population density, income per capita, GDP, GDP per capita, statistics about education levels.","There are a number good options for encoding categorical data with high cardinality. If using Python, the  Category Encoders package  has like a dozen options as of this writing. I wrote a guide to using some of them  here . 
 Category Encoders includes several Bayesian encoders, including Target encoder. 
 Binary and Hashing encoders help reduce dimensionality, but might make it a bit harder for your model to pick up signal in your data.",,,,,59.27018881,51.65979399,60.84754403,54.54652791,61.70324727,,,,
86632,Why is it wrong to train and test a model on the same dataset?,machine-learning,"Yes, you put it quite correctly. 
 As a teacher, you wouldn’t give your students an exam that’s got the exact same exercises you have provided as homework: you want to find out whether they (a) have actually understood the intuition behind the methods you taught them and (b) make sure they haven’t just memorised the homework exercises.","It is wrong because: 
 
 it is fundamentally incorrect  (a theoretical concern) 
 it leads to bad results  (a practical concern) 
 
 
 It is fundamentally incorrect because usually  the objective of testing a model is to estimate how well it will perform predictions on data that the model didn't see . 
 It's quite hard to come up with good estimates of real-world performance,  even when you do everything correctly. If you use training data to estimate the performance the result is worse than useless, it's actively misleading. 
 There's several ways that doing this can lead to bad results. 
 Overfitting 
 If you're training a complex model with small amount of data, your model is very likely to  overfit . In a simplified way, we can say that if the has a lot of ""memory"" (parameters), it memorizes the training data, and fails to understand its underlying structure. 
 Imagine that you're building a model that predicts house price based on the floor area. Your training set looks like this: 
 area  price
30    100001
50    150002
80    200003
 
 You train your model, then ask it to predict the price for a house of area=50, and it tells you that the price should be 150002. Is that impressively accurate? Not really. It's just memorizing the training data. 
 Overfitting is commonly detected through a large difference in performance between the training and test set. If you test on the training set, you're unable to detect overfitting. 
 Concept drift 
 If you make sure you're training a very simple model on a large amount of data, even if there's no overfitting, it's common for models to suffer from  concept drift . 
 This basically means that the underlying structure of the data can change over time. For example, trying to predict how many sales a store is going to make on the weekend after training on data from Monday to Friday. 
 If your test data is not diverse enough along the time dimension vs the training set, you won't catch that problem.","It can happen that the model you train learns ""too much"" or memorizes the training data, and then it performs poorly on unseen data. This is called ""overfitting"". 
 The problem of training and testing on the same dataset is that you won't realize that your model is overfitting, because the performance of your model on the test set is good. The purpose of testing on data that has not been seen during training is to allow you to properly evaluate whether overfitting is happening.","Simple answer:  circular reasoning . The fact that your model ""knows"" the answer to something you've already told it the answer to really doesn't prove anything. 
 Put another way: the entire point of testing is to get some sense of how well your model would do with data it hasn't seen yet, and testing it with data that it  has  already seen doesn't do that.","To express it in a different way, that might be more useful when explaining to impatient stakeholders: 
 Imagine that you go to a travelling fair and a lady with many shawls and a crystal ball tells you, ""I can look at a person and tell them if they are married or not."" You are not sure if this is for real. 
 If she starts pointing at her colleagues from the fair and tells you, ""he is married, she isn't, the other woman also isn't"" - what does this tell you? Nothing. She already knows these people, she knows who is married! To start trusting her ability, you want her to make her guesses about people she's never seen. 
 In data science, you always have the problem whether people (including you!) should trust the model or not. It can prove itself by showing that it can find information which it didn't know beforehand. It has to know its training data by definition, so your only option is to keep some data ""hidden"" from it (the test set). 
 In fact, it is ideal that, if you suspect your data is too uniform, to do a second testing with a different dataset created in a different way, to confirm it is working in general. This is done mostly in science, if data is available, e.g. if you trained and tested data on patients from one hospital, you ideally try it on patients from a different hospital, just in case data was coded differently, or you had selection bias or whatever.","Is it possible that the model starts to learn the images by heart instead of understanding the underlying logic? 
 
 If the model memorizes the training data when that same data is used for the ""test"" set, it would still memorize the training data when different data was used for the ""test"" set.  Using a separate ""test"" set  cannot prevent  that memorization from happening.  More generally, the ""test"" set has no direct impact on model training. 
 However, the separate ""test"" set does allow the researchers to  identify  that the model is indeed memorizing the individual data samples and targets instead learning the underlying patterns.  When the researchers see the loss decreasing on the training set but increasing on the ""test"" set, they know this overfitting is taking place.  At that point they can tune the model's hyperparameters, specifically trying to lower the model's capacity (i.e. number of nodes and/or layers), and then retrain the model to see if the issue has been resolved. 
 Because the researchers use the ""test"" set to tune the model's hyperparameters, the ""test"" data can have an indirect impact on the final model's performance.  It could be that the researchers pick hyperparameters that work well for ""test"" set, but not for the data in general.  For that reason, it is sometimes recommended to use 3 distinct data sets:  the training set used to train the model, the initial ""test"" set used to address overfitting and other issues by tuning hyperparameters (this is more commonly known as the validation set), and a final test set which is only used to evaluate the finalized model (and has no impact, direct or indirect, on model training).","To give a simple illustration of how bad overfitting can be, consider the example of fitting (training) a polynomial of order equal to the number of points of data you have. In this case I've generated data with a slope and some normally distributed random noise added. If you test it with exactly the same x & y values that you used to generate the polynomial fit by looking at the residuals, all you see is the numerical error, and you might naively say it's a good fit, or at least better than the linear fit (plotted in green) which has much larger residuals. If you plot the actual polynomial you get (in red), you'll probably see that it actually does a terrible job of interpolating between this test data (since we know that the underlying process is simply a straight line), like so: 
 
 If you generate a new set of data with the same x-values, you see that as well as failing at interpolating, this performs about the same as the linear fit in terms of residuals: 
 
 And perhaps worst of all, it fails spectacularly when attempting to extrapolate as the polynomial predictably blows up in both directions: 
 
 So if ""prediction"" for your model is interpolating, then overfitting makes it bad at that and won't be detected unless you test it on non-training data. If prediction is extrapolating, then most likely it's  even worse  at that than it is at interpolating, and again you won't be able to tell unless you test it on the right kind of data. 
 Python code used to generate these plots: 
 import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)

nSamples = 15
slope = 10
xvals = np.arange(nSamples)
yvals = slope*xvals + np.random.normal(scale=slope/10.0, size=nSamples)

plt.figure(1)
plt.clf()
plt.subplot(211)
plt.title('""Perfect"" polynomial fit')
plt.plot(xvals, yvals, '.', markersize=10)

polyCoeffs = np.polyfit(xvals, yvals, nSamples-1)
poly_model = np.poly1d(polyCoeffs)
linearCoeffs = np.polyfit(xvals, yvals, 1)
linear_model = np.poly1d(linearCoeffs)
xfit = np.linspace(0, nSamples-1, num=nSamples*50)
#yfit_dense = poly_model(xfit)
plt.plot(xfit, poly_model(xfit), 'r')
plt.plot(xfit, linear_model(xfit), 'g')

plt.subplot(212)
plt.plot(xvals, poly_model(xvals) - yvals, 'r.')
plt.plot(xvals, linear_model(xvals) - yvals, 'g.')
plt.title('Fit residuals for training data (nonzero only due to numerical error)')

#%% Testing interpolation
plt.figure(2)
plt.clf()
test_yvals = slope*xvals + np.random.normal(scale=slope, size=nSamples)
plt.subplot(211)
plt.title('Testing ""perfect"" polynomial fit with new samples')
plt.plot(xvals, test_yvals, '.', markersize=10)
plt.plot(xfit, poly_model(xfit), 'r')
plt.plot(xfit, linear_model(xfit), 'g')

plt.subplot(212)
plt.title('Fit residuals for test data')
plt.plot(xvals, poly_model(xvals) - test_yvals, 'r.')
plt.plot(xvals, linear_model(xvals) - test_yvals, 'g.')

#%% Testing extrapolation
extrap_xmin = -5
extrap_xmax = nSamples + 5
xvals_extrap = np.arange(extrap_xmin, extrap_xmax)
yvals_extrap = slope*xvals_extrap + np.random.normal(scale=slope, size=len(xvals_extrap))

plt.figure(3)
plt.clf()
plt.subplot(211)
plt.title('Testing ""perfect"" polynomial fit extrapolation')
plt.plot(xvals_extrap, yvals_extrap, '.', markersize=10)
plt.plot(xvals_extrap, poly_model(xvals_extrap), 'r')
plt.plot(xvals_extrap, linear_model(xvals_extrap), 'g')

plt.subplot(212)
plt.title('Fit residuals for extrapolation')
plt.plot(xvals_extrap, poly_model(xvals_extrap) - yvals_extrap, 'r.')
plt.plot(xvals_extrap, linear_model(xvals_extrap) - yvals_extrap, 'g.')",In addition to all the good explanation about overfitting I would also quote the  Goodhart's law . Since you trained by optimizing the train loss (some kind of metric) it is has thus become a bad metric to measure the quality of your model.,"The problem is base dataset is used for training of NN. Test data is used for validation test. Now if u have one and only one relevant data set on which u want to make future prediction u have a problem how to do it. U can split that original dataset once u have determined hyperparameters to what ever proportion 50/50%, 70/20%, 80/20% or ""leave one out"" using K-Folds, fixed proportion of Columns, Bootstrap, Random sampling, Stratified ... so far things are clear. In order to NN is able to learn u have to specify target column. if your target column is let's say c1000 and u want to predict c1001 u cant do that on same dataset directly. Now u have to make second dataset from base dataset. U can ""randomize"" in small percentage the whole base dataset or only proportion of so NN doesn't see the ""same data"" and predict future c1000 on that dataset if that suits u. Margin of error will be small enough to not loose achieved accuracy. That's one solution. The other is u shift the whole or only ""test data set proportion"" to left in new dataset so u have prediction target on test file c1001 shifted to c1000 (that's now becomes proper target) on base dataset and on test or prediction dataset. U have now c1000 shifted to left becoming c999 and ""c1000"" becomes now prediction column that is actually that c1001 on base dataset u want to predict. Look at x = kx which we shift to y = kx-1, the same goes for polynomial functions. That is what i use with bootstrap method without randomizing. But that comes with some caveat's. The whole hyperparameter determination that was valid for base dataset goes down the drain (that -1 on test file isn't included in learned model). U now have to train your NN on that shifted to left database and determine hyperparameters for that approach. Now we have solved that c1000 and c1001 prediction obstacle but accuracy will suffer and u will have a lot of work to do to search ""proper"" hyperparameters and tuning work beforehand to do, exploring hidden layers more deeply and more wide. That is something that no one will tell you, but is crucial to perform forecast this way (have not found that information nowhere nor tutorials that covers that approach, u will trial test a lot, tedious and time consuming). The other solution is if u can to transform your dataset to ""time series"" and use time series NN's (that's whole different approach) but doesn't require target column (non linear regression that simply continues beyond that c1000 column) only for binary, numerical values datasets. Maybe that works for multiclass dataset too, don't know didn't try or use. At the end u can try to use both approaches time series NN to forecast c1001 (u wont have high accuracy on time series but still better as guessing or poor predicted c1001) and then feed those results to base dataset making it target and try to forecast on test dataset c1001 normally (small percentage randomized when learning as test file and no randomizing on prediction file so to say preserving original but only shifted -1 column left).To validate that approach u can start predicting from c900 up and for known values validate predictions and tune up your hyperparameters for better accuracy. Hope that clarify that topic and helps due a lot of user is failing to do so. That's one thing, the other is used software, that can be very different in use for same/similar principles.",50,60.48227438,61.5722094,54.2098121,55.34160725,58.51973189,52.404595,53.37628725,58.87793328
84567,Why does my model produce too good to be true output?,machine-learning,"Assuming that these results are obtained on a valid test set with no data leakage, these results don't show overfitting because overfitting would cause great performance on the training set but significantly lower perfomance on the test set. 
 Make sure that your instances between the training and test set are truly distinct: there might be some data leakage, for example if there are duplicate records for the same patient. 
 Another potential flaw is the gold standard labeling: if the patient has this T2DM  in their medical record, it means that they are already diagnosed right? And since the features are also based on the medical record, it's likely that this medical record contains direct indications about this diagnosis (for example in the drugs prescribed to the patient). There are two interpretations about this: 
 
 either the task is purposefully defined by this T2DM label, and in this case you can just enjoy the great performance but it's not technically about detecting diabetes in general. 
 or the goal is to detect patients with diabetes including the ones who are not diagnosed yet, but then it's likely that your gold standard is incorrect for this task. 
 
 
 [edit following updated question] 
 Your update clarifies which exact task you're targeting, and it corresponds exactly to my first interpretation above: given that your goal is to predict which instances satisfy this T2DM criterion and that the features contain direct indications about it: 
 
 I think you're right to keep these features, if a very useful information is available it would be absurd not to exploit it (assuming it's also available in the same form in any future dataset you plan to use, of course) 
 The very high performance you obtain makes perfect sense for this task, it's not a bug. It simply happens that the task is easy, so the system is able to predict the label very well. 
 
 However this also means that you could do even better without any ML: currently the ML method gives you around 99% F-score because it doesn't perfectly represent the criterion used for the gold. But since the gold standard label is based entirely on the features, the most direct way to ""predict"" the label is to apply the criterion. There's no point using a complex ML method to predict an information that you can obtain from the same data more accurately with a deterministic method.","Might be a case of Data leakage.
 For 1370 features, 2475 is a very small dataset for such an extreme result. 
 Please try  -
  Inspecting the misclassified records.
  Try removing the T2DM feature and note the dip
  Repeat the last step for all the features.  You must observe a negligible dip for other features and a very large dip for any feature which is causing the leakage.","It pretty much looks like overfitting. It would be also interesting to know which algorythm did you use. Some are really sensitive to low number of instances / big number of features, and yYou have almost so many features as instances. 
 Trying checking first correlation between features and reduce the number of features with PCA or another method, before fitting your model again.","It sounds like the system can just learn your algorithm for labeling. If that is intended then you can just use that and throw away all the ML. If you want to predict, for example, the diagnosis of icd9=250, then of course there is no point to include icd9 as feature. Alternatively, if there is a history, you can use the record  just before the diagnosis of diabetes  as training example. You said you didn't want to predict whether a patient will be diabetic in the future. But you  do  want to predict whether someone is diabetic right now even if not diagnosed, right?",The best approach is to use a model like Decision Tree to see what actually is happening. Maybe there are couple of features in there that correlate in a big way to the label and the rest of the 1000+ features dont matter at all. It is possible (as someone else too point out) that one of the feature hiding in there (an icd with a certain response code) has a direct bearing the output label.,,,,,50,50,50.75495108,50,52.30454456,,,,
81617,How to combine and separate test and train data for data cleaning?,python,"Add an indicator column while concatenating the two dataframes, so you can later seperate them again: 
 df = pd.concat([test.assign(ind=""test""), train.assign(ind=""train"")])
 
 Then later you can split them again: 
 test, train = df[df[""ind""].eq(""test"")], df[df[""ind""].eq(""train"")]","There are several methods to choose from. If you insist on concatenating the two dataframes, then first add a new column to each DataFrame called  source . Make the value for  test.csv  'test' and likewise for the training set. 
 When you have finished cleaning the combined  df , then use the source column to split the data again. 
 An alternative method is to record all the operations you perform on the training set and simply repeat for the test set. This won't work it you normalise values based on the population.","Method 1:
Develop a function that does a set of data cleaning operation. Then pass the train and test or whatever you want to clean through that function. The result will be consistent. 
 Method 2:
If you want to concatenate then one way to do it is add a column ""test"" for test data set and a column ""train"" for train data set. Perform you operation then use python split to again divide it into 2 dataframe 
 data[data['type']==""test""]","before concatenation  of test and train data. add new column to train and test data called type. And after preprocessing separate them based on column type. Here is a sample code. 
 test = pd.read_csv('test.csv')
train = pd.read_csv('train.csv')

test['type'] = ""test""
train['type'] = ""train""

df = pd.concat([test, train])

preprocess(df)

df.drop(['type'],axis = 1,inplace = True)

train = df[df['type'] == ""train""]

test = df[df['type'] == ""test""]","If you don't remove any rows during data cleaning, you can simply do: 
 train = df.loc[train.index, :]
test = df.loc[test.index, :]",,,,,60.21132691,57.37676518,67.28269967,64.44240307,62.43744129,,,,
80531,Is there any way to explicitly measure the complexity of a Machine Learning Model in Python,machine-learning,"I have not heard of any model agnostic way to measure model complexity. There are several strategies but they are model dependant. 
 You can tackle the problem using different families of models. 
 
 For  linear models  you can count the number of nonzero parameters that is using. Number of features used for the prediction. 
 
 For  decision tree  you can count the maximum depth that the tree achieves. 
 
 For  Neural Networks  you can count the number of parameters that your NN is optimizing. 
 
 For  ensemble methods  (random forest, gradient boosting) you can use an aggregation of the different weak learners used in the model. 
 
 
 For the python implementation there are several implementations depending on for which model you want to measure it. Some of them if you notice are really easy to measure. 
 Its intuitively hard to compare complexity between different model families. What is more complex a linear regression with 4 coefficients or a decision tree with max_depth=3? 
 On the topic of deep learning complexity, Hinton, Oriol, Jeff Dean published a paper  Distilling the knowledge of a Neural Network . Where they talk about simplifying the complexity of a Neural Network.","As you probably know, ""complexity"" is a loaded term in computer science. Normally, complexity is measured in ""big-O notation"" and has to do with how solutions scale in time as the number of inputs grows. For example,  this post  discusses the computational complexity of convolutional layers. 
 In deep learning, however, competing neural network architectures are generally applying the same algorithm (back-propagation) to the same types of problems (e.g., ImageNet classification); the only difference is the architecture. Further, most architectures use similar computational elements (e.g., convolutional layers and linear layers). Thus, it is a convention to use the  number of parameters  as a stand-in for complexity. It is true that this is only an approximation: two networks may have the same number of parameters but require different numbers of operations. But it's generally a good approximation, given that different architectures generally have the similarities noted above, but can have sizes that differ by several orders of magnitude. 
 As a reference, consider Figure 1 in the  EfficientNet Paper . They use the number of trainable parameters as a stand-in for ""model size"" and note that the number of parameters is more-or-less linearly correlated with runtime. 
 As for a Python function that counts the number of trainable parameters, this will depend whether you are using Keras, Tensorflow, PyTorch, etc. In Keras, this is one line:  model.count_params() . In PyTorch, you can calculate it from  model.parameters()  as discussed  here .","It's perhaps a bit naive but the first idea that comes to mind is to simply count the number of parameters which have to be estimated during training: the more values need to be estimated, the more complex the model is, since the hypothesis space is larger. For example a linear model needs only  $n+1$  parameters (with  $n$  the number of features), whereas the number of parameters in an ensemble model needs is the sum of the numbers of parameters for every learner, so it's likely to be higher. This idea could be refined to take  into account the range of values of a parameter. 
 As a very rough approximation, one could simply calculate the size of the object which represents the model in python (assuming the representation of the model is space-efficient, it might not always be the case).","One option is  Bayesian information criterion (BIC)  which is a model selection criterion that attempts to rewards modeling fit, as measured by maximizing likelihood, while penalizing the number of parameters. 
 One implementation of BIC is in the  RegscorePy  package.","As mentioned by other answers here, when we talk about model complexity we are usually thinking about the number of parameters the model learns. When someone talks about comparing to a less complex model, they often mean comparing to an intuitively less complex model (either a model in the same class, e.g. a neural network with fewer neurons, or a model from a simpler class, e.g. a linear model rather than a random forest). 
 One way to think about model complexity between very different models is  Kolmogorov Complexity , and you can approximate this by looking at the amount of space occupied by your saved (e.g. pickled) models. In the example you gave, the ensemble would occupy more disk space than the linear model, unless the ensemble was simpler than the linear model (e.g. an ensemble of two linear models with 10 learned coefficients each versus a linear model with 200 learned coefficients).","1. But, what would it be a way to numerically measure the model's complexity in order to be able to compare two or more models in such terms? 
 You can use the VC dimension to measure the complexity of a model in a numerical format.
See  Vapnik–Chervonenkis dimension on Wikipedia . 
 2. Is there any python implementation that can help with such a task? 
 There is already a stack exchange link that explains about VC dimension.
 How to calculate VC-dimension?",,,,60.02078437,54.13018253,55.50576542,52.68628888,57.78972884,62.2178538,,,
77684,Time-series grouped cross-validation,machine-learning,"As @NoahWeber mentioned, one solution is to: 
 
 split by customer ids  (A) 
 do the time series split on all dataset  (B) 
 keep in the training  (resp. testing)  dataset only the data from training  (resp. testing)  customers split  (A)   and  from training  (resp. testing)  time series split  (B) . 
 
 Below is a code sample I was writing at the same time he answered. 
 import pandas as pd
import numpy as np
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import TimeSeriesSplit

# Generating dates
def pp(start, end, n):
    start_u = start.value//10**9
    end_u = end.value//10**9

    return pd.DatetimeIndex((10**9*np.random.randint(start_u, end_u, n, dtype=np.int64)).view('M8[ns]'))

start = pd.to_datetime('2015-01-01')
end = pd.to_datetime('2018-01-01')
fake_date = pp(start, end, 500)

# Fake dataframe
df = pd.DataFrame(data=np.random.random((500,5)), index=fake_date, columns=['feat'+str(i) for i in range(5)])
df['customer_id'] = np.random.randint(0, 5, 500)
df['label'] = np.random.randint(0, 3, 500)

# First split by customer
rkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=42)
for train_cust, test_cust in rkf.split(df['customer_id'].unique()):
    print(""training/testing with customers : "" + str(train_cust)+""/""+str(test_cust))

    # Then sort all the data (if not already sorted)
    sorted_df = df.sort_index()

    # Then do the time series split
    tscv = TimeSeriesSplit(max_train_size=None, n_splits=5)
    for train_index, test_index in tscv.split(sorted_df.values):
        df_train, df_test = sorted_df.iloc[train_index], sorted_df.iloc[test_index]

        # Keep the right customers for training/testing 
        df_train_final = pd.concat( [ df_train.groupby('customer_id').get_group(i) for i in train_cust ])
        df_test_final = pd.concat( [ df_test.groupby('customer_id').get_group(i) for i in test_cust ])
 
 Note : Generating random dates is based on  this  post 
 Note bis : I tested the generated training/testing dataframes ready for cross-val with this sample code that you can add right after the line  df_test_final : 
 # Test condition 1: temporality
for i in range(len(df_test_final)):
    for j in range(len(df_train_final)):
        if df_test_final.index[i] < df_train_final.index[j]:
            print(""Error with "" + str(i) + ""/"" + str(j))

# Test condition 2: training customers are not in testing final df
for i in train_cust:
    if i in df_test_final['customer_id'].values:
        print(""Error in df_train with "" + str(i) + ""th customer"")
    
# Test condition 2: testing customers are not in training final df
for i in test_cust:
    if i in df_train_final['customer_id'].values:
        print(""Error in df_train with "" + str(i) + ""th customer"")
 
 
 Here is a pseudo-code implementation: 
 function keep_customer_ids( data, ids ):
    goal: this function returns a subset of data with only the events that have
          an id tag that is in ids
    data: labeled events containing features, date and a customer id tag
    ids: the customer ids you want to keep
    for event in data:
        if event has a customer id tag that is in ids, keep it
        else, drop it
    return data

algorithm:
    for the number of cross-val you want:
        customer_train_ids, customer_test_ids = split_by_customers( customer_ids )
        train_data, test_data = split_data_in_time_series_way( data )
        final_train_data = keep_customer_ids( train_data, customer_train_ids )
        final_test_data = keep_customer_ids( test_data, customer_test_ids )
        do_the_fit_predict_things( final_train_data, final_test_data )","Here is a solution based on @NoahWeber and @etiennedm answers. It is based on a juxtaposition of splittings, a 1) repeated k fold splitting (to get training customers and testing customers), and 2) a time series splits on each k fold. 
 This strategy is based on a time series' splitting using a custom CV split iterator on dates (whereas usual CV split iterators are based on sample size / folds number). 
 An implementation within sklearn ecosystem is provided. 
 Let's restate the problem. 
 Say you have 10 periods and 3 customers indexed as follows : 
 example_data = pd.DataFrame({
    'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
    'cutomer': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
    'date': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
})
 
 We do a repeated k fold with 2 folds and 2 iterations (4 folds in total) and within each k fold split we split again with time series split such that each time series split has 2 folds 
 kfold split 1 : training customers are [0, 1] and  testing customers are [2] 
 kfold split 1 time series split 1 : train indices are [0, 1, 2, 3, 10, 11, 12, 13] and test indices are [24, 25, 26] 
 kfold split 1 time series split 2 : train indices are [0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 13, 14, 15, 16] and test indices are [27, 28, 29] 
 kfold split 2 : training customers are [2] and  testing customers are [0, 1] 
 kfold split 2 time series split 1 : train indices are [20, 21, 22, 23] and test indices are [4, 5, 6, 7, 15, 16, 17] 
 kfold split 2 time series split 2 : train indices are [20, 21, 22, 23, 24, 25, 26] and test indices are [7, 8, 9, 17, 18, 19] 
 kfold split 3 : training customers are [0, 2] and  testing customers are [1] 
 kfold split 3 time series split 1 : train indices are [0, 1, 2, 3, 20, 21, 22, 23] and test indices are [14, 15, 16] 
 kfold split 3 time series split 2 : train indices are [0, 1, 2, 3, 4, 5, 6, 20, 21, 22, 23, 24, 25, 26] and test indices are [17, 18, 19] 
 kfold split 4 : training customers are [1] and  testing customers are [0, 2] 
 kfold split 4 time series split 1 : train indices are [10, 11, 12, 13,] and test indices are [4, 5, 6, 24, 25, 26] 
 kfold split 4 time series split 2 : train indices are [10, 11, 12, 13, 14, 15, 16] and test indices are [7, 8, 9, 27, 28, 29] 
 Usually, cross-validation iterators, such as those in sklearn, which are based on the number of folds, i.e., the sample size in each fold. These are unfortunately not suited in our kfold / time series split with real data. In fact, nothing guarantees that data is perfectly distributed over time and over groups. (as we assumed in the previous example). 
 For instance, we can have the 4th observation in the consumer training sample (say customer 0 and 1 in kfold split 1 in the example) that comes after the 4th observation in the test sample (say customer 2). This violates condition 1. 
 Here is one CV splits strategy based on dates by fold (not by sample size or the number of folds).
Say you have previous data but with random dates. Define an initial_training_rolling_months, rolling_window_months. say for example 6 and 1 months. 
 kfold split 1 : training customers are [0, 1] and  testing customers are [2] 
 kfold split 1 time series split 1 : train sample is the 6 first months of customers [0, 1] and test sample is the month starting after train sample for customers [2] 
 kfold split 1 time series split 2 : train sample is the 7 first months of customers [0, 1] and test sample is the month starting after train sample for customers [2] 
 Below a suggestion of implementation to build such a time series split iterator. 
 The returned iterator is a list of tuples that you can use as another cross-validation iterator. 
 With a simple generated data like in our previous example to debug the folds generation, noting that customers 1 (resp. 2) data begins at index 366 and (resp. 732). 
 from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
df = generate_happy_case_dataframe()
grouped_ts_validation_iterator = build_grouped_ts_validation_iterator(df)
gridsearch = GridSearchCV(estimator=RandomForestClassifier(), cv=grouped_ts_validation_iterator, param_grid={})
gridsearch.fit(df[['feat0', 'feat1', 'feat2', 'feat3', 'feat4']].values, df['label'].values)
gridsearch.predict([[0.1, 0.2, 0.1, 0.4, 0.1]])
 
 With randomly generated data like in @etiennedm's example (to debug split, I covered simple cases such as when the test sample begins before the training samples or just after). 
 from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
df = generate_fake_random_dataframe()
grouped_ts_validation_iterator = build_grouped_ts_validation_iterator(df)
gridsearch = GridSearchCV(estimator=RandomForestClassifier(), cv=grouped_ts_validation_iterator, param_grid={})
gridsearch.fit(df[['feat0', 'feat1', 'feat2', 'feat3', 'feat4']].values, df['label'].values)
gridsearch.predict([[0.1, 0.2, 0.1, 0.4, 0.1]])
 
 The implementation : 
 import pandas as pd
import numpy as np
from sklearn.model_selection import RepeatedKFold


def generate_fake_random_dataframe(start=pd.to_datetime('2015-01-01'), end=pd.to_datetime('2018-01-01')):
    fake_date = generate_fake_dates(start, end, 500)
    df = pd.DataFrame(data=np.random.random((500,5)), columns=['feat'+str(i) for i in range(5)])
    df['customer_id'] = np.random.randint(0, 5, 500)
    df['label'] = np.random.randint(0, 3, 500)
    df['dates'] = fake_date
    df = df.reset_index() # important since df.index will be used as split index 
    return df


def generate_fake_dates(start, end, n):
    start_u = start.value//10**9
    end_u = end.value//10**9
    return pd.DatetimeIndex((10**9*np.random.randint(start_u, end_u, n, dtype=np.int64)).view('M8[ns]'))


def generate_happy_case_dataframe(start=pd.to_datetime('2019-01-01'), end=pd.to_datetime('2020-01-01')):
    dates = pd.date_range(start, end)
    length_year = len(dates)
    lenght_df = length_year * 3
    df = pd.DataFrame(data=np.random.random((lenght_df, 5)), columns=['feat'+str(i) for i in range(5)])
    df['label'] = np.random.randint(0, 3, lenght_df)
    df['dates'] = list(dates) * 3
    df['customer_id'] = [0] * length_year + [1] * length_year + [2] * length_year
    return df


def build_grouped_ts_validation_iterator(df, kfold_n_split=2, kfold_n_repeats=5, initial_training_rolling_months=6, rolling_window_months=1):
    rkf = RepeatedKFold(n_splits=kfold_n_split, n_repeats=kfold_n_repeats, random_state=42)
    CV_iterator = list()
    for train_customers_ids, test_customers_ids in rkf.split(df['customer_id'].unique()):
        print(""rkf training/testing with customers : "" + str(train_customers_ids)+""/""+str(test_customers_ids))
        this_k_fold_ts_split = split_with_dates_for_validation(df=df,
                                                               train_customers_ids=train_customers_ids, 
                                                               test_customers_ids=test_customers_ids, 
                                                               initial_training_rolling_months=initial_training_rolling_months, 
                                                               rolling_window_months=rolling_window_months)
        print(""In this k fold, there is"", len(this_k_fold_ts_split), 'time series splits')
        for split_i, split in enumerate(this_k_fold_ts_split) :
            print(""for this ts split number"", str(split_i))
            print(""train ids is len"", len(split[0]), 'and are:', split[0])
            print(""test ids is len"", len(split[1]), 'and are:', split[1])
        CV_iterator.extend(this_k_fold_ts_split)
        print('***')

    return tuple(CV_iterator)


def split_with_dates_for_validation(df, train_customers_ids, test_customers_ids, initial_training_rolling_months=6, rolling_window_months=1):
    start_train_df_date, end_train_df_date, start_test_df_date, end_test_df_date = \
        fetch_extremas_train_test_df_dates(df, train_customers_ids, test_customers_ids)
    
    start_training_date, end_training_date, start_testing_date, end_testing_date = \
        initialize_training_dates(start_train_df_date, start_test_df_date, initial_training_rolling_months, rolling_window_months)
    
    ts_splits = list()
    while not stop_time_series_split_decision(end_train_df_date, end_test_df_date, start_training_date, end_testing_date, rolling_window_months):
        # The while implies that if testing sample is les than one month, then the process stops
        this_ts_split_training_indices = fetch_this_split_training_indices(df, train_customers_ids, start_training_date, end_training_date)
        this_ts_split_testing_indices = fetch_this_split_testing_indices(df, test_customers_ids, start_testing_date, end_testing_date)
        if this_ts_split_testing_indices:
            # If testing data is not empty, i.e. something to learn
            ts_splits.append((this_ts_split_training_indices, this_ts_split_testing_indices))
        start_training_date, end_training_date, start_testing_date, end_testing_date =\
            update_testing_training_dates(start_training_date, end_training_date, start_testing_date, end_testing_date, rolling_window_months)
    return ts_splits


def fetch_extremas_train_test_df_dates(df, train_customers_ids, test_customers_ids):
    train_df, test_df = df.loc[df['customer_id'].isin(train_customers_ids)], df.loc[df['customer_id'].isin(test_customers_ids)]
    start_train_df_date, end_train_df_date = min(train_df['dates']), max(train_df['dates'])
    start_test_df_date, end_test_df_date = min(test_df['dates']), max(test_df['dates'])
    return start_train_df_date, end_train_df_date, start_test_df_date, end_test_df_date 


def initialize_training_dates(start_train_df_date, start_test_df_date, initial_training_rolling_months, rolling_window_months):
    start_training_date = start_train_df_date 
    # cover the case where test consumers begins long after (initial_training_rolling_months after) train consumers
    if start_training_date + pd.DateOffset(months=initial_training_rolling_months) < start_test_df_date:
        start_training_date = start_test_df_date - pd.DateOffset(months=initial_training_rolling_months)
    end_training_date = start_train_df_date + pd.DateOffset(months=initial_training_rolling_months)
    start_testing_date = end_training_date
    end_testing_date = start_testing_date + pd.DateOffset(months=rolling_window_months)
    return start_training_date, end_training_date, start_testing_date, end_testing_date


def stop_time_series_split_decision(end_train_df_date, end_test_df_date, end_training_date, end_testing_date, rolling_window_months):
    no_more_training_data_stoping_condition = end_training_date + pd.DateOffset(months=rolling_window_months) > end_train_df_date
    no_more_testing_data_stoping_condition = end_testing_date + pd.DateOffset(months=rolling_window_months) > end_test_df_date
    stoping_condition = no_more_training_data_stoping_condition or no_more_testing_data_stoping_condition
    return stoping_condition


def update_testing_training_dates(start_training_date, end_training_date, start_testing_date, end_testing_date, rolling_window_months):
    start_training_date = start_training_date
    end_training_date += pd.DateOffset(months=rolling_window_months)
    start_testing_date += pd.DateOffset(months=rolling_window_months)
    end_testing_date += pd.DateOffset(months=rolling_window_months)
    return start_training_date, end_training_date, start_testing_date, end_testing_date


def fetch_this_split_training_indices(df, train_customers_ids, start_training_date, end_training_date):
    train_df = df.loc[df['customer_id'].isin(train_customers_ids)]
    in_training_period_df = train_df.loc[(train_df['dates'] >= start_training_date) & (train_df['dates'] < end_training_date)]
    this_ts_split_training_indices = in_training_period_df.index.to_list()
    return this_ts_split_training_indices


def fetch_this_split_testing_indices(df, test_customers_ids, start_testing_date, end_testing_date):
    test_df = df.loc[df['customer_id'].isin(test_customers_ids)]
    in_testing_period_df = test_df.loc[(test_df['dates'] >= start_testing_date) & (test_df['dates'] < end_testing_date)]
    this_ts_split_testing_indices = in_testing_period_df.index.to_list()
    return this_ts_split_testing_indices","As a first porint, when you say  ""The idea is that this model is deployed and used to model new customers""  I guess you mean  and used to infere on new customers , is it correct?
I can think of two possible options: 
 
 following the properties you impose, you can first make use of the  TimeSeriesSplit cross-validator  by scikit-learn, with wich you  get the time-ordered indices  of each train-validation split, so that you can use them later on the clients IDs you decide to fulfill the second condition, something like:
 
 
 As a second option, you could try to apply clustering on your clients, based on certain features, and build as many models as clients types you get (each cluster having n clients history data). This would solve a possible problem I see in your approach, which is (due to the second restriction) not using a client whole history data both for training and validating","Sort on the customer id. And than do the time series split. If there is any overlapping than drop these rows if possible. 
 These are mutually exclusive conditions, meaning that if you have class 2 for customer id in the beginning of the time series and Right and the end of it, you can not expect not to have to drop these rows in the beginning. Because not doing that would damage one of the two posed conditions.","This feature was requested on scikit-learn and I have added a  PR  for it . The code is awaiting review at this point.
This code was used with some good results on a recent  Kaggle competition  . 
 
 scikit-learn Feature Request
:  https://github.com/scikit-learn/scikit-learn/issues/14257 
 scikit-learn PR :
 https://github.com/scikit-learn/scikit-learn/pull/16236 
 Kaggle Notebook 1  Code block below 
 Kaggle Notebook 2  ( Purged Time Series CV)  : This is an excellent modification with  gap  parameter between different groups .  Feature Request  for the same has been raised on Scikit-learn . 
 
 from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples
from sklearn.utils.validation import _deprecate_positional_args

# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243
class GroupTimeSeriesSplit(_BaseKFold):
    """"""Time Series cross-validator variant with non-overlapping groups.
    Provides train/test indices to split time series data samples
    that are observed at fixed time intervals according to a
    third-party provided group.
    In each split, test indices must be higher than before, and thus shuffling
    in cross validator is inappropriate.
    This cross-validation object is a variation of :class:`KFold`.
    In the kth split, it returns first k folds as train set and the
    (k+1)th fold as test set.
    The same group will not appear in two different folds (the number of
    distinct groups has to be at least equal to the number of folds).
    Note that unlike standard cross-validation methods, successive
    training sets are supersets of those that come before them.
    Read more in the :ref:`User Guide <cross_validation>`.
    Parameters
    ----------
    n_splits : int, default=5
        Number of splits. Must be at least 2.
    max_train_size : int, default=None
        Maximum size for a single training set.
    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.model_selection import GroupTimeSeriesSplit
    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\
                           'b', 'b', 'b', 'b', 'b',\
                           'c', 'c', 'c', 'c',\
                           'd', 'd', 'd'])
    >>> gtss = GroupTimeSeriesSplit(n_splits=3)
    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):
    ...     print(""TRAIN:"", train_idx, ""TEST:"", test_idx)
    ...     print(""TRAIN GROUP:"", groups[train_idx],\
                  ""TEST GROUP:"", groups[test_idx])
    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]
    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\
    TEST GROUP: ['b' 'b' 'b' 'b' 'b']
    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]
    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\
    TEST GROUP: ['c' 'c' 'c' 'c']
    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\
    TEST: [15, 16, 17]
    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\
    TEST GROUP: ['d' 'd' 'd']
    """"""
    @_deprecate_positional_args
    def __init__(self,
                 n_splits=5,
                 *,
                 max_train_size=None
                 ):
        super().__init__(n_splits, shuffle=False, random_state=None)
        self.max_train_size = max_train_size

    def split(self, X, y=None, groups=None):
        """"""Generate indices to split data into training and test set.
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data, where n_samples is the number of samples
            and n_features is the number of features.
        y : array-like of shape (n_samples,)
            Always ignored, exists for compatibility.
        groups : array-like of shape (n_samples,)
            Group labels for the samples used while splitting the dataset into
            train/test set.
        Yields
        ------
        train : ndarray
            The training set indices for that split.
        test : ndarray
            The testing set indices for that split.
        """"""
        if groups is None:
            raise ValueError(
                ""The 'groups' parameter should not be None"")
        X, y, groups = indexable(X, y, groups)
        n_samples = _num_samples(X)
        n_splits = self.n_splits
        n_folds = n_splits + 1
        group_dict = {}
        u, ind = np.unique(groups, return_index=True)
        unique_groups = u[np.argsort(ind)]
        n_samples = _num_samples(X)
        n_groups = _num_samples(unique_groups)
        for idx in np.arange(n_samples):
            if (groups[idx] in group_dict):
                group_dict[groups[idx]].append(idx)
            else:
                group_dict[groups[idx]] = [idx]
        if n_folds > n_groups:
            raise ValueError(
                (""Cannot have number of folds={0} greater than""
                 "" the number of groups={1}"").format(n_folds,
                                                     n_groups))
        group_test_size = n_groups // n_folds
        group_test_starts = range(n_groups - n_splits * group_test_size,
                                  n_groups, group_test_size)
        for group_test_start in group_test_starts:
            train_array = []
            test_array = []
            for train_group_idx in unique_groups[:group_test_start]:
                train_array_tmp = group_dict[train_group_idx]
                train_array = np.sort(np.unique(
                                      np.concatenate((train_array,
                                                      train_array_tmp)),
                                      axis=None), axis=None)
            train_end = train_array.size
            if self.max_train_size and self.max_train_size < train_end:
                train_array = train_array[train_end -
                                          self.max_train_size:train_end]
            for test_group_idx in unique_groups[group_test_start:
                                                group_test_start +
                                                group_test_size]:
                test_array_tmp = group_dict[test_group_idx]
                test_array = np.sort(np.unique(
                                              np.concatenate((test_array,
                                                              test_array_tmp)),
                                     axis=None), axis=None)
            yield [int(i) for i in train_array], [int(i) for i in test_array]",,,,,54.05571278,54.42261494,57.31089495,61.46747302,63.88071348,,,,
77422,Multi-country model or single model,machine-learning,"In the paper of  Hinton - Distilling the knowledge of Neural Networks , the following is mentioned (Section 5) when defining specialist models: 
 
 When the number of classes is very large, it makes sense for the cumbersome model to be an ensemble that contains one generalist model trained on all the data and many “specialist” models, each of which is trained on data that is highly enriched in examples from a very confusable subset of the
classes (like different types of mushroom). 
 
 What they do is they use a general model first and then a specialist model each to focus on a different subset of the classes. 
 You could consider your problem something similar, instead of a specialist to classes, and specialist in countries. This way you could build a country(cluster of countries) specific ensemble of models.","I think the only objective criterion to decide this is to simply compare the performance of the candidate approaches over the validation data. 
 That being said, if I were to blindly choose the approach upfront without any other information, I would choose a single model, where the model is aware of the country of each piece of data. This would let it model the peculiarities of each country while profiting from the combined training data. 
 If you have reasons to believe this is harming the global performance because of the intrinsic difference of some countries, you can apply  boosting  and therefore let the performance of the classifiers speak by itself.",I don't have the theoretical ressources to confirm this but I think it's possible to train a first model on the whole dataset with a limited degree of freedom (high regularization) and with the commmon features that will allow you to capture the global trends and then train local models on the residuals.,"I think that @mirimo's idea of having a regularized model as an offset is very interesting. 
 My proposal is a slight variation where you ensure you don't overfit. 
 The idea is, to obtain the model for group  $j$ , train a model with all groups except  $j$  and use that model as an offset to the model for group  $j$ . This way, we can have a complex model for the general behavior and still not train on the same target twice, thus having a more stable model. 
 The downside is that this is way slower, as, if there are  $J$  groups, it takes around  $J$  times more than the regular training. 
 Edit 
 On top of @Carlos Mougan proposal, we can: 
 
 Train a global model 
 Train a specific model for each country 
 Ensemble both models
The ensemble can have some shrinkage, like:
 $$y_{final} = \frac{y_{global} \cdot m + y_{country} \cdot n_{country}}{m + n_{country}} $$ 
where  $y_{country}$  is the prediction of the country-specific model,  $y_{global}$  the global prediction,  $n_{country}$  the number of samples in a country and  $m$  an hyperparameter to tune, the higher the  $m$  the more we trust on the global model. 
 
 I think this shrinkage is very relevant to the problem.","I think the most important thing you can do to bridge both assumptions is to include the country as a variable in the global model. 
 Should there be any country-specific effects they will simply be modeled as interactions in the global model. This is how the model deals with any other variable anyway and why should country be any different? 
 I think the problem is much more complicated if the data is heavily imbalanced e.g. some products are only sold in one country, etc. However this only becomes a problem at a point when training a global model is infeasible anyway.",First of all in this use case i see  we want our model to learn data or understand the data frist . This is a similar problem of natural language processing like one always try to make model to learn it from data. Here we can do little tricky things where we can declare country as target variable and rest of features as input variable. We can at least  train model to learn mapping for input features to country  so model might have an understanding of the representation of input with respect to the country. we can use this model as ensemble modeling suggested above. I think it would give little accuracy increment and  cost-effective solution  also.,"I don't think there is a unique rule to answer that.  It strongly depends on how pertinent is the country information  regarding other input data and what you want to predict. 
 It is possible to face cases where similar input data in different countries lead to different outputs. In that case, it would be mandatory either to add the country as input information or to create a model per country. 
 In other cases, the country information would not lead to any improvement in the model (so no need to do a specific model per country). 
 Finally, there are cases for which you will find global information (whatever country) and specific information per country. In that case, there are multiple approaches to deal with it. The first and most common is to include the country as input of your global model. As @Fnguyen mentioned, why dealing with the country differently than other inputs? 
 Update 
 If you think that the country has a specific impact on the prediction, here are a  non-exhaustive  list of how you could create models that deal with your assumption: 
 
 Using transfer learning , train a global model to capture general trends, then train the same model on the specific countries starting from the global. You may still not be able to capture specific country effects. 
 Using boosting method : train a first classifier on all countries and then train a model per country that does boosting on the output of the global trained classifier. In that way you will keep the global trends and then use specific country information. 
 Using bagging method : train some classifier(s) on all countries, others on a specific country, and then you can combine them parellel to each others in one big model per country. 
 A specific example using NNs : train a global model and one model per country. Then use per country a model that combines both the global and the specific you trained before and only retrain the 'head'. For instance if using DNNs / CNNs, you only retrain the green part of the final model:
 
 
 The list is non-exhaustive and you must have a good reason to use such approaches which give more importance to the country information. Normally, the machine learning algorithms would do it on their own.",,,59.2405959,64.78427866,53.72773902,65.19564136,61.65962477,59.79549246,63.766274,,
77352,Generate pdf from jupyter notebook without code,pandas,"Try this: 
 jupyter nbconvert --to pdf --TemplateExporter.exclude_input=True  my_notebook.ipynb
 
 This also works for html output.
You will find the documentation for this and other options  here . 
 FYI, for complex notebooks, this may generate errors depending on your version of nbconvert, LaTeX and other components. In that case try to convert to html then print to pdf using a browser.","Make sure you are working with Qt Console (anaconda): 
 Install Jupiter extensions: 
 !pip install jupyter_contrib_nbextensions

!jupyter contrib nbextension install --user 
 
 Enable nbextension: 
 !jupyter nbextension enable codefolding/main
 
 Install pyppeteer: 
 !python -m pip install -U notebook-as-pdf

!pyppeteer-install
 
 MAKE SURE YOUR WORKING DIRECTORY IS WHERE YOUR  Untitled.ipynb  FILE IS SAVED
Save file to HTML format without codes: 
 !jupyter nbconvert Untitled.ipynb --no-input --no-prompt --to html 
 EXPORT TO PDF FORMAT: 
 !jupyter-nbconvert Untitled.ipynb --no-input --no-prompt --to pdfviahtml
 
 PS. Exporting HTML and PDF format are mutually exclusive commands, you can use either.","Make your program  generate  markdown output (you can save images and add references to them) and convert it using pandoc. Or don't convert, upload directly to Gitlab or Github and view.","If you manually create the PDF, there is a simpler solution: add the following code block & execute it 
 from IPython.core.display import HTML
HTML('<style>.input, .jp-InputArea {display: none !important}</style>')
 
 Code will disappear both in the ""normal"" UI and in print preview / export to HTML. Open print preview, generate the PDF - it looks great now, clear the output of this one cell to bring the code back, done","Refer below link. It is the easiest and legitimate way of downloading jupyter notebook as pdf. No third party app or converter needed. 
 https://youtu.be/Q1J7MpI5PQk",,,,,67.10623577,63.81518953,51.82562709,62.35781845,73.68470713,,,,
77335,Anconda R version - How to upgrade to 4.0 and later,graphs,"You need to create a new environment and then you can install R 4.+ in Anaconda. Follow these steps. 
 conda create --name r4-base
 
 
 After activating  r4-base  run these commands 
 conda activate r4-base
conda install -c conda-forge r-base
conda install -c conda-forge/label/gcc7 r-base
 
 Finally, you will notice  r-basa  version 4 will be installed. 
 
 Thereafter, you can install any supported packages. But with this only, you won't have the ability to use it in the Jupyter notebook. You need to install  install.packages('IRkernel')  and Jupyter notebook as well if you want to use it. Otherwise you are good to go with R-Studio. 
 For Jupyter Installation and RKernel. 
 conda install jupyter
 
 Then open the R console. Write in R console 
 install.packages('IRkernel')
IRkernel::installspec()
 
 Congrats! You can use Notebook for Python and R.",Rstudio does not support Anaconda so you are stuck with the version they provide. Your best option is to install R and RStudio outside Anaconda.,"Try install through anaconda prompt and the following path 
 conda install -c conda-forge r-mnormt
 
 Then install qgraph one more time","The following steps seemed to resolve the issue for me on my mac. 
 
 Create an environment using Anaconda. Give it a name, say ""Environment Name"". 
 
 Launch that environment using terminal. ""Environment Name"" > ""Open Terminal"" 
 
 Inside terminal type  the following command. 
 
 brew install r","The code above didn't work for me.  I use macOS.  Instead I used the following code and it worked.  You need to fill in a name of your environment for ""your_name_here"" and choose version numbers for python and R: 
 conda config --add channels conda-forge    
conda config --set channel_priority strict    
conda search r-base  
conda create -n your_name_here python=3.X   
conda activate your_name_here  
conda install -c conda-forge r-base=4.X.X    
conda install jupyter
 
 Now, install RStudio in your environment via Anaconda. In RStudio, write in the R console: 
 install.packages('IRkernel')   
IRkernel::installspec()",,,,,51.07949183,54.47195477,50,50,51.26497762,,,,
77298,How many ways are there to check model overfitting?,machine-learning,"The direct way to check your model for overfitting is to compare its performance on a training set with its performance on a testing set; overfitting is when your train score is significantly above your cv score.  According to your comments, your r2 score is 0.97 on the training set, and 0.86 on your testing set (or similarly, 0.88 cv score, mean across 10 folds). That's somewhat overfitting, but not extremely so; think if 0.88 is ""good enough"" for your requirements 
 
 The r2 score is 1 - MSE of errors / variance of true values. In the example you showed, all of the three true values were the same; i.e. their variance is zero. The r2 score should've been a negative infinite, but apparently sklearn corrects this to 0; you can verify that changing  y_true  to  [0.9, 0.9, 0.90001]  changes your r2 score to a very large negative number (around -2*10**9).  This is why checking r2 against a small sample is not a good idea; the mean of the small sample contains too much important information. 
 
 You added that you want to know which parameters to tune in order to prevent over-fitting. In your edit to your question, you said you're using grid-search over  n_estimators  (3 options),  min_samples_split  (2 options) and  min_sample_leaf  (2 options).  
There are other parameters you can try, and in my experience  max_depth  is important to tune.   This question on Stack Overflow  and  this question on Cross Validated  deal with overfitting, and there are good options there.   I'd add that if you're trying many options, then maybe you'd be better off doing using Bayesian Optimization (there's a package that functions well with SKLearn:  https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html ).","Overfitting can be identified by checking validation metrics such as accuracy and loss. The validation metrics usually increase until a point where they stagnate or start declining when the model is affected by overfitting. 
 If our model does much better on the training set than on the test set, then we’re likely overfitting. 
 You can use Occam's razor test: If two models have comparable performance, then you should usually pick the simpler one. 
 For linear regression, there is an excellent accelerated cross-validation method called predicted R-squared. This method doesn’t require you to collect a separate sample or partition your data, and you can obtain the cross-validated results as you fit the model. Statistical software calculates predicted R-squared using the following automated procedure: 
 
 It removes a data point from the dataset. 
 Calculates the regression equation. 
 Evaluates how well the model predicts the missing observation. 
 And, repeats this for all data points in the dataset. 
 
 Predicted R-squared has several cool features. First, you can just include it in the output as you fit the model without any extra steps on your part. Second, it’s easy to interpret. You simply compare predicted R-squared to the regular R-squared and see if there is a big difference. 
 If there is a large discrepancy between the two values, your model doesn’t predict new observations as well as it fits the original dataset. The results are not generalizable, and there’s a good chance you’re overfitting the model.","-  Use RandomForest as XGBoost is more prone to overfitting and comparatively difficult to tune hyperparameters
 Tune at least these parm -
 param_grid = { 'n_estimators': [ ], 'max_features': [ ], 'max_depth' : [ ], 'criterion' :['gini', 'entropy']} 
 - Try  imputation based on your domain knowledge and using other Features  e.g. Correleation 
 - Scaling is not very much needed with Tree models
 - Monitor another metrics along with  $R^2$  score. I mean  being in the domain you must know how much error is ""too much"" .  $R^2$  rewards useless Features, so be mindful of that and may use adjusted  $R^2$ .
 - Have K=10 only when you have sufficient samples. Otherwise, try K=5,3. If we use K=10 on a small dataset, then the cross-val test-set will be very small and we may see a very high variance in the 10 different predictions. I suspect the same in your result.  We have an output between 0.82 to 0.94 
 array([0.8484691 , 0.86808136, 0.91821645, 0.93616375, 0.94435934,
0.82065733, 0.84856025, 0.8267642 , 0.84561417, 0.89567455] 
 - Feature selection/engineering - A very separate and broad topic in itself. Would only suggest trying multiple things and  trying one thing at a time and maintaining a proper track of which activities resulted in what . It seems from the question that you are trying to do many things randomly.","You should be using an evaluation metric like area under the ROC curve not R^2. R^2 is good for continuous unbounded variables not classification. This is the most important thing you should do. If your outcome variable is highly imbalanced you might want to use precision recall.
 More about Precision-Recall and ROC. 
 You need to do parameter tuning with Grid Search. 
 It might be better to use random forest since sometimes boosting methods can overfit. You should also try logistic regression. 
 I would avoid removing variables before training based on correlation. 
 
 I am happy to help further if you update your question to include correct metrics for classification problems.","When evaluating xgboost (or any overfitting prone model), I would plot a validation curve. Validation curve shows the evaluation metric, in your case R2 for training and set and validation set for each new estimator you add. You would usually see both training and validation R2 increase early on, and if R2 for training is still increasing, while R2 for validation is starting to decrease, you know overfitting is a problem. 
 Be careful with overfitting a validation set. If your data set is not very large, and you are running a lot of experiments, it is possible to overfit the evaluation set. Therefore, the data is often split into 3 sets, training, validation, and test. Where you only tests models that you think are good, given the validation set, on the test set. This way you don't do a lot experiments against the test set, and don't overfit to it.","Evaluating training scores—such as accuracy for classification and (adjusted) R-squared for regression—against test scores can indicate potential overfitting. However, this comparison alone doesn't provide a quantifiable method for assessing overfitting rigorously. 
 To address this, I employ a ratio of errors method: (1 - Rsquared_test)/(1 - Rsquared_train). If this ratio exceeds a predetermined threshold, it suggests overfitting. The choice of threshold, however, remains somewhat subjective. In my practice, I consider a ratio up to 2 as acceptable, a standard I've used and published in previous research. Historically, I've seen others accept ratios as high as 8, though specific references are elusive due to the passage of time. 
 This method offers a quantitative measure of overfitting, providing a defensible criterion that facilitates comparisons with other studies.",,,,60.64205976,55.63503716,51.89344013,51.35569051,55.65876028,55.23712789,,,
76824,Is Python a viable language to do statistical analysis in?,machine-learning,"Python is more ""general purpose"" while R has a clear(er) focus on statistics. However, most (if not all) things you can do in R can be done in Python as well. The difference is that you need to use additional packages in Python for some things you can do in base R. 
 Some examples: 
 
 Data frames are base R while you need to use  Pandas  in Python. 
 Linear models ( lm ) are base R while you need to use  statsmodels  or  scikit  in Python. There are important  conceptional differences  to be considered. 
 For some rather basic mathematical operations you would need to use  numpy . 
 
 Overall this leads to some additional effort (and knowledge) needed to work fluently in Python. I personally often feel more comfortable working with base R since I feel like being ""closer to the data"" in (base) R. 
 However, in other cases, e.g. when I use boosting or neural nets, Python seems to have an advantage over R. Many algorithms are developed in  C++  (e.g.  Keras ,  LightGBM ) and adapted to Python and (often later to) R. At least when you work with Windows, this often works better with Python. You can use things like Tensorflow/Keras, LightGBM, Catboost in R, but it sometimes can be daunting to get the additional package running in R (especially with GPU support). 
 Many packages/methods are available for R and Python, such as GLMnet ( for R  /  for Python ). You can also see based on the Labs of "" Introduction to Statistical Learning "" - which are available  for R  and  for Python  as well - that there is not so much of a difference between the two languages in terms of  what  you can do. The difference is more like  how  things are done. 
 Finally, since Python is more ""general purpose"" than R (at least in my view), there are  interesting and funny things  you can do with Python (beyond statistics) which you cannot do with R (at least it is harder).","Python being more widely used is an important consideration. This will especially become important when applying for a job. Also Python has as many if not more key statistical and ML/AI tools as R, and a larger open-source base to utilize. Python is designed for programmers, R is designed for statisticians. Originally I was a R programmer, but most of my colleagues were using Python so I eventually switched over. 
 Here are some of the basic differences: 
 Python: 
 
 programmer friendly 
 debugging easier 
 More open-source support (stack web sites, etc) 
 
 R: 
 
 Easier and simpler to write scripts 
 Works better with other languages 
 More built in functionality 
 
 Good reference to check out: datacamp.com/community/tutorials/r-or-python-for-data-analysis 
 Also should mention that i have used R code within Python, using Rpy2. If you are using a notebook, just use %%R, after installing the necessary R libraries","I'd like to add two points to the existing answers: 
 
 There is excellent interaction between R and python, with various possibilities for either direction. 
 To me, it's not that much of a decision python vs. R. The decision is to choose the main language appropriately for the project at hand, and then do parts in the other language if that is better for some reason. 
 
 I find the facilities to generate reports much more convenient in R. 
Since lots of my work consists in producing reports about statistical analyses, I mainly use R. 
 To the point that were I to encounter a data analysis + report today that I think is better done in python, I'd set up the report as ""R""markdown and do the python in python chunks.","One thing that can be a gotcha coming from R to Python is that the Python stats ecosystem tends to be more machine learning-ey oriented rather than inferential stats-ey oriented. 
 This can create some hiccups, because some of the defaults in R that are the defaults because people who do inferential stats like in the social sciences always use them, are not the defaults in the main Python libraries. 
 For example, Statsmodels, one of the standard libraries for inferential stats,  doesn't include the intercept by default when you do linear regression , UNLESS you  use the R-style formulas with Patsy , in which case it is included. 
 Another example : Scikit-learn in Python uses the divide-by-n (""population"") formula for standard deviation, while R uses the divide-by-n-1 (""sample"") formula. 
 Those sort of things tend to be really confusing for people new to the ecosystem, and create totally unnecessary cognitive burden.  So that's a tradeoff.","I eventually do plan on moving more towards ML 
 
 One aspect that I would like to add based on what I observed . 
 Things are moving with more focus towards  Deep Learning e.g. Neural Networks  and in this space, most of the dominating Libraries supports Python as first choice.
  Companies manage a separate Python version to open-source, just to maintain the user base even though they themselves use either a C++ compiled version or something different.
It's because of the two-way-additive process  i.e. since Python has gained fame, companies are creating an open-sourced framework/library in Python and easily available Frameworks/Libraries are attracting more users. 
 Stackoverflow 2019 Survey 
 Most Popular Technologies -  Python - 41.7%  $\hspace{1cm}$  R - 5.8% 
 Other Frameworks, Libraries, and Tools -  Pandas and Tensorflow are in top 5-6 
 Most Wanted Languages -  Python is at the top with 25.7% 
 Most Wanted Framework -  Tensorflow at 2nd after NodeJs 
 Same logic goes with Books/Blogs and Tutorials. 
 I will agree that concepts don't change with a programming language but the examples/code provided in the books/blogs definitely accelerate the learning.
 Almost everyone in the Industry will recommend this book to a beginner and I also found it the best.
 Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 2nd Edition by Aurélien Géron","For the love of the flying spaghetti monster, use anaconda to install the needed packages for data science. I have seen both Python and R  being used in the data science setting and both needed additional packages to execute any data science capabilities. Conda made it way easier to install them. 
 From my point of view, Python has a better support for all kind of packages. There are simply more ports to Python than to R, but this may change in the future. 
 https://docs.conda.io/projects/conda/en/latest/user-guide/install/
conda install scikit-learn","As others have pointed out, python is more general, more programmers oriented, with more libraries and better hardware support. I'm not an R user, but python seems faster (c based) and more suitable on processing large files, or extracting big data from sql, most times in my experience is a previous step before apply statistics or AI to data. 
 Of course if you try processing using Dataframes and all data artifacts R like, with pandas or other math libraries, you end with a bad performance as in R. But with python you also have the option to process raw data files, line to line and byte to byte, and optimize processing time on big data sets, use multiprocessing for full machine use, etc.",,,59.9981179,59.48721511,65.30675025,53.15560351,55.46123179,53.48159062,54.31330599,,
76227,AttributeError: module 'tensorflow.python.keras.utils' has no attribute 'to_categorical',keras,"Newer versions of keras==2.4.0 and tensorflow==2.3.0 would work as follows. 
 Import: 
 from keras.utils import np_utils
 
 or 
 from keras import utils as np_utils
 
 and then replace  keras.utils.to_categorical  with 
 keras.utils.np_utils.to_categorical","Include this in your code 
 from tensorflow import keras 
 in place of 
 from tensorflow.python import keras",Use keras>=2.2 and tensorflow >=1.14 to resolve the issue.,"As it already has been said,  to_categorical()  is function. It in keras for tensorflow 2.x can be imported this way: 
 from keras.utils import to_categorical
 
 then used like this: 
 digit=6
x=to_categorical(digit, 10)
print(x)
 
 it will print 
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 
 Where 10 is the number of classes, the input values range is [0;number_of_classes-1]. The output is activated (1) or not active (0) position.","As of tensorflow version 2.9.2, the correct import is: 
 from tensorflow.python.keras.utils.np_utils import to_categorical",,,,,74.64206221,66.44846881,58.24701814,63.19167214,82.32585999,,,,
75449,Is there a way to force a transformer to return a pandas dataframe?,scikit-learn,"Might be late but for anyone with the same question the answer (as almost everything with Scikit-learn) is the usage of  Pipelines 
 from sklearn.impute import SimpleImputer
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import Pipeline
import pandas as pd

df = pd.DataFrame(dict(
    x=[1, 2, np.nan],
    y=[2, np.nan, 0]
))

imputer = Pipeline([(""imputer"", SimpleImputer()),
                    (""pandarizer"",FunctionTransformer(lambda x: pd.DataFrame(x, columns = [""x"", ""y""])))])

imputer.fit_transform(df)","[ Update : As the  answer  just above describes, this feature has been implemented (in  this pull request )]. 
 As of 05 April 2022, this is not available in scikit-learn. 
 The good news is: 
 
 The feature (or a like of it) is being developed:  https://github.com/scikit-learn/scikit-learn/pull/20110 
 There seems to be a way of making  ColumnTransformer  return a dataframe by overwriting (or overriding)  _hstack  method, as mentioned in  https://github.com/scikit-learn/scikit-learn/issues/20035 
 
 Hopefully, scikit-learn will make working with pandas dataframes more convenient soon.","Since sklearn Version 1.2 ,  set_output  can be configured per estimator by calling the  set_output  method or globally by setting  set_config(transform_output=""pandas"") 
 See  Release Highlights for scikit-learn 1.2 - Pandas output with set_output API 
 Example for  set_output() : 
 from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().set_output(transform=""pandas"")
 
 Example for  set_config() : 
 from sklearn import set_config
set_config(transform_output=""pandas"")","The following code snippet returns a Pandas DataFrame, but overwrites the original DataFrame values: 
 from sklearn.impute import SimpleImputer
imp = SimpleImputer(strategy='mean')
cols = df.columns
df[cols] = imp.fit_transform(df[cols])
 
 Note that I'm not sure whether this consumes any additional memory.","What you could do, is to rewrite your favorite preprocessing functions into new custom transformers. This might take time to rewrite, but it surely is helpful when you want everything to be as a dataframe. For example consider an example of a StandardScaler: 
 class DFStandardScaler(TransformerMixin):
    def __init__(self):
        self.ss = None
        self.mean_ = None
        self.scale_ = None
    def fit(self, X, y=None):
        self.ss = StandardScaler()
        self.ss.fit(X)
        self.mean_ = pd.Series(self.ss.mean_, index=X.columns)
        self.scale_ = pd.Series(self.ss.scale_, index=X.columns)
        return self
    def transform(self, X) -> pd.DataFrame:
        # assumes X is a DataFrame
        Xss = self.ss.transform(X)
        Xscaled = pd.DataFrame(Xss, index=X.index, columns=X.columns)
        return Xscaled
    def __str__(self):
         return ""DF_StandardScaler""
    def __repr__(self):
         return ""DF_StandardScaler""
 
 Using the following as  DFStandardScaler().fit_transform(df)  would return the same dataframe which was provided. The only issue is that this example would expect a df with column names, but it wouldn't be hard to set column names from scratch. 
 Here's the sklearn's documentation on custom transformers:
 https://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers",,,,,57.80693614,62.01594438,56.21177498,65.76302354,58.60933242,,,,
75167,If A and B are correlated and A and C are correlated. Why is it possible for B and C to be uncorrelated?,correlation,"Imagine a random point on a plane with coordinates  $(x, y)$ , where  $x, y \in [-1, 1]$ . 
 A = both  $x$  and  $y$  are positive
 
B =  $x$  is positive
 
C =  $y$  is positive 
 It is clear A is correlated with both B and C, which are not themselves correlated (assuming uniform distribution).","EDIT 
 I have a better simulation 
 set.seed(2020)
N <- 250
X1 <- rnorm(N, 0, 1)
X2 <- rnorm(N, 0, 1)
X3 <- X1 + X2
par(mfrow=c(3,1))
plot(X1, X3)
plot(X2, X3)
plot(X1, X2)
cor.test(X1, X3) # 95% confidence interval: [0.6719684, 0.7870920]
cor.test(X2, X3) # 95% confidence interval: [0.5767864, 0.7197146]
cor.test(X1, X2) # 95% confidence interval: [ -0.15596395,  0.09191158]
 
 In this example,  $X_1$  and  $X_2$  are totally independent, so they are uncorrelated. However,  $X_3$  is created as the sum of those two independent variables, meaning that  $X_3$  is correlated with each of  $X_1$  and  $X_2$ . 
 ORIGINAL 
 This should be fairly easy to simulate and graph. 
 library(MASS)
set.seed(2020)
N <- 250
mu <- c(0,0,0)
S <- matrix(c(1, 0.7, 0.5, 0.7, 1, 0, 0.5, 0, 1), 3, 3)
X <- mvrnorm(N, mu, S, empirical=T)
par(mfrow=c(3,1))
plot(X[, 1], X[, 2])
plot(X[, 1], X[, 3])
plot(X[, 2], X[, 3])
 
 I thought I would have to have opposite signs of the nonzero correlations, but that was not required. 
 In this example, think of the marginal variables as having two independent variables  $X_2$  and  $X_3$  influence  $X_1$ , meaning that each is correlated with  $X_1$  but not each other. 
 (And since this simulation is multivariate normal, the lack of correlation does give independence, though that fact relies on the distribution being jointly Gaussian.)","You can see it with a constructive technique: 
 
 Let's say A and B are correlated A and C are correlated B and C is uncorrelated How is it possible for B and C to be uncorrelated when they are both correlated to A? 
 
 Pick B from a random distribution. Dice throws, random values between 1 and 6.
Pick C from a random distribution. Another set of different dice throws, random values between 1 and 6. 
 Clearly, B and C are uncorrelated. And there's no way they could be, moving forward, whatever we take for A. 
 Now, let's take for A the sum of B and C. Clearly, A and B will be correlated as A is B plus some random variable. Clearly, A and C will be correlated a A is C plus some random variable.","I'm not sure if you're looking for an analytical proof, a simulation, or general explanation. But conceptually, 'correlation' in A and B, for example, does not mean  everything in A , as some sort of single entity, is statistically associated (correlated) to  everything in B  also as some single entity.  
 What it means when we say A is correlated to B is that  some  of the variation (changes) in A is able to explain or predict  some  of the variation (changes) in B.  
 In this regard, imagine A is total car sales, B is total car sales by Toyota, and C is total traffic violations.  
 
 As total car sales go up, Toyota will have made more sales (B goes up). 
 As total car sales go up, more cars out there means more traffic violations.  
 However total cars sold by Toyota is too particular to have much explanatory power in predicting total traffic violations (C). As B changes, you won't be able to get much reliability in predicting the direction of traffic violation changes.","There is a nice geometrical proof that correlation is not transitive in  How Not to be Wrong: The Power of Mathematical Thinking , by Jordan Ellenberg.  
 Let  $(a_1, b_1, c_1), (a_2, b_2, c_2), \ldots (a_N, b_N, c_N)$  be your observations, and let  $\mu_A$ ,  $\mu_B$ , and  $\mu_C$  be the means of A, B, and C.
Subtract the means from the observations and arrange them into vectors  $\vec{a} = (a_1-\mu_A, \ldots a_N-\mu_A)$ ,  $\vec{b} = (b_1-\mu_B, \ldots b_N-\mu_B)$ , and  $\vec{c} = (c_1-\mu_C, \ldots, c_N-\mu_C)$ . 
 It turns out that the correlation between any of these two variables is just equal to the cosine of the angle between the two vectors constructed from those variables.  (In this guise the correlation often goes by the name ""cosine similarity""). 
 IF  $B$  and  $C$  are uncorrelated, then  $cos(\theta_{BC}) = 0$ ; i.e.,  $\vec{b}$  and  $\vec{c}$  are perpendicular.  Now, if you let  $\vec{a}$  be in a plane with  $\vec{b}$  and  $\vec{c}$  and between them (see diagram below), then it will form an acute angle with both  $\vec{b}$  and  $\vec{c}$ , meaning that  $0 < \theta_{AB}, \theta_{AC} < \pi/2$ ; therefore  $\cos\theta_{AB}, \cos\theta_{AC} > 1$ . 
 
 You can go a step further and observe that moving  $\vec{a}$  closer to  $\vec{b}$  moves it away from  $\vec{c}$  and vice versa.  Therefore, the smaller of the two correlations (whichever it happens to be) is largest when the  $\vec{a}$  is halfway between  $\vec{b}$  and  $\vec{c}$ .  In this case,  $\theta_{AB} = \theta_{AC} = \pi/4$ , so cor(AB) = cor(AC) =  $\cos \pi/4 = \sqrt{2}/2$ .  This is the most correlated a single variable can be with  both  of two uncorrelated variables.","All the answer above provided counter examples in which correlation is not transitive. They are all very excellent, and there are many more examples. But they do not answer the question WHY correlation is not transitive. I don't think that such an answer exists. There are many relations that are not transitive, and correlation between random variables is just another example. There are many more examples for such relations in statistics, as well as in other fields.   To my opinion, the take home message here is never to assume that a relation is transitive. Always check what's going on.","Let  $X$  and  $Y$  be two independent and identically distributed random variables with zero mean and variance  $\sigma^2$ . Define a new random variable  $Z$  according to the flip of a coin (with probability  $p>0$  of  heads ) as follows:
 $$Z=\begin{cases}X,&\text{if heads}\\Y,&\text{if tails}\end{cases}$$ 
Then, one can easily verify the following results for the correlations: 
 
 $\mathrm{Cor}(Z,X)=E[ZX]=p\sigma^2=\sigma^2-\mathrm{Cor}(Z,Y)$  whereas  $\mathrm{Cor}(X,Y)=0$ .","Take Simple Example - 
 I'm on an annual fishing trip right now. There is a correlation between the time of day I fish and the amount of fish I catch. There is also a correlation between the size of the bait I use and the amount of fish I catch. There is no correlation between the size of the bait and the time of day. Source  here",,69.18233133,59.16669167,75.80447477,54.72899316,58.95745853,59.82981641,52.70642133,63.16045251,
72121,How can I handle a column with list data?,machine-learning,"You basically want to create a column for each product bought, as the presence or absence of each in the list is a feature in itself. See Hadley Wickham’s definition of tidy data. 
 That being said, you seem to have numerous products. To avoid the curse of dimensionality, what I would do is take your binary bought/not features (or count values might be even more effective if you have that data) and do dimensionality reduction to get a reasonable set of features. Latent Dirichlet Allocation (which comes from topic modeling), PCA, t-SNE, and our UMAP are all easy to implement and worth trying. PCA is the least sophisticated and the fastest to run and would be a good baseline. 
 When you have your smaller list of features, you might want to try using a classifier that further selects the most relevant features, like gradient-boosted trees.","You can think of  productList  as a sentence and treat it the same way language is treated in NLP.  
 So yes, if your set of unique products is not too big, then exploding the list and writing each product as a unique column is an approach that can work quite well. You can also look into embedding layers, which extend this idea to lists of items that are ""too big"".  
 If the order of items in the list matters, you probably want to decompose the list into individual rows and look for prediction on sequences. 
 Edit: 
In response to your comment here is an analogy with semantic analysis on tweets: 
 We can think of a tweet as a list of words, e.g.,  ""I am happy"" -> [""I"", ""am"", ""happy""] . These lists vary in length but each word (presumably) comes from the English language (+ some slang and neologisms which we will conveniently ignore). We can take a dictionary of the English language, look up the position of each word in that dictionary, and replace the word with the index of the word in the said dictionary. In our running example, this might look like  [23, 54, 219] . This is the same as your list of product ids relating to individual products. 
 The dictionary only has a finite number of words in it (similarly you only have a finite number of products), so we can OneHot encode each index in the list ( [[0,0,..,1,...], [0,...,1,...,0,..], ...] ). 
 Now there are two options: (1) the order of the vectors in the list does not matter, in which case we would sum them up to obtain a single vector for each example, with which you can proceed as described -, or (2) the order of the vectors in the list does matter, in which case you would split the array into multiple examples, one for each vector in the list, and add another feature denoting the position at which it was found in said list. You now have a dataset where a column contains a vector of the same size as every other column, which you can rewrite as a set of many columns. 
 You can then proceed with any analysis you think is reasonable for your data, e.g., clustering using simple methods, or training a non-linear embedding.","As soon as you do OHE on products, it will add too many extra dimensions.
To handle that, you can take one of the two approaches -  
 
 Reduce the dimension using standard techniques as suggested by  Nicholas 
 You can also try to cluster the product list using the knowledge about the products and their relation to the target variable(i.e. gender). 
 A typical example of this scenario is converting zipcode into state code. 
 
 Create your OHE matrix 
     import numpy as np,pandas as pd
    ###This is your current productlist
    productlist = pd.DataFrame(np.random.randint(1,14807,(1000,14806)))

    ##This is a zero matrix with column count equal to product count, rows = data count
    productlist_ohe = np.zeros((1000,14806)) 

    ##I looped over productlist and make the OHE=1 based on row and product Id
    for index, row in productlist.iterrows():
        for elem in row:
            productlist_ohe[index][elem-1] = 1","What exactly is the aim? Prediction of binary outcome (gender in this case)? If true, you can go down the way suggested by  Nicholas , but instead of doing dimensionality reduction (yourself), you could also treat the problem as a high dimensional one and use Lasso / Ridge / Elastic Net to ""automatically"" select features. In this case there is no need for any feature engineering. 
 Here is an  R implementation  of the method. Similar packages exist  for Python . Also see Ch. 6.3 in  Introduction to Statistical Learning  for a good overview.","There is a technique called Association Analysis where the prototypical example is a grocery store looking for associated products.  A typical grocery store may have half a million distinct items being sold.  Each 'grocery cart' is a list of items bought.  You treat the grocery cart purchases across some period of time as your initial dataset.  Your data has shape [count of total items] (columns) x [count of different grocery carts] (rows). 
 It's a sparse dataset, and the correlation matrix would have shape (columns x columns) far too massive and often not helpful since most products aren't correlated.  What is done instead is you accept some small threshold where if the correlation is smaller than this threshold, you don't compute it.  This allows you to actually mine the data for interesting metrics of interest.  The Apriori algorithm (or perhaps others if you are sophisticated) is used here (behind the scenes if you import the correct module in Python) and allows a regular computer to handle the number crunching.   
 The interesting metrics gained are typically: 
 
 Support  
 Confidence  
 Lift  
 Conviction
(definitions easily found online) 
 
 I have used the following module to do this in the past: 
 from mlxtend.frequent_patterns import apriori, association_rules
 
 Hope this helps",,,,,56.40906592,65.41524066,54.93709503,50,58.32910878,,,,
71816,Please review my sketch of the Machine Learning process,machine-learning,"This process will result in  data leaks . The split needs to happen earlier. Normalizing data before the split means that your training data contains information about your test data. I would put the split at 3. in your flow chart. 
 A common step I think you have missed is  imputation of missing values . I would put that before feature engineering. 
 Overall I think this is a good rough outline for a beginner to follow. It is overly simplistic and leaves a lot out, but I think you know that and you have to start somewhere.","Yes, these are the basics step. Then in each step there is a lot more. If you want to get a bit deeper you can follow this book of Andriy Burkov of  Machine Learning engineering 
 A couple notes in your process: 
 Before get data I Will put, define the question to resolve or something similar, but maybe this parted is granted. 
 Feature Engineering is one of the most important thing in ML, so probably spending a bit more of time there would help. 
 Normalize data helps mainly in Linear models, decision trees model has little/no effect. 
 Integer/Label Encoding is not specially good, there are better things as Target Encoding and Weight of Evidence encoding, have a look.","After 12 ""Choose the model with highest scores.""
Maybe add ""create ensemble of models"" and try to improve accuracy further.","Is this a good routine for a beginner to follow? 
 
 Yes, it's very good. 
 You could add: 
 
 K-fold Cross-validation(""Split Training into Training and Test Data"") 
 Feature selection before ""Choose a few models.""","Is this the end-to-end process?  
 
 Most importantly, you also need to understand the data you are using.
It's not supposed to be a meat-grinder. Add some uni and multivariate
analysis just before splitting your data. Look at the distributions
and frequencies. 
 After you split 70/30 or 80/20 or whatever, are the distributions
approximately similar? 
 I think you should also add touching base with stakeholders/business
people just after feature engineering (and maybe add a loop arrow to
reflect their feedback). 
 Another user mentioned ensemble models / model averaging at the end -
I think that is also important. Wouldn't an ensemble model perform
better that any single model? 
 You are also missing documentation - where are you documenting your
steps? Is it all in your mind? How will others follow what you are
doing? 
 What about four-eyes check aka pair programming? 
 What about version control? In most industries you will need to show
how your models were derived and how they perform against
alternatives. 
 What about edge cases for reasonable results for the best 2-3 models 
 Model explainability - how can you or your users trust the model without understanding how it is operating.",,,,,51.03070214,53.85755986,50,50,50.78320782,,,,
71751,Applying a keras model working with greyscale images to RGB images,python,"Your model is not sufficiently complex to adequately classify the CIFAR 10 data set. CIFAR-10 is considerably more complex than the  Fashion-MNIST data set and therefore you need a more complex model.You can add more hidden layers to your model to achieve this. You should also add DROPOUT layers to prevent over fitting. Perhaps the easiest solution is to use transfer learning. I would recommend using the MobileNet CNN model if you want to try transfer learning. Documentation for that can be found  here . Since CIFAR-10 has 50,000 sample images I do not think you will need data augmentation. First try a more complex model without augmentation and see what accuracy you achieve. If it is not adequate then use the keras ImageData Generator to provide data augmentation. Documentation for that is  here .","I'm using this model (basically building on work of  Chollet ). It uses a pretrained model (VGG16) for a multiclass image recognition problem. 
 from keras.applications import VGG16
import os, datetime
import numpy as np
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import to_categorical
from keras import models, layers, optimizers, regularizers
from keras.callbacks import EarlyStopping
from keras.callbacks import ReduceLROnPlateau
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.normalization import BatchNormalization
from PIL import ImageFile
import statistics
ImageFile.LOAD_TRUNCATED_IMAGES = True

###############################################
# DIR with training images
base_dir = 'C:/pathtoimages'
# Number training images
ntrain = 2000
# Number validation images
nval  = 500
# Batch size
batch_size = 20 #20
# Epochs (fine tuning [100])
ep = 400 #400
# Epochs (first step [30])
ep_first = 30 
# Number of classes (for training, output layer)
nclasses = 30
###############################################
start = datetime.datetime.now()

conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'val')
#test_dir = os.path.join(base_dir, 'test')

datagen = ImageDataGenerator(rescale=1./255)

def extract_features(directory, sample_count):
    features = np.zeros(shape=(sample_count, 4, 4, 512))
    labels = np.zeros(shape=(sample_count))
    generator = datagen.flow_from_directory(
        directory,
        target_size=(150, 150),
        batch_size=batch_size,
        class_mode='binary')
    i = 0
    for inputs_batch, labels_batch in generator:
        features_batch = conv_base.predict(inputs_batch)
        features[i * batch_size : (i + 1) * batch_size] = features_batch
        labels[i * batch_size : (i + 1) * batch_size] = labels_batch
        i += 1
        if i * batch_size >= sample_count:
            break
    return features, labels

train_features, train_labels = extract_features(train_dir, ntrain)
validation_features, validation_labels = extract_features(validation_dir, nval)
#test_features, test_labels = extract_features(test_dir, 1000)

# Labels and features
train_labels = to_categorical(train_labels)
validation_labels = to_categorical(validation_labels)
#test_labels = to_categorical(test_labels)
train_features = np.reshape(train_features, (ntrain, 4 * 4 * 512))
validation_features = np.reshape(validation_features, (nval, 4 * 4 * 512))
#test_features = np.reshape(test_features, (1000, 4 * 4 * 512))

#######################################
# Model
model = models.Sequential()
model.add(conv_base)
model.add(layers.Flatten())
model.add(layers.Dense(4096, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(BatchNormalization())

model.add(layers.Dense(2048, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(layers.Dense(2048, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(BatchNormalization())

model.add(layers.Dense(1024, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(layers.Dense(1024, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(BatchNormalization())

model.add(layers.Dense(512, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(layers.Dense(512, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(BatchNormalization())

model.add(layers.Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(layers.Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(BatchNormalization())

model.add(layers.Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(layers.Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(layers.Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002
model.add(layers.Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.003)))#0.002

model.add(layers.Dense(nclasses, activation='softmax'))
conv_base.trainable = False

#######################################
# Data generators
train_datagen = ImageDataGenerator(
      rescale=1./255,
      rotation_range=40,
      width_shift_range=0.2,
      height_shift_range=0.2,
      shear_range=0.2,
      zoom_range=0.2,
      horizontal_flip=True,
      fill_mode='nearest')

# Note that the validation data should not be augmented!
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
        # This is the target directory
        train_dir,
        # All images will be resized to 150x150
        target_size=(150, 150),
        batch_size=batch_size,
        # Since we use categorical_crossentropy loss, we need binary labels
        class_mode='categorical')

validation_generator = test_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=batch_size,
        class_mode='categorical')

# Model compile / fit
model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.RMSprop(lr=2e-5),
              metrics=['acc'])

# early stopping: https://keras.io/callbacks/#earlystopping
es = EarlyStopping(monitor='val_loss', mode='min', min_delta=0.001, verbose=1, patience=40, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.9, patience=15, min_lr=1e-20, verbose=1, cooldown=3)

history = model.fit_generator(
      train_generator,
      steps_per_epoch=round((ntrain+nval)/batch_size,0),
      epochs=ep_first,
      validation_data=validation_generator,
      validation_steps=20, #50
      verbose=2,
      callbacks=[es, reduce_lr])

#######################################
# Fine tuning
conv_base.trainable = True

set_trainable = False
for layer in conv_base.layers:
    if layer.name == 'block5_conv1':
        set_trainable = True
    if set_trainable:
        layer.trainable = True
    else:
        layer.trainable = False

model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.RMSprop(lr=0.00001), #1e-5
              metrics=['acc'])

history = model.fit_generator(
      train_generator,
      steps_per_epoch=round((ntrain+nval)/batch_size,0),
      epochs=ep,
      validation_data=validation_generator,
      validation_steps=20,
      callbacks=[es, reduce_lr])

#######################################
# Save model
model.save('C:/yourpath/yourmodel.hdf5')
end = datetime.datetime.now()
delta = str(end-start)

# Metrics
acc = history.history['acc']
acc = acc[-5:]
val_acc = history.history['val_acc']
val_acc = val_acc[-5:]
loss = history.history['loss']
loss = loss[-5:]
val_loss = history.history['val_loss']
val_loss = val_loss[-5:]

# End statement
print(""============================================"")
print(""Time taken (h/m/s): %s"" %delta[:7])
print(""============================================"")
print(""Metrics (average last five steps)"")
print(""--------------------------------------------"")
print(""Loss       %.3f"" %statistics.mean(loss))
print(""Val. Loss  %.3f"" %statistics.mean(val_loss))
print(""--------------------------------------------"")
print(""Acc.       %.3f"" %statistics.mean(acc))
print(""Val. Acc.  %.3f"" %statistics.mean(val_acc))
print(""============================================"")
print(""Epochs:    %s / %s"" %(ep,ep_first))","Two things come to mind: 
 You can add a data generator. This will generate new images from your current images by introducing a bunch of small changes (i.e. randomly rotating, zooming, shearing, shifting horizontally/vertically...), forcing the model to learn important distinguishing features between the different classes of images. 
 You can also add dropout layers to combat overfitting. 
 Here is a good example:  https://keras.io/examples/cifar10_cnn/","I think your model is not complex enough to learn from the CIFAR-10 datasets. 
 You can find CIFAR-10 classification datasets results using different models and activation functions  here . 
 Looking from the results, I can see that you will need to use a dense CNN model with Exponential Linear units (ELU) to get better accuracy.","Since you just achieve a training accuracy of 45%, I assume that your model is too simple. What you can do: 
 1) Use more hidden layers: more hidden layers increase the number of parameters and complexity of your model. However, since you are using dense, fully-connected layers you might see that your model gets big and slow pretty quickly. Therefore, I would suggest: 
 2) Use Convolutional layers. They are made for image classification since they allow much more efficient usage of parameters and training of more hidden layers.",,,,,53.98209829,53.28637802,56.32998469,51.71643318,52.66565724,,,,
70164,What is the difference between explainable and interpretable machine learning?,machine-learning,"I found  this article by Cynthia Rudin  which goes a bit more into the difference between the two terms that is in line with your source from O'Rourke. 
 At the core it is about the time and mechanism of the explanation: 
 A priori (Interpretable) vs. A posterio (Explainable) 
 I found this quote to be very helpful and inline with my own thoughts (emphasis mine): 
 
 Rather than trying to create  models that are inherently interpretable , there has been a recent explosion of work on  ‘explainable ML’, where a second (post hoc) model is created to explain the first black box model . This is problematic.  Explanations are often not reliable, and can be misleading , as we discuss below. If we instead use  models that are inherently interpretable , they  provide their own explanations , which are  faithful to what the model actually computes . 
 
 In short an interpretable model is able to output humanly understandable summaries of its calculation that allow us understand how it came to specific conclusions. Due to that a human would be able to actually create a specific desired outcome by selecting specific inputs. 
 A ""merely"" explainable model however does not deliver this input and we need a second model or mode of inspection to create a ""Hypothesis about its mechanism"" that will help explain the results but not allows to rebuild results by hand deterministically.","As for as explanation is concerned, we need explainability/interpretability at every level- 
 
 data explanation- tsne, simple plotting.  
 model explainability- by creating surrogate models  
 global explainability- feature importance for all training data 
 local explainability- explanation of every prediction.  
 
 all types of explainability on iris dataset, will be fun to have a look   
 I myself is confused about new buzz words coming everyday related to XAI( Even after developing our own xai framework) . consider Interpretability and explainability refer to same thing. people has given different names only.","Explainable Machine Learning is the  domain of AI . It  consists of interpretable models .  One could say the difference is that one is a tool and the other is a field of study. 
 In brief, interpretable machine learning is a  tool  used to solve problems present in the  domain  of explainable machine learning. 
 To define your answer: One shall use an interpretable model to help "" explain "" the model and explain why the model gives out the specific results. 
 Detail Explanation: Assume you need a cnn to classify whether there is a dog in the image. The architecture of the cnn would be the interpretable aspect of the machine learning problem. And the final saliency map or heatmap which shows the output and the focus of the cnn would be the explainable part of it.","In my opinion, the interpretability of an ML model refers to the ability to understand how the ML model is formed.
Normally, an ML model is created by using some intuitions. However, if the model is designed based on prior knowledge, like an unroll algorithm, then we know how it works.
Explainability refers to the question Why, like why the model makes a decision that way. For example, a sentiment analysis model concludes a text with a positive label. Why is that? Because the text contains some indicate words like  amazing , etc.","Interpretability  can be seen as a passive chracteristic of the model that referees to which level a given model makes sense for a human observer. 
 Explainability  can be viewed as an active characteristic of a model, denoting any action or procedure taken by a model with the intent of clarifying or detail its internal functions 
 Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI:  https://www.sciencedirect.com/science/article/pii/S1566253519308103",,,,,60.22304403,62.25503948,73.5811988,53.56725973,56.32066659,,,,
69978,How can I appropriately handle cleaning of gender data?,machine-learning,"There are at least two general considerations to make: 
 Domain-related 
 If an attribute potentially has predictive power in your domain and more specifically for your task your models might benefit from a direct encoding. For example: if being trans is correlated with different psychological disorders then I'd include a direct feature for this. This way it is easier for your model to make a prediction since it does not need to combine two features in the first place (e.g. no need to combine ""sex at birth"" and ""gender identification"" to identify a transsexual person (which would not even accurate since ""trans"" is a much broader term than just  sex at birth != gender identity )).  
 Moreover, I'd apply the same thinking to other feature engineering questions. Sex has predictive power for many tasks related to mental disorders, e.g. because mood disorders are more common among women and anti-social personality disorders are more common among men. However, whether these are rather related to the sex at birth or the gender a person identifies with is another question. So if your hypothesis is that in your task the gender a person identifies with is important then, again, it makes sense to include this in addition to the sex at birth. 
 Model-related 
 Different models are able to handle predictors differently. For example, tree-based models can more easily work with two separate attributes  sex == female  and  trans == True  to implicitly derive  trans female == True . However, linear models like neural networks might benefit from having a combined binary feature  female trans .","It is quite an interesting question. I guess that you can call it ""dealing with non-binary gender roles in a binary language"" or something like this. 
 In the past I did once something similar. I created 3 features: 
 
 sex at birth [male,female] 
 sex identification [male,female] 
 Attracted sexually to [male,female]. 
 
 All these features are binary and you can encode it as 0,1. You can achieve most of the gender, sex orientation, sex at bith states with a combination of both, for example, sex at birth=male, sex identification = female will gave you a trans person. sex_birth = male, attracted = male will give you a gay male. 
 A decision tree should be able to distinguish information and classify it correctly with this kind of encoding. 
 You could also do the cartesian product of all features and you will then encode it this way: 
 
 for a male[0], born male[0] attracted to male[0], == [000], 
 for  a male[0], born male[0] attracted to female 1  == [001]. 
 
 Applying one-hot encoding to this will give you 8 features that will include a high percentage of the cases. This encoding will allow trees to distinguish gender with a split and for linear regression to assign weights correctly. 
 It is true, that this is not exactly and you can complain about a lot of things. But in the end, while doing mathematical modelling we are making approximations and we are always missing something. 
 All models are wrong, but some are useful 
 Let me know if you find something better.","No need to drop from the analysis, for sure.  Your analyses should be capable of classifying by domain, even if you just assign them to a third (or fourth or ...) category.  You'll be basically comparing Female:Not Female, Male:Not Male, etc.; keeping them in the dataset means you have a better result when you're comparing those domains. 
 The decision you make depends to some extent on what question you are answering in your analysis.  Are you asking questions relating to gender identity?  Are you focused on a specific gender or sex?  Or are you exploring your data and looking to see what factors are important? 
 If you are focused on one gender identity, say, Female, then you could simply categorize the non-cis-Female non-cis-Male to a third (""Other"") category, for example.  This doesn't give you any information about the trans or otherwise non-cis gendered individuals, but if that's not actually important to your question, then this is the easiest way to handle them. 
 However, if you are exploring, and as you note in your question you're aware this is a possibly significant factor, then you should classify it - likely as a separate variable.  However, consider how you will perform the analysis when you assign these; you may still want to assign ""trans female"" as a separate gender, depending on what makes your analysis easier (while still having a  trans  1/0 flag variable, or  cis  1/0 flag variable, or similar). If you don't have any plans to analyse based on all females (regardless of trans/cis/etc.), then it may be easier to have a separate gender code there to make it easier to analyse rather than having to include the trans/cis flag variable in those analyses.","Some considerations here: 
 How has the data been collected? 
 If it's self-reporting, it's quite likely that most trans people will simply have replied with ""male"", ""female"", or other equivalent terms that give no indication of trans status. If it's reported by others, it's quite likely that the reporter will often not know that the person is trans.  
 If most of the trans men in your data are indistinguishable from cis men, and similarly for women, then - ignoring non-binary cases for the moment - your categorisation options are: 
 
 ""Cis men and trans men"" vs. ""cis women and trans women"" (if you map ""trans man"" to ""man"", etc.) 
 ""Cis men, most trans men, and some trans women"" vs. ""cis women, most trans women, and some trans men"" (if you map ""trans man"" to ""women"", etc.) 
 
 The first of those two seems clearly preferable, IMHO. It might not be the  best  delineation for every application, but at least it's fairly well defined. The alternative is just vague. 
 Are your decisions actually going to matter to the results? 
 It's quite likely that there won't be enough (identifiable) trans and non-binary people for you to get any useful data about ""trans men"", ""trans women"", or ""non-binary people"" as categories. It's also quite likely that these groups will be rare enough that they don't make a big difference to the overall stats for larger categories like ""men"" and ""women"", however defined. 
 If you weren't talking about open-source data, I'd also raise privacy issues with reporting for small sub-populations, but presumably that has already been considered. 
 What is the point of the analysis? 
 If you get past the above considerations... how does gender and trans status relate to whatever it is you're trying to understand? This is likely to be relevant to your decisions. 
 
 Should I just drop them from the dataset entirely, because they might distort it? 
 
 Cis people are likely to have much more influence on your results than trans people. Should we therefore drop cis people from the analysis for fear of distortion? 
 Trans people are people. If your aim is to produce statistics about ""people"" overall, then trans people should be included in those statistics. If some trans people are unusual (in whatever way) and this affects the statistics, then the statistics are simply reflecting the fact that some people are unusual.","If it is an open mental health data set, then those using it would benefit from filtering into as many categories as possibly relevant as the end user may need to specify between the given subsets. 
 In the end, data sets are easy to modify into narrowing categories or maintaining the same categories. 
 If the end user wants to combine those data categories, then they can factor them both into the ""female"" or ""male"" category, otherwise, don't dilute the data.","First I would determine the number of people who fall into male and female and if the number left over will not end up being statistically significant then they would be best to be discarded.  After that if the ""other"" group is big enough maybe split it up, but once again consider if the split groups are big enough for statistical significance otherwise I think you are wasting your time, just keep it as male female and other","Gender Analysis is a pretty common trend in data science, especially when it comes to mental health. But breaking it down into categories can be difficult. 
 I would break it down in to two columns, minimum.  
 One that is designated as either 'Assigned Male at Birth (AMAB)' or 'Assigned Female at Birth (AFAM)'. This is necessary from a medical standpoint as some drugs and side-effects of drugs have different effects depending on the hormones already present in the body. There's also the male study bias, where people who go into scientific studies for drugs are only tested on non-pregnant men. 
 Note that the column above may change later into a broader category, depending on how culture adapts to handle intersex individuals. 
 The second column would have more ambiguous categories, as with the current culture shift people are exploring gender more. It would need to be open to new updates as our culture shifts. Some options for this would be M for man, W for woman, U for unknown, Q for queer, A for agender, F for fluid, ect. One-hot encoding later will help make this easier to 'study'. 
 It would be handy for a person to know the pronouns of the person they're going to interact with, as well as to study the trends within our culture. So having a field for pronouns would be helpful for data analysis later on, as well.","I've been sitting on this idea for decades, never sharing it with anyone.  I don't expect it to be accepted by anybody.  But here we go anyhow: 
 When looking at the question of gender, I came to the conclusion that 8 bits were needed to define gender properly, including groups (which of course can be of both genders) and uncertainties.  The bits are: 
 NBM (80h) - natural born male 
 NBF (40h) - natural born female 
 MTS (20h) - masculinized transsexual 
 FTS (10h) - feminized transsexual 
 PNU (08h) - parts now uncertain 
 PHU (04h) - parts history uncertain 
 PHC (02h) - parts history certain 
 PIT (01h) - parts in transition 
 There's more to this but I won't bother you with further details. 
 For example: a group of cisgendered men and women would be 0C0h. 
 I came up with this before the era of nonbinaries, which would probably need additional bits. 
 Naturally, it doesn't include anything about heteros vs. LGB's -- that's a separate discussion. 
 The reason I'm presenting this is to point out how complex the issue is. 
 You are free, of course, to dismiss this as the rantings of a old woman.","That generally depends on what you are trying to achieve. 
 What people report as their gender is basically the output of a black box function with a lot of input variables. As any endocrinologist can tell you, it's not as simple as ""high testosteron"" vs ""high estrogen"" but more on the order of a hundred different hormones involved, most of which have interesting consequences medically. People with ""all male"" or ""all female"" hormone configurations are seldom, it is usually a mix with a bimodal distribution. 
 As such, correlating gender with any other data will only give you a somewhat noisy view on the variables that went into the black box. You can derive some probabilities from that, and that is usually all you want anyway: slightly better prediction for the majority of cases. Spending effort to optimize for capturing a small group perfectly is going to give diminishing returns here. 
 If you give users a neutral option ""prefer not to say"", you will lose a few rows from privacy minded people, but this also gives an easy out to people who don't believe they neatly fit into these categories. A separate ""other"" option is generally considered rude. 
 For applications where the neutral option doesn't work (e.g. because you're investigating side effects of medication), a simple ""gender"" column is likely oversimplified, and you might get better results from correlating directly to measurements.",56.80540566,53.90083776,60.0991982,51.36020397,52.48233731,50,56.87663405,58.30451377,58.19646306
68450,How can you include information not present in an image for neural networks?,neural-network,"Other answers suggest to put an additional channel, I disagree. I think it's a very computationally intensive, time consuming process. Moreover, it forces non-pixel data to be processed by Conv filters, which doesn't make much sense IMHO. 
 I suggest you to establish a  multi-input model . It would be composed by three parts: 
 
 A Convolutional part, to process pixel data, 
 A Feed-forward part to process non-image data, 
 Another Feed-forward part that elaborates the prediction based on the concatenation of the two outputs above. 
 
 You will need to instantiate them separately, then combine together in a Keras  Model() . You will also need  Concatenate()  layers to combine the two different sources of data. 
 You can read more about the implementation of multi-input Neural Networks  here .","Edit: after the edit in the question, 1) does not relate so much anymore, but 2) still does. 
 
 It depends a bit on the form of the location data. If you have a segmentation mask (i.e. another image with two colors denoting for each pixel if it belongs to the object or not), then going with another channel as n1k31t4 suggested might be a good idea. 
 
 If you have the coordinates or something in a vector form, Figure 2 in  this paper  shows a way to put the information together. Essentially, the authors concatenate the additional info (in your case the location data) to the output of the feature extractor and feed that into the classifier of the CNN.","The simplest thing to try out is to put the information in an extra  channel  of the image. 
 So if you have RGB channels, you could add a four channel which would simply be the location information you have, repeated for every pixel. 
 This creates a lot of redundancy, of course, but means you can take any standard image classifier and it will still work.","In the specific case of knowing the location of the object in the image, one technique would be to crop and pad each training example so that the object is in the exact center. This way the extra information is passed to the neural network implicitly. This is how most face identification neural networks work. 
 If the ""location"" of the object is more abstract, like ""bedroom"" or ""Spain,"" then I'd recommend concatenating the information to each pixel of the image. Don't be afraid to add a large number of extra input channels, neural networks handle this well. For example,  Alpha Go has a 48 channel input layer .","I will definitely try one more approach in this case, which is explained below. 
 
 Will use simple CNN architecture, followed by fully connected layers. 
 Say, now I have fully connected layer(FL) of size 100. 
 Using this FL apply another liner regression model(followed by activation layer...). Structure of linear regression would be: 
 y = w1i.FL(Ni)+ w2i.f1+ w3i.f2...and so on. 
 Ni = ith Neuron 
 f1, f2.. are the non image feature. 
 w1i, w2i, w3i.. are the weights 
 
 
 What I'm trying to achieve is, using the output of each FL
  neuron, I   will create a linear regression model, where other
  features would be your   non-image data. 
 
 Here my assumption is during training, it will boost the weights of neurons based upon the non-image data as well.",,,,,53.19201746,52.57323839,56.09847484,63.29921571,52.58655144,,,,
68327,Why is activation needed at all in neural network?,activation-function,"Generally the activation is part of the model and gets applied for each neuron, so definitely before the error calculation. What the activation function is depends on what task you are solving and where the neuron of interest is. In principle the activation function  $f$  would go to the calculation of the outcome 
 $$ y = f(Wx + b)$$ 
 For output neurons, if you are doing classification, then  $f$  should map between 0 and 1, since you'll interpret the outcome as a probability. For regression  $f$  could be just the identity.  
 For hidden (i.e. non-output neurons), you definitely want to use a non-linear  $f$ . The reason is that the neural network would otherwise be equivalent to a regular linear model. So the non-linear activations are needed to harvest the expressive power of neural networks. 
 For deep learning the most popular  $f$  for hidden neurons would probably be the rectified linear unit (relu) 
 $$ f(x) = \max(0,x)$$","Activation function is applied after (sum of w*x+b) for each neuron in each layer. 
 The role of activation function is to introduce non linearity  ""higher order relation ship"" between inputs and outputs.","Without activation, the model is just a linear model like linear plotting,regression. Where is the ""learning""?  
 2.Calculate the outcome (sum of w*x+b) - we know what it should be because we know what image we gave to the system. 
 The weights are random. The neuron does not know how to bound the value (the firing pattern). Activation should be there as an instruction how to bound the output. Otherwise from layer to layer outcome can be anything.  
 4. Nudge all weights to reduce the error. 
 How we do it? We find out the gradient (direction of minimizing/maximizing) because if you want to optimize the function as desired , you find its values at its derivative/gradient, back propagate the gradient since we want to minimize or maximize some cost function (error or difference in this case). How will you find gradient of a linear function since differentiating it will be a constant. Learning will essentially stop.  
 So we introduce certain non-linearity (activation functions) to the system so that the gradient varies and we get a way to update our weights every time we back propagate.","Mathematically, the weights that sit between two given rows of neurons collectively form a transformation matrix, and a row of neurons forms a vector. To use the network, we use the matrix to transform the vector, giving us a vector representing the next row of neurons. Then we apply an activation function to those neurons. Then we proceed to the next layer and repeat. 
 So what happens when we don't have an activation function? Then we just have a series of matrix transformations, and we can use matrix multiplication to compute a single matrix that does the same thing. So in truth such a network has no hidden layers, and is incapable of deep learning.","Talking about activation... 
As we know that the activation function gives the positive signal only then when the input (sum of weights and inputs) it gets is positive value. Am I right? Consequently the weights we use to multiply pixel input values has to be as big as possible. But why is it this way that only positive value fits to give better probability value? Why smaller weights are not suitable?","Ok, let's put it like that: ""no activation"" means ""linear model"", i.e. a model that can only learn linear associations between your variables. This is a very big limitation!
The whole point of Neural Networks is: almost any regularity we observe in the world is non-linear, therefore we must make them flexible enough to learn all these complex patterns. That's where non-linear transformations come into play. 
 A bit more technically, a layer is like a function: 
 output = f(Weights * Data + Bias) 
 if all your  f() 's in your layers are linear, even the output of the stack of layers such as: 
 ANN_output = f_3( f_2( f_1(Weights * Data + Bias) ) )         # yeah that's a Neural Net 
 will be linear as well. 
 Non-linear activation functions let us overcome this limitation, and allow a model to learn patterns that could be either linear or not. And the more you stack layers (i.e. the more you nest functions into one another) the more complex the final non-linear transformation becomes, making the model able to learn more complicated things.",,,,60.57397415,55.17270961,53.08663334,53.88020196,53.27212249,54.56414361,,,
66732,is it possible (and/or logical) to set feature importance for xgboost?,machine-learning,"You may duplicate some features in your dataframe. I certainly was looking for two things as you mention,one of them is putting some features closer to the root node, and also want some features to appear more on branches. I understand your concern is to make your trees more aware to some features. I may suggest something there. XGBoost samples each feature uniformly, which it would be nicer if we can say that some features are more important and should be used more. Short hack would be duplicating the columns while decreasing the colsample_bytree ratio. XGBoost for now doesn't support weighted features since it draws features uniformly. I've seen it on another place, there's no specific sampling technique for features(columns in XGBoost) in the documentation. Check colsample_bytree :  https://xgboost.readthedocs.io/en/latest/parameter.html  (Credits: Heard that technique from a webinar of  https://www.kaggle.com/aerdem4  ) 
 Generally, newbies like me starts to create a lot features via engineering, however also may discard that important features becomes minority. Although XGBoost parameters can deal with it while hyperparameter search, it seems really not beneficial creating trees with no rain data because of the domination of high number of engineered wind data when problem is predicting drought . 
 If your concern is specifying domain knowledge, you may also define a domain knowledge via ""feature interaction constrainst"" in XGBoost, documentation. I hope I'll edit the answer when I become %100 sure what I'm doing with feature interactions:  https://xgboost.readthedocs.io/en/latest/tutorials/feature_interaction_constraint.html#:~:text=Feature%20interaction%20constraints%20are%20expressed,but%20with%20no%20other%20variable . 
 I'm aware these doesn't answer your primary question, it seems not possible for now. Also, I'm not sure that features put on the top are more important, this doesn't seem always true. 
 Edit:  XGBoost has added feature_weights to the DMatrix, in 1.3.0! However, 1.3.0 is not stable yet. I'll edit here when I find/apply feature weights.  https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor.fit","Sometimes, when you have important categories in your population it is best to split the data set by catégories and train different model on them. This might be some way to go if you really have different populations and different associated behaviour. This might be helpful to avoid some category imbalance problems. However this might not be practical as you would have to tune multiple models. 
 Regarding xgboost, it is designated to handle important data sets rapidly. It’s usually not desirable to try to influence the learning process to choose what feature it will pick first. 
 If you have one feature that it should pick first because of expert knowledge but it doesn’t : 
 
 look what feature are above in term of importance and if there might be a problem with them. 
 try to improve your feature with feature engineering.","In fact, Tree algorithms chose the feature they cut on by calculating a metric, evaluating which cut is the best. Most famous metrics are Gini or Entropy. 
 So the goal of it is to automatically make the best splits, knowing what's in the data. Force features to be on top of the tree would mean downgrading performance, since the trees makes its cuts in variables that create the most significance.","Xgboost calculates feature importance automatically. I did not find any method to set it manually. You can choose 2 options to solve the problem: 
 
 set weight of important samples 
 split you data with very important feature and train multiple models.","It's not possible to set feature importances in advance, but you can reduce your model’s dependence on specific features. Since earlier trained trees have a greater influence on the final predictions, you can reduce the importance of certain features by excluding them in the initial trees. For example, suppose you train an XGBoost model with 1,000 trees, and features F1 and F2 have the highest feature importances. If you exclude F1 and F2 from the first 200 trees, the importances of these features will be significantly reduced. 
 This strategy can help improve generalization, especially if your model heavily relies on F1 and F2, and you expect a distribution shift between the training and test data. Here’s how to implement this: 
 
 Train the first 200 trees without F1 and F2. 
 
 Save the model's JSON file. 
 
 Modify the JSON file : 
 
 Append  (otherwise, you need to change the indices in each tree structure) the features you excluded from the first 200 trees back into the feature set. 
 Adjust the  feature_num  to reflect the total number of features (after adding F1 and F2 back). 
 You don’t need to modify the tree index splits; they will remain the same. 
 
 
 Retrain the model  by loading the pre-trained model with 200 trees, but this time, include F1 and F2 in the training data: 
 model.fit(ddf_X_train, ddf_y_train, xgb_model=previous_model) 
 
 
 As a result, the final model will show much smaller feature importances for F1 and F2.","In XGBoost, it is currently possible to set feature importance by influencing the selection process during tree construction. 
 Instead of considering all available features for each split, the optimisation algorithm selects a subset of features at random before each split. You can control the size of this subset as a proportion of the total number of features. Also, you can assign different probabilities to each feature, affecting how likely they are to be sampled, thereby setting their importance. 
 Note this is not the same ""feature importance"" that you can obtain from the trained model with  get_score() . 
 See parameters  colsample_bynode  and  feature_weights  in the  documentation . Also see a very  basic example .",,,,66.25697416,59.95747868,52.49196517,66.87110745,65.00757927,70.43721017,,,
66651,"In industry, what type of new data science algorithms does one develop?",predictive-modeling,"I am no data scientist, only an aspiring one for two years, moving from my background in software engineering and mathematics. So I took some courses, had some interviews, read a lot on the subject online. My opinion: 
 New algorithms are developed in research centers and at universities. And even then, most algorithms used in companies are already developed, and just optimized even more.
Don't get your hopes up or be afraid that you have to reinvent gradient descent backpropagation. 
 With developing algorithms they most likely mean data extraction, data cleaning, data preparation for reporting statistics and presenting graphs. Maybe programmatically, maybe just using tooling. 
 The presented data may give more insights in simple relations in the domain, and maybe insights in more complex questions that can be asked.  
 You may get to define data flows, get to compare and select machine learning algorithms and tune its parameters. And continuously evaluate model performance in practice.","In industry its usually variations (but important ones) of the ground ideas. 
 Look at this boosting timeline: 
 
 (Ada)Boosting  Formally  by two profesors in 2003 
 xgboost by  DLMC  Distributed Machine Learning Community in 2014 
 lightgbm by  Microsoft  in 2017 
 catboost by  yandex  in 2017 
 +- all the variations in between that did not caught up 
 
 Building on ""basic"" idea they removed all the negative ones while modifying/defining new sequence of steps to be executed. 
 To answer your question.  As long as you have some non-trivial (debatable what it is) new (or variation of) sequence of steps to be executed, with appropriate degree of generalisation you got youself an algorithm. So the type of new algo is dependable on the field you work in.","It is indeed extremely rare for someone to develop a novel algorithm to solve their problem. In my experience it is more important to understand the business domain, how to normalize the data and choose what loss function should be minimized. 
 But it is very valuable to have experience with various kinds of algorithms so that you can pick the right tool for the job. 
 If a job listing says that a person ""must develop new algorithms"" I'd read it more like ""must develop new programs / software / scripts"".","You might be interested in the annual  Kaggle survey  about the state of Machine Learning and Data Science. 
 Some key results related to your question: 
 
 The most commonly used algorithms were linear and logistic regression, followed closely by decision trees and random forests. Of more complex methods, gradient boosting machines and convolutional neural networks were the most popular approaches. 
 
 Some numbers from the  report  (2020): 
 
 Linear or Logistic Regression - 83.7% 
 
 Decision Trees or Random Forests - 78.1% 
 
 Gradient Boosting Machines (xgboost, lightgbm, etc.) - 61.4% 
 
 Convolutional Neural Networks - 43.2% 
 
 Bayesian Approaches - 31.4% 
 
 Recurrent Neural Networks - 30.2% 
 
 Neural Networks (MLPs, etc.) - 28.2% 
 
 Transformer Networks (BERT, gpt-3, etc.) - 14.8% 
 
 Generative Adversial Networks - 7.3% 
 
 Evolutionary Approaches - 6.5%","None. 
 What you do in business is program algorithms to solve biz problems. 
 Most all common problems have algorithms and processes already.  So what you would do is adopt and apply it to your particular situation. 
 Data science being the buzzword du jour, and very poorly understood by almost everyone as to what it is and how to use it, is more a fancy word to get you to apply your programming skills to databases in some fashion. 
 Often this will be some sort of data 'mining' that looks for patterns so that management can ASSume that correlation means causation and then use that as a rule to make decisions. 
 This is how the last wall street meltdown occurred.  They  made BAD ASSumptions that were close enough for most cases to ignore the errors in it but when that black swan hit they went bankrupt and the government had to bail them out with our tax money.",,,,,57.3068461,56.65148996,57.44850063,52.34757255,54.75385698,,,,
66577,How can we extract fields from images?,machine-learning,"I have a similar use-case and a working product based on  tensorflow object-detection api  and  pytesseract  for OCR. On top of the extracted text, I perform regex for validation of the extracted information and cleaning it to meet requirements of other processes.  
 Steps : 
1. Annotate images with some tool like  labelimg . 
I annotated a set of 1K images, similar to yours, with 23 different classes. The dataset is unbalanced with some classes appearing almost in every image to some classes appearing in only as few as 60. However, there are ways to ensure that this imbalance does not affect the performance of the network. 
2. Choose a model from  tf model zoo  (I use this  frcnn  model) and  retrain  the last two layers using transfer learning. 
3.  Export the inference graph , perform object detection to identify the region of interest, and run OCR on the region of interest to extract the text. 
I'd recommend storing the extracted data in a dictionary with class of the object as key and the extracted text as value. 
4. Finally, have regex validate the text in the extracted field and perform any manipulation/transformation that is necessary. 
 The trained model can be deployed to production with help of  tfserving . The same trained network can be deployed into a mobile app as well - look for tutorials on tensorflowlite for this. 
 Hope my answer helps you! I had a tough (but interesting) time gathering the knowledge required to get a production grade product that currently serves hundreds of request everyday. I would recommend reading completely all the links I have shared in this answer, and feel free for more questions. Good luck!","I think you already have some OCR in place? I don't know if you also have the x-y locations and size of the recognized texts? 
 I hope you have a model that knows (has learned) occurrences of 'invoice #' as a label. 
 And maybe you can machine learn to recognize values that could be invoice numbers. 2034, 200.00 could be invoice numbers, 'Date' and 'Service fee' not. 
 You could machine learn relations between objects, probably with the help of a distance function. 
 I would say that a string value that contains mostly digits, is near a label that matches 'invoice #', and also has a similar size, is the most likely invoice number. 
 564 could be an invoice number, but it is too far away from invoice # (further than 2034). 
 'Date' is close to invoice number, but it does not match an expected string for invoice numbers, since it is mostly letters.","I would suggest that you should use a pre-trained OCR model and train your own custom model which only outputs required data. 
 Training method: 
 Just use a pre-trained OCR model like  this , and remove the tail of the model and add your custom output layer with the required number of fields (in your it's case invoice and date). After this, freeze the head of the model and train your custom model with the data you have. 
 Note: 
 
 The accuracy of the model can be improved if you train this custom model by using as many different invoice templates as possible. Because this custom OCR model will have to learn to figure out the position of the invoice and date by itself. 
 If the model's output is incorrect for certain template you can always generate your own (synthetic) data by editing the template and add many examples of that particular template. 
 
 Using the pre-trained model, you can get pretty decent results with less training data. If you haven't used a pre-trained model,  here  is a more generalized way to use style transfer in PyTorch. I hope it will help you.","A completely different answer: 
 I am currently following a course  Computer Vision and Image Analysis . 
 With your problem in mind you could follow along. Depending on previous knowledge you could skip a few sections. (I skipped immediately to Beyond Classification/Object Detection) 
 Globally, you could train an image classification model that could recognize regions of interest, with a classification of the content. Where the course addresses people, cars, buses, in your problem you have images, labels, content (of various types). You may need to experiment, and maybe have 'label-value-pair' as a class, or even 'label-multiline'.
Or maybe labels and values separately work better? Or even a third option that you identify all combinations 'label' 'value' 'label-value' 
 You should probably define a low-wide boundary box for horizontal label/value pairs, and a more square one for vertical aligned label/value pairs. 
 You should end up with labeled regions of interest. 
 If you are happy with these regions, then the second step could be OCR. For the OCR you could use a similar problem division to recognize separate characters, and label the separate characters. And then you still have to combine the characters to words or values.","More or Less this would be helpful 
 Link:  Extracting information from documents 
 Approach & Algorithm from the above blog 
 Approach 
 The algorithm looks for phrases that look like a date. Then it picks the one which appears in the highest position in the document. In the corpus we used, almost every date contained the month written as a word (e.g. April), the day written in digits (13) followed by the year (1994). Sometimes, the day was printed before the month (e.g. 4th September, 1984). The algorithm looks for the patterns M D Y and D M Y where M is a month given as a word, D is a number representing the day and Y a number representing a year. 
 Software Tools 
 Our implementation runs in a Jupyter Notebook with Python 3. We use Tesseract version 4, for doing OCR through the wrapper pytesseract. Since the software sometimes gets a letter of the month wrong (e.g., duly instead of July), we accept all strings which almost look like a month in the sense that only a few letters need to be changed to reach a valid month. The number of these operations is called the Levenshtein distance, a common string metric in natural language processing (NLP). For example, the Levenshtein distance of duly and July is 1. Similarly for Moy, Septenber or similar errors. We use python-Levenshtein. For detecting numbers (years and days), we use regular expressions. We process all the tables in Pandas and use tqdm to have a neat progress bar. 
 Algorithm step by step 
 Similar Questions from Stackoverflow: 
 
 How to Extract Information from the Image 
 How to Extract Information from the Image(PNG)",,,,,64.78585537,50,51.11941253,52.9045366,57.33327227,,,,
66350,What would I prefer - an over-fitted model or a less accurate model?,machine-learning-model,"There are a couple of nuances here. 
 
 Complexity question very important - ocams razor 
 CV - is this trully the case 84%/83% (test it for train+test with CV) 
 
 Given this, personal opinion: Second one. 
 Better to catch general patterns. You already know that first model failed on that because of the train and test difference. 1% says nothing.","It depends mostly on the problem context.  If predictive performance is all you care about, and you believe the test set to be representative of future unseen data, then the first model is better.  (This might be the case for, say, health predictions.) 
 There are a number of things that would change this decision. 
 
 Interpretability / explainability.  This is indirect, but parametric models tend to be less overfit, and are also generally easier to interpret or explain.  If your problem lies in a regulated industry, it might be substantially easier to answer requests with a simpler model.  Related, there may be some ethical concerns with high-variance models or non-intuitive non-monotonicity. 
 
 Concept drift.  If your test set is  not  expected to be representative of production data (most business uses), then it may be the case that more-overfit models suffer more quickly from model decay.  If instead the test data is just bad, the test scores may not mean much in the first place. 
 
 Ease of deployment.  While ML model deployment options are now becoming much easier and more sophisticated, a linear model is still generally easier to deploy and monitor. 
 
 
 See also 
 Can we use a model that overfits? 
 What to choose: an overfit model with higher evaluation score or a non-overfit model with lower one? 
 https://stats.stackexchange.com/q/379589/232706 
 https://stats.stackexchange.com/q/220807/232706 
 https://stats.stackexchange.com/q/494496/232706 
 https://innovation.enova.com/from-traditional-to-advanced-machine-learning-algorithms/ 
 (One last note: the first model may well be amenable to some sort of regularization, which will trade away training accuracy for a simpler model and, hopefully, a better testing accuracy.)","The first has an accuracy of 100% on training set and 84% on test set. Clearly over-fitted. 
 
 Maybe not. It's true that 100% training accuracy is usually a strong indicator of overfitting, but it's also true that an overfit model should perform worse on the test set than a model that isn't overfit. So if you're seeing these numbers,  something  unusual is going on.   
 If both model #1 and model #2 used the same method for the same amount of time, then I would be rather reticent to trust model #1. (And if the difference in test error is only 1%, it wouldn't be worth the risk in any case; 1% is noise.)
But different methods have different characteristics with regard to overfitting. When using AdaBoost, for example, test error has often been observed not only to not increase, but actually  continue decreasing  even after the training error has gone to 0 (An explanation of which can be found in  Schapire et. al. 1997 ).  So if model #1 used boosting, I would be much less worried about overfitting, whereas if it used linear regression, I'd be  extremely  worried. 
 The solution in practice would be to not make the decision based only on those numbers. Instead, retrain on a different training/test split and see if you get similar results (time permitting). If you see approximately 100%/83% training/test accuracy consistently across several different training/test splits, you can probably trust that model. If you get 100%/83% one time, 100%/52% the next, and 100%/90% a third time, you obviously shouldn't trust the model's ability to generalize. You might also keep training for a few more epochs and see what happens to the test error. If it is overfitting, the test error will probably (but not necessarily) continue increasing.","These numbers suggest that the first model is not, in fact, overfit.  Rather, it suggests that your training data had few data points near the decision boundary. Suppose you're trying to classify everyone as older or younger than 13 y.o. If your test set contains only infants and sumo wrestlers, then ""older if weight > 100 kg, otherwise younger"" is going to work really well on the test set, not so well on the general population.  
 The bad part of overfitting isn't that it's doing really well on the test set, it's that it's doing poorly in the real world. Doing really well on the test set is an indicator of this possibility, not a bad thing in and of itself.  
 If I absolutely had to choose one, I would take the first, but with trepidation. I'd really want to do more investigation. What are the differences between train and test set, that are resulting in such discrepancies? The two models are both wrong on about 16% of the cases. Are they the same 16% of cases, or are they different? If different, are there any patterns about where the models disagree? Is there a meta-model that can predict better than chance which one is right when they disagree?","Obviously the answer is highly subjective; in my case clearly the SECOND. Why? There's nothing worse than seeing a customer running a model in production and not performing as expected. I've had literally had a technical CEO who wanted to get a report of how many customers have left in a given month and the customer churn prediction model. It was not fun :-(. Since then, I strongly favor high bias/low variance models.","It seems a lot of people misunderstand overfitting here. Overfitting is not the gap between train and test performance. Overfitting is when you add complexity to a model, and there is no return on investment, or, most times, a loss in return. 
 See page 38 of Elements of Statistical Learning for the graph below. An overfit model is one that is to the right of the minimum of test error. Notice that for the best-fitting model, the gap between train and test is still relatively high. The correct answer is to choose the model with 84% accuracy, assuming you know that the 1% difference is statistically significant. I'm of course also assuming interpretability is not of concern here.","If your options are indeed ""100% on train / 84% on validation"" vs ""83% on train / 83% on validation"", I'd feel safer with the second one - but really, I'd take a third option: Try and tweak the first model to reduce overfitting (with the usual  methods), hopefully squeezing a bit more accuracy out of it.","Primarily, go for CV for the training and test set. If you still get the same type of result, then choose the second model. 
 The first model has a very large difference in accuracy between the training and test set.
It is a very specific model.
There is a chance that the high accuracy on the test set appeared due to data leakage. 
 The second model is a more general purpose model with acceptable accuracy results on both sets.",,51.29057841,56.80490541,54.98344187,52.73694087,53.65651384,55.88528103,51.34770381,54.62420076,
66216,GridSearch without CV,python,"GridSearchCV is built around cross validation, but if speed is your main concern, you may be able to get better performance using a smaller number of folds. 
 From the docs: 
 
 class sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False) 
 cv: int, cross-validation generator or an iterable, optional 
 Determines the cross-validation splitting strategy. Possible inputs for cv are: 
     None, to use the default 5-fold cross validation,

    integer, to specify the number of folds in a (Stratified)KFold,

    CV splitter,

    An iterable yielding (train, test) splits as arrays of indices.
 
 For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. 
 
 cv  defaults to 5, so changing it to 2 should provide a significant speedup for you. This will weaken the cross validation significantly. 
 Alternatively, you may be able to pass in a single test/train split for your value of  cv . This would effectively disable cross validation and remove the benefits that it provides.","By passing a callable for parameter  scoring , that uses the model's oob score directly and completely ignores the passed data, you should be able to make the GridSearchCV act the way you want it to.  Just pass a single split for the  cv  parameter, as @jncranton suggests; you can even go further and make that single split use all the data for the training portion, and the testing portion won't even get used in the above setup.  (Does sklearn perform a check to prevent passing  cv=1 ?) 
 I haven't had a chance to try this out yet: 
 def oob_scorer(estimator, X, y):
    return estimator.oob_score_

model = GridSearchCV(estimator=RandomForest(...),
                     param_grid={...},
                     scoring=oob_scorer,
                     cv=PredefinedSplit([-1]*TRAIN_SET.shape[0]),
                     ...
                     )
 
 scikit docs: 
 Fixed split 
 Custom scorer 
 Related Qs: 
 Scikitlearn grid search random forest using oob as metric? 
 RandomForestClassifier OOB scoring method 
 I'm not sure the hackiness of this approach is worth it; it wouldn't be terribly difficult to make the grid loop yourself, even with parallelization. 
 
 EDIT: Yes, a cv-splitter with no test group fails.  Hackier by the minute, but you can split off just a single test point, or add a dummy test set, or... 
Here's a working example.  It does seem the oob_score is being used, and the test set has just a single sacrificial point:
 https://github.com/bmreiniger/datascience.stackexchange/blob/master/GridSearchNoCV_oob.ipynb","Alternatively, just implement a simple Grid Search algorithm yourself. The book ""Introduction to Machine Learning with Python"" by Mueller and Guido includes an example using an  SVC : 
 # naive grid search implementation
from sklearn.svm import SVC

X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)
print(""Size of training set: {} size of test set: {}"".format( X_train.shape[0], X_test.shape[0]))

best_score = 0

for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:
    for C in [0.001, 0.01, 0.1, 1, 10, 100]:
        # for each combination of parameters, train an SVC
        svm = SVC(gamma=gamma, C=C)
        svm.fit(X_train, y_train)
        # evaluate the SVC on the test set
        score = svm.score(X_test, y_test)
        # if we got a better score, store the score and parameters
        if score > best_score:
            best_score = score
            best_parameters = {'C': C, 'gamma': gamma}

print(""Best score: {:.2f}"".format(best_score))
print(""Best parameters: {}"".format(best_parameters))","There are a few ways of making this faster:   
 
 Decrease the CV value, as mentioned by @jncraton 
 Decrease the search space for the hyperparameters (test only a few parameters or decrease the ranges for parameters) 
 
 Additionally, you might consider using a more efficient way of hyperparameter searching by using hyperopt or nevergrad.","If speed is the only only issue then i have few suggestions that will definitely improve the algorithm run time by 5-10times(which i experienced), without compromising on any other input: 
 1) Increase the number of jobs submitted in parallel, use (n_jobs = -1) in the algorithm parameters. This will run the algo in parallel instead of series(and will cut down by time by 3 to 4 times. (chk the below code). 
 class sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, **n_jobs=None**, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)
 
 2) You can use RandomSearchCV in place of grid search. This also work on similar principal but must more optimized version(actually it randomly searches for optimum parameters unlike grid search that does it for all combinations). 
This will cut down algo run time by 4-5 folds again. 
 3) Combination of RandomSearchCV with n_jobs = -1, this will help to cutdown time by 8-10 times.  
 Please try it on your problem and feedback if it solved your problem or not ?",,,,,50,50,50,50,51.15996879,,,,
66015,Identifying similar data points,machine-learning,"It's generally treated as a  binary classification problem , often called  credit scoring . You are trying to know whether a specific entity will default with you or not. The fact that the entity has defaulted with another company can be used as feature of your problem. As mentionned by others, there are other approaches, but I honestly think they are in theoretical infency, and can't seem to find any literrature on their practical implementation to credit scoring.  
 Then you need to  build an historical database  to learn on, with an associated  label . There are two main approaches : either you construct a table based on periodic (yearly) data and set an horizon for the default. It allows you to build a label by observing what happened in the past. Say you have yearly data and consider a 1 year horizon. You will have multiple line for a company : one for each year, say 2010 to 2018, the label will be built on their status the following year, 1 if they defaulted respectively in 2011, ..., 2019, 0 otherwise. Another approach is to consider life-time default, it is better in theory, and probably better match what you want to do, but it is unlikely you will be able to do that, notably because it require complete data and you usually don't keep those that defaulted.  
 You can  learn  on that database, but this is a complex process including  variable selection, model selection, model training, model validation . Pretty much standard data science, there is a lot of introductory material you should look at, starting with :  an introduction to statistical learning , which is free). Then you can go look at ressources oriented towards credit scoring like  Credit Scoring and its application , then to supervised learning applied to credit scoring like deep learning for mortgage risk:  https://arxiv.org/abs/1607.02470 ). This will help you  measure the risk  associated with your current population. More so if you  calibrate your model in probability  such that the output of your model reflect the probability of default. 
 However if you want to use that model for  making decisions  about lending, then the problem will be made more complex, because : 
 1) Your  initial decision will have an impact on the credit situation  of said entity. So you actually would need to assess the impact of both legs of your decision on the caracteristic of the entity before putting it in your prediction model.  
 2) You need to  take costs  into account. The costs are usually asymetrical and hard to predict. For loans for exemple, not doing the loan will cost you interests on the nominal (it loops to (1) as the % interest you may ask will modify the contract and hence the situation of the entity), but also associated returns (a bit of money for maintaining their account for exemple), while a default would cost you a proportion of the outstanding amount, which is another order of magnitude, and that proportion is often rather difficult to predict (given insurances for exemple).  
 I am not really aware for advanced literature for points 1) and 2), despite working in the field, I even suspect there is a complete black-out on those topics as they are the one that provide competitive advantages to a given credit lender.   
 3) You want to  avoid social discrimination , mostly for ethic and legal purpose, which is counterproductive to statistical discrimination. There is not much literature on the topic in general, and fewer in credit scoring, except maybe :  https://arxiv.org/abs/1610.02413 . Depending on your jursidiction this may give you some headaches.","First, I don't think this problem can be addressed as a binary classification problem. Indeed, your current label (customer has or has not defaulted with others) is not reliable, as some customers who have not defaulted in the past might do so in the future, with you or others. 
 Consequently, what comes to me as the first thing to try is  one-class SVM  (see the  scikit-learn implementation ), because you can only rely on a single class (the 30% of your dataset which correspond to the  default  class). OCSVM is useful when you have only one class in your dataset, and also popular in problems where the classes are so imbalanced that some are likely to have too simple patterns. 
 Theoretically, training an OCSVM over your  default  class and applying it to the other 70% will let you see which currently reliable customers are likely to default in the future. The distance to the separating hyperplane will even enable you to sort customers regarding their potential to default. 
 However, in practice,  it won't be an easy task : you will especially find it hard to choose a kernel and its hyperparameters without a proper validation dataset, including a certain number of customers who will not default. To prepare this validation dataset, I trust that you can collect some past data of both customers who defaulted and not. As soon as you can do this, find the best kernel / hyper-parameters with a classical validation approach, trying to lower the false negatives rate (use the  recall  score as your main guide). 
 Please note that your problem is such that you will thus build a model to predict the risk of default for  other  customers. You might want to include data about customers who defaulted with your own company, and see if it changes the results.","It's a very broad task and there is no single answer. You have to experiment.
Look for active learning-based approach.
Also Single Class SV<","Saurabh without the data this is a little difficult to answer but here goes  
 I would agree to Leevo's thought process here since 30% of your data is folks who have defaulted elsewhere and you want to use that to understand who from the remaining 70% and anyone new would default in the future. Clustering would be the best approach here and since the trend of the data plays an important part usage of Mahalanobis distance over euclidean distance might give a good cluster consistency if you try k-means clustering here. Another alternative is to try using association rule mining to understand what variable combination contribute to customers defaulting at others and then use those rules that association rule mining throws out on the new data set.","I think clustering is the way to go, but first you have to preprocess data in the proper way. 
 I would start encoding your observations using an  Autoencoder for dimensionality reduction . This DL technique allows you to compress your data, or to take a ""summary of it"". The compressed representation of your data is often use as a preliminary step in outlier detection tasks, and I think your problem is highly compatible with this approach.
If you don't want to use Deep Learning, you can recur to other dimensionality reduction techniques.  PCA  might work, for example. ( t-SNE  is stochastic, and I don't recommend it outside data visualization in lower dimensions.) 
 Once you have represented your data in a dense, lower dimensional space, there's a number of things you can do. 
 
 Employ some measure of distance between observations in this compressed space. If your goal is to understand how different two observations are, that would be the main thing to do. The most common and simple is  Euclidean distance , but there's an awful lot of measures you can use (Manhattan, Minkowski, Mahalanobis, ... you name it!). 
 You can run clustering techniques. The one I use almost any time is, of course,  k-Means clustering . With this, you can try to identify groups of more similar observations, this can give you hints on how to spot ""good"" and ""bad"" observations.
An alternative is  DBSCAN  clsutering, that allows you to classify some observations as outliers (the drawback of this model is that you have many hyperparameters to tweak.) 
 Additionally, you can train a classifier that is fed with the compressed representation of your data. Labeling observations of a dataset as ""good"" and ""bad"" could be time consuming, but it might be worthwile if you don't just want to check how different two customers are, but also get a straight classification.",,,,,51.00748349,50.44528886,50,51.63418785,52.82619148,,,,
65968,Retrieve dropped column names from `sklearn.impute.SimpleImputer`,scikit-learn,"I've got the same issue today, and it's a shame your post got no answers. I think this question is not well addressed in the sklearn documentation. I can show you my workaround to this issue: 
 headers = X.columns.values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

empty_train_columns =  []
for col in X_train.columns.values:
    # all the values for this feature are null
    if sum(X_train[col].isnull()) == X_train.shape[0]:
        empty_train_columns.append(col)
print(empty_train_columns)
 
 The idea is to keep all your column names, and after you split your data check which of them completely empty in your training set. If I'm not wrong the Imputer respects the column order so, for example, you can correlate every feature with its importance if you are using Decision-Tree-based models. 
 I'm not satisfied with this ugly piece of code but I couldn't find a more elegant (and simple) solution.","Since the original question, scikit-learn (version 1.1, May 2022) has implemented  get_feature_names_out  methods for most (if not all) transformers. Now, column names can easily be retained using it: 
 import pandas as pd
data = pd.DataFrame({'col1': [1,np.nan,2], 'col2': [3,4,5]})

from sklearn.impute import SimpleImputer
si = SimpleImputer()
pd.DataFrame(si.fit_transform(data),
             columns = si.get_feature_names_out())
 
    col1  col2
0   1.0   3.0
1   1.5   4.0
2   2.0   5.0","SimpleImputer drops columns consisting entirely of missing values.  It is indeed unpleasant when trying to associate original columns; the sklearn devs have been discussing this: 
 https://github.com/scikit-learn/scikit-learn/issues/16426 
 Vincent's answer is good, if you are working directly: just detect and remove the offending all-missing columns, since they don't contribute anything to your model.  If you need something more automatic (e.g., you have a highly-missing column that in cross-validation leads to some training folds being all-missing), then perhaps use a  ColumnTransformer , where the  columns  argument is a callable that checks for all-missing?  Then you can use the ColumnTransformer's  get_feature_names  method to find out when/if a column was removed.","As per my experience, It didn't drop any columns. It replaced the name of the columns from actual column names to 1, 2, 3... 
 To put the actual column names after the imputation 
  imp = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')
 imp.fit(df)
 df = pd.DataFrame(imp.transform(df), columns = df.columns)","Kaggle has a great way to solve the missing column names issue with SimpleImputer. 
 my_imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))

# Imputation removed column names; put them back
imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns

print(""MAE from Approach 2 (Imputation):"")
print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))
 
 https://www.kaggle.com/code/alexisbcook/missing-values/tutorial",,,,,57.01434241,65.13140345,62.71890226,70.00876015,69.79037926,,,,
65736,Why does Keras need TensorFlow as backend?,keras,"This makes more sense when understood in its historical context. These were the chronological events: 
 
 April 2009   Theano   0.1 is released . It would dominate the deep learning framework scene for many many years. 
 June 2015  Keras  is created  by  François Chollet . The goal was to create an abstraction layer to make Theano easier to use, enabling fast prototyping. 
 August 2015   Google hires François Chollet . 
 November 2015   Tensorflow  is released by Google, with much inspiration from Theano and its declarative computational graph paradigm. 
 December 2015  Keras  is refactored  to allow for pluggable backend engines, and now it offers backend implementations for Theano and Tensorflow. 
 
 Other backends were later supported by Keras (CNTK, MxNet), but they never got much traction. 
 Time passes by and the overlap between Tensorflow and Keras grows. Tensorflow ends up duplicating many of the functionalities in Keras (apart from the multiple APIs within Tensorflow that also had big overlaps). 
 
 September 2017  Theano  is discontinued . 
 November 2017   Keras is bundled with Tensorflow  as  tf.keras . From this point on there are 2 different Keras: the one bundled with Tensorflow and the one that supports multiple backend engines. Both are maintained by the same people and are kept in sync at API level. 
 
 At some point, the roadmap for Tensorflow 2.0 is defined, choosing to pursue an imperative model like  PyTorch . The person leading the Tensorflow API refactoring is François Chollet. This refactoring included a reorganization of the functionality to avoid duplications. 
 
 November 2018  some crucial functionalities of Tensorflow  are to be moved to   tf.keras ,  generating a heated debate 
 September 2019   Keras 2.3 is announced to be the last release of the multi-backend version of Keras 
 
 Now,  THE ANSWER  to your question: Tensorflow is the most used Keras backend because it is the only one with a relevant user base that is under active development and, furthermore, the only version of Keras that is actively developed and maintained is one with Tensorflow. 
 So, summing up: 
 
 At the beginning of Keras, the overlap with Tensorflow was small. Tensorflow was a bit difficult to use, and Keras simplified it a lot. 
 Later, Tensorflow incorporated many functionalities similar to Keras'. Keras became less necessary. 
 Then, apart from the multi-backend version, Keras was bundled with Tensorflow. Their separation line blurred over the years. 
 The multi-backend Keras version was discontinued. Now the only Keras is the one bundled with Tensorflow. 
 
 Update : the relationship between Keras and Tensorflow is best understood with an example: 
 The dependency between Keras and Tensorflow is internal to Keras, it is not exposed to the programmer working with Keras. For example, in the source code of Keras, there is an  implementation of a convolutional layer ; this implementation calls package  keras.backend  to  actually run the convolution computation ; depending on the Keras configuration file, this  backend  is set to use the Tensorflow backend implementation in  keras.backend.tensorflow_backend.py ; this Keras file just  invokes Tensorflow to compute the convolution 
 Update : new important events in the timeline: 
 
 August 2021 : Tensorflow 2.6.0  no longer has Keras as part of it . Keras has now its own PIP package ( keras ) and lives on its own  github repo . 
 
 November 2023 :  Keras 3.0  has again support for multiple backends, including Tensorflow, JAX and Pytorch (and also Numpy only for inference).","Keras is an application programming interface (API). It is a single interface that can support multi-backends, which means a programmer can write Keras code once and it can be executed in a variety of neural networks frameworks (e.g., TensorFlow, CNTK, or Theano). 
 TensorFlow 2.0 is the suggested backend starting with Keras 2.3.0 .","Lets go back to basics here. 
 It is not possible to only use Keras without using a backend, such as Tensorflow, because  Keras is only an extension for making it easier to read and write machine learning programs . All the actual calculations needed to create models are not implemented in Keras, which is why you need to use a backend library for anything to work. 
 When you are creating a model in Keras,  you are actually still creating a model using Tensorflow , Keras just makes it easier to code.","Additionally:  Think of it as an abstraction layer. 
 Keras gives nice and intuitive way to build and think about neural network, but you have to understand thats not how computer takes orders. Hiding this complexity behind Tensorflow allows us to think naturally about building a neural network and not all the details behind implementation. 
 (On a general note thats why python is so popular, cause it abstracts the complexity away, and allows you to think and write down solution more naturally and intuitively)","The first point to note is that Keras can potentially use many backends (e.g. Theano before it was discontinued, Microsoft Cognitive Toolkit, to name a couple). It just so happens that Keras has proven to be the most popular among the community. As a result, TensorFlow has adapted to the extent that Keras is now the default API in TensorFlow 2.0. 
 One of the biggest changes is the way libraries are now loaded using  tf.keras . 
 Consider this example. Let's say one wishes to run a Sequential model using Keras. To do so, one must import the relevant libraries. 
 In the first version of TensorFlow, it would be done as follows: 
 from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor
 
 The model is defined as such: 
 model = Sequential()
model.add(Dense(8, activation='relu', input_shape=(4,)))
model.add(Dense(1, activation='sigmoid'))
 
 Now, let's contrast this to the  TensorFlow 2.0  notation: 
 from tensorflow.keras import models
from tensorflow.keras import layers

model = models.Sequential()
model.add(layers.Dense(8, activation='relu', input_shape=(4,)))
model.add(layers.Dense(1, activation='sigmoid'))
 
 The Sequential function is now being defined using models, and layers is the only other library imported. Whereas in TensorFlow v1.0, Sequential, Dense, and KerasRegressor all had to be imported separately to generate the model. 
 Using the above example as a reference point, one can say that Keras now uses TensorFlow as a backend most frequently - simply because it has proven to be the most popular. As a result, TensorFlow has adapted to making the syntax for calling Keras more user-friendly, and thus Keras has become the default API in v2.0. 
 You might also find this article of use for further information on this topic:  https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/","Keras used to use 2 backends(Theano and Tensorflow), but now only supports Tensorflow because of the discontinuation of Theano. The reason why Keras uses Tensorflow as it's backend is because it is an abstraction layer. 
 It is the easiest way to get started with AI and machine learning because all of the core algorithms are implemented in tensorflow and keras allows you to just call the classes/functions without adding any additional code. Great starter library for beginners and AI enthusiasts who have little coding experience.","Imagine you have a basic maths framework, a lot of functions doing addition, subtraction, multiplication and division. 
 Imagine in everyday life you often need to compute averages. 
 Then you make a function (using the functions from the framework, inside it), that will take an array of numbers as parameters an return the mean. 
 The framework is actually doing the work, it's still a lot of additions and a division, but your API-like function is a way nicer way to do what you need. 
 Let's say you were using Numpy (an algebra framework on CPU) to do your stuff. Numba is it's equivalent but on GPU.
If in your code you had a lot of ""numpy.add(a, b)"" everywhere you needed an addition, you would need to change it everywhere to ""numba.add(a, b)"", so a lot of shitty work. But if instead you were using your homemade function ""add(a, b)"", then you just have to change the framework you use inside your function, easy peasy ! So yeah you understood correctly, it's better to update the API than the framework. To come back to this dumb example, Numpy is a ""CPU computing framework"", so it wouldn't make any sense to change it to use the GPU (Numba was created for it). But your custom function can easily be modified, as it's purpose is to ""do the job the best way for you"". So it's good practice to stick to using your ""API"" everywhere, even if it sometimes seems unnecessary. 
 Now picture Keras as this function, and Tensorflow as the algebra framework.
Sure, most of the time you can use directly the framework, but if you want your code to be cleaner, you'll use your API. 
 As of today Keras and Tensorflow are bundled together and Tensorflow interface is getting closer to it, but that was the idea. 
 If you can do the same model as easily with Keras than directly from Tensorflow, it might seems better to get rid of the ""useless"" middle-man (Keras), but beware! If one day Tensorflow implements a better way of doing it, Keras will use it, while using directly Tensorflow you'll need to update your code... 
 I over simplify everything, I know, but you seems to have a hard time to distinguish framework and API. 
 You can see that the API is dumb, meaning that the API on itself is using the algebra framework and would be useless without it. Or it would need to implement all those operation and become a fully-fledged framework instead of a simple API. The API needs the framework to work, as Keras needs TensorFlow.",,,77.50050402,68.86200965,69.25255429,53.65857795,66.44442889,70.54234543,60.82898393,,
65585,Decision tree with final decision being a linear regression,python,"I think the easiest way to do this would be to have a decision tree where the final decision results in a linear formula. 
 
 Setting aside whether this is actually easiest/best, this type of model does exist, usually called ""model-based recursive partitioning"".  See e.g.  https://stats.stackexchange.com/q/78563/232706 
There are several packages in R for this:  partyfit  (and the older simpler  party ),  mob ,  Cubist ; unfortunately, there don't seem to be any in Python yet.   Here's a discussion  on including it in sklearn, from mid-2018.","You are looking for  Linear Trees . 
 Linear Trees  differ from  Decision Trees  because they compute linear approximation (instead of constant ones) fitting simple Linear Models in the leaves. 
 For a project of mine, I developed  linear-tree :  a python library to build Model Trees with Linear Models at the leaves. 
 
 linear-tree is developed to be fully integrable with scikit-learn. 
 from sklearn.linear_model import *
from lineartree import LinearTreeRegressor, LinearTreeClassifier

# REGRESSION
regr = LinearTreeRegressor(base_estimator=LinearRegression())
regr.fit(X, y)

# CLASSIFICATION
clf = LinearTreeClassifier(base_estimator=RidgeClassifier())
clf.fit(X, y)
 
 LinearTreeRegressor  and  LinearTreeClassifier  are provided as scikit-learn  BaseEstimator . They are wrappers that build a decision tree on the data fitting a linear estimator from  sklearn.linear_model . All the models available in  sklearn.linear_model  can be used as linear estimators. 
 Compare Decision Tree with Linear Tree:","I would suggest using spline regression.  or some polynomial regression. 
 Why? what you are basically approximating is the (almost) step-wise function. See  here 
 More info in  here  and a great background introduction in  here .","Even after your update, I think Noah's hint to spline regression is the best way to approach the problem. Here is a brief example in R: 
 # Generate data
x <- -50:100
y <- 0.001*x^3
plot(x,y)
df = data.frame(y,x)

# Linear regression
reg_ols=lm(y~.,data=df)
pred_ols = predict(reg_ols, newdata=df)

# GAM with regression splines
library(gam)
reg_gam = gam(y~s(x,5), data=df)
pred_gam = predict(reg_gam, newdata=df)

# Plot prediction and actual data
require(ggplot2)
df2 = data.frame(x,y,pred_ols, pred_gam)
ggplot(df2, aes(x)) +                    
  geom_line(aes(y=y),size=1, colour=""red"") +  
  geom_line(aes(y=pred_ols),size=1, colour=""blue"") +
  geom_line(aes(y=pred_gam),size=1, colour=""black"", linetype = ""dashed"")  
 
 So I have some function which is the data generating process (red line in the plot) and I want to get a good fit on this function. OLS (linear regression) will not work well (blue line in plot) but a GAM with splines will fit very well (black dashed).  
 
 This model looks like  $y_i=\beta_0 + \beta_1 x_{1,i} + u_i$  (so 2D-like), but of course you can expand the model to  $y_i=\beta_0 + \beta_1 x_{1,i} + ... + \beta_k x_{k,i} + u_i$ , where  $k$  is the number of ""variables"" in the model (so kD-like).  
 Since you seem to be on Python, you would need to look for a Py implementation of GAMs, such as  Statsmodels  of  PyGAM .  
 Chapter 7 of ""Introduction to Statistical Learning"" covers splines regression. You may have a look at the  Python Lab  for  the book .","Create a  new column  based on your formula and train your decision tree.
Then, based on the outcome class of the tree, pass the data to different regression. 
 This would be a sort of  Stacking . 
 Please treat this just an engineering approach, it might not be a good solution based on your data and need.",,,,,65.82219829,66.16578992,54.99618293,54.04473135,60.0042503,,,,
65311,Could GANs be used to augment data?,deep-learning,"GAN's and traditional augmentation techniques are fundamentally different in a way: A GAN produces (and combines) patterns previously seen in a dataset, data augmentation adds patterns to the data.  
 Well thought out data augmentation tries to add variations that could exist in the data. For instance: In arial photography rotation around the z-axis is very trivial and it could be smart to add that generously, on the other hand of the spectrum: In computer generated cartoons for instance, you'd reasonably do not expect much gaussian noise, and you might want to use that sparsely. 
 GAN's don't add information, they mostly add the same localised patterns that your (I'm assuming) ConvNet to train also has. The benefit a GAN offers is that you can use unlabelled data to train your convolutional layers with concepts you'd might expect in the domain.  
 Other techniques that go in that same direction, and you might want to check out, are weakly supervised learning and auto-encoding (which is imho very close to your original GAN idea).","Standard data augmentation techniques. 
 First of all, by  standard data augmentation  I'll be referring to techniques like flipping an image (up/down, left/right), affine transformations (rotations, translations, crops, shears, etc.), adjusting the brightness/contrast of an image, adding noise to the image (salt&pepper, gaussian, etc.) and so on. 
 Before describing the pros/cons of standard vs GAN augmentation, we should note on  why data augmentation is effective . In short, deep neural networks have the capacity of memorizing smaller datasets leading them to overfit. They benefit from more images and a  higher variety  in the images. Data augmentation is a method of generating new images from the existing ones, that have the same semantic content as the originals. E.g. if I have a  cat  image and I flip it, it is still a  cat ; the network, however, thinks that this is a new image. These techniques are so effective, that they are even be used in large datasets, which don't have the aforementioned problems, to boost their performance even more.  
 What are the problems of standard data augmentation techniques? 
 The main issue is that the augmentation strategies we can use vary depending on the input images. For instance, the  mnist  dataset is one of the most popular datasets in machine learning, for recognizing handwritten digits. In this case, we  can't  flip the images or rotate them too much. Another case is medical images which adhere to strict formats. For example MRIs are centered, aligned, laterally/horizontally asymmetric and somewhat normalized regarding brightness and contrast. This severely limits what augmentations we can accomplish. This makes their application ad-hoc in most cases. 
 Cons of using standard data augmentation techniques 
 
 Might  damage  the semantic content of the image (e.g. rotating too much might cause a ""6"" to turn into a ""9"", or translating too much might cause the object of interest to fall out of the image). 
 Augmentation schemes are  dependent  on the problem . 
 Empirical/Ad-hoc  application. 
 Naive method: looks at one image at a time,  can't  gather information from the  whole  dataset. 
 
 These techniques might motivate us to use a more advanced data augmentation technique, i.e. generate synthetic images with GANs. In fact, GAN-augmentation, if done properly will  solve  all of these problems. 
 However, they too have their drawbacks. 
 Cons of using GANs for data augmentation. 
 
 They require  training . Training a GAN can take a lot of time and it isn't the easiest thing to do. 
 They  can't  be applied  on-line . Instead after training, you need to generate a pool of synthetic images and add them to your original dataset.  
 
 One final remark 
 Even though I was comparing one technique vs the other, I'd like to point out that  using one technique does not exclude the other . In fact we found that by combining both standard and GAN-based augmentation helps even more than each one individually. 
 If you're interested more you can read  this study  we did which focuses on the use of GANs for data augmentation in medical images.","While all the answers here are on point, I'd like to add a new perspective to this. 
 You can view common data augmentation techniques as rounding out your data's distribution curve. That is to say - if some feature of your data set should look like a gaussian curve, but does not just yet, data augmentation techniques would help increase the data samples and the variance to shape the curve towards its ideal, naturally occurring, or statistically expected form. So your starting point is more data samples, and you end at a better-looking more-desirable (based on your application) distribution curve. 
 Generative Adversarial Networks, on the other hand, start at the distribution curve that you already have and generate data samples that are in line with (or agree to) the already established distribution curve. Similar to a random number generator with a predefined distribution to conform by - uniform, gaussian, Raleigh etc. 
 So here is a  general guideline  (not always applicable) that you could use to frame this discussion.
Use data augmentation techniques when you do not see the expected statistical properties within your data. This could be due to a lack of variance and/or a shortage of data. Traditional data augmentation techniques might be what you need. 
 Once you have the ideal (or closest to ideal) distribution within your data, OR you know what the ideal distribution of your data should be, use GANs to synthetically generate more data for semi-supervised learning, further training and improving model generalization and robustness. 
 For more information on data augmentation, check out our blog  here .","It depend how good your GAN is. 
 if generator is sufficiently good trained and it produces reeally good false positive examples than that can be a good augmentation technique. 
 Check  this  out, and  this  one.","Standard data augmentation techniques add noise to the data. 
 For example in images - 1) Adding gaussian noise. 2) Taking the negative of the samples. 3) Transitioning the image vertically/horizontally 
 might help augment the data but may not represent the  real data distribution  you are trying to fit your model to.  
 A generative model tries to capture/fit the real data distribution rather than the samples which are picked from it. So that the discriminator gets fooled by it. Hence it is said to be generative ADVERSARIAL. 
 That is the thing about GANs, getting a 50/50 prob of being real on each of the generated samples and real samples tells that the discriminator cannot distinguish between them and hence the generated data distribution has completely replicated the real data distribution(ideally). That indeed is a necessary (if not exhaustive) condition required for a generator to augment data. 
 Hence a completely trained generative model(although very tough to train via GANs) is better to augment your data.",,,,,70.62855419,69.52127869,62.35625962,61.11226269,70.95770917,,,,
65241,Why is the decoder not a part of BERT architecture?,nlp,"The need for an encoder depends on what your predictions are conditioned on, e.g.: 
 
 In causal (traditional) language models (LMs), each token is predicted conditioning on the previous tokens. Given that the previous tokens are received by the decoder itself, you don't need an encoder. 
 In Neural Machine Translation (NMT) models, each token of the translation is predicted conditioning on the previous tokens and the source sentence. The previous tokens are received by the decoder, but the source sentence is processed by a dedicated encoder. Note that this is not necessarily this way, as there are some decoder-only NMT architectures, like  this one . 
 In masked LMs, like BERT, each masked token prediction is conditioned on the rest of the tokens in the sentence. These are received in the encoder, therefore you don't need an decoder. This, again, is not a strict requirement, as there are other masked LM architectures, like  MASS  that are encoder-decoder. 
 
 In order to make predictions, BERT needs some tokens to be masked (i.e. replaced with a special  [MASK]  token. The output is generated non-autoregressively (every token at the output is computed at the same time, without any self-attention mask), conditioning on the non-masked tokens, which are present in the same input sequence as the masked tokens.","BERT is a pretraining model to do the downstream tasks such as question answering, NLI and other language tasks. So it just needs to encode the language representations so that it could be used for other tasks.That's why it consists only of encoder  parts. You can add the decoder while doing your specific task and this decoder could be anything based on your task.","In short, Bidirectional  Encoder  Representations from Transformers (BERT) is not designed for decorder-related tasks. 
 
 I can't see how BERT makes predictions without using a decoder unit, which was a part of all models before it including transformers and standard RNNs. How are output predictions made in the BERT architecture without using a decoder? How does it do away with decoders completely? 
 
 If I undertand correctly, you are asking how BERT predicts its output. If you read the paper, BERT uses a technique called masked language modeling (MLM). During training, BERT randomly masks some of the tokens in a sentence and then tries to predict what the original word was. For example, in the sentence ""I want to buy a [MASK]"", BERT might be trained to predict that the masked word is ""car"" based on the context of the other words in the sentence. 
 The MLM task only requires an encoder because it involves encoding the input text into a fixed-length vector representation that can be used for downstream tasks such as sentiment analysis, question-answering, and language generation. 
 BERT also uses another technique called next sentence prediction. During training, BERT is given pairs of sentences and is asked to predict whether the second sentence follows logically from the first. This allows BERT to capture the relationships between sentences and to understand the broader context of a given text. 
 During inference, BERT uses its pre-trained weights to encode the input sentence and then passes the encoded representation through one or more neural network layers to generate the output. The exact details of how BERT generates its output depend on the specific task it is being used for, but in general, it uses a combination of the encoded input, attention mechanisms, and learned weights to produce the final output. 
 
 To put the question another way: what decoder can I use, along with BERT, to generate output text? If BERT only encodes, what library/tool can I use to decode from the embeddings? 
 
 I see you have edited your questions. In this newly added question, I can see that you have found that BERT only provides you with embeddings. You need other architectures like decorders along with these embeddings for your downstream tasks. 
 Here are some examples from the smiley face  https://huggingface.co/learn/nlp-course/chapter1/6","First need to understand what problems BERT can solve or what kind of inference/prediction it can achieve. 
 
 BERT Neural Network - EXPLAINED! 
 
 
 Encoder in Transformer itself can learn: 
 
 Relations among words (what word is most probable in a context). For instance, what word will fit in the  BLANK  in the context  I take [BLANK] of the opportunity . 
 
 Relations among sentences. For instance A: ""In a glossary store"" can follow B: ""I bought the ingredients"". 
 
 
 Having these traits or capabilities, BERT can predict a word to follow a sequence of words. BERT can classify if a text is negative or positive. As far as you can achieve the predictions you want with the Encoder part only, you do not need the Decoder. 
 Hence it would be better focus on what problems require Decoder. Or what problems BERT cannot solve.","I only understood tranformer decoders and encoders after I watched this video. Check it out: 
 https://www.youtube.com/watch?v=0_4KEb08xrE","BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based language model that is trained to predict the next word in a sequence given the context of the previous words. It does this by using a bidirectional encoder to process the input text and generate a fixed-length representation of the input. The decoder is not a part of the BERT architecture because it is not designed to generate text as output. Instead, it is used to encode the input text into a fixed-length representation that can be fed into a downstream task such as question answering or language translation. 
 In a typical language model, the decoder is responsible for generating text as output based on the context provided by the encoder. However, in the case of BERT, the encoder is trained to generate a fixed-length representation of the input text, and this representation is used as input to a downstream task that is responsible for generating text as output. The BERT model itself does not have a decoder component and is not designed to generate text directly.",BERT is a stack of deep bidirectional transformer encoders that read the input sequence and generate meaning representations called embeddings. It uses multi-head attention to decide the meaning.,,,57.78165648,61.10026963,67.91115306,64.27795429,57.66126034,62.33096126,54.57326286,,
65089,Real purpose of pooling,cnn,"Both your original intuitions and the other answers contain important and valid points: 
 
 Reduce the feature maps size, hence reducing the overall computational needs. 
 Give flexibility by filtering the important features from the unimportant ones, increasing the receptive field and reducing the risk of overfitting. 
 
 However, as you pointed out, the same effects could be achieved by other means. 
 The differential point other answers miss is the key aspect:  pooling has those benefits while having zero trainable parameters and being fast to compute .","So the only real reason for using pooling is to reduce to the size of the feature representation thus leading to smaller memory and computational footprint as the networks deeper.  yes, cost efficiency but also: 
 Generalisation : We get rid off small--unimportant details when we combine several values into one representative one. Hence what you really get is  reduced chance of overfitting","Both the common answer and your analogy are wrong. Pooling layers are added after convolution and ReLu layers.
These layers gets the feature maps of image that where are the important features are. The problem with these features is the get this specific place of feature in feature map. A minor change in image will reflect a total change in feature maps. to solve this problem you can downsample the feature maps which is done by Pooling layers. Try reading about pooling and its types like max pooling and average pooling. It reduce the feature importance in a feature map by downsampling.","I think these common answers are quite right but abstract. In my opinion, pooling ""select the most important feature"" or ""increase the receptive field"" by droping nearby information, such as Maxpooling. This may makes sense because in some case like image classification we just need some important feature to classify, hence we could drop redundant local feature. Or by AveragePooling, we can make the feature maps more stable to local change. 
 The last FC layer did select the most important feature to use, however, if every layer's output is more useful for the final task, you can get a more accruate result, we want the former network to learn reliable features, FC layer just map them. 
 As for your second idea, the receptive field could be increased by increasing the kernel size. But if you use larger kernel size, you lose local features. That's why dialated convolution is proposed, it can increase receptive field without losing information as pooling. 
 By the way, many researcher don't use pooling or just use them at the end of the network recent year, as Hinton said: 
 
 The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.","Pooling layers are used to reduce the dimensions of the feature maps.
Thus, it reduces the number of parameters to learn and the amount of
computation performed in the network. 
 The pooling layer summarises the features present in a region of the
feature map generated by a convolution layer. So, further operations
are performed on summarised features instead of precisely positioned
features generated by the convolution layer. This makes the model
more robust to variations in the position of the features in the
input image. 
 
 So somewhat it can be agreed with your conclusion, but without statistical evidence, we should not confirm that proposed hypothesis about memory optimization.",,,,,54.09385638,57.32267206,62.18003394,58.37714088,56.16742737,,,,
64764,Monitoring machine learning models in production,machine-learning,"The changes in distribution with respect to training time are sometimes referred to as  concept drift . 
 It seems to me that the amount of information available online about concept drift is not very large. You may start with its  wikipedia page  or some blog posts, like  this  and  this . 
 In terms of research, you may want to take a look at the  scientific production of João Gama , or at chapter 3 of  his book . 
 Regarding software packages, a quick search reveals a couple of python libraries on github, like  tornado  and  concept-drift . 
 Update : recently I came across  ML-drifter , a python library that seems to match the requirements of the question for scikit-learn models.","While reading this  Nature paper:Explainable AI for Trees: From Local Explanations to Global Understanding . The section 2.7.4  ""Local model monitoring reveals previously invisible problems with deployed machine learning models"", says the following: 
 
 Deploying machine learning models in practice is challenging
because of the potential for input features to change after deployment. It is hard to detect when such changes occur,
so many bugs in machine learning pipelines go undetected, even in core software at top tech companies [78]. We
demonstrate that local model monitoring helps debug model deployments by decomposing the loss among the model’s
input features and so identifying problematic features (if any) directly. This is a significant improvement over simply
speculating about the cause of global model performance fluctuations 
 
 Then they do 3 experiments with the Shapley values provided by the TreeExplainer 
 
 We intentionally swapped the labels of operating rooms 6 and 13 two-thirds
of the way through the dataset to mimic a typical feature pipeline bug. The overall loss of the model’s predictions
gives no indication that a problem has occurred (Figure 5A), whereas the SHAP monitoring plot for room 6 feature clearly shows when the labeling error begins 
 
 Figure 5C shows a spike in error for the general
anesthesia feature shortly after the deployment window begins. This spike corresponds to a subset of procedures
affected by a previously undiscovered temporary electronic medical record configuration problem (Methods 17). 
 
 Figure 5D shows an example of feature drift over time, not of a processing error. During the training period and early
in deployment, using the ‘atrial fibrillation’ feature lowers the loss; however, the feature becomes gradually less useful
over time and ends up hurting the model. We found this drift was caused by significant changes in atrial fibrillation
ablation procedure duration, driven by technology and staffing changes 
 
 
 
 Current deployment practice is to monitor the overall loss of a model over time, and potentially statistics of input features. TreeExplainer enables us to instead directly allocate a model’s loss among individual features","What you're describing is known as concept drift and there are quite a few software startups bringing a solution to market (us included - happy to show you what we have).  
 
 A very simplistic way of detecting drift is monitoring the differences between distributions of the predicted dataset and the training dataset using a Kolmogorov-Smirnov test or Wasserstein distance.  
 For radical changes in distribution, what you might do is create a model to understand the datasets unique patterns and have an outlier detector to determine true radical changes to the distribution as opposed to also identifying false positives.  
 This is an interesting use case - are you able to share an example?","You can have a look at Anodot's MLWatcher. Few of the highlights of this tool are as follows. 
 
 MLWatcher collects and monitors metrics from machine learning models in production. 
 This open source Python agent is free to use, simply connect to your BI service to visualize the results. 
 To detect anomalies in these metrics, either set rule-based alerting, or sync to an ML anomaly detection solution, such as Anodot, to execute at scale. 
 Distribution of input features. 
 
 You can have a look at their complete features  here .","There is all kinds of solutions right now. Mainly you can divide it into two: 
 
 Monitoring features as part of a bigger AI platform 
 A dedicated monitoring solution 
 
 Few factors to examine before choosing between the two options: 
 
 What is your scale of your usage in ML models? 
 What's the impact of your models? are they part of your core business or is it only enrichment \ niche of your business? 
 What is your DS team size? 
 How many platforms do you use to deploy models to production? Do you have only one standard way to deploy? 
 
 The general theme is, the bigger the ml operation is, and the more you need it to be agnostic to the deployed platform, go for a dedicated solution.
If your ml operations are still very limited and your serving platform already has few monitoring features in place, so it might good enough for you for now. 
 When examining a specific solution, consider the following points: 
 Integration  - How complicated is it? 
 Measurement  - Does it offer both data (input \ inference \ label) stability measurement? 
 Performance analysis  - Does it provide you the ability to close the loop and see performance analytics (BTW... in most cases, even if you can get performance metrics, you probably won't be able to base your monitoring on top of it, cause in reality such performance information usually available only with delay time after the predictions were made). 
 Resolution  - Can the system detect and measure such metrics on a higher resolution? (sub-segments of your entire datasets)? In many cases, drift or technical issues will occur only for a specific subset of your data. 
 Alerts  - Does the solution include also a statistical alert mechanism? Eventually, it's hard to track all the KPIs mentioned above, and every dataset behaves differently, so thresholds are hard to define. 
 Dashboard  - Does the solution contain a clear UI dashboard? 
 API  - Can you consume such production insights directly from API? It can be very beneficial to build automation on top of it. 
 BTW... Here is a  blog post , I wrote, talking about the different elements that should be converted when monitoring ml and reviewing current solutions","There are few good startups and open sources that offer solutions for ML monitoring (I actually work at a startup in the field). 
 You can find here a few  comparison tools  to compare between some of them according to different features. I recommend the  airtable  by Ori on the top of the list, and  mlops.toys  (This is an open-source created by some of my colleagues so maybe I'm biased, but I love it). 
 The MLCompendium is, in general, a good source for information in many subjects in the ml field. 
 I really can't recommend the best tool for you because  it depends on your exact needs : 
 
 Do you look for monitoring on the way as part of a full pipeline tool, or some super-advanced tool specifically for monitoring to expand your existing pipeline? 
 Do you work with Tabular data? NLP? Vision? 
 What is the frequency of your predictions? 
 Do you need to monitor all your data or just a segment of it? 
 etc... 
 
 In addition, this short blog post a colleague of mine wrote on  Concept Drift Detection Methods  may help you as well. You can find many more articles on the subject in the link to the MLCompendium I attached above.","If I understand your query correctly,you are looking for MLFLOW where you can track your experimentation and vizualize them using APIs  MLFLOW","ML monitoring is offered by some new companies, although it is currently a super-niche field.
 One of the OSS tools that can help you in ML monitoring is Evidently for sure. Before finalizing, I also recommend reading  OSS vs Commercial Tools For Model Monitoring? A Complete Guide To Help You  blog by Censius.
 Machine and deep learning compendium  and  MLOps Radar  are great resources to start with MLOps. I also love Datarevenue’s  ML toolkit  for tool selection decisions.",,53.99488271,63.13781037,53.74962841,66.10223376,62.02542594,59.28622525,50,62.14019001,
64302,When should I NOT scale features,feature-scaling,"Scaling often assumes you know the  min/max  or  mean/standard deviation , so directly scaling features where these information is not really known, can be a bad idea. 
 For example,  clipped signals  may hide this info, so scaling them can have a negative result because you may distort its true values. 
 Below is an image of 1) a signal that can be scaled, and 2) a  clipped signal  that scaling should not be done.","The example that comes to mind is images ; I’ve never heard of scaling pixel intensities before processing with CNN. Presumably it’s useful to maintain mean differences between the features — eg it could be signal that the top right corner is usually less red , etc .","If the features are correlated, don't scale them. You can damage your data applying scaling to each feature separately. It depends on your data, problem and operator you'll be aplying.","An immediate example is  standard scaling  or  whitening data  before a  PCA . By normalizing each variance, these scalings erase the relative magnitude of the eigenvalues of the covariance matrix. Hence it defeats the purpose of a PCA.","The majority of features, especially in physical sciences, have names, definitions, values and units (s, m, kg, etc.), not only names and values. Knowing this, it is easy to manually or even automatically create new features basing on the units. It makes no sense to add meters to seconds, but (x1^2+x2^2+x3^2)^0.5,  where x1, x2, x3 are the space coordinates of the same unit is potentially a very valuable feature (distance). Scaling  before  the ~creative feature engineering stage successfully destroys these (hidden for many) dataset properties and decreases the chance to find new valuable features.","In a regression problem and based on algorithm of your choice (such as multiple linear regression, or symbolic regression) you don't need to scale your data. As I examined in several problems, scaling hurts the model fit when data is scaled. However for SVM ans ANN you may need to scale your data 
 There is another case that one needs to select  right scaling method , which is a dataset with both categorical and numerical variables. 
 If one uses min/max method, model may be confused to determine that  1  is for numerical or categorical feature (discrete/continuous); especially if one wants to do  clustering !
So the right method may be  standardizing  (I am working on such a problem now)",,,,70.28427613,55.81192101,76.82123685,58.1882889,60.50591949,59.89818809,,,
63325,Cosine similarity vs The Levenshtein distance,similarity,"As mentioned in other answers, traditionally cosine is used to measure similarity between vectors whereas Levenshtein is used as a string similarity measure, i.e. measuring the distance between sequences of characters. 
 Nevertheless they both can be used in non-traditional settings and are indeed comparable: 
 
 the vectors compared with cosine can for instance contain frequencies of characters or characters n-grams, hence making it a string similarity measure 
 one can replace the sequence of characters with a sequence of strings or a sequence of n-grams, thus making Levenshtein a more general distance measure. 
 
 The main conceptual difference between Cosine and Levenshtein is that the former assumes a ""bag-of-words"" vector representation, i.e. compares unordered sets, whereas the latter takes into account the order of the elements in the sequences. 
 In the context of comparing sequences of words many combinations are possible. In case that's what you're looking for you might be interested in this paper:  https://www.aclweb.org/anthology/C08-1075/  (full disclosure: I'm one of the authors).","I think the answers you got are technically correct, but don't address the big picture. 
 In the data science world, cosine similarity is mainly used for documents which have been encoded by an embedding. Documents could be anything from a single sentence or a tweet, to a paper with dozens of pages of text. 
 Embeddings include things like doc2vec, BERT, and similar, and they try to reflect some level of semantic knowledge into their encoding. That is, words that tend to have similar meanings will end up close together in the high-dimensional embedding space. And documents with similar contexts will also end up close together in this space. 
 So, to answer your question: 
 Levenshtein distance  has no knowledge of semantics, it's simply an edit distance and nothing more. And in general you only use it if you have no other choice. For example, compare: 
 3000 N Main Street
3000 N Maan Street
3001 N Main Street
9000 N Main Street
3000 S Main Street
 
 All of the lines after the first are an edit distance of 1 from the first line. That is, one character is changed. As you can see, which character is changed -- in this context -- can make a HUGE difference so using Levenshtein on the entire street address is useless. 
 So if ""similar"" means ""would type nearly the same text"" it might be useful, though it's hard to apply well and if you're looking for things like ""fat finger"" mistakes (typos) you might want to use a different edit distance that accounts for swaps of two adjacent characters. 
 Cosine similarity  (where ""similarity"" is the inverse of ""distance"") is in general used on embeddings. The Bag of Words approach that the accepted answer uses for pedagogical purposes is clever but I've never seen or heard of it before now. It does allow you to use Cosine distance as an approximation of editing distance but it's really not used that way in practice. 
 In practice, you're using Cosine distance on an embedding that encodes semantic information: you're looking for words or documents (collections of words) that are using similar words in similar contexts. This would not be useful for the street address example, above, but you could imagine it being very useful on news articles or scientific papers, etc. 
 You could use Euclidean distance in the embedding space -- comparing the vector for each document directly -- but there can be issues with magnitude. And cosine similarity measures only the relative directions of the documents, not their magnitude, which is in general more useful and more what you expect when you want to compare two documents in terms of their ""topic"" or ""meaning"", etc. 
 So if ""similar"" means ""talking about something similar or in a similar way"" than you'll probably end up using a Cosine similarity measure with an embedding. 
 The accepted answer is technically creating an embedding, but in general I think the term ""embedding"" in data science is referring to something like doc2vec, BERT, GLoVE, etc, which reflect co-occurrences and other factors from which a semantic-like quality emerges.",The first one is for computing the similarity between objects considering their representations as vectors. The second one is for computing the similarity between sequences of characters.,"Cosine similarity uses vectors and can calculate similarity for sets and multisets (=bags). If used for similarity of sequences (of characters, words, sentences, lines, ...) the comparison is unordered and each kind of element is a feature = dimension in the vector space. Thus the letters of the word 'banana' are transformed into a set [a, b, n] or a bag {a: 3, b: 1, n: 2}, where the set can be thought as bag {a: 1, b: 1, n: 1} and the same calculation can be used. Each character is treated as a dimension of the vectors. Thus with supporting Unicode the vectorspace can have potentially 0x10FFFF ~ 1.1 million dimensions, but for comparison of two strings you need only a subset of size <= len1 + len2. That's implemented as sparse vector. To bring some sequential order into cosine similarity applied to sequences, we can use 2-grams or 3-grams. This can be very efficient for searching similar words in large dictionaries as candidates for spelling correction, e.g. limit the search to minimal similarity 0.7, or get the top 20 similar words. 
 Out of the candidates you can use the slower, but more precise Levenshtein or LCS.","To answer directly to your question, I would say that one could use Cosine similarity when dealing with vectors (for instance the distance between (1,2,3) and (4,5,6)) and one could use the Levenshtein distance when dealing with strings (""distance"" between ""aaaaa"" and ""aaaba"").  
 Concretely they don't really apply to the same context and are not used for the same applications. If you want to test if two different piece of texts are quite similar, it could be reasonable to use the Levenshtein distance. If you want to know if two vectors are quite similar to each other in a 3 dimensional space, it might be a good idea to use the cosine similarity.",,,,,64.09969584,67.69004608,53.54365328,63.71533112,75.99274351,,,,
63101,Collaborating on Jupyter Notebooks,jupyter,"CoCalc  provides Jupyter notebooks with  realtime collaboration , unlike  Colab ,  Kaggle , etc.  You just make a project drag and drop  ipynb  and data files, add collaborators, and everybody can edit everything simultaneously.  You can also share content publicly at  the share server .   I think  CoCalc  is currently the overall most mature of the realtime Jupyter collaboration platforms (and it is the only open source one  4 ), but  Deepnote  is another option that is more focused on data science (but is closed source). 
 Note , while  CoCalc  has very flexible payment plans in order to use the real-time collaboration (RTC) functionality via internet,  Deepnote  is for free for students, e.g. via the  Github Student Developer Pack .","There are several collaboration platforms with hosted notebooks that can be shared like: 
 
 Google Colab 
 Kaggle Kernels 
 Deepnote 
 Binder 
 Curvenote 
 Noteable 
 Etc. 
 
 However the base idea of collaborating and sharing notebooks is actually a base function of jupyter. As you might have noticed it is a server-hosted application which by default opens a local server for you to work on. 
 By  simply hosting that server  (e.g. on AWS, your internal servers, etc.) you can collaborate on the notebooks directly and interactively.","Using a notebook, you can always convert it to a python script if you just go to ""File > Download as > Python (.py)"". Then, you can share it with your teammates and have handwritten comments on a printed form of it, regardless of how unusual this practice sounds.","In GitHub that would mean commenting on HTML or JSON level (internal markup for .ipynb files), not on the document level. 
 
 This is the crux of the problem. I built  ReviewNB  specifically to peer review Jupyter Notebooks on GitHub. It integrates directly with your repositories on GitHub and provides visual diff and commenting support (see screenshot below). 
 For straight up multi-user collaboration you can also setup  JupyterHub  so everyone can login to the same server, although I'd recommend using GitHub and installing Jupyter locally.","Here is the latest work on JupyterLab.
Shared editing with collaborative notebooks in JupyterLab 
 To try it → install the alpha release: 3.1.0a7 & set the flag  --collaborative 
 https://github.com/jupyterlab/jupyterlab/pull/10118","I've seen a tool  https://github.com/jupytercalpoly/jupyterlab-comments 
in  https://youtu.be/L36omEBu2HU?t=1837  which aims at providing real-time collaboration comments for Jupyter notebooks.","Try  JetBrains Datalore . It's free up to 120 hours of computation per month on a small CPU and up to three collaborators (editors), and unlimited viewers. 
 Top features (all included in the free plan): 
 
 Python auto-complete (like in PyCharm) 
 Secrets management, ability to access your AWS S3 storage 
 Slider widgets, interactive plots 
 Online collaborative editing 
 History checkpoints and the ability to track changes in files 
 The ability to publish static copies of notebooks and share them by a link 
 Cells from Datalore can be embedded in Medium posts and other publishing/writing platforms that support embedding","You could take a look at  Curvenote  it is different from many of the tools listed above as it focusses on the collaboration and communication part of research work and integrates with Juptyer. It doesn't ask you to change your computational environment. 
 
 It's true that Jupyter is based around creating notebooks that can stand alone (in terms of content+code+results) and be shared with more context than just the code. But it can still be difficult to collaborate around or to communicate with, especially when that involves communicating to different audiences e.g non coding stakeholders and collaborators. 
 Curvenote gives you version control, real time commenting, sharing and writing tools that make it much easier to present and share results to others and get feedback on them. 
 Also it allows you to keep working in Jupyter like before, whether that's locally, on an organisation's JupyerHub or one of the hosted Jupyter environments like Sagemaker, Coiled, Pangeo, etc...","From JupyterLab 3.1, file documents and notebooks have collaborative editing using the Yjs shared editing framework. Editors are not collaborative by default; to activate it, start JupyterLab with the --collaborative flag. 
 
 So what you need to do is to just install Jupyer Lab in your environment and then call it in this way: 
 jupyter lab --collaborative
 
 Source:  https://jupyterlab.readthedocs.io/en/stable/user/rtc.html",69.19687713,69.0693234,54.93263263,65.9724133,65.24063654,82.55164646,59.4347068,67.75283538,72.11989264
63060,Is it a best practice to exclude retweets from the data set?,machine-learning,"No. I do not believe so and I can explain a few reasons why. 
 
 If an entity wants to create waves in twitter with false tweets retweets are probably apart of the plan. 
 If you want to detect tweets generated by bots looking at the statistical data on said tweets and retweets like time stamps could be relevant to detecting if the tweet is generated by a bot. 
 If You have a way of checking retweets by bots then removing all retweets would also remove that data.  
 
 You should remove retweets if.  
 
 The project is focused on analysis of text to determine if a tweet is bot or not. 
 There is no labeled human or bot retweet data.","There might be a chance that the retweet has an entirely different context compared to the original tweet.
It is also possible that some retweets with different opinion/comment gain more popularity than the original one. 
 In these cases I don't think you can classify them as fake tweets. 
 You can classify tweets as fake when they are widely retweeted but with no context,
One such example is retweets due to a giveaway or charity. 
 If you can figure out how to separate the spam retweets and original tweets it would help for better analysis and accurate results.","Removing retweets would make sense to me, if you want a model where each single original post has the same weight.
However, it doesn't make sense to me if you want to measure each original tweets impact, except if you add one of the below features: 
 
 amount_of_total_people_reached  (measured in e.g. total impressions from the original tweet and all retweets) 
 amount_of_people_reached_via_retweets  (measured in e.g. total impressions from retweets) 
 amount_of_retweets 
 
 The exact purpose of the model and how you want it to fit to the real wolrd matters here I suppose.","To me it depends on what you want to focus on : do you want to create a model dealing with original posts that are fake news, and then make an algorithm finding the original from a retweet then applying your model ? Or do you just want a model that takes one tweet, not looking if it's a retweet or not, and trying to guess if it's fake or not. 
 In the first case, you should remove them, because you'll have many information about the people retweeting fake news, while you only want to find info about origin posters, which will make your model biaised.
In the second case, of course, since that's exactly what your model aims to do, you should keep them.","Removing retweets from your data set could potentially help improve the model's performance by reducing noise and focusing on the original tweets, but it could also result in the loss of valuable information that could be useful for identifying fake tweets. 
 One approach you could consider is to keep the retweets in your data set, but to also include additional features in your model that capture information about the retweet, such as the number of times the tweet has been retweeted, the user who initially tweeted the message, and any other relevant metadata. This could allow your model to use the information from the retweets in a more structured and controlled manner, potentially improving its overall performance. 
 Experiment with different approaches and compare their performance to determine the best approach for your specific use case.",,,,,65.74178945,63.34083362,62.26374922,58.44073636,69.38557009,,,,
62867,Detect geolocation match a GeoJson pattern,time-series,"No. 
 Clearly this is  not  a clustering problem, nor a dynamic time warping problem. 
 Instead, what you are looking for is nearest edge  search .","Depending on how many data points your pattern has and how often you have to do it the easiest way might be a brute force approach and just calculate the distance of all points and check if the minimum is in reach. If your input data is too sparse, you could also create first interpolate points to have a decent density of points.","The best method would be to calculate the  Haversine distance  between the new point and the GeoJSON object ( Point, LineString, Polygon, MultiPoint, MultiLineString, and MultiPolygon ). Haversine distance is the great-circle distance over the surface of a sphere between two points.","Assume that you have enough GPS data to form a pattern. 
 
 Get the road pattern by using regression analysis 
 
 Check whether the new point fits into the pattern. 
 
 Get the nearest road GPS & calculate the distance between two locations.","This is not an unsupervised learning problem, so k-means or other clustering techniques will not be helpful. In the world of statistical learning techniques, I would rather recommend a  semi-supervised  approach for  one-class classification problems  (PU learning), because you have patterns that can be learned as the primary class (""in the pattern""), and data to classify into ""in"" or ""out of"" the pattern. 
 I don't know so much about such techniques, but anyway it feels like they may be too complicated for your case, and would involve somehow computing distances between the new point to classify and the patterns. So if it is not too computationally expensive, you should probably simply calculate the minimum distance to the pattern and see if it is within the expected range.",,,,,50,53.09596913,50,58.6007091,59.85886563,,,,
62658,How to get sentence embedding using BERT?,tensorflow,"Which vector represents the sentence embedding here? Is it  hidden_reps  or  cls_head ? 
 
 If we look in the  forward()  method  of the BERT model, we see the following lines explaining the return types: 
 outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here
return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)
 
 So the first element of the tuple is the ""sentence output"" - each token in the input is embedded in this tensor.  In your example, you have 1 input sequence, which was 15 tokens long, and each token was embedding into a 768-dimensional space. 
 The second element of the tuple is the ""pooled output"".  You'll notice that the ""sequence"" dimension has been squashed, so this represents a pooled embedding of the input sequence. 
 So they both represent the sentence embedding.  You can think of  hidden_reps  as a ""verbose"" representation, where each token has been embedded.  You can think of  cls_head  as a condensed representation, where the entire sequence has been pooled. 
 
 Is there any other way to get sentence embedding from BERT in order to perform similarity check with other sentences?  
 
 Using the  transformers  library is the easiest way I know of to get sentence embeddings from BERT. 
 There are, however, many ways to measure similarity between embedded sentences.  The simplest approach would be to measure the Euclidean distance between the pooled embeddings ( cls_head ) for each sentence.","In your example, the hidden state corresponding to the first token ( [CLS] ) in  hidden_reps  can be used as a sentence embedding. 
 By contrast, the pooled output (mistakenly referred to as  hidden states of each [cls]  in your code) proved a bad proxy for a sentence embedding in my experiments.",There is very cool tool called  bert-as-service  which does the job for you. It maps a sentence to a fixed length word embeddings based on the pre trained model you use. It also allows a lot of parameter tweaking which is covered extensively in the documentation.,"For anyone coming to this question from Google, I'll share my experience with building sentence embeddings.
With a standard Bert Model you have three options: 
 
 CLS: You take the first vector of the  hidden_state , which is the token embedding of the classification  [CLS]  token 
 Mean pooling: Take the average value across each dimension in the 512  hidden_state  embeddings, making sure to  exclude   [PAD]  embeddings 
 Max pooling: Take the max value across each dimension in the 512  hidden_state  embeddings, again  exclude   [PAD] 
 
 If you're using the standard BERT, mean pooling or CLS are your best bets, both have worked for me in the past. 
 However, there are BERT models that have been fine-tuned specifically for creating sentence embeddings. They're called  sentence transformers  and one of the easiest ways to use one of these is via the  sentence-transformers  library. 
 Generally these models use the  mean pooling  approach, but have been fine-tuned to produce good sentence embeddings, and they far outperform anything a standard Bert Model could do. 
 If you wanted to fine-tune your own BERT/other transformer, most of the current state-of-the-art models are fine-tuned using  Multiple Negatives Ranking loss  (ps I wrote that article). For this the model learns to distinguish between similar sentence pairs, and after a pretty short training session (just over an hour for me on RTX 3090) you can produce a good quality sentence transformer model. 
 That being said, there are already many great pretrained models out there, there's a list of some of the  better models here , although it isn't fully up to date - for example,  flax-sentence-embeddings/all_datasets_v3_mpnet-base  performs better on benchmarks than any of those listed.","In order to avoid confusion by anyone reading this page: 
 pooled_output in the BERT model is NOT a pooling operation applied on the hidden states of all tokens in the sequence. 
 The pooled_output is generated by applying an additional dense layer on top of  the [CLS] token hidden state. 
 This pooled_output is the basis on which classification tasks are done in the original BERT paper. 
 From the TF implementation  https://github.com/google-research/bert/blob/master/modeling.py  line 227: 
         first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)
        self.pooled_output = tf.layers.dense(
            first_token_tensor,
            config.hidden_size,
            activation=tf.tanh,
            kernel_initializer=create_initializer(config.initializer_range))
 
 So to answer the main question:
As other already responded both the [CLS] token and the pooled_output are a fixed-length vector of the whole sentence so you can use both to embed and compare sentences. The difference between them is that the pooled_output is a result of a dense vector applied on top of the [CLS] token representation.","bert-as-service provides a very easy way to generate embeddings for sentences. 
 It is explained very well in the  bert-as-service repository : 
 Installations: 
 pip install bert-serving-server  # server
pip install bert-serving-client  # client, independent of `bert-serving-server`
 
 Download one of the pre-trained models available at  here . 
 Start the service: 
 bert-serving-start -model_dir /your_model_directory/ -num_worker=4 
 
 Generate the vectors for the list of sentences: 
 from bert_serving.client import BertClient
bc = BertClient()
vectors=bc.encode(your_list_of_sentences)","This is an excellent guide on using sentence/text embedding for similarity measure. Important : BERT does not define sentence level - so basically anything between [CLS] and [SEP] is a piece of text for which you can use output embedding. 
 https://github.com/VincentK1991/BERT_summarization_1/blob/master/notebook/Primer_to_BERT_extractive_summarization_March_25_2020.ipynb 
 This approach uses [CLS] token value for 768 dimension or basically the cls_head in your question. 
 As S-BERT is mentioned earlier , it contends taking [CLS] token's embedding does not work very well for text matching , natural language inference etc. They finetune BERT on a loss objective , such that sentences which entail one another has higher similarity score and they use mean pooling of token embedding rather than taking the [CLS]. In the end you will get the same result - a vector of [1,768]. 
 Following link has an excellent tutorial for this -
 https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/",,,69.5417941,64.12168833,58.40129006,67.33767169,53.37132096,61.0452394,61.57069893,,
60657,Keras model producing same output,python,"It happens often, that some model only predicts one class. The reason usually is, that the model is unable to distinguish the two classes well, and resorts to one (often the majority) class. 
 With your data, you may have a hard time to fit a NN with reasonable results. I suggest you check boosting which usually works okay with small data. Make sure you tune parameters well. 
 Here  is an application of lightgbm to the iris data. 
 You could also check  Logit with L1 regularization .","A reason might be that you are running a single-layer neural net. Ideally, you should have more than one layer, and use the sigmoid activation function.","Probably because you are using a single hidden layer which is unable to learn that many good parameters to differentiate between the classes. 
 try adding 1-2 more Dense layers. 
 you can try out this configuration as a baseline 
 
 model.add(Dense(128, input_shape=(22,), activation='relu')) 
 model.add(Dense(64, input_shape=(22,), activation='relu')) 
 
 at the output layer sigmoid will be fine because it is a binary classification. 
 
 model.add(Dense(1, activation = 'sigmoid') 
 
 Share your results so that we can explore more on this.","Pin pointing exact reason would be impossible without looking at the data.
Few things you can try: 
 
 Add more layers, neurons in layers to make the model more expressive. Other has pointed this out already. 
 As you have only 195 data points, try to see if there is a scope to use transfer learning. 
 Look at the data, if imbalanced you can do following:

 
 Upsampling/Downsampling 
 Use weighted cost, misclassification of less represented class will be penalised heavily compared to the other class. 
 Be careful with your error metric. Accuracy may not be the right choice. 
 
 Play with the learning rate.","You are using a softmax function with only one neuron in your last layer. 
 It is normal that your output is the same, one property of softmax function is that the sum of output across neurons equals 1 (1 neuron = 1 always). 
 Use a sigmoid function or set your last layer to 2 neuron.",,,,,51.23775058,50,52.1113601,50.57987501,52.24922082,,,,
57367,Classification training using probabilites and not raw classes (factors),classification,"Beta Regression 
 You could use beta-regression. I have no practical experience with this type of regression. However, it might be the right method for your task. As far as I understand, the link function is chosen so to restrict  $\hat{y} \in [0,1]$ . 
 Here is an  R implementation , where the docs say:  
 
 Fit beta regression models for rates and proportions via maximum
  likelihood using a parametrization with mean (depending through a link
  function on the covariates) and precision parameter (called phi). 
 
 Example: 
 library(""betareg"")
data(""GasolineYield"", package = ""betareg"")
summary(GasolineYield$yield)

Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.0280  0.1165  0.1780  0.1966  0.2705  0.4570 

br = betareg(yield ~ batch + temp, data = GasolineYield)
preds = predict(br, newdata=GasolineYield)
summary(preds)

Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.04571 0.10309 0.16364 0.19655 0.26429 0.50792 
 
 Regression Models For Ordinal Data 
 Ordinal Logistic Regression  could be used for this problem since classes are ordered and multinomial classification does not take the order of classes into consideration. In practice, this algorithm doesn't scale to many classes or many observations because its computationally expensive. 
 Here is an example of fitting a cumulative link model (CLM) such as the proportional odds model to data using the  ordinal package in R . 
 require(""ordinal"")
fm1 <- clm(rating ~ contact + temp, data=wine)
summary(fm1)

formula: rating ~ contact + temp
data:    wine

link  threshold nobs logLik AIC    niter max.grad cond.H 
logit flexible  72   -86.49 184.98 6(0)  4.01e-12 2.7e+01

Coefficients:
       Estimate Std. Error z value Pr(>|z|)    
contactyes   1.5278     0.4766   3.205  0.00135 ** 
tempwarm     2.5031     0.5287   4.735 2.19e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Threshold coefficients:
    Estimate Std. Error z value
1|2  -1.3444     0.5171  -2.600
2|3   1.2508     0.4379   2.857
3|4   3.4669     0.5978   5.800
4|5   5.0064     0.7309   6.850
 
 Regression with a Logistic Link Function 
 As suggested by Ben Reiniger in the comments of the question, another alternative is simply to use a Logistic Link function in a regression model. 
 An example would be using xgboost with reg:logistic as the objective function. However, many libraries may not support this behavior as they need the target be either one or zero.","What you are describing is just the cross entry loss (also known as relative entropy or kullback-leibler divergence). If you have target probabilities that are one-hot you get the NLL form of it that is most commonly seen, however it is actually a loss that tries to match probability distributions. Simple solution to your problem would be a linear layer followed by softmax and then for example torch.nn.KLDivLoss or the equivalent in your favorite framework.","Never tried this, but I think it will work: Let's say you are using a neural network. Continue to use your normal y values (1, 0, etc), but make sure to hold in memory the respective probabilities for each classification (they all should be greater than 1/number of classes). Then, once the algorithm has calculated the loss for the sample, multiply the loss by the probability for that sample, making sure to do this before any backpropagation. I think you would have to do this at a fairly low level. Tensorflow should be able to get you there, though I'm less sure if something like Keras would.","I believe if you could tell me what are your actual number of classes for classification that would have helped,any way lets suppose you have n classes one way to easily increase your accuracy is to simply use an optimized rounder function here i.e a function that takes your predicted probabilities as input and you have your target classes with there respective probability vectors to compute your accuracy now try to find the optimal coefficient value that partitions the [0,1] range in such a way that it maximizes your accuracy(or any metric that you are optimizing, partitions should be as many as there are classes you can use DP to solve this problem and get your coefficinets the best ones) after this you could use this same approach on predictions from other various models and then stack them and it should give you much better results.Another possible approach is to run many ml algorithms such as tree based regressors or ensemble based regressors or a mix of them and then predict probability of the class and using the results from various other models combine the results to form a new training set(keep the probabilities as it is) and then train a meta learner on top of it and for me this gave me much much better results this is actually the true principle behind using ensemble or stacking try to use models that vary in accuracy as well as stability and i think you should get much better results.","What exactly is the problem you are trying to solve? Are you trying to map the probabilities to classes? If so, why not just assign the observation to the class with the highest probability? 
 If you really want to use a classification method for some reason, have you considered boosted trees? 
 There are a few advantages to this approach: 
 
 No need to normalize inputs 
 No need to calibrate probabilities prior to inputting them into the model 
 This method is robust to correlation between input features 
 
 There are open source libraries available depending on your language of choice. I know it sounds simple, but based on your question, this seems like the simplest solution that can quickly meet your needs.",,,,,53.00948952,50,53.96330454,56.7209518,54.23620748,,,,
57005,Why there is no exact picture of softmax activation function?,neural-network,"Softmax is a multivariable function, generally. You wouldn't take a softmax of a single variable just like you wouldn't take a maximum of a single variable. It's difficult to plot functions of more than 2 variables because our eyes see in 3 dimensions. 
 A sigmoid is a 1 dimensional function. In your picture, they are applying a sigmoid to every variable separately.","Softmax is in a fundamentally different category from the other activation functions you showed in your answer (ReLU, ELU, tanh, etc.), which are all univariate non-linearities. 
 As  Brady Gilg  said, softmax is a multivariate function, so it's not as easy to plot it for anything above 2 dimensions, but we can at least plot it for 2 dimensions (a 2-neuron layer) and compare it to a sigmoid activation layer: 
 Sigmoid 
 
 
 For sigmoid, each neuron sums its inputs, passes the sum through the sigmoid function, and then to the output.  Each input  $x_i$  is passed independently through a sigmoid function to produce an output  $y_i$ .  There is no interaction between neighboring neurons in a layer, and you can have a layer with only one neuron if you want. 
 So in the plots above, you can see that each output of the sigmoid layer depends only on one input.   $y_0$  depends only on  $x_0$ , for instance, and  $x_1$  has no effect on it.  (You wouldn't normally even include  $x_1$  in the plot; you would just plot the relationship between  $y_0$  and  $x_0$ , but I've shown it to contrast with softmax.)  Likewise,  $y_1$  depends only on  $x_1$  and ignores  $x_0$ . 
 The outputs of the layer form a vector  $Y = [y_0, y_1, \dots, y_k]$ , where:
 $$y_i = F(x_i) = \frac{1}{1 + \exp(-x_i)}, \quad i = 0, 1, 2, \dots$$ 
 This maps each input  $x_i$  to the range  $[0, 1]$ , independently of other inputs. 
 Softmax 
 
 
 Softmax, on the other hand, is a multivariate nonlinearity, so  every  output depends on  every  input in the layer. 
 The entire input vector  $X = [x_0, x_1, \dots, x_k]$  is processed together to produce a normalized output vector  $Y = [y_0, y_1, \dots, y_k]$ , where: 
 $$y_i = F(x_i) = \frac{\exp(x_i)}{\sum_{j=0}^k \exp(x_j)}, \quad i = 0, 1, \dots, k$$ 
 In the plots above, you can see that  $y_0$  is affected by both  $x_0$  and  $x_1$ .  Both inputs  $x_0$  and  $x_1$  influence both outputs  $y_0$  and  $y_1$ . 
 Similar to sigmoid, this maps each input  $x_i$  to the range  $[0, 1]$ , but unlike sigmoid it also ensures the outputs collectively sum to 1, i.e.,  $\sum_{i=0}^k F(X_i) = 1$ .  So the threshold of the softmax output shifts depending on the number of inputs.  We can see this if we plot one output while adjusting one input and keeping all other inputs at 0: 
 
 Name 
 The name ""softmax"" is a bit misleading, because it's not really a ""softened version of  max ([x0, x1, x2])"". (That would be  LogSumExp , also called ""RealSoftMax"".) 
 Some say that it's really a ""softened"" version of  argmax ([x0, x1, x2]) and  should therefore be called softargmax() , but I don't really agree with that either, since argmax returns a single index value, while softmax returns a vector of probabilities.  There are several softened versions of single-output argmax out there, which are more deserving of that name:  1   2   3 
 ""Softmax"" is really a softened version of a  one-hot representation  of the argmax.  So it should  really  be called  softonehotargmax() .  😬","Softmax isn't a continuous mathematical function such as logistic(sigmoid), tanh or ReLU. It is used to map outputs of the last layer of a Neural Network into a probability distribution, i.e., summation of softmax squashed layer's outputs will be 1 (unity). 
 Unlike other activation functions, softmax takes a list/array of input and maps them into probability distribution.
Example : softmax([ 2.0 , 1.0 , 0.1 ]) will return [ 0.7 , 0.2 , 0.1 ] 
 And 0.7 + 0.2 + 0.1 = 1, so if we pass softmax list with only one element, its probability of occurrence will be 1(unity), i.e., softmax([ any_value ]) will return 1. 
 Therefore, softmax is helpful only when multiple outputs have to be squashed. 
 function softmax(list){
// list is an array of outputs of last layer neurons of a Neural Network 
// numerators is an array of Math.exp() applied to each element of list array
    var numerators = list.map(function(e){ return Math.exp(e); });
// denominator is summation of each element of numerators array
    var denominator = numerators.reduce(function(p, c){ return p + c; });
// returning numerators array after dividing each element by denominator
    return numerators.map(function(e){ return e / denominator; });
}
 
 Softmax is helpful in classification problems .","The softmax function is used in the last layer of CNN network. Softmax is an activation function like tanh and ReLU, the difference is that this technique can interpret the incoming inputs as output probabilities. The method guarantees that the output probabilities will be in a range of 0 and 1, and the sum of them is 1, thus the scores are interpretable as a percentage rate for each class. The function uses  this  formula, also you can use these lines of code to compute softmax: 
 logits = [2.0, 1.0, 0.1]
exps = [np.exp(i) for i in logits]
sum_of_exps = sum(exps)
softmax = [j/sum_of_exps for j in exps]
print(softmax)","I'm late for my own party... 
 so in short it is  possible  to plot after the implementation of the SoftMax activation function, however it could be varied because: 
 A softmax function is a very different activation function (AF) from the rest of AFs because it deals with probability. in short: 
 
 ....that turns numbers aka logits (numeric output of the last linear layer of a multi-class classification neural network) into probabilities that sum to  1 . Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes.   ref. 
 
   pic credit 
which could be related to the other cost function  cross entropy loss  too. 
 It is the probability distribution over an N number of classes. This AF is usually applied to the  end  of the neural network where we want it to calculate probability. 
 
 
 
 
 
 
 
 
 Fig. 1: Understandingtion function.  ref 
 
 
 
 I found nice  article  plots and compared both AFs results one can see there is not much difference between the sigmoid function graph and softmax function graph: 
 
 
 
 
 
 
 
 
 
 Fig. 2: Softmax activation function explained with code (Go) Understanding why and how to use the softmax activation function.  ref 
 Fig. 2: Comparison between sigmoid and softmax outputs:.   ref 
 
 
 
 Apart from some related posts:  post 1  ,  post 2 , there are some posts explained this AF exclusively : 
 
 The Softmax function and its derivative 
 The Softmax Activation Function, Explained 
 #13 Softmax Function 
 
 
 
 Interestingly I found different AFs plots in NN including  Soft-...   from an  FB source  someone shared, which I would like to share here due to the nice vision could give about this post: 
 
 SoftPlus 
 SoftSign 
 SoftShrink",,,,,60.72647755,58.74485624,62.17637757,64.77577283,75.51345414,,,,
56676,Can machine learning learn a function like finding maximum from a list?,machine-learning,"Maybe , but note that this is one of those cases where  machine learning is not the answer . There is a tendency to try and shoehorn machine learning into cases where really, bog standard rules-based solutions are faster, simpler and just generally the right choice :P 
 
 Just because you can, doesn't mean you should 
 
 Edit : I originally wrote this as ""Yes, but note that..."" but then started to doubt myself, having never seen it done. I tried it out this afternoon and it's certainly doable: 
 import numpy as np
from keras.models import Model
from keras.layers import Input, Dense, Dropout
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from keras.callbacks import EarlyStopping

# Create an input array of 50,000 samples of 20 random numbers each
x = np.random.randint(0, 100, size=(50000, 20))

# And a one-hot encoded target denoting the index of the maximum of the inputs
y = to_categorical(np.argmax(x, axis=1), num_classes=20)

# Split into training and testing datasets
x_train, x_test, y_train, y_test = train_test_split(x, y)

# Build a network, probaly needlessly complicated since it needs a lot of dropout to
# perform even reasonably well.

i = Input(shape=(20, ))
a = Dense(1024, activation='relu')(i)
b = Dense(512, activation='relu')(a)
ba = Dropout(0.3)(b)
c = Dense(256, activation='relu')(ba)
d = Dense(128, activation='relu')(c)
o = Dense(20, activation='softmax')(d)

model = Model(inputs=i, outputs=o)

es = EarlyStopping(monitor='val_loss', patience=3)

model.compile(optimizer='adam', loss='categorical_crossentropy')

model.fit(x_train, y_train, epochs=15, batch_size=8, validation_data=[x_test, y_test], callbacks=[es])

print(np.where(np.argmax(model.predict(x_test), axis=1) == np.argmax(y_test, axis=1), 1, 0).mean())
 
 Output is 0.74576, so it's correctly finding the max 74.5% of the time. I have no doubt that that could be improved, but as I say this is not a usecase I would recommend for ML. 
 EDIT 2 : Actually I re-ran this this morning using sklearn's RandomForestClassifier and it performed significantly better: 
 # instantiation of the arrays is identical

rfc = RandomForestClassifier(n_estimators=1000, verbose=1)
rfc.fit(x_train, y_train)

yhat_proba = rfc.predict_proba(x_test)


# We have some annoying transformations to do because this .predict_proba() call returns the data in a weird format of shape (20, 12500, 2).

for i in range(len(yhat_proba)):
    yhat_proba[i] = yhat_proba[i][:, 1]

pyhat = np.reshape(np.ravel(yhat_proba), (12500,20), order='F')

print(np.where(np.argmax(pyhat, axis=1) == np.argmax(y_test, axis=1), 1, 0).mean())
 
 And the score here is 94.4% of samples with the max correctly identified, which is pretty good indeed.","Yes. 
Very importantly, YOU decide the architecture of a machine learning solution. Architectures and training procedures don't write themselves; they must be designed or templated and the training follows as a means of discovering a parameterization of the architecture fitting to a set of data points. 
 You can construct a very simple architecture that actually includes a maximum function: 
 net(x) = a * max(x) + b * min(x)
 
 where  a  and  b  are learned parameters.  
 Given enough training samples and a reasonable training routine, this very simple architecture will learn very quickly to set a to 1 and b to zero for your task. 
 Machine learning often takes the form of entertaining multiple hypotheses about featurization and transformation of input data points, and learning to preserve only those hypotheses that are correlated with the target variable. The hypotheses are encoded explicitly in the architecture and sub-functions available in a parameterized algorithm, or as the assumptions encoded in a ""parameterless"" algorithm. 
 For example, the choice to use dot products and nonlinearities as is common in vanilla neural network ML is somewhat arbitrary; it expresses the encompassing hypothesis that a function can be constructed using a predetermined compositional network structure of linear transformations and threshold functions. Different parameterizations of that network embody different hypotheses about which linear transformations to use. Any toolbox of functions can be used and a machine learner's job is to discover through differentiation or trial and error or some other repeatable signal which functions or features in its array best minimize an error metric. In the example given above, the learned network simply reduces to the maximum function itself, whereas an undifferentiated network could alternatively ""learn"" a minimum function. These functions can be expressed or approximated via other means, as in the linear or neural net regression function in another answer. In sum, it really depends on which functions or LEGO pieces you have in your ML architecture toolbox.","Yes - Machine learning can learn to find the maximum in a list of numbers.  
 Here is a simple example of learning to find the index of the maximum: 
 import numpy as np
from sklearn.tree import DecisionTreeClassifier

# Create training pairs where the input is a list of numbers and the output is the argmax
training_data = np.random.rand(10_000, 5) # Each list is 5 elements; 10K examples
training_targets = np.argmax(input_data, axis=1)

# Train a descision tree with scikit-learn
clf = DecisionTreeClassifier()
clf.fit(input_data, targets)

# Let's see if the trained model can correctly predict the argmax for new data
test_data = np.random.rand(1, 5)
prediction = clf.predict(test_data)
assert prediction == np.argmax(test_data) # The test passes - The model has learned argmax","Learning algorithms 
 Instead of learning a function as a calculation done by a feed-forward neural network, there's a whole research domain regarding learning  algorithms  from sample data. For example, one might use something like  a Neural Turing Machine  or some other method where execution of an algorithm is controlled by machine learning at its decision points. Toy algoritms like finding a maximum, or sorting a list, or reversing a list, or filtering a list are commonly used as examples in algorithm learning research.","I will exclude educated designs from
my answer.  No it is not possible  to use an out of the box machine learning (ML) approach to  fully  represent the maximum function for  arbitrary  lists with arbitrary precision. ML is a data-based method and it is clear that you will not be able to approximate a function at regions where you do not have any data points. Hence, the space of possible observations (which is infinite) cannot be covered by finite observations. 
 My statements have a theoretical foundation with Cybeko’s Universal Approximation Theorem for neural networks. I will
quote the theorem
from
Wikipedia: 
 
 In the mathematical theory of artificial neural networks, the
  universal approximation theorem states[1] that a feed-forward network
  with a single hidden layer containing a finite number of neurons can
  approximate continuous functions on compact subsets of  $\mathbb{R}^n$ , under mild
  assumptions on the activation function. The theorem thus states that
  simple neural networks can represent a wide variety of interesting
  functions when given appropriate parameters; however, it does not
  touch upon the algorithmic learnability of those parameters. 
 
 The most important part is the bounded subset of  $\mathbb{R}^n$ . This additional statement restricts the application of approximating the maximum function for  $x\in \mathbb{R}$ . This restriction is manifesting itself in the poor fit of the model from the answer with the most upvotes. 
 If your space of observations is compact then you might be able to approximate the maximum function with a finite data set. As the top voted answer made clear you should not reinvent the wheel!","Here's an expansion on my comment.  To preface, absolutely @DanScally is right that there's no reason to use ML for finding a maximum of a list.  But I think your ""it might give me an understanding of what machine learning can do in general"" is good enough reason to delve into this. 
 You ask about more general machine learning, but I'll focus on neural networks.  In that context, we must first ask whether the actual functions produced by a neural network can approximate (or evaluate exactly)  $\max$ , and only then can we further inquire whether any of the (common?) training methods can fit a NN approximating  $\max$ . 
 
 The comments, and @MachineLearner's answer brought up universal approximation theorems: on a  bounded domain , a neural network can approximate any reasonably nice function like  $\max$ , but we can't expect a priori to approximate  $\max$  on arbitrary input, nor to exactly calculate  $\max$  anywhere. 
 But, it turns out that a neural network  can exactly  sort arbitrary input numbers.  Indeed,  $n$   $n$ -bit integers can be sorted by a network with just two hidden layers of quadratic size.   Depth Efficient Neural Networks for Division and Related Problems , Theorem 7 on page 955; many thanks to @MaximilianJanisch in  this answer  for finding this reference. 
 I'll briefly describe a simplification of the approach in that paper to produce the  $\operatorname{argmax}$  function for  $n$  arbitrary distinct inputs.  The first hidden layer consists of  $\binom{n}{2}$  neurons, each representing the indicator variable  $\delta_{ij} = \mathbf{1}(x_i < x_j)$ , for  $i<j$ .  These are easily built as  $x_j-x_i$  with a step activation function.  The next layer has  $n$  neurons, one for each input  $x_i$ ; start with the sum  $\sum_{j<i} \delta_{ji} + \sum_{j>i} (1-\delta_{ij})$ ; that is, the number of  $j$  such that  $x_i>x_j$ , and hence the position of  $x_i$  in the sorted list.  To complete the argmax, just threshold this layer. 
At this point, if we could multiply, we'd get the actual maximum value pretty easily.  The solution in the paper is to use the binary representation of the numbers, at which point binary multiplication is the same as thresholded addition.  To just get the argmax, it suffices to have a simple linear function multiplying the  $i$ th indicator by  $i$  and summing. 
 
 Finally, for the subsequent question: can we can train a NN into this state.  @DanScally got us started; maybe knowing the theoretical architecture can help us cheat into the solution?  (Note that if we can learn/approximate the particular set of weights above, the net will actually perform well outside the range of the training samples.) 
 Notebook in github / Colab 
 Changing things just a little bit, I get better testing score (0.838), and even testing on a sample outside the original training range gets a decent score (0.698).  Using inputs scaled to  $[-1,1]$  gets the test score up to 0.961, with an out-of-range score of 0.758.  But, I'm scoring with the same method as @DanScally, which seems a little dishonest: the identity function will score perfectly on this metric.  I also printed out a few coefficients to see whether anything close to the above described exact fit appears (not really); and a few raw outputs, which suggest the model is too timid in predicting a maximum, erring on the side of predicting that none of the inputs are the maximum.  Maybe modifying the objective could help, but at this point I've put in too much time already; if anyone cares to improve the approach, feel free to play (in Colab if you like) and let me know.","Yes, even as simple machine learning as ordinary linear least squares can do this if you use some applied cleverness.  
 (But most would consider this quite horrible overkill).  
 (I will assume we want to find max of abs of input vector): 
 
 Select a monotonically decreasing function of absolute value, for example  $$f(x) = \frac{1}{x^2}$$ 
 Build diagonal matrix of  $f({\bf r})$ . Let us call it  $\bf C_r$ 
 Build vector full of ones  $\bf S$ . 
 Build and solve equation system  $(\epsilon {\bf I}+10^3{\bf S}^t{\bf S}+{\bf C_r})^{-1}(10^3 {\bf S}^t)$ 
 Let us call result vector  $\bf p$ , it will be a probability measure (sums to 1), we can reweigh it nonlinearly, for example  $$p_i = \frac{p_i^k}{\sum|p_i|^k}$$ 
 Just calculate scalar product with index vector and round.",,,51.72523258,58.12303615,61.26472107,68.45441461,55.48360488,59.25224331,52.96309501,,
56476,What is the best question generation state of art with nlp?,machine-learning,"For your first part of the question as to which question generation approaches are good - Neural question generation is being pretty popular (as of 2018/2019) among NLP enthusiasts but not all systems are great enough to be used directly in production. However, here are a few recent ones which reported the state-of-art performances in 2019 and have shared their codes too: 
 
 https://github.com/ZhangShiyue/QGforQA 
 https://github.com/PrekshaNema25/RefNet-QG 
 
 This one is a 2020 one (now that NLP performances have been improved with Transformers)
3.  https://github.com/patil-suraj/question_generation 
 Besides, if you want more control as to understand and fix for wrongly generated questions, I would suggest the more traditional rule-based approach like the below which is more reliable than the above neural ones ones and generates a larger amount of question-answer pairs than the above 2: 
 
 http://www.cs.cmu.edu/~ark/mheilman/questions/ 
 https://bitbucket.org/kaustubhdhole/syn-qg/src/master/ 
 
 To answer your second question, if your QG model is generating an answer, then it makes sense to use cosine similarity. Assuming your question generation is at the sentence level, you will mostly have short answer spans and hence averaging Glove or Paragram word vectors might serve you better results than the Universal Sentence Encoder.","It seems that state of art methods use neural encoder-decoder models [1] 
 [1] Neural Question Generation from Text: A Preliminary Study :  https://arxiv.org/pdf/1704.01792.pdf 
 There is an open source implementation of the paper written with Pytorch on github :  https://github.com/magic282/NQG","I have had great success with  Haystack . Haystack provides state-of-the-art T5 models with many features such as Retriever, Reader, Generator, preprocessing, etc to simplify many NLP processes. Moreover, they also have the option of using pipelines which in turn use DAG (directed Acyclic Graphs) that determine how to route the output of one component into the input of another. Their tutorials are really good. Check out them! 
 Disclaimer: I am in no way affiliated with Haystack, just a dude using their OSS.","Keep an eye out on  https://github.com/sebastianruder/NLP-progress/blob/master/english/question_answering.md  which is regularly updated, the main repository  https://github.com/sebastianruder/NLP-progress  also shows the state-of-the-art on other tasks","To answer your question, I would say transformer based  T5 models  have performed state of the art in this particular problem statement i.e.  Question Generation . Do give a try to this awesome repo:  https://github.com/patil-suraj/question_generation . This guy is a regular contributor in HuggingFace.",,,,,69.24490207,65.38484187,57.42804619,61.35226871,65.72894754,,,,
56444,Do I need to standardize my one hot encoded labels?,machine-learning,"NO, you do not standardize labels 
The purpose of standardization is to bring features with disparate ranges into a  standard  range. When the data is not standardized, features with large numerical values will tend to have a larger influence (weight) than those that are smaller numerically. 
 Consider the Automobile Data Set ( https://archive.ics.uci.edu/ml/datasets/automobile )  from UC Irvine. Among the features, it has height, lenght, width, weight, # cylinders, along with a number of other label and numerical features. Height ranges from from 47.8 to 59.8 and weight ranges from 1488 to 4066. You'll want to standardize them, but not label features such as body-style or engine-type.","From what I can tell, there isn't a ""right"" answer to the title question.  Most people I know wouldn't bother.  (Indeed, one often-used scaler puts the data into the range  $[0,1]$  anyway.)
 https://stats.stackexchange.com/questions/290929/standardizing-dummy-variables-for-variable-importance-in-glmnet 
 https://stats.stackexchange.com/questions/359015/ridge-lasso-standardization-of-dummy-indicators   
 For the second question, normalization/standardization is always (er, always that I've seen, and certainly for your example) applying an increasing function, so that the ordering is preserved.  For your normalization of binary variables then, always 0 gets mapped to the negative value and 1 to the positive one.","While there may not be any added value, is there is any harm in applying standardization to features which were already one-hot encoded ?","It depends on what sort of model you're training and how you're going to be consuming these one hot encoded vectors. For example, if you're going to use a deep learning based approach, you probably don't need the one-hot encoded vectors, and you can use an embedding table to project to a higher dimensional space. 
 If you're going to send this into a linear classifier, I'd like to think that it should not matter too much since the coefficients will be learned accordingly. Performing normalization does make the inference time a bit harder since you need to remember to apply the same normalization to ensure that the data statistics remain unchanged, so there's that. 
 This has also been discussed here before  Should one hot vectors be scaled with numerical attributes 
 tl;dr try it and see the impact it has on your dataset and problem.","No you don't need to. 
 To elaborate my answer: 
 
 Your one hot encoded labels has already converted into categories. 
 And you can not use, or it's useless to use standardization techniques over them.",,,,,58.87021769,52.9156579,76.83259397,66.04971179,82.15241019,,,,
55991,"In the context of Deep Learning, what is training warmup steps",machine-learning,"As the other answers already state: Warmup steps are just a few updates with low learning rate before / at the beginning of training. After this  warmup , you use the regular learning rate (schedule) to train your model to convergence. 
 The idea that this helps your network to slowly adapt to the data intuitively makes sense. However, theoretically, the main reason for warmup steps is to allow adaptive optimisers (e.g. Adam, RMSProp, ...) to compute correct statistics of the gradients. Therefore, a warmup period makes little sense when training with plain SGD. 
 E.g. RMSProp computes a moving average of the squared gradients to get an estimate of the variance in the gradients for each parameter. For the first update, the estimated variance is just the square root of the sum of the squared gradients for the first batch. Since, in general, this will not be a good estimate, your first update could push your network in a wrong direction. To avoid this problem, you give the optimiser a few steps to estimate the variance while making as little changes as possible (low learning rate) and only when the estimate is reasonable, you use the actual (high) learning rate. 
 
 Add-on 
 As Michael pointed out, my answer might suggest that running one epoch with zero learning rate should do the job. However, there are a few reasons why this might not work as well: 
 
 One epoch with  lr=0  is just a lost epoch in the end. If you're iterating through the data you can as well have a very small learning rate to do at least something. 
 After this first ""useless"" epoch, you would have accurate statistics. However, if you directly jump in with a high learning rate, your network (and therefore also the gradients) might change a lot. Therefore, you want to slowly increase the learning rate so that the statistics get a chance to move along with the updates. 
 
 Disclaimer: these arguments are intuitions/speculations rather than plain facts. Feel free to let me know if this does work in practice!","This usually means that you use a very low learning rate for a set number of training steps (warmup steps). After your warmup steps you use your ""regular"" learning rate or learning rate scheduler. You can also gradually increase your learning rate over the number of warmup steps. 
 As far as I know, this has the benefit of slowly starting to tune things like attention mechanisms in your network.","If your data set is highly differentiated, you can suffer from a sort of ""early over-fitting"". If your shuffled data happens to include a cluster of related, strongly-featured observations, your model's initial training can skew badly toward those features -- or worse, toward incidental features that aren't truly related to the topic at all. 
 Warm-up is a way to reduce the primacy effect of the early training examples. Without it, you may need to run a few extra epochs to get the convergence desired, as the model un-trains those early superstitions. 
 Many models afford this as a command-line option. The learning rate is increased linearly over the warm-up period. If the target learning rate is p and the warm-up period is n, then the first batch iteration uses 1 p/n for its learning rate; the second uses 2 p/n, and so on: iteration i uses i*p/n, until we hit the nominal rate at iteration n. 
 This means that the first iteration gets only 1/n of the primacy effect. This does a reasonable job of balancing that influence. 
 Note that the ramp-up is commonly on the order of one epoch -- but is occasionally longer for particularly skewed data, or shorter for more homogeneous distributions. You may want to adjust, depending on how functionally extreme your batches can become when the shuffling algorithm is applied to the training set. 
 source","It can assume also other meaning but the learning rate schedule process. For example in YOLOv3, during warm up epochs the ground truth bounding boxes are forced to be the same size of the anchors. 
 At the end of the day, the warm up procedure aim to soften the impact of the first epochs of learning that can mislead the entire training process. This is not mathematically proven (as far as I know), it just a solid intuition that actually happens to result in better performances.","Warm up steps is just a parameter in most of the learning algorithms which is used to lower the learning rate in order to reduce the impact of deviating the model from learning on sudden new data set exposure.  
 For eg:-
       If you are giving warm up steps as 500 for a iteration of 10,000 epochs, For the first 500 iterations the model will learn the corpus with minimal learning rate than the rate which you've specified in the model. From 501 th iteration model will use the learning rate as itself which given.",,,,,62.8115416,74.56866979,52.52110896,51.89316927,54.65966879,,,,
55901,"In a Transformer model, why does one sum positional encoding to the embedding rather than concatenate it?",nlp,"When you concatenate, you have to define  a priori  the size of each vector to be concatenated. This means that, if we were to concatenate the token embedding and the positional embedding, we would have to define two dimensionalities,  $d_t$  for the token and  $d_p$  for the position, with the total dimensionality  $d = d_t + d_p$ , so  $d>d_t$  and  $d>d_p$ . We would be decreasing the total size we devote to tokens in favor of positional information. 
 However, adding them together is potentially a super case of the concatenation: imagine that there is an ideal split of  $d$  into  $d_t$  and  $d_p$  in terms of minimizing the loss; then, the training could converge to position vectors that only take  $d_t$  elements, making the rest zero, and the positions were learned and happened the same, taking the complementary  $d_p$  elements and leaving the rest to zero. 
 Therefore, by adding them, we leave the optimization of the use of the  $d$  dimensions to the optimization process, instead of assuming there is an optimal partition of the vector components and setting a new hyperparameter to tune. Also, the use of the vector space is not restricted by a hard split in the vector components, but takes the whole representation space.","the first few bits of the embedding are completely unusable by the
network because the position encoding will distort them a lot 
 
 This confused me very much at first because I was thinking of the model using a pre-trained word embedding. And then an arbitrary initial chunk of that embedding gets severely tampered with by the positional encoding. 
 However, in the original transformer model at least, the embedding was trained from scratch, so this does not apply. An initial chunk of the overall embedding will be used for positional information, and the rest will be used for word information. 
 This still doesn't explain why we use this method instead of concatenation -- see the other answers for that -- but it does explain why the method isn't crazy. 
 That said, it may be that the method works well even with pre-trained word embeddings, I don't know. If so, it's hard to explain.","The confusion here is that we believe positional embedding is a more complicated version of adding positional information to the word embedding; however, it is not actually. Adding new dimensions to each embedding increases the dimensionality of the problem. On the other hand, please note that the added positional embedding is (almost) static, as shown in this image for a 2D positional embedding: 
 
 The added positional embeddings are the same for all the inputs, and the transformer can separate the positional information from the actual word embedding through the training process. Therefore, the positional embedding doesn't mess with the word embedding information, and adding them is a more efficient way of adding the positional information that concatenates them.","The following is conjecture, not fact. 
 If you look at how much each scalar in the the positional embedding vector changes as a function of position... you'll find that many of the scalars barely change at all. You can visualize this with any positional embedding plot, where the x axis is usually the [512] length of the vector, and the y axis is the position of the token. 
 For example, this image is from Jay Alammar's well regarded ""The Illustrated Transformer"" 
 
 Let's try to do this mathematically as well. The implementation of PE's that Jay references is at this Google GitHub repo: 
 https://github.com/tensorflow/tensor2tensor/tree/23bd23b9830059fbc349381b70d9429b5c40a139 
 Running the function on a PE/WE of length 512 and max sentence length of 128, let's look at how much the final value in the vector actually changes from the first position, to the 64th position, to the final position. Answer: not much. 
 print(signal[0, 0, -1])
print(signal[0, 63, -1])
print(signal[0, 127, -1])

tf.Tensor(1.0, shape=(), dtype=float32)
tf.Tensor(0.99998015, shape=(), dtype=float32)
tf.Tensor(0.99991935, shape=(), dtype=float32)
 
 Ditto for a value 16 steps away from the final location: 
 print(signal[0, 0, -16])
print(signal[0, 63, -16])
print(signal[0, 127, -16])

tf.Tensor(1.0, shape=(), dtype=float32)
tf.Tensor(0.9984067, shape=(), dtype=float32)
tf.Tensor(0.9935305, shape=(), dtype=float32)
 
 I saw elsewhere that BERT's WEs are typically roughly the range of [-2, 2], so adding a 0.007 delta from the PE would not move the WE very much at the -16th position. 
 So what I think is probably happening is that only ~256 of the PE vector's values are actually moving around as a function of the position... the rest are ~constant. Then the learned WE (Transformers don't use prelearned WE like word2vec or glove), figures out to only use the other ~256 elements. So really... it's conceptually a concat. 
 notebook here 
 https://colab.research.google.com/drive/14RGALTsPIYGAuIByXGutK-aYN-PikWzF","The best answer I have seen is  this Reddit answer  by  pappypapaya : 
 
 In attention, we basically take two word embeddings (x and y), pass
one through a Query transformation matrix (Q) and the second through a
Key transformation matrix (K), and compare how similar the resulting
query and key vectors are by their dot product. So, basically, we want
the dot product between Qx and Ky, which we write as: 
 (Qx)'(Ky) = x' (Q'Ky). 
 So equivalently we just need to learn one joint
Query-Key transformation ( Q'K ) that transform the secondary inputs  y 
into a new space in which we can compare  x . 
 By adding positional encodings  e  and  f  to  x  and  y , respectively, we
essentially change the dot product to 
 (Q(x+e))' (K(y+f)) = 
(Qx+Qe)' (Ky+Kf) = 
(Qx)' Ky + (Qx)' Kf + (Qe)' Ky + (Qe)' Kf = 
x' (Q'Ky) + x' (Q'Kf) + e' (Q'Ky) + e' (Q'K f) 
 
 where in addition to the original  x' (Q'Ky)  term, which asks the question ""how much attention should we pay to word  x  given word  y "", we also have  x'(Q'Kf) + e'(Q'Ky) + e'(Q'K f) , which ask the additional questions,
""how much attention should we pay to word  x  given the position  f  of
word  y "", ""how much attention should we pay to  y  given the position  e 
of word  x "", and ""how much attention should we pay to the position  e  of
word  x  given the position  f  of word  y "". 
 Essentially, the learned transformation matrix  Q'K  with positional
encodings has to do all four of these tasks simultaneously. This is
the part that may appear inefficient, since intuitively, there should
be a trade-off in the ability of  Q'K  to do four tasks simultaneously
and well. 
 HOWEVER, MY GUESS is that there isn't actually a trade-off when we
force  Q'K  to do all four of these tasks, because of some approximate
orthogonality condition that is satisfied of in high dimensions. The
intuition for this is that randomly chosen vectors in high dimensions
are almost always approximately orthogonal. There's no reason to think
that the word vectors and position encoding vectors are related in any
way. If the word embeddings form a smaller dimensional subspace and
the positional encodings form another smaller dimensional subspace,
then perhaps the two subspaces themselves are approximately
orthogonal, so presumably these subspaces can be transformed approx.
independently through the same learned  Q'K  transformation (since they
basically exist on different axes in high dimensional space). I don't
know if this is true, but it seems intuitively possible. 
 If true, this would explain why adding positional encodings, instead
of concatenation, is essentially fine. Concatenation would ensure that
the positional dimensions are orthogonal to the word dimensions, but
my guess is that, because these embedding spaces are so high
dimensional, you can get approximate orthogonality for free even when
adding, without the costs of concatenation (many more parameters to
learn). Adding layers would only help with this, by allowing for
nonlinearities. 
 We also ultimately want  e  and  f  to behave in some nice ways, so that
there's some kind of ""closeness"" in the vector representation with
respect to small changes in positions. The sin and cos representation
is nice since nearby positions have high similarity in their
positional encodings, which may make it easier to learn
transformations that ""preserve"" this desired closeness. 
 (Maybe I'm wrong, and the approximate orthogonality arises from
stacking multiple layers or non-linearities in the fully-connected
parts of the transformer). 
 tl;dr: It is intuitively possible that, in high dimensions, the word
vectors form a smaller dimensional subspace within the full embedding
space, and the positional vectors form a different smaller dimensional
subspace approximately orthogonal to the one spanned by word vectors.
Thus despite vector addition, the two subspaces can be manipulated
essentially independently of each other by some single learned
transformation. Thus, concatenation doesn't add much, but greatly
increases cost in terms of parameters to learn.","It is been a while, but I think anyone ending up here might also be interested in the reading of the following paper: 
 
 What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding  (Yu-An Wang, Yun-Nung Chen) 
 https://www.aclweb.org/anthology/2020.emnlp-main.555 
 
 I am not changing the accepted answer as this article is  not  specific.","So the question is about why positional embeddings are directly added to word embeddings instead of concatenated. This is a particularly interesting question. To answer this question, I will need to firstly separate the differences between sequential networks like RNNs and Transformers, which then introduces this problem nicely. 
 In RNNs, we feed in data (let's say a sequence of words) into the model in a sequential manner. This means that in the context of inputting in a sequence of words, the model does arguably obtain the order the tokens as it is fed in one by one.
 With transformers, on the other hand, all of the words in the sequence are fed in all at once. This means that, so far, transformers do not have any notion of word ordering. Therefore, we need positional embeddings to tell the model where each word belongs in the sequence. 
 I believe the reason why we add them to word embeddings is because we want to maintain a similar input into the model as an RNN, which takes in word embeddings as its input as well.
I think your question is a very good one to ask, and maybe you should experiment with having a more compressed word embedding with its positional embedding and compare your approach against the more ""traditional"" approach and see what results you yield. I'll be excited to see them.","Why everyone compares RNN and Transformers, when you should actually compare Feedforward Neural Networks with Transformers? I am really sorry, I cannot comment @shepan6 answer, so I will post an answer. 
 
 This means that, so far, transformers do not have any notion of word ordering. - @shepan6 
 
 This is totally wrong and misleading. Transformers are just FNNs. Order of input matter. Please stop spreading disinformation. I know two ablation studies about positional encoding - one in ""Attention is all you need""  [arxiv:1706.03762]  and the other in ""Convolutional Sequence to Sequence Learning""  [arxiv:1705.03122] . Both authors conclude that there is no or negligible difference in performance of 1) different positional encoding; and 2) present/missing positional encoding. 
 From paper ""Attention is all you need"": 
 
 We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). 
 
 From paper ""Convolutional Sequence to Sequence Learning"": 
 
 Table 4 shows that position embeddings are helpful but that
our model still performs well without them.",,60.39077565,69.42020576,62.07031594,54.01675334,64.46779752,60.75506415,61.36118422,66.09529786,
54908,Data normalization before or after train-test split?,normalization,"Normalization across instances should be done after splitting the data between training and test set, using only the data from the training set. 
 This is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance. 
 [Precision thanks to Neil's comment] 
When normalizing the test set, one should apply the normalization parameters previously obtained from the training set as-is.  Do not  recalculate them on the test set, because they would be inconsistent with the model and this would produce wrong predictions.","As @Erwan said, you should normalize the training set and then use the same normalization steps on the test set.  So your code should look like: 
 from sklearn.preprocessing import StandardScaler

x_train, x_test, y_train, y_test = train_test_split(
    X_features, 
    Y_feature, 
    test_size=0.20,
    random_state=4)

scaler = StandardScaler()

normalized_x_train = pd.DataFrame(
    scaler.fit_transform(x_train),
    columns = x_train.columns
)

LR = LogisticRegression(
    C=0.01,
    solver='liblinear'
).fit(normalized_x_train, y_train)

normalized_x_test = pd.DataFrame(
    scaler.transform(x_test),
    columns = x_test.columns
)
y_test_pred = LR.predict(normalized_x_test)","Answer to your question: 
Do Normalization after splitting into train and test/validation. The reason is to avoid any  data leakage . 
 Data Leakage:   
 Data leakage is when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed. 
 You can read about it here :
 https://machinelearningmastery.com/data-leakage-machine-learning/","I think it doesn't matter whether you do it before or after since
data leakage is only possible when classification results or your output information are also somehow flowing in the input model. 
 But since you are applying normalization on the input parameter and not the output no leakage could possibly happen.","Sklearn's own documentation indicates that: 
 
 ""Note We here choose to illustrate data leakage with a feature
selection step. This risk of leakage is however relevant with almost
all transformations in scikit-learn, including (but not limited to)
StandardScaler, SimpleImputer, and PCA."" 
 ""As with any other type of preprocessing, feature selection should
only use the training data. Including the test data in feature
selection will optimistically bias your model."" 
 ""10.2.2. How to avoid data leakage Below are some tips on avoiding
data leakage: 
 Always split the data into train and test subsets first, particularly
before any preprocessing steps."" 
 
 https://scikit-learn.org/stable/common_pitfalls.html",,,,,64.15773715,77.03505965,58.69468879,52.59391012,57.69617117,,,,
54601,Binary classification as a 2-class classification problem,machine-learning,"tl;dr 
 The two approaches you mention are  equally effective . 
 Why? 
 First of all, I'll assume you're referring to a neural-network-type model. You should note that Convolutional Neural Networks are  discriminative models , meaning that they are trained to  find the differences  between two classes (e.g.  dog  and  not-dog ).  
 
 Would the second approach be less effective than the first? 
 
 Just to disprove this claim, consider the following. You have a 2-output classification network; let's name the outputs  dog  and  not-dog . Now imagine, as you say, that the network can only identify dog patterns. All it would have to do was to give a positive weight to those patterns for the  dog  class and a negative weight for the  not-dog  class. In this sense the  not-dog  class would be trained as the opposite of the dog class, which would  not  make it less effective than a single-output binary classifier. 
 Even if the  not-dog  class could not be trained and remained constant, due to the fact that we're using a softmax activation, predictions would be generated just by the relative difference between the  dog  output and the constant  not-dog  output. This is  exactly  like having a single-output binary classifier.","You can use a anomaly detection autoencoder architecture. Basically, what you will do is, 
 
 Create an autoencoder architecture wih convolutional layers. 
 We train the model on images of dogs. 
 So when we feed a image of a dog on the trained autoencoder, it will produce a relatively small loss. 
 When a image other than a dog is fed to the model, the loss value will be high. 
 
 Thus, we can detect a presence of the dog in the image using the loss value.","Binary classification  is  2-class classification. The difference you listed is actually  between  soft  and  hard classification . In soft classification you have a continuous distribution on all the classes, in hard classification the output is a one-hot encoded vector. 
 At some point, your model must be able to express some classification (either ""dog"" or ""not-dog""), therefore you would end up with hard classification anyway.","Agree with Schubam's answer, treat this as an anomaly detection system, train only on dog images , and flag any image that has high loss as a non-dog image. and yes use 2 classes dog and not-dog, just like cancer detection.","The first and second approach is the same, as learning which images are not-dog is part of binary classification.",,,,,54.55578046,50,73.38004184,51.12466325,62.71721852,,,,
54477,How to get high accuracy in CNNs?,neural-network,"I have a few suggestions to improve your accuracy (these are hardly original with me): 
 
 Use an  ImageDataGenerator  from  keras.preprocessing.image  to augment your data. 
 Use k-fold validation to use your training data more thoroughly. 
 Try the  keras.optimizers.Adam()  optimizer.","You can check this great article  Link . I think you you will get a good intuition.
Although this link gives you very generalize ideas for improving the overall model, it would be a great help for building a good model in the future.","Following things you can do to improve accuracy 
 
 Perform data augmentation check  this 
 Transfer learning is also a good approach in the case of availability of the large pre-trained models. In your case, you can try VGG16, Inception V3, ResNet52 & others. Refer  this article  for the same","Here  is the same dataset examined in a Udacity course. Based on your code you can try the following: 
 Your output is softmax categorization, so you might want to try sparse categorical cross entropy as output instead of binary cross entropy. 
 Also, is there a reason you are using SGD with a much lower learning rate compared to default parameters? As suggested before, try adam optimizer with default settings. 
 And finally, you can also try different paddings for the convolutional layers.","Adding multiple layers before pooling will increase the feature extraction, softmax function is used when there are more than 2 categories/classes, try with sigmoid fuction, 
 Also, Data Augmentation will help you with tuning hyper-parameters(tilt, width-shift, shear, rotation etc). Go through the documentation. 
 Give a try with this code, If RMSprop yieds less accuracy try adam, 
 model = Sequential()
model.add(Conv2D(32, 3, 3, border_mode='same', input_shape=input_shape, activation='relu'))
model.add(Conv2D(32, 3, 3, border_mode='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, 3, 3, border_mode='same', activation='relu'))
model.add(Conv2D(64, 3, 3, border_mode='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, 3, 3, border_mode='same', activation='relu'))
model.add(Conv2D(128, 3, 3, border_mode='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(256, 3, 3, border_mode='same', activation='relu'))
model.add(Conv2D(256, 3, 3, border_mode='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))    
model.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.0001),metrics=['accuracy'])",,,,,52.42046204,51.17433781,52.47409585,50,50.57867088,,,,
54232,BERT vs Word2VEC: Is bert disambiguating the meaning of the word vector?,word2vec,"BERT and ELMo are recent advances in the field. However, there is a fine but major distinction between them and the typical task of word-sense disambiguation: word2vec (and similar algorithms including GloVe and FastText) are distinguished by providing knowledge about the constituents of the language. They provide semantic knowledge, typical about word types (i.e. words in the dictionary), so they can tell you something about the meaning of words like ""banana"" and ""apple"". 
 However, this knowledge is at the level of the prototypes, rather than their individual instances in texts (e.g. ""apple and banana republic are american brands"" vs ""apple and banana are popular fruits""). The same embedding will be used for all instances (and all different senses) of the same word type (string). 
 In past years, distributional semantics methods were used to enhance word embeddings to learn several different vectors for each sense of the word, such as Adaptive Skipgram. These methods follow the approach of word embeddings, enumerating the constituents of the language, but just at a higher resolution. You end up with a vocabulary of word-senses, and the same embedding will be applied to all instances of this word-sense. 
 BERT and ELMo represent a different approach. Instead of providing knowledge about the word types, they build a context-dependent, and therefore instance-specific embedding, so the word ""apple"" will have different embedding in the sentence ""apple received negative investment recommendation"" vs. ""apple reported new record sale"". Essentially, there is no embedding for the word ""apple"", and you cannot query for ""the closest words to apple"", because there are infinite number of embeddings for each word type.  
 Thus, BERT and ELMo handle the differences between word-senses but will not reveal what are the different senses of the word.","I think there are a few misconceptions in your statements. Please take into account the following 
 
 BERT does not provide word-level representation. It provides sub-words embeddings and sentence representations. For some words, there may be a single subword while, for others, the word may be decomposed in multiple subwords. The representations of subwords cannot be combined into word representations in any meaningful way. 
 ELMO does provide word-level representations. 
 Word embeddings don't reflect meaning but co-occurrence statistics within the context window. This means that two antonyms will probably have similar representations, as they can appear in similar contexts within short context windows. 
 Distances over continuous representation spaces do not mix well with untangled concepts, like meaning, e.g. should two antonyms be closer than two unrelated words? 
 
 That being said... 
 
 If you want a model that does word sense disambiguation (WSD), you should train a model on a WSD dataset if there is available data. 
 If you have no data, you could try a nearest neighbor approach like in  this article , where the authors specifically show their approach with ""bank"" for different contextualized word embeddings (see figure below). You should also have a look at  this other article , where they also study the geometry of BERT representations in relation to WSD.","The most important question to ask is : for which purpose do you need that?  
 
 You are not right when claiming that Word2Vec creates vectors for words without taking into account the context. The vectors are created in fact using a window (the size of the windo is one of the settings in W2V) in order to get the neighbouring words, so yes, it takes into account the context! That is why you can have those famous equations : king-man = queen-woman, high cosine similarity between two synonyms or words having the same part of speech, etc, etc. 
 If you wish to have two meanings, why not create two models, with data from two separate sectors (banking and general, let us say). In this case, each model will recognize bank with a different meaning.  
 
 Once again, what exactly is you need here ?","One possible way to disambiguate multiple meanings for a word is to modify the string literal during training.  
 For  bank , the model would learn  bank_FINANCE  and separately learn  bank_RIVER . Creating separate tokens from the string literal is a way to allow the tokens to capture different meanings and disambiguate those separate meanings. 
 This is commonly done for parts-of-speech,  excuse  can be  excuse_NOUN  and  excuse_VERB . 
 Adding context-specific metadata to disambiguate the meaning of the same string literal would work in a similar way for both word2vec and BERT.","One option is to add an additional neural network model from the output of standard BERT. During training, standard BERT would learn the sentence embeddings. The additional neural network would learn to classify the different senses of the same string literal based on the context of the sentence.  
 One version of this was done in "" Using BERT for Word Sense Disambiguation "".",,,,,64.97825897,62.02843128,59.27291627,66.58788182,63.22865167,,,,
53995,What does embedding mean in machine learning?,machine-learning,"In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well. 
 Typically, when I stumble upon jargon I'm not familiar with I first turn to Google, and if it can't be found I ping my colleagues and data science forums.","According to all answers(Thank you) and my google search I got a better understanding, So my newly updated understanding is: 
 The embedding in machine learning or NLP is actually a technique mapping from words to vectors which you can do better analysis or relating, for example, ""toyota"" or ""honda"" can be hardly related in words, but in vector space it can be set to very close according to some measure, also you can strengthen the relation ship of word by setting: king-man+woman = Queen. 
 so we can set boy to (1,0) and then set girl to (-1,0) to show they are in the same dimension but the meaning is just opposite. And all nouns that just diff in gender can be parallel~ 
 My initial guess that embedding is extracting features from something is close but not specific enough. 
 And for my last point when you met a jargon in some special area how to quickly get the essential meaning of it, I still didn't find a very good way, maybe a website that can explain the meaning of jargon in that area will save great time for us.","For me embedding is used to represent big sparse matrix into smaller dimensions, where each dimension(feature) represent a meaningful association with other elements in the embedding matrix. 
 Consider an example of NLP. Where each sentence broken down into words(also called token). Such set of different words make a vocabulary for NLP. Generally vocabulary have millions of words. All such words can be uniquely represented as OneHotEncoding. 
 
 Demerits of OneHotEncoding representation of words: 
 
 In case of large vocabulary, OneHotEncoding representation needs a
  big chunk of memory and computationally become very expensive. 
 OneHotEncoding is used to represent categorical values, where each entity is 
  independent to other one, whereas words in vocabulary represent some association in 
  terms of similar meanings or in some other way. OneHotEncoding not utilizing that 
  capability for NLP. 
 
 
 In order to overcome both the issues, we use word Embedding, where each word represented in lesser dimension, where each dimension represent some sort of features and hence each dimension will have some values.","As well known, machine only identify 0 and 1. Therefore, we, for an instance, ""encode"" characters and symbols with ASCII codes. 0 & 1 can only code two characters. To make sure the code is unique for all characters, we have to use a series number to code them. The hard ware experienced 8, 16, 32, and 64 bits (now 64 bits is most popular). 
 Similarly, in my understanding, embedding is to convert non-digital inputs in a preset order, like words, videos, images, etc. into vectors. These vectors can be used to machine learning easily. The dimension of metrics depends on the features of objects. Few features would result in low dimension, vice versa. 
 For an instance, we would recommend movies to users. If users have only one feature, let's say UserID, and movies only has one feature, movie name. We can build up a matrix with UserID as row, and movie name as columns. If a user watched some movies, those intersection would be 1, otherwise would be 0. 
        | movie1 | movie2 | movie3 | movie 4 | ... | movie n |
user1  |   1    |   0    |   0    |    1    | ... |    0    |
user2  |   0    |   0    |   0    |    0    | ... |    1    |
 ...   |  ...   |  ...   |  ...   |   ...   | ... |   ...   |
userm  |   1    |   1    |   0    |    0    | ... |    0    |
 
 Above is a m X n matrix. Each row represents one user. Thus, for user1, we have a vector (1 0 0 1 ... 0), user2 (0 0 0 0 ... 1), similarly for the rest of users. If the movie has only name without any further features, it is hard for machine to learn and train. Now, if we arrange the movies from left to right based on the suitable age group, we then can recommend movie2 and 3 to user1. 
 Based on above example, we can classify the movies in different features (let's say t features), like animated cartoon, action movie, war movie, horror movie, etc. We can build a m X n X t matrix. Based on the watch history, machine can classify a user's watched movies into different group and recommend similar movies.","The LSA community seems to have first used the word “embedding” in Landauer
et al. (1997), in a variant of its mathematical meaning as a mapping from one space or mathematical structure to another. In LSA, the word embedding seems to have described the mapping from the space of sparse count vectors to the latent space of SVD dense vectors. Although the word thus originally meant the mapping from one space to another, it has metonymically shifted to mean the resulting dense vector in the latent space. and it is in this sense that we currently use the word. 
 
 For the more generalized and frequently met questions, I'd like to recommend you read textbooks, especially classic ones. 
 Refrence: 
 Speech and Language Processing: An introduction to natural language processing","Embeddings are vector representations of a particular word. 
 In Machine learning,  textual content has to be converted to numerical data to feed it into Algorithm.  
 One method is one hot encoding but it breaks down when we have large no of vocabulary. The size of word representation grows as the vocabulary grows. Also, it is sparse. 
 With embedding (fixed size vectors with lower dimension), the size of word representation can be controlled. Also, the vector representation stores the semantic relationship b/w words. There are pretrained embeddings Word2Vec, Glove etc available which can be used just as a lookup.  Embeddings improve the performance of ML model significantly.",,,,60.22906234,58.44100469,54.3923928,53.52004653,54.69339123,59.22086833,,,
53956,Distance between users,recommender-system,"No need for algorithms, or recommendation systems.  You have: 
 
 For each user a have a bunch of features. 
 
 As long as they're numeric, or can be made numeric (e.g. aggregating the values or one-hot-encoding them), you already have distances.  What you may not have is the proper variance across the feature space, i.e. features are scaled in different orders of magnitude. 
 If you know the exact weight of the features in relation to user similarity you may try to tune (scale) the features by hand. Otherwise you can simply make every feature have mean 0 and standard deviation 1.  In other words, per feature subtract the mean from all points and divide by the current standard deviation. ( sklearn  has a  StandardScaler  that does exactly that.) 
 In the scaled dataset, from any point (user), you can just calculate  euclidean distance  to any other point.  And the closest the points the more similar the pair of user will be.  i.e. top  $N$  similar users to a user are just the  $N$  closest points. 
 Plain  euclidean distance  works in many cases.  If euclidean distance does not work for the problem at hand, then you can explore more complex possibilities: starting from  manhattan distance , through  minkowski distance  (combination of euclidean and manhattan distances).","As others also pointed out as long as you have numeric data (or data that can be converted to numeric) you can use some sort of distance measure between the users. 
 Simple solution is the  euclidean distance  (or some other like minkowski or manhattan). The gotcha with them that they are sensitive for having different scales in your variables. You can solve it by normalizing your data but keep in mind that in this case you will assign equal importance for each feature. You may want to adjust it manually based on your domain knowledge. 
 If you have a high number of features in a sparse space it's worth considering using  cosine similarity  that will more focus on the  direction of your data point (customer features). 
 You may also want to do  PCA  before thereby reducing your dimensionality and eliminating the fact that certain features can be similar to each other (so they correlate). 
 If you want to experiment with a more sophisticated solution you can try to do an  autoencoder . A type of neural network where your input and output are the same (user features) but in the hidden layers you suppress the dimensionality thereby having a more dense representation of the data. In that representation features may also bear a semantic meaning. On this more dense representation you can calculate again some of the distance metrics proposed at the beginning.","The main thing one thinks about is some measure of distance. Treating each variable as an axis, users can be reprsented as datapoints in a multidimensional space. Euclidean distance is the most common, but you can use Manhattan, Minkowsky, Mahalanobis, ... there are countless formulas. 
 Common alternatives are (dis)similarity measures, such as  cosine similarity  and  KL divergence . They all return measures of ""how different"" two arrays are. 
 Here  you can find the Python implementation of the most common similarity measures.","Alex, what about trying clustering. It may give you more information about your dataset.  
 So you can start with  sklearn.cluster.Kmeans  that will take some users randomly as centers of clusters and then look for other similar users checking if the distance between their features is close to central users.  
 If you are sure that clusters may be of different size (in some clusters features are very close while in some clusters features can be more distributed) you can try  sklearn.cluster.DBSCAN  where you can as well apply different  sklearn.metrics.pairwise_distances","I would use the  unsupervised  nearest neighbors (kNN) ; for instance in scikit-learn:  https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors 
 It allows you to choose between many distances (or similarity measures), including e.g. Jaccard similarity for categorical variables. 
 You can use the  fit()  method on your set, then return all K nearest neighbors to a given user with the  kneighbors()  method.","All you need to do is to develop a distance metric for every feature, that is a function which tells you how far apart two different elements of the same feature are. Once you have such a function for every feature, you can calculate the distance between users by calculating the sum between their features.",,,,73.73473284,60.37367983,60.0312599,63.62725208,58.21570017,62.56621829,,,
53954,How to make LightGBM to suppress output?,python,"As @Peter has suggested, setting  verbose_eval = -1  suppresses most of  LightGBM  output (link:  here ). 
 However,  LightGBM  may still return other warnings - e.g.  No further splits with positive gain . This can be suppressed as follows (source:  here  ): 
 lgb_train = lgb.Dataset(X_train, y_train, params={'verbose': -1}, free_raw_data=False)
lgb_eval = lgb.Dataset(X_test, y_test, params={'verbose': -1},free_raw_data=False)
gbm = lgb.train({'verbose': -1}, lgb_train, valid_sets=lgb_eval, verbose_eval=False)","Solution for sklearn API (checked on v3.3.0): 
 import lightgbm as lgb


param = {'objective': 'binary', ""is_unbalance"": 'true',
         'metric': 'average_precision'}
model_skl = lgb.sklearn.LGBMClassifier(**param)

# early stopping and verbosity
# it should be 0 or False, not -1/-100/etc
callbacks = [lgb.early_stopping(10, verbose=0), lgb.log_evaluation(period=0)]

# train
model_skl.fit(x_train, y_train,
              eval_set=[(x_train, y_train), (x_val, y_val)],
              eval_names=['train', 'valid'],
              eval_metric='average_precision',
              callbacks=callbacks)","To suppress (most) output from LightGBM, the following parameter can be set. 
 Suppress warnings:   'verbose': -1  must be specified in  params={} . 
 Suppress output of training iterations:   verbose_eval=False  must be specified in the  train{}  parameter. 
 Minimal example: 
 params = {
            'objective': 'regression',
            'learning_rate' : 0.9, 
            'max_depth' : 1, 
            'metric': 'mean_squared_error',
            'seed': 7,
            'verbose': -1,
            'boosting_type' : 'gbdt'
        }

gbm = lgb.train(params,
                lgb_train,
                num_boost_round=100000,
                valid_sets=lgb_eval,
                verbose_eval=False,
                early_stopping_rounds=100)","Follow these points. 
 
 Use  verbose= False  in  fit  method. 
 Use  verbose= -100  when  you call the classifier. 
 Keep  silent = True  (default).","I read all the answers and issues, and tried all these approaches and yet LGBM still outputs some info (which drives me crazy). If you want to completely suppress any output during the training try this out: 
 with open(os.devnull, ""w"") as f, contextlib.redirect_stdout(f):
    gbm = lgb.cv(param, lgb_dataset)",,,,,58.33606071,52.42885737,67.35846398,50,65.02429656,,,,
53388,Time Series:Outlier Detection,machine-learning,"You could compute mean and standard deviations in  sliding windows , and use those to remove outliers.  
 For example, taking windows of, say, length 100, you can compute the mean and std for for these 100 successive observations, and see whether any point falls above the 3 sigma rule. In this case, the circled outlier would still be recognized as such, while the others should not, as they are not so outlying w.r.t. neighboring data (i.e. within the window that includes them and their neighboring observations).","I don't know of any package that is capable of doing what you want to achieve, but there could be a package.  
 On way to programmatically deal with this problem is to calculate the difference between two (or multiple for smoothing) consecutive data points. Then you could do filtering based on these values by using a threshold.","You could first differentiate the series, then apply the classical method based on standard deviation you mentioned.","You could use array slicing. If the data you show here is available in an array you can find the index of the ouliers you want to remove, and simply take them out.  
 You can also split the data into two different data sets, and apply outlier detection algorithms to the dataset incluing the spike you want removed.","You can try naive anomaly detection technique (SH-ESD) developed by researchers at Twitter. Here is the link of  research paper 
and  implementation  of technique in R. There are some python libraries under development. 
 They have implemented piece-wise median method that is less sensitive to outliers and caught the perfect outliers in the time-series data. 
 The algorithm is pretty simple to understand.",,,,,55.79005814,50,53.44328211,60.50067318,64.60473482,,,,
53353,Methods to detect this kind of outliers,machine-learning,"I recently had a similar problem (removing abnormal peaks from a time series). That's what I suggest you: 
 
 Get the smoothed trend. There are several techniques you can employ, such as various forms of exponential smoothing. 
 Find the difference between actual trend observations and smoothed ones. 
 Normalize this distribution of distances (using Z-score, i.e.  sklearn 's  StandardScaler ) 
 Substitute the observations that lie k standard deviations away from the mean (that is 0). The choice of x can be arbitrary or data-driven; in my case I chose k = 3 (i.e. a very conservative anomaly removal). You can use the smoothed values as a substitute. In your case, the interpolation could be perhaps a good choice (that's up to your preference). 
 
 This will automatically remove abnormal peaks like the one you displayed.","Following is simple example to exclude outliers: 
 import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.DataFrame(np.random.randn(100, 1))
#print(df)

from scipy import stats
df1=df[(np.abs(stats.zscore(df)) < 2).all(axis=1)]
#print(df1)
plt.plot(df,'r', label='Outliers')
plt.plot(df1, 'b' ,label='Non-outliers')
plt.legend()
 
 output: 
 
 description: 
 
 For each column, first, it calculates the  z-score()  of each value in the column, relative to the column mean and standard deviation. 
 np.abs(stats.zscore(df))  takes the absolute of Z-score because the direction does not matter, only if it is below the threshold=2. 
 all(axis=1)  ensures that for each row, all column satisfy the constraint. 
 Finally,  df[]  result of this condition is used to index the dataframe. 
 
 Approach 2: 
You can use  quantile()  and  between() 
 x = pd.Series(np.random.normal(size=20)) # with outliers
#print(x)
o = x[x.between(x.quantile(.25), x.quantile(.75))] # without outliers
#print(x)
plt.plot(x,'r', label='Outliers')
plt.plot(o, 'b' ,label='Non-outliers')
plt.legend()
 
 
 I also recommend you to check it out these answers:  answer1  and  answer2","The kind of outliers you are describing are referred to as  ""point"" outliers  in literature, as they are abnormal w.r.t their individual value, unlike ""context"" and ""collective"" outliers, who are abnormal only when considering their neighbors (context).  
 For point outliers detection, there are several methods one can use, depending on whether you are interested in an  offline  or  streaming  detection, for example. 
 This repository  includes many available ready-to-use algorithms to detect such outliers, for both scenarios. 
 If you are interested in reading more details about how these techniques work, some  relevant papers  are: 
 https://ieeexplore.ieee.org/abstract/document/7954844 
 https://ieeexplore.ieee.org/abstract/document/7424283/","For each data point calculate the distance to it's left + right neighbour using euclidean distance, by comparing those distances you should see such kind of outliers.","You can also go for Boxplot graph.   Refer BoxPlot graph for more Explanation 
 This will help you out in segregating different values and in case some of the samples have outliers, it will get plotted bit far than the other samples. 
 
 So steps could be: 
 
 Draw boxplot of your 2D data.  How to create BoxPlot ? 
 Intuitively select the acceptable range i.e. samples outside-dimensions(range) consider as an outliers. 
 Drop samples exist outside such range. 
 
 
 JFI:   
 
 
 Generally we consider outliers are those values, which doesnt fall
  with in Variance range of the data. Though I'm not the specialist,
  but boxplot consider the same thing while deciding the Outliers. 
 ""Intuitively select the acceptable range"", we can also say Z-score() in statistical term.",,,,,50,57.3155333,71.26416394,64.42371983,56.64073365,,,,
53181,Large no of categorical variables with large no of categories,classification,"I used to work a lot with such data in the past. 
When you have many categorical features with high cardinality,  you must avoid one-hot encoding  them because it consumes too much memory and above all trees built from them will be too deep and worse than trees built without one-hot encoding. 
You need a Tree based model library implemented with native categorical feature support (i.e. not needing the categorical features to be one-hot encoded). 
I suggest you to use one of those 4 implementations : 
-  CatBoost 
-  LightGBM 
-  H2O GBM 
-  H2O Random Forest 
Scikit learn and XGBoost implementations still need one-hot encoded categorical variables so I don't recommend using one of these libraries if your dataset has high cardinality categorical variables (i.e. with more than about 10 levels/categories). 
See  this  :   
 
 LightGBM offers good accuracy with integer-encoded categorical
  features. LightGBM applies Fisher (1958) to find the optimal split
  over categories as described here. This often performs better than
  one-hot encoding.","""I was thinking of keeping these top k frequently occurring values and encoding all the other values as another category ""others"". Would it be okay to do so?"",  for me also it should be ok. 
 
 I have few more comments: 
 
 You can also try with new features. That is combine multiple
  categorical features, instead of using all of them independently. 
 Since you have many categorical features, one hot encoding of every categorical feature will generate a huge list of features, which
  may overfit your model. You can also try without One Hot Encoding as
  it's not always mandatory for Decision tree.","Hi and welcome to the forum. Just as an idea: 
 The strategy you suggested (extracting important classes in the X and leave the rest as ""other"") may work and it‘s worth a try. Make sure you don‘t throw away information by keeping the levels in the ""other"" category.  
 The alternative is (of course) to one-hot encode all classes.  
 In any case, I would check if regulation (by L2 or L1 norm) is useful. 
 Some background: L1 regulation can shrink features to zero, L2 regulation can shrink features but they never become zero. 
 So say you have a lot of one-hot encoded features but you have no idea which one are really important. Just let the computer do the job of selecting features by L1 regulation. 
 Method wise, the problem sounds like a candidate for boosting. Boosting is similar to random forest, but in addition, the algorithm tried to focus on especially hard to predict rows/observations. This is done by growing a lot of very small trees. 
 Prominent algorithms are LightGBM, Catboost, XGboost. They all offer support for regulation. 
 So I would start with ""a lot"" one-hot encoded features and boost with L1 regulation to get rid of irrelevant features.","Encoding categorical variables so that  you do not lose information and do not add non-existent relationships  between the categories is fundamental to achieving good performance from your model. 
 You can not simply LabelEncode your categories because they add non-existent relationships. For example, if you have a feature named 'color' which takes on three values - Red, Blue and Green and you encode Red as 1, Blue as 2 and Green as 3, this will imply relationships such as  
 $$Red < Blue < Green$$ 
 $$Blue = (Red + Green)/2$$ 
 Because of the  curse of dimensionality , you can not use OneHotEncoder as that will inflate your feature space beyond your model's comprehension because of a relatively smaller dataset. 
 The approach of taking only  top k frequently  occurring categories will be suboptimal as you loose information. A better approach would be to use a  FrequencyEncoder  which replaces categories with their frequencies (or counts) of occurrence. 
 More sophisticated approaches  like  Target Encoding and Contrast Coding Schemes  which take target variable into account when encoding categories are known to perform better in cases of high cardinality categories. 
 If some of your categories have string values which are not domain-specific you can convert them into meaningful vectors using pre-trained  Word2Vec . If any of your categories contain long text (e.g. description of an event), you can  train your own Word2Vec  on sentences built from values of such a category. By taking a weighted-average of the vectors from a sentence, you can encode sentences. 
 As far as models are concerned,  with the right kind of encoding, Random Forests and Gradient Boosted Machines will work just fine . SVMs and Vanilla Neural Networks are also worth considering. To boost your accuracy you can definitely use  XGBoost .","If you're using python and sklearn I'd suggest you take a look at  http://contrib.scikit-learn.org/categorical-encoding/ 
 There's a large number of different encoding schemes you can experiment with. Many of them have parameters such as  smoothing  for the TargetEncoder which you can select using hyper-parameter optimisation to ensure you don't overfit.",,,,,62.24977145,56.47790926,51.25022794,61.79326814,53.6994568,,,,
52859,30 Years Of Excel Test Data,python,"I see three main ways of comparing columns: automated comparisons of column names (e.g. regex,  Levenshtein distance), comparing content (e.g. compare mean and standard deviation of the data for the column; if the mean value of a column is 10,000 then it probably isn't recording Front Axial Temperature), and manual comparison. You can combine these, for instance clustering on column names and content, then manually looking at the contents of each cluster. The smaller the number of different column names, the more you can rely on manual examination. You may also be able to get other metadata sources, such as looking for documentation for whatever process generated the files.","It is difficult to give you a good answer without knowing the dataset. However this is how I would approach the problem:  
 Create the base columns you are using for your final headers. Loop through the headers of each file and match them to the base column with the most common amount of characters.","To me, the fastest way to analyze tabular data (rows and columns) from the same or different Excel files is Tableau. 
 You can get Tableau for free (but can't save your files). 
 Simplified: load your Excel files and create a key to join (e.g. a new column called 'axel temp font'). Join on this key. Once you've done this successfully you can bring the columns and rows (called measures in Tableau) into your view to analyze. 
 Bonus trick: once you're happy with your view (columns and rows) you can download it as a CSV. Now you have exactly the data you want in CSV.  
 https://onlinehelp.tableau.com/current/pro/desktop/en-us/joining_tables.htm 
 Good luck,","How many useful Excel files are we talking about? More importantly, how many potential variants for the columns names?  
 Because if it's less than say a thousand, you're probably better off manually curating the columns names: it's going to take less time and provide you with more accurate data than implementing and testing a sophisticated string matching system. You can't rely on the automatic matching, so you would need to evaluate how correct the result is. Using any predefined string similarity method will leave you with many mistakes. 
 I would proceed in the following way: 
 
 Automatically extract all the column names from all the files 
 Manually group the ones which represent the same information 
 Automatically replace the variants with a standardized version of the name","After a few weeks of looking at what the data consists of, I wrote some modules in python to automate most of the work. These modules are using pyexcel and openpyxl (which has a faster read function). I was able to write some functions to match specific templates within a percentage of error using a collection of keywords. Below are the steps I took breaking the data set down. 
 1) Remove all non excel data from the file systems. 
 File Count : 100,300 -> 30,000
 
 2) Remove all files containing specific keywords in the filename I did not need. 
 File Count : 30,000 -> 23,000
 
 3) Remove all files that do not match a specific pattern in the template. This is the most expensive operation as each file needed to be opened (via python) and checked for specific row and column values. I also had an issue with Windows 10 because the file paths exceeded 260 characters in most cases. To resolve this conflict, I put everything on an external drive and transferred the files to a Linux VM. 
 File Count : 23,000 -> ~1000
 
 4) At this point I can extract data using a similar method to the one above, there are multiple excel templates so step 3 will need to be repeated again. 
 File Count : 1000 -> ?
 
 *This worked for me and I abstracted the logic so it isn't tied to my data set. I will post the code on  GitHub  (repo: Anacell) once the project is done and I removed all proprietary information. 
 **I also wanted to mention that there were a few inaccuracies in my initial question. As the years progressed data became more structured and formatted with templates. While there are multiple undocumented templates it was easier to take advantage of them (once found) than if nothing was structured.",,,,,50.57759864,50,55.58861121,53.96766989,55.57861588,,,,
52628,Generate timeseries data,time-series,"ATM I know of  TSimulus  and  TimeSynth  to generate data programatically in a controlled manner (instead of generating random data). 
 TSimulus allows to generate data via various  generators . 
 TimeSynth is capable of generating  signal types 
 Harmonic functions(sin, cos or custom functions)
Gaussian processes with different kernels
    Constant
    Squared exponential
    Exponential
    Rational quadratic
    Linear
    Matern
    Periodic
Pseudoperiodic signals
Autoregressive(p) process
Continuous autoregressive process (CAR)
Nonlinear Autoregressive Moving Average model (NARMA)
 
 and  noise types 
 White noise
Red noise
 
 If you are looking for a graphical way to generate data  TimeSeriesMaker  is the only tool able to do this.","If you know Python use Faker. 
 https://github.com/joke2k/faker","You may apply  Wolfram Language  to your project. There is a free  Wolfram Engine  for developers and if you are developing in Python then with the  Wolfram Client Library for Python  you can use these functions in Python. 
 A good place to start is the  Time Series Processing  guide or the  Random Processes  guide; both of which contain a link to the  Time Series Processes  guide. 
 Use  RandomFunction  with any of the processes you wish to simulate.  For example, with a  MAProcess 
 SeedRandom[9]
res1 = RandomFunction[MAProcess[2, {.3, -.5}, 1], {0, 100}]
 
 
 
 
 The returned  TemporalData  object is known throughout the language so further process or analysis can continue from here. For example, visualise the result with  ListLinePlot 
 ListPlot[res1, Filling -> Axis]
 
 
 
 
 The values can be directly viewed with the  ""Values""  property.  Using  Short  to limit the output. 
 res1[""Values""] // Short
 
 
 {2.00335, 1.15942, <<98>>, 2.7685}
 
 
 Multiple paths can be generated as well. For example, generate two paths with  ARMAProcess 
 SeedRandom[11]
res2 = RandomFunction[ARMAProcess[0.1, {0.8, 0.1}, {-0.4}, 0.1], {0, 100}, 2]
 
 
 
 
 Visualise with  ListLinePlot 
 ListLinePlot[res2]
 
 
 
 
 ""Paths""  property will return the paths as list.   
 res2[""Paths""] // Short
 
 
 {{{0,0.275976}, <<99>>, {100, 0.766177}}, {<<1>>}}
 
 
 ""Components""  property returns  TemporalData  objects for each path. 
 res2[""Components""]
 
 
 
 
 ListLinePlot /@ res2[""Components""]
 
 
 
 
 Hope this helps.","If your data is financial time series (or similar to it) and your language of choice is Octave/Matlab, then you might be interested in a recent blog post of mine. In this post there are short descriptions of 4 different ways of creating synthetic time series data with links to other posts which contain both code and more detailed descriptions of the methods. 
 The blog post is  here .","If you want to train a model with simulated timeseries data, you first need to obtain the characterstics/properties of your underlying data (your ""sample""). Means you have to check /make assumptions about the mean and variance.  Remember you use the returns/relative change -- NOT the actual LEVEL INPUT in time series!  
 Why? Cause level time series has ""memory"" effect - this means you will get correlations even if you are comparing 2 time series that are drawn from two independent and random distributions. This ""mistake"" is often called in statistics ""spurious regression"". 
 Check  for more details/examples.  
 However as mentioned before you can use: 
 
 (G)ARCH /ARMA -- Python implementation  here 
 
 Maybe check first if your data follows such a process 
 
 
 or: 
 
 Simulate data using Monte Carlo Simulation --  Python implementation  here","@vipin bansal  You can also try out  TimeSynth  .
Allows you to create both signals and noise e.g. Gaussian Noise, Sinusoidal signal.",,,,56.19393545,50,50.80777184,51.40536963,60.07042628,50,,,
52445,Rationale behind most published works in medical imaging trying to reduce false positives,image-classification,"TL;DR: diseases are rare, so the absolute number of false positives is a lot more than that of false negatives. 
 Let's assume that our system has the same false positive and false negative rate of 1% (pretty good!), and that we're detecting the presence of new cancers this year: 439.2 / 100,000 people, or 0.5% of the population. [ source ] 
 
 No cancer, no detection: 99.5% x 99% = 98.5% (98.505%) 
 No cancer, detection: 99.5% x 1% = 1.0% (0.995%) 
 Cancer, detection: 0.5% x 99% = 0.5% (0.495%) 
 Cancer, no detection: 0.5% x 1% = 0.005% 
 
 So we can see that we have a problem: for everyone who has cancer, two people who didn't have cancer wind up with invasive surgery, chemotherapy or radiotherapy.  
 For every person who fails to have a present cancer detected, two hundred people receive actively harmful treatment they didn't need and can't really afford.","You know the story of the boy who cried wolf, right? 
 It's the same idea. After some classifier gives false alarms (cries wolf) so many times, the medical staff will turn it off or ignore it. 
 ""Oh,  this  again! NOPE!"" 
 At least with the bioengineering group I've worked with, the emphasis is on reducing FPR specifically because the goal is to make a tool that will alert physicians to potential pathology, and they've told us that they will ignore a product that cries wolf too much. 
 For a product that aids physicians, we have to appeal to their psychology, despite the legitimate argument that missing the wolf on the farm is worse than crying wolf.  
 Edit : Decreasing false positives also has a legitimate argument. If your computer keeps crying wolf while getting the occasional true positive (and catching most of the true positives), it's effectively saying that someone might be sick. They're in the hospital. The physician knows that the patient might be sick.","Summary:  the question probably* isn't whether  one  false negative is worse than  one  false positive, it's probably* more like whether 500 false positives are acceptable to get down to one false negative. 
 * depends on the application 
 
 Let me expand a bit on @Dragon's answer: 
 
 Screening  means that we're looking for disease among a seemingly healthy population. As @Dragon explained, for these we need extremely low FPR (or high Sensitivity), otherwise we'll end up with many more false positives than true positives. I.e., the Positive Predictive Value (# truly diseased among all diagnosed positive) would be inacceptably low.   
 Sensitivity (TPR) and Specificity (TNR) are easy to measure for a diagnostic system: take a number of truly (non)diseased cases and measure the fraction of correctly detected ones.   
 OTOH, both from doctors' and patients' point of view, the  predicitive values  are more to the point. They are the ""inverse"" to Sensitivity and specificity and tell you among all positive (negative) predictions, what fraction is correct. In other words, after the test said ""disease"" what is the probability that the patient actually does have the disease.  
 As @Dragon showed you, the incidence (or prevalence, depending on what test we're talking about) plays a crucial role here. Incidence is low in all kinds of screening/early cancer diagnosis applications. 
To illustrate this,  ovarian cancer screening for post-menopausal women has a prevalence of 0.04 % in the general population and 0.5 % in high-risk women with family history and/or known mutations of tumor suppressor genes BRCA1 and 2 [Buchen, L. Cancer: Missing the mark. Nature, 2011, 471, 428-432]   
 So the question is typically not whether  one  false negative is worse than  one  false positive, but even 99 % specificity (1 % FPR) and 95 % sensitivity (numbers taken from the paper linked above) then means roughly 500 false positives for each false negative.  
 As a side note, also keep in mind that early cancer diagnosis in itself is no magic cure for cancer.  E.g. for breast cancer screening mammography, only 3 - 13 % of the  true  positive patients actually benefit from the screening . 
So we also need to keep an eye on the number of false positives for each  benefitting  patient. E.g. for mammography, together with  these numbers , a rough guesstimate it that we have somewhere in the range of 400 - 1800 false positives per benefitting true positive (39 - 49 year old group).    
 With hundreds of false positives per false negative (and also maybe hundreds or even thousands of false positives per patient benefitting from the screening) the situation isn't as clear as ""is one missed cancer worse than one false positive cancer diagnosis"": false positives do have an impact, ranging from psychological and psycho-somatic (worrying that you have cancer in itself isn't healthy) to physical risks of follow-up diagnoses such as biopsy (which is a small surgery, and as such comes with its own risks). 
Even if the impact of  one  false positive is small, the corresponding risks may add up substantially if hundreds of false positives have to be considered.   
 Suggested reading: Gerd Gigerenzer: Risk Savvy: How to Make Good Decisions (2014). 
 Still, what PPV and NPV are needed to make a diagnostic test useful is highly dependend on the application. 
As explained, in screening for early cancer detection the focus is usually on PPV, i.e. making sure you do not cause too much harm by false negatives: finding a sizeable fraction (even if not all) of the early cancer patients is already an improvement over the status quo without screening. 
OTOH,  HIV test in blood donations focuses first on NPV (i.e. making sure the blood is HIV-free). Still, in a 2nd (and 3rd) step, false positives are then reduced by applying further tests before worrying people with (false) positive HIV test results. 
 Last but not least, there are also medical testing applications where the incidences or prevalences aren't as extreme as they usually are in screening of not-particularly-high-risk populations, e.g. some differential diagnoses.","From a personal perspective, rather than a data science experience, a false positive has a higher impact on the patient's quality of live than a false negative (at least in most applications of medical image processing. We're not talking about lab results here). 
 Let's look at a concrete example:  tumor screening . 
 A false negative  means that an early-stage tumor has more time to grow and develop into malicious cancer. Overall this process takes a long time and each subsequent screening has a higher chance to detect it, but realistically the long-term health of a patient suffers. 
 Additionally, there's always a human involved in diagnosing. Medical image processing at it's current technological stage is meant to be a  help  for medical personell,  not a substitute . It's often meant to point out lesions or changes in tissue that are so subtle that a human might overlook them. There's no chance a doctor would overlook an advanced stage tumor. They don't need image processing for that. 
 In terms of medical procedures, if a tumor doesn't become inoperable before the next screening, there's no big difference between removing an early-stage tumor or one that had a little more time to grow. The amount of tissue removed is more, but the kind of operation is often the same. (This assumes that the patient does regular health screenings.) 
 A false positive  has many implications that are not all directly related to an ailment: 
 
 Additional procedures. After an imaging process yields a positive result, more tests are conducted for which blood or tissue is extracted (biopsy). Objectively speaking the body of the patient is damaged to be able to verify the imaging result. 
 Fear. Lab tests take time. The person affected often lives through several days, sometimes weeks, of uncertainty weather or not the lesion is actually cancer. Many people who have experienced such a false positive describe this event as ""traumatizing"" and suffer from health-related anxiety for a long time. 
 Time investment. If verifying the imaging result via lab tests or similar takes several examinations, the patient and doctors have to invest time for them. Even if it takes only one test, there are several people involved, including nurses, doctors and lab technicians. In a time when doctors are chronically overworked, this should be avoided if possible. 
 Unnecessary medication. In the worst case the patient is treated for an ailment they don't even have and their body is put under unnecessary strain by side effects of medication. 
 Loss of effect. Medical personell will ignore true positive results if a procedure yields too many false positives (as explained in other answers). 
 
 This risk-benefit-evaluation shows that a false negative includes less risk for a patient than a false positive. Therefore the priority of reducing false positives is generally higher.","Clinician's time is precious 
 From within the field of medicine, clinicians often have a wide variety of illnesses to try to detect and diagnose, and this is a time consuming process. A tool that presents a false positive (even if at a low rate) is less useful because it's not possible to trust that diagnosis, meaning every time it makes that diagnosis, it needs to be checked. Think of it like the WebMD of software - everything is a sign of cancer! 
 A tool that presents false negatives, but always presents true positives, is far more useful, as a clinician doesn't need to waste time double-checking or second guessing the diagnosis. If it marks someone as being ill with a specific diagnosis, job done. If it doesn't, the people which aren't highlighted as being ill will receive additional tests anyway. 
 It's better to have a tool that can accurately identify even a single trait of an illness, than a tool that maybe fudges multiple traits.","False Positive Rate (FPR) also known as false alarm rate (FAR);  A large False Positive Rate can produce a poor performance of the Medical Image Detection System.  A false positive is where you receive a positive result for a test, when you should have received a negative results.  For example A pregnancy test is positive, when in fact the person isn't pregnant.","In all likelihood, everyone on this thread already knows that this is a problem at the core of Bayesian analysis. Solely for the benefit of future pilgrims who might think of false positives as somehow solely a problem in radiology, I hope this comment will provide a little more general perspective.","Several studies demonstrated [1, 2] that radiologist fatigue levels and performance are related to environmental factors such as number of False-Positives and False-Negatives. However, the number of False-Positives is  usually  higher than the number of False-Negatives if we consider solely the diagnosis ( e.g. , breast, lung cancers, etc.) over medical imaging analysis. This is due to the fact that typically, clinicians do not want to take risks [2] in the presence of an ambiguous case. Thus, clinicians will diagnose that case with their higher probability for the worse case scenario. In breast cancer diagnosis, such medical decision will translate into a patient's biopsy in which patients are physically and mentally affected, as well as affected by healthcare costs. On the other hand, False-Negatives are severe medical errors that can lead to patient mortality. 
 Although False-Negatives are more severe, the improvement window is much lower. For instance, in breast cancer diagnosis, about 8% to 10% of cases are yielding False-Negatives and between 30% to 50% are yielding False-Positives [3]. These results show a much more promising output relation for working on False-Positives than False-Negatives. Moreover, it is much safer for the Data Science community to work on False-Positives improvement for now, as it instantiates lower levels of patient mortality. 
 References 
 [1] Stephen Waite, Srinivas Kolla, Jean Jeudy, Alan Legasto, Stephen L. Macknik, Susana Martinez-Conde, Elizabeth A. Krupinski, Deborah L. Reede, Tired in the Reading Room: The Influence of Fatigue in Radiology, Journal of the American College of Radiology, Volume 14, Issue 2, 2017, Pages 191-197, ISSN 1546-1440, DOI:  https://doi.org/10.1016/j.jacr.2016.10.009 
 [2] Francisco Maria Calisto, Carlos Santiago, Nuno Nunes, Jacinto C. Nascimento, Introduction of human-centric AI assistant to aid radiologists for multimodal breast image classification, International Journal of Human-Computer Studies, Volume 150, 2021, 102607, ISSN 1071-5819, DOI:  https://doi.org/10.1016/j.ijhcs.2021.102607 
 [3] Bray F, Ferlay J, Soerjomataram I, Siegel RL, Torre LA, Jemal A. Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA Cancer J Clin. 2018 Nov;68(6):394-424. doi: 10.3322/caac.21492. Epub 2018 Sep 12. Erratum in: CA Cancer J Clin. 2020 Jul;70(4):313. PMID: 30207593.",,54.94216959,54.18177277,62.26745669,64.23058651,53.69625794,66.39873859,55.4072392,61.15574823,
52066,Why decision tree needs categorical variable to be encoded?,machine-learning,"...why is encoding needed on categorical variables? 
 
 That isn't true; decision trees can be built on both continuous and categorical features.  ( Why don't tree ensembles require one-hot-encoding?  )   Some implementations , however, do not support categorical variables (notably  sklearn  ( for now ,  update ) and  xgboost  ( their old politics ,  update )). 
 Now, there is a question of efficiency: the number of bipartitions of the set of categories is exponential in the number of categories, so a complete search of the possible splits is only practical for categorical variables with few categories. 
There turns out to be a (surprising?) simplification though: if the underlying problem is a regression with MSE, or a binary classification with cross-entropy or Gini index, then the optimal split can be found by ordering the categories according to their average response value and treating it now as an ordinal variable split.  (That said, still having many categories, especially small ones, might lead to heavy overfitting.)  See  Elements of Statistical Learning , section 9.2.4. 
 Some implementations perform the exhaustive bipartition search but cap the number of categories allowed.   LightGBM  and  rpart  perform the ordered search.  ( some R discussion ,  LightGBM ). 
 All of the above is based on CART-based trees, but there is also another thread of trees by Quinlan (IDE, C4.5, etc.).  Those will create higher arity splits for categoricals, with one child node for each category.","As I understand it, decision trees use the rules  < threshold_value  or  >= threshold_value  to group observations together, where  threshold_value  is the value of a variable which minimises the cost function for a particular split.  (It's equally likely that the tree uses  <=  and  >  but that's just semantics). 
 This obviously works fine for numeric variables, but it does not work well with categorical variables - especially when the categorical variable cannot be ordered in a meaningful way. Therefore we need to numerically encode the categorical variable.",This is needed because not all the machine learning algorithms can deal with categorical data. Many of them cannot operate on label data directly. They require all input variables and output variables to be numeric. That's why We need to encode them.,"A categorical variable should be encoded as number, anyway. You can encode it as sequence: 1, 2, 3... for example. It this case it's called ordinals. It's always possible to encode categorical variable as ordinals by mapping a number for each category. But it doesn't always make sense, because categories aren't always sequential. 
 
 For example, if you have 'low', 'middle', 'high' as values of categorical variable, it's reasonable to treat it as ordinals and encode it as 1, 2, 3. So, if algorithm splits the variable, and {1} goes to one branch and {2,3} to the other, it'd reasonable because 'middle' and 'high' are clearly separated from 'low' and they can form two different categories. 
 But if the values are 'spoon', 'fork', 'knife', it doesn't make sense to encode it as numbers, because tree algorithm would split the numerical value of the variable and it doesn't make sense why 'knife' and 'fork' should go to one branch and 'spoon' to the other. But that would be the case, if they are encoded as 1, 2, 3 and the it's split as {1} in one branch and {2, 3} in the other. So, in this case it's better to one-hot encode them. This way algorithm would treat them as non-sequential binary variables and they would have the same chance to fall into one separate branch with each other.","From what I understand from the sklearn long trove of discussion about implementing this in their base tree model: 
 To define an optimal way to split the categorical data into two groups you would have to look at all possible split. Enumerating all possible split grows up with 2 to the power of the number of categories wich is rapidly intractable. This is why you need specific algorthms to proceed to near-optimal splits / perform encoding to avoid this enumeration problem.",,,,,65.77444451,67.24135832,58.75423119,64.31256009,54.02556595,,,,
51520,Time series - is it necessary to retrain the model when new time series data is present,machine-learning,"It is good practice to retrain model once in a while. That ""in a while"" depends on the specific task, of course. This is due to the fact that countless unmeasurable things change continuously. 
 Think for example about a model to forecast companies' revenues: the rules of the game change continuously, the market today is not like the market one year ago, not to mention five years ago. This is the first example that I thought about, but it's true for pretty much any possible application of ML. 
 That is why it was observed that models' performance deteriorates in time. For that reason, it is good practice to keep your models updated with fresh data.","In some cases where you know that the underlying process slightly changes, a good course of action is to have an adaptive forecasting model. Thus, model parameters are slightly re-calibrated at each new observation (analogous to online learning). An example would be forecasting power output for a wind turbine: due to some factors such as blades getting dirty and weather slowly changing, it is actually in your best interest to have a time adaptive model. 
 Also, local learning algorithms are essentially re-trained at each new observation, since kernel weights are re-estimated. This includes algorithms like k-NN, kernel regression, local linear regression, etc. In this case the training is always conducted when the prediction is needed; the only requirement is that you update your historical observations. 
 Regarding retraining batch models I have not seen general consensus, only empirical evidence. In some applications, such as electricity price forecasting, a lot of researchers re-train models using a rolling approach. The length of the rolling window usually is 2 times the length of the longest seasonal period. Sometimes this approach is coupled with exponentially decaying weights. However, I have noticed that this applies usually to researchers from econometric background rather than machine learning/ computational intelligence.","Adding the citation: 
 
 ""For time series models, this is not the case. Instead, we have to
retrain our model every time we want to generate a new forecast."" 
 
 Taken from here -  3 facts about time series forecasting","You need to retrain your model every time you want to generate a new prediction: 
 For most ML models, you train a model, test it, retrain it if necessary until you’ve gotten satisfactory results, and then evaluate it on a hold out data set. After you’re satisfied with the model’s performance, you then deploy it to production. Once in production, you score new data as it comes in. Eventually after a few months, you might want to update your model if a significant amount of new training data comes in. Model training is a one time activity, or done at most at periodic intervals to maintain the model’s performance to take int account new information. For instance, the visual properties of cats are stable over time. We don’t expect cats to look different next week, or next year, or even ten years from now. Given enough data, the model we trained this week is good enough for the foreseeable future as well. 
 For time series models, this is not the case. Instead we have to retrain our model every time we want to generate a new forecast. To understand why this happens consider this...trends, and related seasonal variations, change over time!!","You are right, as usual in ML, it entirely depends on your data and you model ability to generalise. 
 There is no general rule. Ideally models should be recalibrated often. That notion of 'often' entirely depends on your problem. But in practice there are statistical and operationnal considerations that should be taken into account.  
 Statistical considerations revolve around the availability of new data (if you have a yearly data collection process, it would be a bad idea to recalibrate mid-year), and the evolution of the underlying data generation process. You can try to observe the evolution of the underlying process trough studying basic statistics about the distribution of your variable / of your output, but this will only give you a partial answer. The only real answer you can get to those statistical considerations is to recalibrate the model and compare the results to the previous one... so at this point you might as well use the new model (if you haven't any operrational considerations).  
 Operrationals considerations revolve aorund the difficulties of the recalibration process and putting the new model into production. Is this process automated ? does it require a specific data exctraction ? Is the recalibration easy to do ? Are the ressources (human / hardware) available to do that ?  
 Note that some constraint may also be regulatory : in some domains there are some internal guidelines / external regulation that give a minimal frequency of model recalibration.",,,,,56.22151777,54.42727625,72.06298297,65.67847073,53.03172425,,,,
51358,How to find the relationship between two categorical variables?,statistics,Traditional statistics like Chi-squared tests and Cramer's V can be used to determine relationship between two categorical features.,"It can be a logistic regression, with Graduation as a binary dependent variable (1 for Yes and 0 for No) and Race as independent.  You would encode Race as five separate dummy (1/0) variables, with each subject in your dataset having a 1 in one of those variables and 0 in the other four.  Then you would run the regression with one of those five dummies omitted from the regression as the reference category.  You would use the output to draw conclusions, such as odds of graduating for students of race A are X% higher than the odds for students of Race B.","There is a Scikit-Learn module ""LogisticRegression"" which will easily perform this calculation (albeit its only for Python not R, which might be a problem). The advantage of Scikit is that if you wanted to perform a linear SVC ... its easy because it is just another module. Furthermore, if you wanted to assess the data within an ML framework, its an easy extension. Again, you would need to learn a bit of Python to import your data.","As bradS mentioned, you can use Cramers_V. 
 def cramers_v(confusion_matrix):
    """""" calculate Cramers V statistic for categorial-categorial association.
        uses correction from Bergsma and Wicher,
        Journal of the Korean Statistical Society 42 (2013): 323-328
    """"""
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))
    rcorr = r - ((r-1)**2)/(n-1)
    kcorr = k - ((k-1)**2)/(n-1)
    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))","One suggestion is to label encoding on these categorical variables and find the correlation.
Or I believe we can use count plot for the same.",,,,,66.65777134,54.31837559,50,50,67.14243622,,,,
49185,Categorical vs continuous feature selection/engineering,machine-learning,"For encoding categorical features, there is two common ways: 
 
 Ordinal encoder 
 
 This is the way you mentioned as 'encoded as integers'. In this method, an integer starting from 0 is assigned to each category. The problem of this method is that it randomly prioritize categories. So in cases when there is no priority among categories, this encoding is meaningless as you mentioned. The only case it work is when assigning larger integer to some categories is meaningful. 
 
 One-hot encoder 
 
 This method makes a feature vector (one-hot vector) for each categorical feature which is the same size as the number of categories. The method assigns each component of the vector to one of the categories. For each data sample, it assigns 1 to component which its corresponding category is present at the sample and assigns 0 to other components. The benefit of this method is that unlike ordinal encoder it does not prioritize any category. 
 So in your case, I highly recommend that you use one-hot encoder.","What you are looking for are called  dummy variables , they convert your categorical data into a matrix where the column is 1 if the person belongs to a category or 0 otherwise. 
 The variable ID is not convertible because you don't want your model to overfit over your ID data (meaning: You don't want your model to remember the result for every ID, you want your model to be general). 
 import pandas as pd
dataset2 = pd.get_dummies(dataset)","As others have said, dummy variables is one method. Another method is to take quantitative statistics from the populations having that property. For instance, you can create a ""marital situation average"" column, and populate it with the average value of the target variable among people with the same marital situation as that subject.  
 If you are using a tree method, simply assigning integers to each category will approximate dummy variables, especially if there are only a few categories. For instance, if the only categories for marital situation are Married, Single, Divorced, and Widowed, and you assign them 0, 1, 2, 3 respectively, then the only possible splits are Married vs. Everything else, Widowed vs. Everything else, or Married/Single vs Divorced/Widowed. So two thirds of the splits are effectively dummy variables, and the last one will turn into a dummy variable as soon as you split on that variable again.","One possibility to deal with categorical inputs is to introduce the category input vector  $\boldsymbol{t}$ . The category input vector of the  $n^{\text{th}}$  observation is given by 
 $\boldsymbol{t}_n=[t_{1n}, t_{2n},...,t_{Kn}],$  in which  $K$  is the number of categories. If the continuous input vector  $\boldsymbol{x}_n$  is belonging to category  $k$ , then  $t_{1i}=1$  for  $i=k$  and  $t_{1i}=0$  for  $i\neq k$ .  
 This type of encoding is called  one hot encoding  for classification.","There could be a number of ways of handling categorical data but what I have seen so far is to create a numeral mapping of the categorical data and then one-hot-encode the mappings to feed into the neural network.  
 If you are working with  Keras , you can use the  to_categorical  function to transform your mappings accordingly.  
 >>> from keras.utils import to_categorical

>>> y = [0,1,0,1,1]
>>> oh_y = to_categorial(y, num_classes=2)
>>> print(oh_y)

[[1,0],[0,1],[1,0],[0,1],[0,1]]
 ```",,,,,52.8503665,51.60579079,50,52.5902441,56.3151067,,,,
48531,How much of data wrangling is a data scientist's job?,data-wrangling,"This is a situation that many blogs, companies and papers acknowledge as something real in many cases. 
 In this paper  Data Wrangling for Big Data: Challenges and Opportunities , there is a quote about it 
 
 data scientists spend from 50 percent to 80 percent of their time 
 collecting and preparing unruly digital data. 
 
 Also, you can read the source of that quote in this article from The New York Times,  For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights 
 Unfortunately, the real world is not like Kaggle. You don't get a CSV or Excel file that you can just start the Data Exploration with a little bit of cleaning. You need to find the data in a format that is not suitable for your needs. 
 What you can do is make use of the old data as much as you can and try to adapt the storing of new data in a process that will be easier for you (or a future colleague) to work with.","Feels like most of the work is not related to data science at all. Is this accurate? 
 Yes 
 I know this is not a data-driven company with a high-level data engineering department, but it is my opinion that data science requires minimum levels of data accessibility. Am I wrong? 
 You're not wrong, but such are the realities of real life. 
 Is this type of setup common for a company with serious data science needs? 
 Yes 
 
 From a technical standpoint, you need to look into ETL solutions that can make your life easier. Sometimes one tool can be much faster than another to read certain data. E.g. R's readxl is orders of mangnitudes faster than python's pandas at reading xlsx files; you could use R to import the files, then save them to a Python-friendly format (parquet, SQL, etc). I know you're not working on xlsx files and I have no idea if you use Python - it was just an example. 
 From a practical standpoint, two things: 
 
 First of all, understand what is technically possible. In many cases,
the people telling you know are IT-illiterate people who worry about
business or compliance considerations, but have no concept of what is
and isn't feasible from an IT standpoint. Try to speak to the DBAs or
to whoever manages the data infrastructure. Understand what is
technically possible. THEN, only then, try to find a compromise. E.g.
they won't give you access to their system, but I presume there is a
database behind it? Maybe they can extract the data to some other
formats? Maybe they can extract the SQL statements that define the
data types etc? 
 Business people are more likely to help you if you can make the case that doing so is in THEIR interest. If they don't even believe in what you're doing, tough luck...","Feels like most of the work is not related to data science at all. Is this accurate? 
 
 This is the reality of any data science project. Google actually measured it and published a paper ""Hidden Technical Debt in Machine Learning Systems""  https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf 
 
 Result of the paper reflects my experience as well. Vast majority of time is spent in acquiring, cleaning and processing data.","Feels like most of the work is not related to data science at all. Is this accurate? 
 Wrangling data is most definitely in the Data Scientist job description.  At some level you have to understand the data generating process in order to use it to drive solutions.  Sure, someone specialized in ETL could do it faster/more efficient, but being given data dumps is not uncommon in the real world.  If you don't like this aspect of data science, there may be an opportunity to work more closely with IT resources to get the data properly sourced into a warehouse you have access to.  Alternatively, you could find a job that already has data in better order. 
 I know this is not a data-driven company with a high-level data engineering department, but it is my opinion that data science requires minimum levels of data accessibility. Am I wrong? 
 I think the minimum level is txt files.  If you have access to the data via text files, you should have access to the data in the database (push back on this with superiors). 
 Is this type of setup common for a company with serious data science needs? 
 Yes.  You are the data SCIENTIST; you are the expert.  It is part of your job to educate others on the inefficiencies of the current data structure and how you can help.  Data that isn't usable isn't helping anyone.  You have an opportunity to make things better and shape the future of the company.","As another recent starter in Data Science, I can only add that I don't think you're experience is unique, my team of about 10 apparently hasn't done any DS in over a year (one small project that occupied 2 of the team). This is due to the promise of an effective pipeline the team's been working on, but still just isn't quite delivering the data. Apparently retention has been fairly poor in the past and there's continuous promise of a holy-grail MS Azure environment for future DS projects. 
 So to answer: 
 1) Yes totally accurate 
 2) No you're correct, but it's an uphill battle to get access to the data you want (if it even exists). 
 3) I'm sure there's companies out there who are better than others. If you can't stand it at your current company, 2 years is a decent length of time, start looking for brighter things (be careful how you phrase your desire to leave your current job, something like ""looking to work with a more dynamic team"" would sound better than ""my old company won't give me data"").","If you look at this from the perspective of  ""this isn't my job, so why should I do it""  then that's a fairly common, general problem not specific to data science. Ultimately, your job is to do whatever the boss tells you to do, but in practice there is little reason for the boss to be dictatorial about this and usually they can be persuaded. Or at least they will give you a sincere explanation of why it has to be that way. But as far as appealing to authority, there is no official definition of ""Data Science"" that says you can only do at most X% data cleaning. The authority is whoever is paying you, so long as they have the legal right to stop paying you. 
 You could also look at it from another perspective: Is this a good use of your time? It sounds like you took a job to do some tasks (which you mean by ""data science"") but you are having to do another thing (which you call ""data wrangling""). Job descriptions and personal feelings are a bit beside the point here because there is something more pertinent: The company presumably pays you a good amount of money to do something that only you can do (the data science). But it's having you do other things instead, which could be done by other people who are some combination of more capable, more motivated or less expensive. If the data wrangling could be done by someone making half your salary, then it makes no sense to pay you twice as much to do the same thing. If it could be done  faster  by someone paid the same salary, the same logic applies. Therefore it is a waste of resources (especially money) to have the company assign this task to you. Coming at it from this perspective, you might find it much easier to make your superiors see your side of things. 
 Of course, at the end of the day,  somebody  has to do the data wrangling. It may be that the cheapest, fastest, easiest way of doing it -- the best person for the job, is you. In that case, you're kind of out of luck. You could try to claim it's not part of your contract, but what are the odds they were naive enough to put something that specific in the contract?","Perhaps to put it simply: 
 
 When creating variables and binning numerics, would you be doing that blindly, or after analysing your data? 
 When peers review your findings, if they had questions about particular bits of data, would it embarrass you to not know them? 
 
 You need to work with and understand your data - which includes simple stuff from fixing inconsistencies (NULLs, empty strings, ""-"") to understanding how a piece of data goes from collected to being displayed. Processing it includes knowing the same pieces of information, so it is partially work you would have had to do anyway. 
 Now, it sounds like this company could benefit from setting up some sort of free MySQL (or similar) instance to hold your data. Trying to be flexible when you're designing your wrangling code is also a good idea - having an intermediate dataset of processed data I think would be useful if you're allowed to (and can't do it in MySQL). 
 But of course you're still setting up things from scratch. This is not an easy process, but this ""learning experience"" is at least good to put in your CV.","1) Feels like most of the work is not related to data science at all. Is this accurate?
In my opinion, Data Science cannot pull out from Data wrangling. But, as you said, the question would come on how much percentage of Data Wrangling is required to do by a Data Scientist. It depends on Organizations bandwidth and the person interest in doing such work. In my experience of 15 to 16 years as DS, I always, spent around 60% to 70% in data wrangling activity and spent to a max of 15% of time in real analysis. so take your call. 
 2) I know this is not a data-driven company with a high-level data engineering department, but it is my opinion that data science requires minimum levels of data accessibility. Am I wrong?
Again it depends on organization's security policies. They cannot leave everything to you and they have their own security issues to reveal the data to a person who is temporary employee (sorry to use this words :-() 
 3) Is this type of setup common for a company with serious data science needs?
I feel these kind of companies require most attention from Data Scientists to make feel that data driven modeling is the future to sustain their business. :-) 
 I have given my inputs in thinking of businesses instead of technical stand points. :-)
Hope I am clear in my choice of words.","In his talk ""Big Data is four different problems"", Turing award winner Michael Stonebraker mentions this particular issue as a big problem ( video ,  slides ) 
 He says that there are a number of open problems in this area: Ingest,    Transform(e.g. euro/dollar),
    Clean(e.g.-99/Null),
    Schema mapping (e.g. wages/salary),
    Entity consolidation (e.g. Mike Stonebraker/Michael Stonebreaker) 
 There are number of companies/products trying to solve this problem such as Tamr, Alteryx, Trifacta, Paxata, Google Refine working to solve this problem. 
 Until this area matures, a lot of the data scientist job will indeed be data wrangling.",63.77389995,54.25183831,52.92951181,64.69391448,53.36148384,63.89555367,55.44658162,66.3518627,62.68158113
48246,How to compute f1 in TensorFlow,machine-learning,TF Addons computes the  F1 score  and more generally the  FBeta Score,"In tf 2.0+: 
 f1 = 2*(tf.compat.v1.metrics.recall(labels, predictions) * tf.compat.v1.metrics.precision(labels, predictions)) / ( tf.compat.v1.metrics.recall(labels, predictions) + tf.compat.v1.metrics.precision(labels, predictions))
 
 In previous versions you can use the contrib.metrics submodule (deprecated in 1.14): 
 tf.contrib.metrics.f1_score(labels, predictions)","To compute  f1_score , first, use this function of python  sklearn  library to produce confusion matrix. After that, from the confusion matrix, generate  TP ,  TN ,  FP ,  FN  and then use them to calculate: 
 Recall = TP/TP+FN   and   Precision = TP/TP+FP 
 And then from the above two metrics, you can easily calculate: 
 f1_score = 2 * (precision * recall) / (precision + recall) 
 OR 
 you can use another function of the same library  here  to compute  f1_score  directly from the generated  y_true  and  y_pred  like below: 
 F1 = f1_score(y_true, y_pred, average = 'binary')
 
 Finally, the library links consist of a helpful explanation. You should read them carefully.","F1 score can be defined as a custom metric. Keras will evaluate this metric on each batch/epoch as applicable. 
 import keras.backend as K

def f1_metric(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val
    
    
model.compile(...,metrics=['accuracy', f1_metric])","Keras is in early stages of supporting the  F1Score  and  FBetaScore  score, but it currently only in nightly-builds. Example from the docs: 
 metric = tf.keras.metrics.FBetaScore(beta=2.0, threshold=0.5)
y_true = np.array([[1, 1, 1],
                   [1, 0, 0],
                   [1, 1, 0]], np.int32)
y_pred = np.array([[0.2, 0.6, 0.7],
                   [0.2, 0.6, 0.6],
                   [0.6, 0.8, 0.0]], np.float32)
metric.update_state(y_true, y_pred)
result = metric.result()
result.numpy()",,,,,57.79255236,50,53.24076127,50,50,,,,
47405,What to set in steps_per_epoch in Keras' fit_generator?,keras,"As mentioned in Keras'  webpage  about  fit_generator() : 
 
 steps_per_epoch : Integer. Total number of steps (batches of samples)
to yield from generator  before declaring one epoch finished  and
starting the next epoch. It should typically be equal to
 ceil(num_samples / batch_size) . Optional for Sequence: if unspecified,
will use the len(generator) as a number of steps. 
 
 You can set it equal to  num_samples // batch_size , which is a typical choice. 
 However,  steps_per_epoch  give you the chance to ""trick"" the generator when updating the learning rate using  ReduceLROnPlateau()   callback , because this callback checks the drop of the loss once each epoch has finished. If the loss has stagnated for a  patience  number of consecutive epochs, the callback decreases the learning rate to ""slow-cook"" the network. If your dataset is huge, as it is usually the case when you need to use generators, you would probably like to decay the learning rate within a single epoch (since it includes a big number of data). This can be achieved by setting  steps_per_epoch  to a value that is  less than   num_samples // batch_size  without affecting the overall number of training epochs of your model. 
 Imagine this case as using mini-epochs within your normal epochs to change the learning rate because your loss has stagnated. I have found it very useful  in my applications .","I think it would be nice to have the following relation hold 
 steps_per_epoch * batch_size = number_of_rows_in_train_data
 
 This will result in usage of all the train data for one epoch.
Also, consider using  fit()  instead of  fit_generator()  if you need to have fast performance, but take into account that  fit()  might use more memory.","For example, if you have 100 training samples, then  num_samples  = 100, or the number of rows of  x_train  is 100. 
 You can specify your own batch size. In this case, say  batch_size  = 20. As a result, you can set your  steps_per_epoch  = 100/20 = 5 because in this way you can make use of the complete training data for each epoch. 
 If you also want to ask the scenario you want to set  steps_per_epoch  !=  num_samples / batch_size  (for example, when  num_samples  cannot be fully divided by  batch_size ), please refer to this post:  https://github.com/keras-team/keras/issues/10164","Let's clear it : 
 Assume you have a dataset with  8000 samples (rows of data)  and you choose a  batch_size = 32  and  epochs = 25 
 This means that the dataset will be divided into (8000/32) =  250 batches , having  32 samples/rows in each batch.  The model weights will be updated after each batch. 
 one epoch will train 250 batches or 250 updations to the model. 
 here  steps_per_epoch  = no.of batches 
 With 50 epochs, the model will pass through the whole dataset 50 times. 
 Ref -  https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/","This is the formal definition : It should typically be equal to the number of unique samples of your dataset divided by the batch size. 
 This is how it works, 
 When you provide  's'  steps per epoch ,
Each  's'  step will have  'x'  batches each consisting  'n'  samples are sent to fit_generator, 
 So, if you specify  5  steps per epoch, each epoch computes  'x'  batches each consisting of  'n'  samples 5 times, then the next epoch is started!",,,,,72.91353857,69.16525867,68.61253013,61.57409591,75.3066238,,,,
46858,How does Max Pooling (of size 2x2) change the size of receptive field in CNN?,cnn,"The effect of adding a max pooling or convolutional layer to any FCN:  
 I assume you are referring to a single path, fully convolutional neural network (FCN). In that case, let's refer to the stride rate in each of the consecutive layers  $l$  as  $s_l$ .  If the size of the receptive field was  $(n,n)$  after the first  $L$  layers, then adding a max pooling or a convolutional layer of size  $(k, k)$  will increase the size of the receptive field to 
 $$
 \left ( n + (k-1)\prod_{i=1}^{L} s_i,\ n + (k-1)\prod_{i=1}^{L} s_i \right ).
$$ 
(Derivation below) 
 
 The effect of adding a max pooling layer of size (2,2): 
 If you add a max pooling layer of size  $(2,2)$  (... therefore  $k=2$ ) to a network with all strides rates set to one ( $s_i = 1$ ), then the receptive field size increases to   $(n+1, n+1)$ . The receptive field size will then only increase by one in each dimension; the size of the receptive field will  not  be doubled. 
 There are a few key points to understand this result: 
 
 The receptive field of a node is the set of all input nodes that  could possibly  influence that node's response. 
 In the absence of striding, neighboring nodes in a CNN are influenced by  nearly  the same set of inputs (i.e. - their receptive fields are nearly identical). 
 Pooling these neighboring nodes' receptive fields creates an output with a receptive field that is essentially the same as any one of the neighboring nodes that feed into it. 
 
 
 Worked out example to emphasize the above points:  
 Let's consider a one-dimensional CNN consisting of a convolutional layer of size 3 followed by a max pooling layer of size 2: 
 
 We note the following: 
 
 The first node of the middle layer could be influenced by inputs 1, 2, and/or 3. 
 
 Therefore the receptive field of the first node of the middle layer is the set  $\left \{ \text{Input } 1, \text{Input } 2, \text{Input } 3  \right \}$ , and the size of the receptive field is three. 
 
 
 The receptive field of middle layer node 2 is   $\left \{ \text{Input } 2, \text{Input } 3, \text{Input } 4  \right \}$  and also has a size of three. The pattern continues with middle layer nodes 3 and 4. 
 
 Output layer node 1 can be influenced by the receptive field of middle layer node 1 (which is influenced by inputs 1,2,3) and middle layer node 2 (which is influenced by inputs 2,3,4); its receptive field is simply the union of these two receptive fields. 
 
 The receptive field of output layer node 1 is  $\left \{ \text{Input } 1, \text{Input } 2, \text{Input } 3, \text{Input } 4  \right \}$ , and thus has a size of 4. 
 Inputs 2 and 3 each count once toward the receptive field size despite influencing output node 1 from two different paths. 
 The last pooling layer does little to expand the receptive field size since the middle layer has many overlapping elements. 
 
 
 
 
 How exactly do strides come into play? 
 Stride rates larger than one greatly  reduce  the number of common receptive field elements between neighboring nodes in all the layers that follow. With fewer common receptive field elements between neighboring nodes, adding a convolutional or pooling layer will increase the receptive field size more than it would have otherwise. 
 
 Computing the receptive field size for a single path, fully convolutional neural network: 
 For a single path, fully convolutional neural network, we can compute the receptive field size analytically.  We consider such a network that has  $L$  layers. The receptive field size of a node in the output layer along any one of the dimensions is given by 
 $$
r_0 = 1 + \sum_{l=1}^L \left ( \left ( k_l - 1 \right ) \prod_{i=1}^{l-1} s_i\right )
$$ 
 where 
 
 $r_0$  is the receptive field size after  $L$  convolutional / pooling layers, 
 $k_l$  is the kernel size of the convolutional / pooling operation at layer  $l$ , 
 and  $s_i$  is the stride rate at layer  $i$ . 
 
 Note:  the product ends at  $l-1$ . The stride rate  $s_L$  in the last convolutional layer has no effect on the receptive field size. 
 
 Deriving the results toward the top: 
 Suppose we have a CNN with  $L$  layers that has a receptive field of  $(n,n)$  and each layer  $l$  has a stride of  $s_l$ . Then adding a max pooling layer of size 2x2 creates a  $(L+1)$  layer with kernel size  $k_{L+1}=2$ . Therefore, the receptive field size along either dimension will now be given as
 $$
\begin{align}
r_0' &= 1 + \sum_{l=1}^{L+1} \left ( \left ( k_l - 1 \right ) \prod_{i=1}^{l-1} s_i\right )\\
 &= 1 + \sum_{l=1}^{L} \left ( \left ( k_l - 1 \right ) \prod_{i=1}^{l-1} s_i\right ) + \left ( k_{L+1} - 1 \right )\prod_{i=1}^{L} s_i \\
&= n + \left ( k_{L+1} - 1 \right )\prod_{i=1}^{L} s_i \hspace{0.5 in} \text{(the first $L$ layers have a receptive field size of $n$)}\\
 &= n + \prod_{i=1}^{L} s_i \hspace{1.4 in} \text{($k_{L+1} = 2$)}
\end{align}
$$","Before anything, let us try to understand what Max-pooling actually does. Intuitively, Max-Pooling takes the maximum of the value inside the kernel as the maximum value is something that causes a larger impact from the picture. When the size of the kernel is  $2x2$ , half of the values denote the actual value so the receptive field increases. A visualization could be found in  this video . Also you can look at  this video  too.","If I am not wrong, the receptive field of a layer increases by 2 after a Max Pool layer. The way I understand it is that while backpropogation, the flow of gradients go from an image of size  $m$  to  $m*2$ . I am not sure whether this is right or wrong.","I know this is an old question, but I guess it needs a more detailed answer than the ones given before. 
 The way a max pooling layer changes the size of the receptive field depends both on the strides and on the size of the max pooling filter.
The receptive field is doubled if the max pooling layer has a pool size of (2,2)  and also  a strides of (2,2).","The receptive field only increases by 2x if we consider only one layer. 
 If we consider the whole network (convolutions, etc) things get complicated because the convolutions overlap on the input image. 
 Check out  this step-by-step visualization .",,,,,75.95196346,67.72467631,69.02451383,85.47628662,58.00117779,,,,
46744,When to use mean vs median,statistics,"It depends what question you are trying to answer.  You are looking at the rate of change of a time series, and it sounds like you are trying to show how that changed over time. The mean gives the reader one intuitive insight: they can trivially estimate the number of followers at any date  $d$  days since the start by multiplying by the mean rate of change. 
 The downside to this single metric is that it doesn't illustrate something which is very common in series such as this: the rate of change is not fixed over time. One reasonable metric for giving readers an idea of whether the rate of change is static is giving them the median. If they know the minimum of the series (presumably zero in your case), the current value, the mean and the median, they can in many cases get a ""feel for"" how close to linear the increase has been. 
 There is a great cautionary tale in  Anscombe's quartet  - four completely different time series which all share several important statistical measures. Basically it always comes back to what you are trying to answer. Are you trying to find users which are likely to become prominent soon? Users which are steadily accruing followers year by year? One hit wonders? Botnets? 
 As you've probably guessed, this means it's not possible to universally call mean or median ""better"" than the other.","The arithmetic mean is denoted as  $\bar{x}$ 
 $$\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i $$ 
 where each  $x_i$  represent an unique observation. The arithmetic mean measures the average value for a given set of numbers. 
 In contrast to this, the median is the value which falls directly in the middle of your dataset. The median is especially useful when you are dealing with a wide range or when there is an outlier (a very high or low number compared to the rest) which would skew the mean. 
 For example, salaries are usually discussed using medians. This due to the large disparity between the majority of people and a very few people with a lot of money (with the few people with a lot of money being the outliers). Thus, looking at the 50% percentile individual will give a more representative value than the mean in this circumstance. 
 Alternatively, grades are usually described using the mean (average) because most students should be near the average and few will be far below or far above.","Simply to say, If your data is corrupted with noise or say erroneous no.of twitter followers as in your case, Taking mean as a metric could be detrimental as the model will perform badly. In this case, If you take the median of the values, It will take care of outliers in the data. Hope it helps","I find myself explaining this a lot and the example I use is the famous Bill Gates version. Bill Gates is in your data science class. Your instructor asks you: what is the average income or net worth of this class? Bill Gates sheepishly obliges and tells you what his income is. Now when you say the average income of your group is a zillion dollars - technically correct but does not describe the reality - that Bill Gates is an outlier skewing everything.  
 So you line up all the people in your group in ascending or descending order - whatever the person in the middle is making - that is your median. In this example, everybody but Bill Gates is likely to be in spitting distance of that median, and Bill Gates will be the only one making anything close to the mean.  
 Now say buddy Bill Gates is hiring a money manager. Based on the returns they produced so far. Should he look at their average returns over a 10 year period or their median return or a combination of the two? Did they outperform the market each year? Some years? How does portfolio size factor in? In the case of Twitter followers, Obama would have a different growth compared to someone with say 500K-1MM followers. As @l0b0 alludes to in his excellent answer - it all depends. Are you measuring follower growth or the rate of change of follower growth and what is the question you are trying to answer, strategy/product you are trying to develop - accordingly you pick mean or median. Getting the mean and median is always the easy part.  It's always better to never ever have the average of 2.1 kids. Have a whole number of kids. But what can you say about population growth rates if mean number of kids is 2.1 and median is 1 or 2? Or median is 3 or more? Is growth accelerating or decelerating? What is mode doing? Compute all the basics first - and then ask the reason why you are using mean versus median.",Often median is more robust to extreme value to mean. Try to think it as a minimization task. Median corresponds to absolute loss while mean corresponds to square loss.,"I was trying to find a  center  where the absolute difference with all the other values are minimized. It turned out to be median. 
 $$
  f(center) = \sum | center - x | 
$$ 
 After we sort all the values, we can make a graph with the sum of absolute difference in Y axis and the center values in the X axis. The graph will be a inverse bell shape. And the bottom of the curve is where  center=median  . 
 It is hard to get it from intuition.",,,,62.97618965,64.7972926,55.66401343,66.48318792,63.51449624,56.81449069,,,
46483,Products classification by name,classification,"If you have enough data and reasonable number of classes, you can definitely train your model. The grouping of words that you have done is similar to an approach called bag-of-words model. You can use that to build a classifier using Naive Bayes or SVM etc.
On a different note, you can also look at the KNN algorithm because it looks fit for your use case. You can have a look at  this  paper","I think, and have done similar problem too, that this problem can be solved in this way: 
1. Generate NGrams 
2. Create 1 hot encoding matrix 
3. Pass to Naive Bayes or Random forest 
 It would automatically count the words count (you can apply TFIDF too) and based on that weightage will be calculated. 
Examples:  
 
 https://medium.com/data-from-the-trenches/text-classification-the-first-step-toward-nlp-mastery-f5f95d525d73 
 https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/ 
 This is detailed one:
 https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/","You can also try  Tfidf  Vectorizer from Sklearn which would be helpful in your case, As Tfidf vectorization inherently is able to learn and differentiate between the frequently occurring words and rarely occurring words by calculating the product of term frequency and inverse document frequency. Check  here  for more details. On top of this featurization, You can try Naive Bayes as it's pretty fast and seems to work well for text data as it uses with conditional probabilities. Use performance metrics such as confusion matrix to get a better sense of what is happening as accuracy is not a good measure when your data is imbalance. Hope it helps","This should be doable with pre-trained word vectors + document/sentence vectors.  Tutorial :  https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e 
 
 All Product labels with ""similar meanings"" should cluster in a short distance.  
 After product name ha been transformed into a vector, vector can be fed into a logistic regression classifier (Or a shallow neural network).  
 Tutorial :  https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4","The steps are the following: 
 
 Prepare your dataset . Put everything in a dataframe. Divide it in train and test (or even train, cv and test). Use of the order of 10k samples for the test set, or 10-20%, whatever is smaller. Consider using  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html 
 Encode your input.  You can convert the input, which is a string, to a bag of words, or to a TFIDF. Consider using  https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html . 
 Instantiate and train your model . You can use for example a simple logistic regression model, for example  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html . 
 Test the performance of the model.  Use the test set to understand how well your model is doing. You can use for example the accuracy ( https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html ) or the precision and recall for each class ( https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html  and  https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html ). Understand well what they mean ( https://en.wikipedia.org/wiki/Precision_and_recall ). 
 
 Your pseudocode should be something like: 
 -> Divide train and test set. Output: X_train, y_train, X_test, y_test
-> Instantiate tfidf and the desired model (e.g. logistic regression).
-> Fit tfidf with X_train (e.g. use .fit_transform) and get the X_train_transformed
-> Fit the model (e.g. using .fit) with X_train_transformed and y_train
-> Use X_test to get a prediction y_pred of the model (first pass it through tfidf and then through the model object, e.g. using .predict)
-> Use y_pred and y_test to get some metrics to understand the performance of the model.
 
 Hope this works for you.",,,,,50,50,51.87826541,58.7167813,50,,,,
46437,How to Write Multiple Data Frames in an Excel Sheet,pandas,"an example to write in same sheet: 
 import pandas as pd

data1 = """"""
class    precision   recall 
<18      0.0125         12           
18-24    0.0250         16 
25-34    0.00350         4
""""""
data2 = """"""
class    precision   recall 
<18      0               0           
18-24    0.25            6 
25-34    0.35            5
""""""

#create 2 df for sample
df1 = pd.read_csv(pd.compat.StringIO(data1), sep='\s+')
df1.name = ""Dataframe1""
df2 = pd.read_csv(pd.compat.StringIO(data2), sep='\s+')
df2.name = ""Dataframe2""
print(df1);print(df2)

writer = pd.ExcelWriter('e:\\test.xlsx',engine='xlsxwriter')
workbook=writer.book
worksheet=workbook.add_worksheet('Result')
writer.sheets['Result'] = worksheet
worksheet.write_string(0, 0, df1.name)

df1.to_excel(writer,sheet_name='Result',startrow=1 , startcol=0)
worksheet.write_string(df1.shape[0] + 4, 0, df2.name)
df2.to_excel(writer,sheet_name='Result',startrow=df1.shape[0] + 5, startcol=0)
writer.save()
 
 output: 
 
 if you want to write in different sheets: 
 import pandas as pd

# Create a Pandas Excel writer using XlsxWriter as the engine.
writer = pd.ExcelWriter('e:\\test.xlsx', engine='xlsxwriter')

# Write each dataframe to a different worksheet. you could write different string like above if you want
df1.to_excel(writer, sheet_name='Sheet1')
df2.to_excel(writer, sheet_name='Sheet2')

# Close the Pandas Excel writer and output the Excel file.
writer.save()","Several dataframes to same sheet from  here  and  here  with selected sheet: 
 writer = pd.ExcelWriter('pandas_multiple.xlsx', engine='xlsxwriter')

# Position the dataframes in the worksheet.
df1.to_excel(writer, sheet_name='Sheet1')  # Default position, cell A1.
df2.to_excel(writer, sheet_name='Sheet1', startcol=3)
df3.to_excel(writer, sheet_name='Sheet1', startrow=6)

writer.save()","Here's a universal function for writing any amount of Pandas dataframes to a single Excel sheet: 
 import pandas as pd

def write_dataframes_to_excel_sheet(dataframes, dir, name):
    with pd.ExcelWriter(f'{dir}/{name}.xlsx', engine='xlsxwriter') as writer:
        workbook = writer.book
        worksheet = workbook.add_worksheet('Result')
        writer.sheets['Result'] = worksheet

        COLUMN = 0
        row = 0

        for df in dataframes:
            worksheet.write_string(row, COLUMN, df.name)
            row += 1
            df.to_excel(writer, sheet_name='Result',
                        startrow=row, startcol=COLUMN)
            row += df.shape[0] + 2
 
 Here's an example of use: 
 # Create sample dataframes
df1 = pd.DataFrame([(1, 2, 3), (4, 5, 6)], columns=('A', 'B', 'C'))
df1.name = ""Dataframe1""
df2 = pd.DataFrame([(7, 8, 9), (10, 11, 12)], columns=('A', 'B', 'C'))
df2.name = ""Dataframe2""
dataframes = (df1, df2)

write_dataframes_to_excel_sheet(dataframes, '/Users/foo/Documents', 'bar')","You can refer to  this   docs for better understanding , by using the parameter   if_sheet_exists =""overlay"" in ExcelWriter 
 with ExcelWriter(""path_to_file.xlsx"", mode=""a"" 
                  ,if_sheet_exists=""overlay"",) as writer:
                   df1.to_excel(writer, sheet_name=""Sheet1"")
                   df2.to_excel(writer,sheet_name=""Sheet1"", startrow=df1.shape[0] + 5, startcol=0)
 
 And with the help of  startrow  , startcol  parameter in the to_excel Function to place you you DataFrame suitably","You can open the excel editor and write to it and then save 
 writer = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')
data.to_excel(writer, sheet_name='Sheet1',
                  encoding='utf-8', index=False)
writer.save()
 
 Please refer this answer  https://stackoverflow.com/questions/34744863/python-how-to-use-excelwriter-to-write-into-an-existing-worksheet",,,,,63.66647398,66.90046809,65.28349968,65.60628066,62.01459642,,,,
46050,Additive vs Multiplicative model in Time Series Data,r,"Calculate one day returns. 
 Plot histogram of daily returns. 
 Calculate  $log(\frac{price_{i+1}}{price_i})$ . 
 Plot histogram of above logarithm. 
 If second plot is more likely to be normally distributed then choose multiplicative model. Else, choose additive model. 
 
 You can also perform statistical test for normal distribution and check, which one has higher p-value. 
 Explanation: 
 Additive model is used when the variance of the time series doesn't change over different values of the time series. 
 On the other hand, if the variance is higher when the time series is higher then it often means we should use a multiplicative models. 
 Additive model: 
 $return_i = price_i-price_{i-1}=trend_i-trend_{i-1}+seasonal_i-seasonal_{i-1}+error_i-error_{i-1}$ 
 If error's increments have normal iid distributions then  $return_i$  has also a normal distribution with constant variance over time. 
 Multiplicative model: 
 If log of the time series is an additive model then the original time series is a multiplicative model, because: 
 $log(price_i)=log(trend_i \cdot seasonal_i \cdot error_i)=log(trend_i)+log(seasonal_i)+log(error_i)$ 
 So the return of logarithms: 
 $log(price_i)-log(price_{i-1})= log(\frac{price_i}{price_{i-1}})$ 
 must be normal with constant variance.","I want to know which model between additive and multiplicative best suits the above data. 
 
 It is hard to tell just by looking at it.   
 A multiplicative decomposition roughly corresponds to an additive decomposition of the logarithms. 
The additive decomposition is the most appropriate if the magnitude of the seasonal fluctuations, or the variation around the  trend-cycle , does not vary with the  level of the time series . When the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series, then a multiplicative decomposition is more appropriate. Multiplicative decompositions are common with economic time series. 
 An alternative to using a multiplicative decomposition is to first transform the data  until  the variation in the series appears to be  stable over time , then use an additive decomposition. So, basically you need to check for heteroskedasticity, eliminate that if it is there by transformations and do an additive decomposition of the transformed series.  
 Most common transformations are log or square root of the series and are special cases of  Power transform . 
 Reference:
 Forecasting principles and practice","To determine whether a time series is additive or multiplicative, we can use  seasonal_decompose  which provides us three separate components (trend, seasonality, and residual). We can check the variance of seasonality and residual components for additive and multiplicative decompose. The seasonality and residual components with constant variance represent the time series well. 
 from statsmodels.tsa.seasonal import seasonal_decompose
# Multiplicative Decomposition
mul_decompose = seasonal_decompose(df['value'], model='multiplicative', extrapolate_trend='freq')
# Additive Decomposition
add_decompose = seasonal_decompose(df['value'], model='additive', extrapolate_trend='freq')
 
 
 Using this image we can very clearly observe variance of seasonality, and the residual component is constant for multiplicative decompose. So the time series is a multiplicative time series.","The easiest way to identify if a timeseries is additive or multiplicative will be decomposition. 
Also it depends on which kind of additivity and multiplicativity you are looking for ?
is it trend or seasonal ?  
 Below steps to be followed:
1) Decompose the time series using stl() or decompose() functions in R 
2) Look seasonality component(if looking for seasonal additive/multiplicative) or trend(if looking for trend additive/multiplicative).
3) IF the variance in the graph is constant through out from central line then its additive else multiplicative. 
 Simple.. isn't it? ..... :) Hope this will help.","Gain method given above which plot the histogram for gain and conclude whether it is additive or multiplicative is flawed for the following. 
 Consider the following case which is x*sinx which is clearly a multiplicative model.
 
 but when I plot histogram of the gain without taking any log, it turns out to be normal which suggest additive model which is wrong. 
 
 The better thing is to check the variance over time. If it is increasing over time then underlying model may be is multiplicative but if it is constant then it surely an additive model",,,,,64.97808873,63.72829886,63.05138193,70.65637886,65.22124187,,,,
45578,Why underfitting is called high bias and overfitting is called high variance?,variance,"How can one understand it intuitively? 
 
 Underfitting is called ""Simplifying assumption"" (Model is HIGHLY BIASED towards its assumption). your model will think linear hyperplane is good enough to classify your data which may not be true. consider you are shown a picture of cat 1000 times, Now you are blindfolded, No matter Whatever you are shown the 1001th time, probability that you will say cat is very high(You are HIGHLY BIASED that the next picture is also gonna be a cat). Its because you believe its gonna be a cat anyway. Here you are simplifying assumptions 
 In statistics, Variance informally means how far your data is spread out.
Overfitting is you memorise 10 qns for your exam and on the next day exam, only one question has been asked in the question paper from that 10 you read. Now you will answer that one qn correctly just like in the book, but you have no idea what the remaining questions are(Question are HIGHLY VARIED from what you read). In overfitting, model will memorise the entire train data such that it will give high accuracy on train but will suck in test. Hope its helps","Let us assume our model to be described by  $y = f(x) +\epsilon$ , with  $E[\epsilon]=0, \sigma_{\epsilon}\neq 0$ . Let furthermore  $\hat{f}(x)$  be our regression function, i.e. the function whose parameters are the ones that minimise the loss (whatever this loss is). Given a new observation  $x_0$ , the expected error of the model is
 $$ 
E[(y-\hat{f}(x))^2|x=x_0].
$$  
This expression can be reduced (by means of more or less tedious algebra) to 
 $$
E[(y-\hat{f}(x))^2|x=x_0] = \sigma_{\epsilon}^2 + (E[\hat{f}(x_0)]-f(x_0))^2 + E[\hat{f}(x_0)-E[\hat{f}(x_0)]]^2
$$ 
where the second term is the difference between the expected value of our estimator  $\hat{f}$  and its true value (therefore the  bias  of the estimator) and the last term is the definition of variance. 
 Now for the sake of the example consider a very complex model (say, a polynomial with many parameters or similar) which you are fitting against the training data. Because of the presence of these many parameters, they can be adapted very closely to the training data to even the average out (because there is many of them); as a consequence the bias term is reduced drastically. On the other hand, though, it is generally the case that whenever you have many parameters their least square estimations come with high variance: as already mentioned, since they have been deeply adapted to the training data, they might not generalise well on new unseen data. Since we have many parameters (complex model) a small error in each of them sums up to a big error in the overall prediction. 
 The converse situation may happen when one has a model that is very static (imagine very few parameters): their variances do not sum up very much (because there is few of them) but the trade-off is that their estimation of the mean might not correspond closely to the true value of the regressor. 
 In the literature one refers to the former behaviour as  overfit , to the latter as  underfit . In the description I have given you can see that they  may  be related to the complexity of the model but need not necessarily be, namely you may as well have particularly complex models that do not necessarily overfit (because of the way they are constructed, one above all is random forest) and simple model that do not necessarily underfit (for instance linear regressions when the data are actually linear).","Check out the answer provided by Brando Miranda in the following Quora question: 
 ""High variance means that your estimator (or learning algorithm) varies a lot depending on the data that you give it."" 
 ""Underfitting is the “opposite problem”. Underfitting usually arises because you want your algorithm to be somewhat stable, so you are trying to restrict your algorithm too much in some way. This might make it more robust to noise but if you restrict it too much it might miss legitimate information that your data is telling you. This usually results in bad training and test errors. Usually underfitting is also caused by biasing your model too much."" 
 https://www.quora.com/What-is-meant-by-high-variance-low-bias-and-why-does-this-lead-to-overfitting-in-machine-learning","A model based on simple assumptions ( biased ) will probably fit the data badly (under-fitting) whereas a more complex, flexible model that can  vary  more may fit the training data so well (over-fitting) that it becomes less good at predicting new data.","Let's say the problem is predicting whether you will pass or fail in subject C based on your grades in subject A and subject B. Suppose you had a model which takes inputs  $x$  and outputs predictions  $y$ . For each  $x$ , there is a true target  $t$  (i.e. what the ""correct"" prediction is). So  $x$  are the grades in course A and course B, and  $y$  and  $t$  are binary, indicating pass or fail. 
 Suppose you train your model on a dataset  $D$ . The output of your model  $y$  for any given  $x$  will differ based on what  $D$  you train it on. (i.e. sampling all the students with student id's ending in 5 vs. all the students with student id's ending in 0). In this sense,  $y$  is a random variable, where the randomness comes from the choice of the dataset  $D$ . If you  overfit , you will memorize the peculiar aspects of the dataset that do not generalize. So if you are provided with different  $D$ 's, and trained your model on all of them, for a  fixed   $x$ , your prediction  $y$  will vary a lot depending on which  $D$  you trained your model on (since the model remembers all the details about each  $D$ ). The  variability  of  $y$  is due to  overfitting . 
 Next consider the case where you have a very basic model, that just takes the average of the two courses A and B and if it's above some threshold, predicts the student will pass subject C. Suppose course A was actually English, course B was Differential Geometry, and course C was Linear Algebra, and the optimal prediction given  $x$  is to predict  $y^*$ . One would expect students did well in course B could also do well in course C. You can think of  $y^*$  in this scenario as having lots to do with the grades in course B. 
 But your model, being as simplistic as it is, on  average , predicts  $E[y|x]$ , since it routinely fails to capture the  importance  of subject B and the  unimportance  of subject A for predicting subject C.Your model is  biased  towards predicting  $E[y|x]$  rather than  $y^*$ , since it is  underfitting  (i.e. failing to capture the relevant structure of the data that helps it make good predictions on average).",,,,,55.65426991,55.89742595,59.40491172,50,51.68981318,,,,
45533,What is the ideal database that allows fast cosine distance?,feature-extraction,"If you need to scale beyond 1000 entries in the future, a brute-force approach to find the exact neighbors will become increasingly prohibitive from a computational standpoint. To future-proof your solution, I would recommend looking into the well-researched field of approximate nearest neighbors (ANN) techniques. Obviously there is a speed/accuracy tradeoff, but as of this writing, there is really no other way to scale your search to millions or billions of entries. 
 The big tech companies rely almost exclusively on techniques like these. Think about... 
 
 Facebook querying similar faces to suggest people to tag in your photos 
 Spotify recommending semantically similar songs 
 Google searching images similar to one you uploaded 
 
 This  article  is an excellent overview of current state-of-the-art algorithms along with their pros and cons. Below, I've linked several popular open-source implementations. All 3 have Python bindings 
 
 Facebook  FAISS 
 Spotify  Annoy 
 Google  ScaNN","If it's only a few thousand entries each with a 1,000 features, you may just be able to keep it in RAM if you are running this on some kind of server. Then when you get a new feature vector, just run cosine similarity routine. An easy way to do this is just use something standard like pandas and scikit-learn. 
 Alternatively you can keep everything in SQL, load it into something like pandas and use scikit-learn. 
 I'm actually not sure you'll get much of a speed up, if any, by writing the computation in SQL itself.","If you are afraid that the dataset is big that a regular database might not handle it, you could consider an alternative implementation such as  SimHash . 
 From Wikipedia, 
 
 In computer science, SimHash is a technique for quickly estimating how
  similar two sets are. The algorithm is used by the Google Crawler to
  find near duplicate pages. It was created by Moses Charikar. 
 
 Here is the  research paper  from Google and here several implementations in  Python","A brute force approach has O(n) search complexity no matter if you do it in Python or a database. For faster queries you need a tree-structured multidimensional lookup table, for example, a  k-d tree . For Python, there are implementations of a k-d tree in both  SciPy  and  Scikit-Learn : 
 
 https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html 
 https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html 
 
 If you need a standalone database solution, see: 
 
 PostgreSQL with  Cube  extension supplemented with a GIST index created on a table for fast vector queries, details:  https://dba.stackexchange.com/a/163915/177276 
 ElasticSearch with its  cosineSimilarity  function, read more  here ; possibly extended with this  plugin  for speedup - although both perform an exhaustive search. 
 A dedicated DB solution for vector similarity search, like  AquilaDB  that utilizes Facebook's  FAISS  and Spotify's  Annoy  libraries internally.","Is there a reason why you need to do this in SQL? Most architecture patterns would advise against keeping formulas and logic in the database layer. Why not create another layer - outside the database - with a language that can do the computations you need?  
 You can also do the calculations ahead of time and store them in a cached lookup table on the database. Do all the computations you need and then import them into your database and then just run standard SQL SELECT statements to pull the results at run-time.","See:
 Vector Similarity Search with Azure SQL database and OpenAI 
 /*
    From GitHub project: Azure-Samples/azure-sql-db-openai
*/
ALTER   function [dbo].[SimilarContentArticles](@vector nvarchar(max))
returns table
as
return with cteVector as
(
    select 
        cast([key] as int) as [vector_value_id],
        cast([value] as float) as [vector_value]
    from 
        openjson(@vector)
),
cteSimilar as
(
select top (5)
    v2.ArticleDetailId, 
    sum(v1.[vector_value] * v2.[vector_value]) / 
        (
            sqrt(sum(v1.[vector_value] * v1.[vector_value])) 
            * 
            sqrt(sum(v2.[vector_value] * v2.[vector_value]))
        ) as cosine_distance
from 
    cteVector v1
inner join 
    dbo.ArticleVectorData v2 on v1.vector_value_id = v2.vector_value_id
group by
    v2.ArticleDetailId
order by
    cosine_distance desc
)
select 
    (select [ArticleName] from [Article] where id = a.ArticleId) as ArticleName,
    a.ArticleContent,
    a.ArticleSequence,
    r.cosine_distance
from 
    cteSimilar r
inner join 
    dbo.[ArticleDetail] a on r.ArticleDetailId = a.id",would be one that uses an inverted index and stores the vectors in a compact format such as sparse vectors or quantized vectors.,,,50,52.15166891,52.01989145,54.02874744,56.4511605,58.03329539,50,,
45174,How to use sklearn train_test_split to stratify data for multi-label classification?,machine-learning,"Try this: 
 from skmultilearn.model_selection import iterative_train_test_split

X_train, y_train, X_test, y_test = iterative_train_test_split(x, y, test_size = 0.1)
 
 Since you're doing multilabel classification, it's very likely to get unique combinations of each class, which is what causes the error with  sklearn .  You have to use a special library for  multilabel stratified splitting . 
 More details   on how to use  skmultilearn  package",The error you're getting indicates it cannot do a stratified split because one of your classes has only one sample. You need at least two samples of each class in order to put one in the training split and one in the test split. You should examine what your class breakdown is to find the culprit.,"As said by @chenjesu: what you really want is likely to consider each labels when performing stratification (rather than only the combinations of labels which are often only seen once). 
 Unfortunately, the  scikit_multilearn  function that was pointed out is extremely slow for medium to large-sized datasets. E.g. 3 min for 100k examples (and never seems to finish if you use  n_labels=20 ): 
 # pip install scikit-multilearn
from sklearn.datasets import make_multilabel_classification
X,Y = make_multilabel_classification(n_samples=100000, n_classes=100, n_labels=10)

# %%time
from skmultilearn.model_selection import iterative_train_test_split
X_train, y_train, X_test, y_test = iterative_train_test_split(X,Y,test_size=0.20)
# CPU times: total: 2min 45s
 
 I would use instead use the following function that uses  the iterative-stratification package . This only requires 2 seconds on the same data: 
 # pip install iterative-stratification
from sklearn.datasets import make_multilabel_classification
X,Y = make_multilabel_classification(n_samples=100000, n_classes=100, n_labels=10)

%%time
X_train, y_train, X_test, y_test = multilabel_train_test_split(X,Y,stratify=Y, test_size=0.20)
# CPU times: user 2.31 s
 
 where: 
 
from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
from sklearn.utils import indexable, _safe_indexing
from sklearn.utils.validation import _num_samples
from sklearn.model_selection._split import _validate_shuffle_split
from itertools import chain

def multilabel_train_test_split(*arrays,
                                test_size=None,
                                train_size=None,
                                random_state=None,
                                shuffle=True,
                                stratify=None):
    """"""
    Train test split for multilabel classification. Uses the algorithm from: 
    'Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of Multi-Label Data'.
    """"""
    if stratify is None:
        return train_test_split(*arrays, test_size=test_size,train_size=train_size,
                                random_state=random_state, stratify=None, shuffle=shuffle)
    
    assert shuffle, ""Stratified train/test split is not implemented for shuffle=False""
    
    n_arrays = len(arrays)
    arrays = indexable(*arrays)
    n_samples = _num_samples(arrays[0])
    n_train, n_test = _validate_shuffle_split(
        n_samples, test_size, train_size, default_test_size=0.25
    )
    cv = MultilabelStratifiedShuffleSplit(test_size=n_test, train_size=n_train, random_state=123)
    train, test = next(cv.split(X=arrays[0], y=stratify))

    return list(
        chain.from_iterable(
            (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays
        )
    )","There is a seperate module for classes stratification and no one is going to suggest you to use the train_test_split for this. This could be achieved as follows: 
 from sklearn.model_selection import StratifiedKFold


train_all = []
evaluate_all = []
skf = StratifiedKFold(n_splits=cv_total, random_state=1234, shuffle=True)
for train_index, evaluate_index in skf.split(train_df.index.values, train_df.coverage_class):
    train_all.append(train_index)
    evaluate_all.append(evaluate_index)
    print(train_index.shape,evaluate_index.shape) # the shape is slightly different in different cv, it's OK

# Getting each batch
def get_cv_data(cv_index):
    train_index = train_all[cv_index-1]
    evaluate_index = evaluate_all[cv_index-1]
    x_train = np.array(train_df.images[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)
    y_train = np.array(train_df.masks[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)
    x_valid = np.array(train_df.images[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)
    y_valid = np.array(train_df.masks[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)
    return x_train,y_train,x_valid,y_valid

# Training loop
for cv_index in range(cv_total):
    x_train, y_train, x_valid, y_valid =  get_cv_data(cv_index+1)
    history = model.fit(x_train, y_train,
                        validation_data=[x_valid, y_valid], 
                        epochs=epochs)

 
 This is a simple code snippet for using StratifiedKFold with your code. Just replace the required parameters and hyper-parameters accordingly.","If you are using  pandas , you could try this: 
 
 Explode on y -> e.g. 
 
  df.explode('labels')
 
 
 Perform regular  train_test_split  with stratification 
 Regroup on  'cleaned_text'  & aggregate over  'labels'  -> e.g. 
 
  df.groupby('cleaned_text').agg({'labels': 'sum')","This is because one of your class has only one record. train_test_split is unable to decide where to put that in train or test part. you can do either of following: 
 
 you create one/more copy of that record or 
 remove from the main data or 
 keep that in a variable and then remove it, then do train_test_split and append it to training data",,,,69.63004853,60.07391652,71.30991921,54.55306052,61.67012335,67.73850535,,,
45165,"How to get accuracy, F1, precision and recall, for a keras model?",machine-learning,"Metrics have been removed from Keras core. You need to calculate them manually. They removed them on  2.0 version . Those metrics are all global metrics, but Keras works in batches. As a result, it might be more misleading than helpful.  
 However, if you really need them, you can do it like this 
 from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])

# fit the model
history = model.fit(Xtrain, ytrain, validation_split=0.3, epochs=10, verbose=0)

# evaluate the model
loss, accuracy, f1_score, precision, recall = model.evaluate(Xtest, ytest, verbose=0)","You could use the  scikit-learn classification report . To convert your labels into a numerical or binary format take a look at the  scikit-learn label encoder . 
 from sklearn.metrics import classification_report

y_pred = model.predict(x_test, batch_size=64, verbose=1)
y_pred_bool = np.argmax(y_pred, axis=1)

print(classification_report(y_test, y_pred_bool))
 
 which gives you (output copied from the scikit-learn example): 
              precision  recall   f1-score    support

 class 0       0.50      1.00      0.67         1
 class 1       0.00      0.00      0.00         1
 class 2       1.00      0.67      0.80         3","You can also try as mentioned below. 
 from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
y_pred1 = model.predict(X_test)
y_pred = np.argmax(y_pred1, axis=1)

# Print f1, precision, and recall scores
print(precision_score(y_test, y_pred , average=""macro""))
print(recall_score(y_test, y_pred , average=""macro""))
print(f1_score(y_test, y_pred , average=""macro""))","See the docs of  keras 
 import tensorflow as tf 

model.compile( ..., metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])])","Try  this  with  Y_test ,  y_pred  as parameters.",,,,,63.95957762,54.35263814,56.82524417,61.00254057,50,,,,
44883,Deep network not able to learn imbalanced data beyond the dominant class,deep-learning,"1) A five-layer neural network is one heck of a complex model for a data set with less than 1 million points. (I’m trying to find a good link for this, but the intuition is that your choice of model should be driven by the complexity of the available data, and not by what you think the real target function is like.) If this is for a real-world project, a tool like XGBoost might work better on this data set — out of the box, you’ll spend less time dealing with problems related to imbalanced classes, poorly scaled data, or outliers. Of course if this is specifically for learning about neural networks, that advice isn’t much help! 
 2) For a class distribution that’s as skewed as your data, you might get more mileage from re-sampling the training data rather than re-weighting the classes during training. Down-sample the majority class first (just throw away majority samples at random); if that’s not satisfactory then try something more complicated like using SMOTE to up-sample the minority classes. Try taking this to the extreme; build a (collection of) new training sets by randomly sampling only 1,000 points from each class. 
 The intuition here is that, for neural networks, as far as I know, re-weighting classes basically means re-scaling the gradient steps during training based on the class weights. If the classes are skewed 10:1, that makes sense: we take a step that’s 10 times as far for a minority sample. If the classes are skewed 1000:1, as in your case, it makes less sense — we’ll take 1,000 small steps as we optimize on the majority class, and then a single gigantic step in an essentially random direction when we happen to see a minority sample, followed by 1,000 tiny steps trying to un-do this work, etc. We don’t see enough minority samples to allow information about their class to average out.","One important thing to check is if by any chance you have NaNs in the input data. 
 Had the same problem, turned out NaN's had crept in the input, working great now!","Is your input data standardised? What happens when you run it with only one hidden layer? What happens when you set learning rate to 1e-6 or 1e-5? What's your result when you run this through a logistic regression? What does the confusion matrix look like?  
 When you are using  class_weights=  you should also use  weighted_metrics=  for your convenience.","Why don't you try with gradient boosting or adaBoost? 
They should perform well in unbalanced data as, during the training, they tend to give weights to misclassified observations, improving then the performance.
Lemme know","I think it is probably doing  something , just not enough to change the overall classification. Have you inspected the estimated probability distributions of the minority class test examples for different  class_weight s? I imagine that the probabilities of the true classes are somewhat higher for these examples, even though the predicted most likely class is still the majority class.  
 If this is the case, then you can determine a value (through cross-validation or estimated on a held-out set) to subtract from the element of the estimated probability distribution corresponding to the top class before taking an  argmax . i.e. test a range of values and pick the one that provides the best  $F_1$  score or whatever else you want to optimise. Something like this: 
 # train neural net
# ... 

y_proba = model.predict(X_val)
best_score = 0.
best_value = 0.

# try every value from 0. to 1., in increments of 0.01
for i in np.linspace(0., 1., 101):
    alt_y_proba = y_proba - np.array([i, 0., 0., 0., 0.])
    alt_y_proba = np.clip(alt_y_proba, 0., 1.) # ensure no negative values!

    y_pred = np.argmax(alt_y_proba, axis=1)
    score = # some score function goes here

    if score > best_score:
        best_score = score
        best_value = i

print(best_value)    
 
 Yes, it's a little hacky, but it may give you good results. You can re-normalise  alt_y_proba  to be a proper probability distribution again if you want, but it won't change the classification. Just make sure that the dataset you're optimising this subtraction value with is not used in the training of the neural net, as this may introduce overfitting.","I just figured out the same issue. Try to pass a dictionary like: 
 {
   0: 1.0, 
   1: 10, 
   2: 20, 
   3: 20, 
   4: 20
}
 
 to  class_weight  in  model.fit()  and it will solve the problem. Understood that it says a list will also work in the docs - but seems like it does not work as well for me.",,,,56.80353216,50.71025643,52.14500013,50.71871556,51.70454678,51.31159636,,,
44846,Face Recognition (Scalability Issue),machine-learning,"The problem you describe may be tackled with  online machine learning  where you continuously update your model as new data arrive,  avoiding the computationally intensive part of retraining. 
 For deep neural networks, there is some work in this  direction . 
 scikit-learn  and  Vowpal Wabbit  also provide some online algorithms.","I am facing the similar issue. But my problem is a little different, whenever I trained a svm classifier model, my old classes got disturbed because of new classes. I have used scikit-learn svm model with higher C value, it partially solved my problem.","You can use another approach using facenet from google to achieve scalability. I have shifted images, model and classifier to database. And used this technique to achieve scalability. Link is below  enter link description here  . In my case only the classifier gets updated everytime there is new image incrementally.","There's dedicated package for online learning called  creme . You can find some quick guide on this  blog .  Docs  are also nice. 
 I would stick with CNN, which are great to extract features from data. I'd start with pretrained model, just chop off last layers and leave conv ones. Then take those features to  creme .","One method is to use  Online Learning  as previous answers have suggested. 
 You can use transfer learning to use a pre-trained model (CNN), that has been trained on a large dataset, and use it to generate  feature vectors  for the members of your system. 

 
 When an existing user comes in you can use these  feature vectors  for identifying the user (member) using SVM. 
 When a non-member (new user) comes in his  feature vector  will not match with any of the members. If you want him to register you can store his  feature vector  along with previous vectors to identify him in the future. 
 
 
 I have used similar technique in one of my previous projects. I used the  face_recognition  library from ageitgey:  https://github.com/ageitgey/face_recognition . You can get it from  pip  as well.  
 Note:  I know it is very late to answer this question now. But I home my additions with the previous answers can help someone now or in the future as well.",,,,,50,54.19481373,58.70693191,50,55.41646789,,,,
44300,How to protect data from internal data scientists?,anonymization,"You could checkout the OpenMined Pysyft library which is a library for encrypted, privacy preserving deep learning built over Pytorch. PySyft decouples private data from model training. 
 Github link to the Pysyft library -  https://github.com/OpenMined/PySyft","Your question: 
You seem to be asking a managerial/policy question phrased like a data-science question.  The policy question is ""how do I keep customer data private from internal data scientists without harming its usability"". 
 The data-science question is something like ""how do I transform data so that the privacy and identifiability of its original form can not be deduced, while not disabling other analytical processes"".  This is the seed of the zero-information paradox. 
 tl;dr 
I think your policy-person is asking a question equivalent to ""how do I make my computer hacker-proof"", where the only perfect answer is not to have the computer.  There are going to be levels of ""resistant"" but there is no such thing as ""hacker-proof"". 
 Problem proposition: 
One of the problems with this question is that vast majority of policy-asker specialized technical expertise is nearly nothing compared to the people you are trying to ""selectively impede"".  Explaining an answer to them that they can understand might keep an idiot out, but doesn't actually stop data exfiltration. 
 Consider how data aggregation with cell phones works. 
 https://eclecticlight.co/2015/08/24/data-aggregation-how-it-can-break-privacy/ 
 The many policy-folks asking the question can get an answer they think means ""yes"" when in fact it means ""no"", and a persistent of clever data person can figure it while the policy-person can't. 
 Simple example: 
Lets make a process where we replace first name with a number.  ""Smith"" becomes 1, ""Jones"" becomes 2, etcetera.  Is that process reversible using only the output?  Given only a list of numbers can I get back to the names? Yes, though it varies.  If I look at the frequency of last names and compare them with number frequencies I should be able to do a decent job of de-anonymizing the common names.  Saying this again, if 15% of last names are ""Smith"" and 15% of my output list of numbers are ""1"" then there is a really good chance that 1 means Smith. 
 That is a toy example, but the MAC address of your cell phone is known and sold.  If all the data in the world is anonymized except the MAC, and I can go to a 3rd part and buy a list of MAC to identity mappings, then your data isn't anonymized at all.  You missed the baby in that bathwater.","If your data is completely numeric, have you considered removing column names from the data? It's completely possible that your staff could carry out their modeling functions without having to know what the numbers are at any stage. You would have to do some data prep to make sure that correlated columns have been accounted for, but even that could be still addressed with the ""anonymous"" columns.  
 If you give your staff a dataset with randomized column names that would still preserve your desired privacy, the data would effectively be useless.","Members of your data science team should be familiar with various forms of data anonymization. Depending on the nature of your data, it usually involves removing or obfuscating all data that has the potential to identify a person/client/other. Feature scaling, encoding, and name swapping (as  @I_Play_With_Data  had mentioned) can help reduce the possibility of revealing personal data or identifying the input source (individual persons or other entities).  
 While there's usually data that can be dropped or obfuscated entirely without impacting the results (like encoding or removing a client's SSN from a dataset), there are often features which are more difficult to handle in a correct manor. If you decide to encode categorical data, there are multiple ways that this can be done and the data scientists will need to be made aware of any assumptions made during the process (i.e. that null values were encoded or that a certain set of columns represent a single original feature). There are a number of things that can go wrong if you're too aggressive in your attempts to anonymize data, so often it's best to delegate the task to people with experience on both ends of the business, for example: a member of the compliance department familiar with data science or a member of the data science team who is also a subject matter expert.","To add onto other answers: 
 Pseudonymization is a way to anonymize specific data mostly customer data by removing personalized ID variables like names, email, etc. and adding a randomized but unique ID. 
 This process allows you to add data and share data via a joint ID while keeping knowledge of other data completely on one side. 
 A common use case is market research where a client company only has its own customer data and the pseudonymous ID and the research company only has the surveyed data and the pseudonymous ID. They can then share only parts of the data that they need to share without exposing more sensitive data. 
 This could be helpful in your case if your DS team needs to model with customer data but should not know the customer names, etc. while still being able to report for example individual lead scores back to you. 
 You do not destroy the data!  You simply add a new unique but meaningless identifier.",,,,,51.42078903,59.13791796,54.27437433,59.82818738,57.79600359,,,,
44228,Estimating the accuracy of a model?,machine-learning-model,"Is there any way to estimate the accuracy of my model on the new dataset? 
 
 No. 
 If you want to evaluate the classification accuracy of your model,  you need to know the ground truth  to compare, simple as that. Estimating the similarity of the datasets is totally irrelevant,  unless  there is prior knowledge that  feature similarity   leads to  label similarity .","I have not heard about such an estimation. Think about it like this: You need a method, that given a similar dataset will predict labels for another dataset. Then, you will use the labels as the groundtruth. But how such a method is different from a machine learning model?  
 In other words, any estimated accuracy on such labels will tell you not how close is your model to the real annotations, but how is it close to another model used to generate the pseudo-groundtruth. This will make sense if your 'label generator' performs with a near 100% accuracy, but I am afraid such tasks are very rare. At least, outside labs.","I'm not a machine learning expert by any means, but I feel like that's what the error measurement is on your training data. Or at least, in theory. 
 If it turns out that this isn't the case, then the data you used for training wasn't distributed in a way that represented the thing you're trying to model  or  you overfit your training data. 
 You can get a good ""estimate"" for the performance of your model by using a validation set, which basically helps you to determine if either of these two things have occurred.","Please read about  semi-supervised learning  and  label propagation .
It's what you was looking for.
Bunch of code: 
 from sklearn.semi_supervised import LabelPropagation
label_prop_model = LabelPropagation()

random_unlabeled_points=#
features=#

labels = old_data_set_labels
labels[random_unlabeled_points] = -1
label_prop_model.fit(features, labels)
 
 Sources: 
 
 https://scikit-learn.org/stable/modules/label_propagation.html 
 https://datawhatnow.com/pseudo-labeling-semi-supervised-learning/ 
 https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html","Technically there is no direct way to do that. 
But let's us say you are doing some binary classification and the response rate in your training data set is 30% .
Now when you use your model to score the new data set (without labels). Ideally average score should be around 30% (considering new data set is similar to training data set) . 
 In case of regression problems, the response rate can be replaced by average of dependent variable.","There is no direct way to estimate the accuracy. I would highly suggest  Question on bias-variance tradeoff and means of optimization  for how variance/bias is evaluated/defined in ML modelling, which can at least provide some context of how ""accurate"" (I am hesitant to use that word) should be defined here.",,,,75.1605031,63.52598715,53.50635811,51.28576642,50.68058972,62.25252537,,,
43313,Why 100% accuracy on test data is not good?,machine-learning,"I see two ways to go:
1 - There is an error
2 - There is no error. 
 1 - Look for the error 
 
 You probably have commited Data Leakage.  You have added the target in one of the features and the model found out.  
 The validation is not right, you have a time series and you have done random validation. 
 Your test has only a few instances or it is unique. 
 The test is repeated from the train.  
 
 2 - There is no error 
 If the prediction is right and you have 100% accuracy, then no need to do Machine Learning. Open the model find where is taking the decision and don't do machine learning, do classical modeling. 
 For example if your model is a decision tree, just plot it or print it and get the decision and put them yourself. 
 This sometimes happens when modeling a previous developed algorithm. The new ML model is able to learn what was going on before.","Shortest possible explanation: You might be overfitting your data.  
 Sure, that is happening in the TEST set, not the training one... but what if, by mistake, you have leaked data from the train set into the test one (this happens, trust me). 
 When you get 100% accuracy, it is most likely a form of overfitting, and that is ultimately a bug. Again, even on the test set... it might just be a data leaking.","Not sure if this is going to be a satisfactory answer... 1st thing that comes to mind whenever I get a 100% accuracy on test data is ""I must have done something wrong"". 
 Most commonly, I've added a feature to the dataset that is actually a sort of proxy of the target variable. That is, I've made a silly mistake. 
 But sometimes it is not a silly mistake, like adding a feature. I don't really remember the source thus I cannot link to it. But I heard in a podcast about some guys that were trying to classify patients of cancer from some features (I mean not from images of cancer cells or anything like that), and they built a model that was pretty simple and surprisingly good (not 100% accuracy though). The point was they included some kind of id of the patient as a feature, and that id somehow contained information about the hospital that treated them. There were a few hospitals that treated the very bad cases, thus the model was learning that anybody going to those few hospitals was really sick, and not really learning about who was sick. 
 Hope it helps.","If you have a model that has  $100%$  accuracy on unseen test data there are some situations that I will explain.  
 
 If you have a situation that does not change over time and there are no exceptions in that, the mentioned accuracy is very satisfactory and you can say with confidence that your model has learnt what it should do. As an example, you can implement an  LSTM  network which can be able to find the sum of binary numbers. In this case, due to the fact that you always know the some of typical  $10 + 01$  values is always  $100$ , and it is a fact that does not change,  $100$ % is acceptable. 
 There are cases where the situations of the current time are different from those of tomorrow's. It means that the behaviour of the nature that you are going to model it is not a function but a distribution. This means for the current feature space you may have different outcomes. This is the case where in the current feature space the distribution of different classes, i.e. in classification tasks, overlap. This means that it is impossible that you have  $100%$  accuracy because the nature you are going to model is not a function. If I want to explain it again there are two situations. first, the nature of the distribution is time variant or invariant. The former case is something that is affected by the previous outcomes. The latter case can have contradictory outcomes with the same input features due to the overlap of distributions.","Training a heavy model on a small dataset, all from the same distribution, could cause an overfit. Even on the test set. When you deploy that model on real world data, you may experience a model drastic performance drop. (ie: getting high variance). 
 Edited: Rethinking the question again and this is what I was trying to explain. Imagine you're trying to train an bird classifier that can predict if there's a bird in a picture or video frame. You train and test your model with high quality and clear images of bird maybe surfed from the internet of taken professionally,.. You get an 100% accuracy on train and test set... But when you deploy your model... It may not perform as well because the images the model would be using may not be as clear as the training and test data.
100% test accuracy isn't bad but not a final performance metric...",,,,,53.64116699,57.46421352,58.26281495,55.62611371,58.07506201,,,,
43191,Validation loss is not decreasing,machine-learning,"The  model is overfitting  right from epoch 10, the validation loss is increasing while the training loss is decreasing. 
 Dealing with such a Model: 
 
 Data Preprocessing:  Standardizing and Normalizing the data. 
 Model compelxity:  Check if the model is too complex. Add dropout, reduce number of layers or number of neurons in each layer. 
 Learning Rate and Decay Rate:  Reduce the learning rate, a good starting value is usually between 0.0005 to 0.001. Also consider a decay rate of 1e-6. 
 
 There are many other options as well to reduce overfitting, assuming you are using Keras, visit  this link .","Yes this is an overfitting problem since your curve shows point of inflection.
This is a sign of very large number of epochs. 
In this case, model could be stopped at point of inflection or the number of training examples could be increased.  
 Also, Overfitting is also caused by a deep model over training data. In that case, you'll observe divergence in loss between val and train very early.","Another possible cause of overfitting is improper data augmentation. If you're augmenting then make sure it's really doing what you expect. 
 I had a similar problem, and it turned out to be due to a bug in my Tensorflow data pipeline where I was augmenting before caching: 
     def get_dataset(inputfile, batchsize):
        # Load the data into a TensorFlow dataset.
        signals, labels = read_data_from_file(inputfile)
        dataset = tf.data.Dataset.from_tensor_slices((signals, labels))
    
        # Augment the data by dynamically tweaking each training sample on the fly.
        dataset = dataset.map(
                    map_func=(lambda signals, labels: (tuple(tf.py_function(func=augment, inp=[signals], Tout=[tf.float32])), labels)))
    
        # Oops! Should have called cache() before augmenting
        dataset = dataset.cache()
        dataset = ... # Shuffle, repeat, batch, etc.
        return dataset
            
    training_data = get_dataset(""training.txt"", 32)
    val_data = //...
    
    model.fit(training_data, validation_data=val_data, ...)
 
 As a result, the training data was only being augmented for the first epoch. This caused the model to quickly overfit on the training data. Moving the augment call after cache() solved the problem.","I had this issue - while training loss was decreasing, the validation loss was not decreasing. I checked and found while I was using LSTM: 
 
 I simplified the model - instead of 20 layers, I opted for 8 layers. 
 Instead of scaling within range  (-1,1) , I choose  (0,1) , this right there reduced my validation loss by the magnitude of one order 
 I reduced the batch size from 500 to 50 (just trial and error) 
 I added more features, which I thought intuitively would add some new intelligent information to the X->y pair","It may be that you need to feed in more data, as well. If the model overfits, your dataset may be so small that the high capacity of the model makes it easily fit this small dataset, while not delivering out-of-sample performance. In other words, it does not learn a robust representation of the true underlying data distribution, just a representation that fits the training data very well. 
 Solutions as stated above: 
 
 reduce model complexity: if you feel your model is not really overly complex, you should try running on a larger dataset, at first. 
 regularization: using dropout and other regularization techniques may assist the model in generalizing better. 
 
 I propose to extend your dataset (largely), which will be costly in terms of several aspects obviously, but it will also serve as a form of ""regularization"" and give you a more confident answer. In case you cannot gather more data, think about clever ways to augment your dataset by applying transforms, adding noise, etc to the input data (or to the network output).","It's not severe overfitting. So, here is my suggestions: 
 1- Simplify your network! Maybe your network is too complex for your data. If you have a small dataset or features are easy to detect, you don't need a deep network. 
 2- Add Dropout layers. 
 3- Use weight regularization. Here is the link for further information:
 https://keras.io/api/layers/regularizers/",,,,62.38291672,51.45390137,50.73556893,78.03829083,50,50,,,
43148,How Do I Learn Neural Networks?,machine-learning,"If you want a good and solid start for deep learning, I would suggest to start with the book ""Deep Learning"" by Ian Goodfellow et al.  After that you'll have a good base that you can expend by the many different tutorials, articles and courses available online. 
 However, I would also add that before doing that, you should take some basic ""machine learning"" class (should be available at your University). Many people these days go straight to deep learning and implementing Neural networks because it is relatively easy, but than they lack the understanding to improve it or use it to its fullest potential.","As others suggested are very good resources. If you want in-depth Knowledge, I would suggest a course by  Andrew Ng  on Coursera. It covers in-depth knowledge of the basics of ML and if you are confused about whether you begin with AI, ML, or deep learning You could follow the blog link in my profile. I recently posted  how to go with these technologies . 
 PS: I am not advertising here my blog. I am just helping. If you want to follow you may follow otherwise just go with Andrew Ng.","I have a Master's in Computer Science and my thesis was about time-series prediction using Neural Networks.  
 The book  Hands on machine learning with Scikit and Tensorflow  was extremely helpful from a practical point of view. It really lays things very clearly, without much theory and math. I strongly recommend it. 
 On the other hand, the  book  by Ian Goodfellow is also a must (kind of the bible of DL). There you'll find the theoretical explanations, also it will leave you much much more knowledgeable with regards to deep learning and the humble beginning of the field till now.  
 Another, as others have suggested, is of course,  Deep Learning with Python  by Chollet. I indulged reading this book. Indeed it was very well written, and again, it teaches you tricks and concepts that you hardly grasp from tutorials and courses online. 
 Furthermore, I see you are familiar with Matlab, so maybe you have taken some stats/probability classes, otherwise, all these will overwhelm you a bit.","I suggest starting with  Google’s Crash Course on ML  if you want to revisit the basics. I then suggest to follow  fast.ai’s ML and DL lessons . For reading I suggest  Introduction to Machine Learning  by Alex Smola and S.V.N. Vishwanathan.
Have a nice day!","I highly suggest you to read this great book: hands on machine learning with Scikit and Tensorflow. Neural networks are presented succinctly in chapters 9 and 10. There are a lot of examples for you to practice. To effectively understand the script of examples you should have background of Python programming. 
Have a nice day!","Deep Learning with Python  by François Chollet is a great, high-level introduction into deep learning by the author of Keras.","There are many good websites for self-learning. Following are 2 examples: 
 Machine Learning Mastery | Deep Learning (Keras) 
 An Introductory Guide to Deep Learning and Neural Networks (Notes from deeplearning.ai Course #1) 
 These are especially helpful for practical aspects, maybe less so for theoretical background.","To add to the above references (the Deep Learning book by Goodfellow et al. is a must if you want to go deep into the subject), an excellent hands-on book is  dive into deep learning  that gives a state of the art approach (computer vision, NLP) using the gluon API (mxnet framework, see also  the straight dope ). I also highly recommend the resources in the PyTorch software ( tutorials ).","I really love that book!!  This  is a great resource as well. 
 Try to take each model apart, and put it back together, with your own specific datasets being fed in. If you can use your own data, rather than the sample data that they give you, and make it work, I think you will have the skills you need to do whatever you want to do.",58.09947266,51.00299966,55.95097788,51.50170447,57.64107867,53.31395134,67.04716347,52.38570655,50
42621,Data science related funny quotes,machine-learning,"Q: How many machine learning specialists does it take to change a light bulb? 
 A: Just one, but they require a million light bulbs to train properly. 
 Q: How many machine learning specialists does it take to change a fluorescent light bulb? 
 A: That wasn't in the training data!","Neural Network are not black boxes. They are a big pile of linear algebra : 
 
 image from  xkcd","I find this funny because it's true. 
 
 source 
 
 Cute funny... 
 
 
 This one always cracks me up for no reason...","If you torture data long enough, it will tell you whatever you want to hear. 
 Statistics shows that statistics cannot be trusted.","Question:  What's the different between machine learning and AI? 
 Answer: 
 If it's written in Python, then it's probably machine learning. 
 If it's written in PowerPoint, then it's probably AI.","Frequentists vs. Bayesians 
 
 Transcript: 
 
 Did the sun just explode? 
  (It's night, so we're not sure) 
 [[Two statisticians stand alongside an adorable little computer that is suspiciously similar to K-9 that speaks in Westminster typeface]] 
 Frequentist Statistician:  This neutrino detector measures whether the sun has gone nova. 
 Bayesian Statistician:  Then, it rolls two dice. If they both come up as six, it lies to us.  Otherwise, it tells the truth. 
 FS:  Let's try.  [[to the detector]]   Detector! Has the sun gone nova? 
 Detector:  <<roll>>  YES. 
 Frequentist Statistician: 
 FS:  The probability of this result happening by chance is  $\frac1{36}=0.027$ .  Since  $p< 0.05$ , I conclude that the sun has exploded. 
 Bayesian Statistician: 
 BS:  Bet you $50 it hasn't. 
 
 Title text: 
 
 'Detector! What would the Bayesian statistician say if I asked him whether the–'  [roll]  'I AM A NEUTRINO DETECTOR, NOT A LABYRINTH GUARD. SERIOUSLY, DID YOUR BRAIN FALL OUT?'  [roll]  '... yes.'",​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​,"Let me embrace thee, sour adversity, for wise men say it is the wisest course. 
 Yann Le Trump! 😂😂😂","Unsure whether they qualify, but there are some fun facts taken from various sources: 
 Beginning  from Yann Lecun : 
 
 Geoff Hinton doesn't need to make hidden units. They hide by
themselves when he approaches. 
 Geoff Hinton doesn't disagree with you, he contrastively diverges 
(from Vincent Vanhoucke) 
 Shakespeare and Bayes are in a boat, fishing. Bayes is trying to figure out which net to cast when Shakespeare says:
""loopy or not loopy? that is the question"". 
 Deep Belief Nets actually believe deeply in Geoff Hinton. 
 Geoff Hinton discovered how the brain really works. Once a year for 
the last 25 years. 
 Bayesians are the only people who can feel marginalized after being integrated 
 And now the legend: 
 
 
 One from  Reddit : 
 YOLO: you only LEARN once 
 P.S: Ian Goodfellow and Jurgen Schmidhuber are co-authoring a paper  (to be presented at NIPS 2019)  on Inverse GANs (More jokes on the topic  here )",50.48200314,50,67.7860897,51.04281385,50,50,,50,50
42599,What is the relationship between the accuracy and the loss in deep learning?,neural-network,"There is no relationship between these two metrics. 
 
 Loss can be seen as a  distance  between the true values of the problem and the values predicted by the model. The larger the loss, the larger the errors you made on the data. 
 Accuracy can be seen as the  count  of mistakes/misclassifications you made on the data. The larger the accuracy, the fewer misclassifications you made on the data. 
 
 That means: 
 
 large loss and small accuracy means you made huge errors on a lot of data (worst case) 
 small loss and small accuracy means you made small errors on a lot of data 
 small loss  with a large accuracy means you made small errors on a few data (best case) 
 large loss but  a large accuracy means you made huge errors on a few data (your case; the third model) 
 
 For your case, the third model can correctly predict more examples (large accuracy), but on those where it was wrong, it made larger errors (large loss - the distance between true value and predicted values is greater). 
 NOTE: 
 Don't forget that loss is a  subjective  metric, which  depends  on the problem and the data. It's a distance between the true value of the prediction, and the prediction made by the model. 
 
 The  significance  of the loss value is relative to the data itself; if your data are between 0 and 1, a loss of 0.5 is huge, but if your data are between 0 and 255, an error of 0.5 is low. 
 The  acceptability  of a loss value depends on the problem; consider cancer detection, where an error of 0.1 is unacceptably huge for this problem, whereas an error f 0.1 for image classification is fine.","Actually,  accuracy  is a metric that can be applied to classification tasks only. It describes just what percentage of your test data are classified correctly. For example, you have binary classification cat or non-cats. If out of 100 test samples 95 is classified correctly (i.e. correctly determined if there's cat on the picture or not), then your accuracy is 95%. By the way, Confusion matrix describes your model much better then accuracy. 
 Loss  depends on how you predict classes for your classification problem. For example, your model use probabilities to predict binary class cat or non-cats between 1 and 0. So if probability of cat is 0.6, then the probability of non-cat is 0.4. In this case, picture is classified as cat. Loss will be sum of the difference between predicted probability of the real class of the test picture and 1. In reality log loss is used for binary classification, I just gave the idea of what loss is.","The other answers give good definitions of accuracy and loss. To answer your second question, consider this example: 
 We have a problem of classifying images from a balanced dataset as containing either cats or dogs.
Classifier 1 gives the right answer in 80/100 of cases, whereas classifier 2 gets it right in 95/100. Here, classifier 2 obviously has the higher accuracy. 
 However, in the 80 of images classifier 1 gets right, it is extremely confident (for instance when it thinks an image is of a cat it is 100% sure that's the case), and in the 20 it gets wrong it was not at all confident (e.g. when it said a cat image contained a dog it was only 51% sure about that). In comparison, classifier 2 is extremely confident in its 5 wrong answers (it's 100% convinced that an image which actually shows a dog is a cat), and was not very confident about the 95 it got right. In this case, classifier 2 would have worse loss.","Someone says that accuracy has no relationship to the loss, but from a theoretical perspective, there IS a relationship. 
 Accuracy is  $1 - (error\ rate)$  and the error rate can be seen as the expectation of the 0-1 loss:
 \begin{equation}
l_{01}(f(x), y) := 
\begin{cases}
0 & (f(x) = y) \\
1 & (f(x) \neq y)
\end{cases}
\end{equation} 
 \begin{equation}
error\ rate = \mathbb{P}_{x, y} \left[ f(x) \neq y \right] =  \mathbb{E}_{x, y} \left[ l_{01}(f(x), y) \right] 
\end{equation} 
where  $f$  is the model,  $x$  is its input and  $y$  is the ground truth label for  $x$ . 
 In order to maximize the accuracy, we want to minimize the error rate.
However, due to the incontinuity of the 0-1 loss, it is practically impossible. Instead, a variety of ""surrogate loss"" is used. The surrogate loss function  $l$  is required to have some properties: 
 
 $l$  is continuous. 
 $l$  is convex. 
 $l$  bounds  $l_{01}$  from above. 
 
 Surrogate losses with these properties allow us to minimize them via the well-known gradient descent algorithm. 
 Popular classes of those surrogate losses include the hinge loss that is used in support vector machine (SVM) and the logistic loss that is used in logistic regression and standard neural networks. 
 So, from a theoretical viewpoint, the accuracy and the loss displayed in every epoch of your training have some relationship. That is, 
 
 Accuracy has a direct connection with the error rate, which we want to minimize in the training. 
 Loss (usually the cross entropy loss, which is equivalent to the logistic loss in a sense) is a  surrogate loss  that bounds the error rate.","In deep learning, during the training process, you typically monitor both the training and validation accuracy and loss to assess the performance of your model. Here's a brief explanation of each: 
 Training Accuracy and Loss: 
 Training Accuracy: This metric measures the percentage of correctly classified samples in the training dataset. It indicates how well the model is performing on the data it is being trained on.
Training Loss: This metric represents a measure of how well the model is performing on the training data. It quantifies the difference between the predicted values and the actual values in the training dataset. The goal during training is to minimize this loss function. 
 Validation Accuracy and Loss: 
 Validation Accuracy: This metric measures the percentage of correctly classified samples in a separate validation dataset that the model hasn't seen during training. It provides an estimate of how well the model is generalizing to new, unseen data.
Validation Loss: Similar to training loss, validation loss measures how well the model is performing on the validation dataset. It helps to identify overfitting or underfitting. Like training loss, the goal is to minimize this loss function, but its value might increase if the model starts overfitting to the training data.
During the training process, these metrics are often plotted over epochs (iterations over the entire dataset) to visualize the performance of the model. Ideally, you want to see both training and validation accuracy increasing and both training and validation loss decreasing. If the training accuracy continues to increase while the validation accuracy stagnates or decreases, it might indicate overfitting. Similarly, if the training loss decreases while the validation loss increases, it might also indicate overfitting. 
 Monitoring these metrics helps in tuning hyperparameters, selecting models, and understanding how well the model is learning from the data.","""There is no relationship between these two metrics."" isn't really accurate. Of course, there is a relationship between those two. Indeed, not a linear one. As @JérémyBlain noted, one can't really decide how well your model is based on the loss. That's why loss is mostly used to debug your training. Accuracy, better represents the real world application and is much more interpretable. But, you lose the information about the distances. A model with 2 classes that always predicts 0.51 for the true class would have the same accuracy as one that predicts 0.99. – 
 
 Adding to the comment of the currently top answer. The question itslef might imply the outmost interpreation of the question about the application to the real world that still offers such words handled metrics, but if interpret about the mechnism of the method these ouput values comes from, beside what I just quoted, could we not mention that the loss function (and its full training or validation dataset response ""surface"") is what the method is trying to minimize over the dataset sample (not sample data point, sample data set).  If gradient descent for example, it would be about greedily looking at ""slope"" of the discretize sampling version of such function, etc... and the loss function or the problem might have been design so that in spite of looking locally for the optimal direction of udating it would find the overall method more probably global optimum.  The loss function at any datapoint of the sample data set is determining in the global optimization search that yield the final object or tools that spits out the variables of the question.
It is a metric of how well that under the hood segment or view pointof the core of the method has accomplished so optmization (whith appropriate generalization of whatever task the problem was well posed as, skipping details). My point is that it is totally related to the method used to produce the final inference user input output tool. It does give some sense that that lower level of how good a job it did.  But the previous answers assuming only the last level tool usage, are right that it would not be needed for your end user.  In all cases though, all those metrics have the target phenomenon reality and its data sampling method knowledge, and then the partitions of that data into training and validation (etc..) and then the method of optmization and then the various internal metrics as determinant dependent variable. in that sense.  The ""quoted-quoted"" statement is more rethorical or explanatory assuming an end user premise to the questoin, than the technical truth. I hope I am not slapshing or rambling out of context here. I thought this would be a nice supobptimal answers to expand for those coming in having that different context assumptions in mind.","Ayúdenme a aclarar esta duda por favor 
 Después de ver este comentario:
------si sus datos están entre 0 y 1, una pérdida de 0,5 es enorme, pero si sus datos están entre 0 y 255, un error de 0,5 es bajo.
Tengo las siguientes dudas: 
 Yo hice un modelo con la ""Y"" categórica, son 4 categorías y las clasifique: ""0"",""1"",""2"" y ""3"", ¿esta bien como lo clasifique (empieza en 0 y no en 1? y ¿mis datos estarían entre 0 - 4 para evaluar el test loss y accuracy? 
 este fue mi resultado:
Test Loss: [0.9032219648361206, 3.8990705013275146]",,,58.36875052,57.27910904,55.74218009,65.27948651,62.37923669,55.08631911,53.29386717,,
41846,"In a binary classification, should the test dataset be balanced?",machine-learning,"The answer to your first question:  
 
 should the test dataset be balanced as well? 
 
 is, like many answers in data science, ""it depends."" 
 And really, it depends on the audience for, and interpretability of the model metrics, which is the thrust of your second question: 
 
 What is the impact of an imbalanced test dataset on the precision, recall, and overall accuracy metrics? 
 
 Personally, if the metrics will just be used by you to evaluate the model, I would use the  sensitivity  and  specificity  within each class to evaluate the model, in which case, I care less about the balance of the classes in the test data as long as I have enough of both to be representative. I can account for the prior probabilities of the classes to evaluate the performance of the model. 
 On the other hand, if the metrics will be used to describe predictive power to a non-technical audience, say upper management, I would want to be able to discuss the overall accuracy, for which, I would want a reasonably balanced test set. 
 That said, it sounds like your test set is drawn independently of the training data.  If you are going to balance the training data set, why not draw one balanced data set from the raw data and then split the training and test data?  This will give you very similar class populations in both data sets without necessarily having to do any extra work.","There are several ways of balancing classes. Either you can increase the number of samples of minority class or decrease the number of samples of majority class.
Once you have balanced your classes, for a start you take 80% of it as train and test it on remaining 20%. In your case 4519 samples are 1 and 18921 are 0 so lets say you upsample 4519 and now you have 15,000 samples of class 1. You will take 80% of (15000+18921) and train your model. Test it on remaining 20%. 
 Recall becomes important when you are let's say identifying fraudulent transactions out of several millions. You will have very few classes labelled as 1 (fraudulent). So even you simply label all the classes in your test data as 0 you will be more than 99% accurate but you should ask yourself the question ""Out of all the actual classes with label 1 how many did i get right from the trained model"". That is what recall gives you.","Your model could have very dissimilar performances at detecting ""0"" or ""1"" samples without you noticing if the performance is not high enough. 
 I would include more samples in the training and subsample ""0"" (to keep balance of ""0"" and ""1"" in each step) during the training and would evaluate the model with a balanced test set.","A fast solution: If you use SKLearn's Random Forest, I highly suggest ""class_weight"" => “balanced”.  
 The argument will automatically weigh classes inversely proportional to their frequency.","I don't think test dataset should follow the ratio of classes of training one.  Class ratio just follows that of real population s.t. test result properly reflects what'll actually happen with the actual predictions in the real world.  Class balancing/sampling in training phase with training dataset is independently optimized to the evaluation performance over the test dataset. 
 And the class weight techniques become useful in case the classes are so imbalanced in your test dataset, to avoid any confusion matrix paradox.",,,,,64.50828475,54.59696301,57.40971313,53.72952839,65.33218686,,,,
41698,How to apply class weight to a multi-output model?,neural-network,"I wansn't able to use the  class_weight  parameter yet, but in the mean time i've found another way to apply class weighting to each output layer.   
 Current solution 
 In  this  keras issue they have supplied an easy method to apply class weights via a custom loss that implements the required class weighing. 
 def weighted_categorical_crossentropy(y_true, y_pred, weights):
    nb_cl = len(weights)
    final_mask = K.zeros_like(y_pred[:, 0])
    y_pred_max = K.max(y_pred, axis=1)
    y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))
    y_pred_max_mat = K.cast(K.equal(y_pred, y_pred_max), K.floatx())
    for c_p, c_t in product(range(nb_cl), range(nb_cl)):
        final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])
    return K.categorical_crossentropy(y_pred, y_true) * final_mask
 
 where  weights  is a  CxC  matrix (where  C  is the number of classes) that defines the class weights. 
More precisely,  weights[i, j]  defines the weight for an example of class  i  which was falsely classified as class  j . 
 So how do we use it? 
 Keras allows to assign a loss function for each output. 
so we could assign each output a loss fucntion with the correct  weights  matrix.   
 For example, to satisfy the request i made in the question we could suggest the following code. 
 # Define the weight matrices
w1 = np.ones((2, 2))
w1[1, 0] = 10
w1[1, 1] = 10

w2 = np.ones((3, 3))
w2[0, 0] = 5
w2[0, 1] = 5
w2[0, 2] = 5
w2[2, 0] = 10
w2[2, 1] = 10
w2[2, 2] = 10    

# Define the weighted loss functions
from functools import partial
loss1 = partial(weighted_categorical_crossentropy, weights=w1)
loss2 = partial(weighted_categorical_crossentropy, weights=w2)

# Finally, apply the loss functions to the outputs
model.compile(loss={'output1': loss1, 'output2': loss2}, optimizer='adam')
 
 And that accomplishes the request :) 
 Edit 
 There is a small edition that needs to be made. 
The loss functions must have a name, so we can supply this with the following: 
 loss1.__name__ = 'loss1'
loss2.__name__ = 'loss2'","Pass a dictionary in the following format to class_weight parameter in fit_generator: 
 { 'output1': {0: ratio_1 , 1: ratio_2} , 'output2': {0: ratio_3 , 1: ratio_4}}
 
 You can use  class_weight  from  sklearn.utils  to calculate class weights from your data 
 EDIT: This approach works in TF 2.1.0 and earlier versions only. Thanks for replies. 
 References: 
 https://github.com/keras-team/keras/issues/4735#issuecomment-267473722 
 https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html","You can pass the weights as follows: 
 class_weight={'Output_1':None,'Output_2':[1,2,1]}
 
 where  Output_2  is a softmax with 3 classes.","You have a list of outputs. You can simply pass a list of class_weight for each output as follows: 
 class_weight = [{0: 1, 1: 10},{0: 5, 1: 1, 2: 10}]","I have configured it as follows: 
 class_weights = {0: weight_1_for_0 , 1: weight_1_for_1,2: weight_2_for_0 , 3: weight_2_for_1} 
 This is for a neural network with two output neurons and it seems to be functioning correctly, but I am not entirely certain that the weights are being applied correctly in TensorFlow 2.11. Here is a portion of the code: 
 weighted_history = weighted_model.fit(
    train_features,
    train_labels,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=[early_stopping],
    validation_data=(val_features, val_labels),
    # The class weights go here
    class_weight=class_weights)```","Here's my solution for sparse categorical crossentropy for a Keras model with multiple outputs in TF2. I think it looks fairly clean but it might be horrifically inefficient, idk. 
 First create a dictionary where the key is the name set in the output Dense layers and the value is a  1D constant tensor. The value in index 0 of the tensor is the loss weight of class 0, a value is required for all classes present in each output even if it is just 1 or 0. 
 Compile your model with  
 model.compile(optimizer=optimizer,
              loss={k: class_loss(v) for k, v in class_weights.items()})
 
 where  class_loss()  is defined in the following manner  
 def class_loss(class_weight):
  """"""Returns a loss function for a specific class weight tensor

  Params:
    class_weight: 1-D constant tensor of class weights

  Returns:
    A loss function where each loss is scaled according to the observed class""""""
  def loss(y_obs, y_pred):
    y_obs = tf.dtypes.cast(y_obs, tf.int32)
    hothot = tf.one_hot(tf.reshape(y_obs, [-1]), depth=class_weight.shape[0])
    weight = tf.math.multiply(class_weight, hothot)
    weight = tf.reduce_sum(weight, axis=-1)
    losses = tf.compat.v1.losses.sparse_softmax_cross_entropy(labels=y_obs,
                                                              logits=y_pred,
                                                              weights=weight)
    return losses
  return loss
 
 If someone has a better suggestion than using  tf.compat.v1  then please let me know. I don't feel confident that it will stick around through future versions of Tensorflow. 
I also posted this answer here:  https://github.com/keras-team/keras/issues/11735#issuecomment-641775516 
 EDIT: Be aware that this is for an output with a linear output rather than a softmax output! You have to softmax the outputs afterwards if you want softmax values (but if you just want the predictions ranked then logits still work).","I found an efficient solution using matrix multiplication. You can use this instead. Here weights is the corresponding weight matrix 
 def weighted_efficient_loss(y_true, y_pred, weights):
        weights = K.constant(np.array(weights))
        return K.categorical_crossentropy(y_true, y_pred) * K.sum((K.dot(y_true,weights) * y_pred), axis=1)",,,65.11070714,65.72102101,68.79026685,71.18846625,67.19005716,67.27647505,56.48445194,,
41113,DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20,machine-learning,"You can do this to get rid of the deprecation messages 
 from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

ct = ColumnTransformer(
    [('one_hot_encoder', OneHotEncoder(), [0])],    # The column numbers to be transformed (here is [0] but can be [0, 1, 3])
    remainder='passthrough'                         # Leave the rest of the columns untouched
)

x = np.array(ct.fit_transform(x), dtype=np.float)","For now, there is nothing you should need to do. The code should work even with these warnings.  Technically, they are not errors.   
 If you want to build some model based on this example, you should probably resolve them.  Most of the information you need is in the warning. For example: 
 In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly. 
 So when you move to  sklearn  version 0.22, you don't need to use both the  LabelEncoder()  and the  OneHotEncoder() , you can do it all in the  OneHotEncoder() , but you will probably need to review the version specific documentation to figure out how to do this and meet you specific needs when the version is released. 
 For now, don't do anything.","# Importing the Libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#import dataset
dataset = pd.read_csv(""Data.csv"")
X = dataset.iloc[:,:-1].values
Y = dataset.iloc[:,3].values

#Taking care of Missing data
from sklearn.impute import SimpleImputer  
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer = imputer.fit(X[:,1:3])
X[:,1:3] = imputer.transform(X[:,1:3])

#Encoding Categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
labelencoder_X = LabelEncoder()
X[:, 0] = labelencoder_X.fit_transform(X[:, 0])
transformer = ColumnTransformer([('one_hot_encoder', OneHotEncoder(), [0])],remainder='passthrough')
X = np.array(transformer.fit_transform(X), dtype=np.float)
labelencoder_Y = LabelEncoder()
Y = labelencoder_Y.fit_transform(Y)","Document for  ColumnTransformer 
Example to check it.
    # TODO: create a LabelEncoder object and fit it to each feature in X 
 # import preprocessing from sklearn
from sklearn import preprocessing
# 1. INSTANTIATE
# encode labels with value between 0 and n_classes-1.
le = preprocessing.LabelEncoder()
# 2/3. FIT AND TRANSFORM
# use df.apply() to apply le.fit_transform to all columns
X_2 = X.apply(le.fit_transform)
X_2.head()
 
 If you wish to see an end to end example please  check .","In OneHotEncoder, use the parameter handle_unknown, it should look something like this, and now onehotencoder is auto on the dataset X, so you can remove the categorical_features or instead keep auto, removing it will solve the error: 
 onehotencoder = OneHotEncoder(categorical_features=[i], handle_unknown='ignore')

onehotencoder = OneHotEncoder(handle_unknown='ignore')
 
 Thanks.",,,,,54.09693572,53.92201243,50.79901783,50.81098691,58.65733483,,,,
40462,How to prepare the varied size input in CNN prediction,machine-learning,"Conventionally, when dealing with images of different sizes in CNN(which happens very often in real world problems), we resize the images to the size of the smallest images with the help of any image manipulation library (OpenCV, PIL etc) or some times, pad the images of unequal size to desired size. Resizing the image is simpler and is used most often.  
 As mentioned by Media in the above answer, it is not possible to directly use images of different sizes. It is because when you define a CNN architecture, you plan as to how many layers you should have depending on the input size. Without having a fixed input shape, you cannot define architecture of your model. It is therefore necessary to convert all your images to same size.","There is a way to include both image sizes. You can preprocess your images so that they are re-sized to the same dimensions.  
 Some of the freely available code that shows this: 
 img_width, img_height = 150, 150

train_data_dir = '/yourdir/train'
validation_data_dir = '/yourdir/validation'
nb_train_samples = 
nb_validation_samples = 
epochs = 50
batch_size = 16

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)



model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.3))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])


train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

 
 This uses the Keras image flow API for data augmentation on the fly, and the data generators at the bottom of the code will adjust your images to whatever dimensions you specify at the top.","There is a  concatenate  function in Keras ( docs  and  docs ). 
Also see  this paper . Its application can be seen  here  and  here . 
 This method can be used to have multiple input channels with different image sizes.","One way is to pad the images while training. That is to say, while training, Keras will expect all tensors in a batch to be of the same size. However, while inference, if you use only a single image, it can be of any size. So what you can do while training is to pad your 100 x 100 images so that their new dimension after padding becomes 240 x 360. 
 You can have a look at  this tutorial .","At least, as far as I know, you can't. The reason is clear. In neural networks, you attempt to find appropriate weights to diminish a typical cost function. You have to find appropriate weights for a specified number of predefined weights. When you specify an input shape, the rest of the network weights will depend on the weights of input. You can't change the input size of a network. In other words, you can't feed your network with different input sizes for convolutional networks. A typical solution for dealing with such situations is to resize the input.","There are some ways to deal with it but they do not solve the problem well. You can use black pixels, special values for nan, resizing and a separate mask layer that says where the information on the picture is. But most likely they are working not so well. Otherwise the image datasets would have images of different sizes. Separate layers for masks is used in the currently best image recognition neural network (SENet. Hu et al. Winner of ImageNet in 2017). But they use masking for zooming into the picture and not for different image sizes.",,,,60.21558346,53.60412013,53.88684437,52.80394626,56.51395538,52.35539949,,,
40089,What is the reason behind taking log transformation of few continuous variables?,machine-learning,"This is done when the variables span several orders of magnitude. Income is a typical example: its distribution is ""power law"", meaning that the vast majority of incomes are small and very few are big.  
 This type of ""fat tailed"" distribution is studied in logarithmic scale because of the mathematical properties of the logarithm: 
 $$log(x^n)= n log(x)$$ 
 which implies 
 $$log(10^4) = 4 * log(10)$$ 
 and  
 $$log(10^3) = 3 * log(10)$$ 
 which transforms a huge difference  $$ 10^4 - 10^3 $$  in a smaller one  $$ 4 - 3 $$ 
Making the values comparable.","Mostly because of skewed distribution. Logarithm naturally reduces the dynamic range of a variable so the differences are preserved while the scale is not that dramatically skewed. Imagine some people got 100,000,000 loan and some got 10000 and some 0. Any feature scaling will probably put 0 and 10000 so close to each other as the biggest number anyway pushes the boundary. Logarithm solves the issue.","You should look at the  lognormal distribution . 
 People may use logs because they think it compresses the scale or something, but the principled use of logs is that you are working with data that has a lognormal distribution. This will tend to be things like salaries, housing prices, etc, where all values are positive and most are relatively modest, but some are very large. 
 If you can take the log of the data and it becomes normalish, then you can take advantage of many features of a normal distribution, like well-defined mean, standard deviation (and hence z-scores), symmetry, etc. 
 Similarly, addition of logs is the same as multiplication of the un-log'd values. Which means that you've turned a distribution where errors are additive into one where they're multiplicative (i.e. percentage-based). Since techniques like OLS regression require a normal error distribution, working with logs extends their applicability from additive to multiplicative processes.","In addition to the other answers, another side-effect of taking  $\log{x}$  is that if  $0 < x  < \infty$ , again for example with loans or incomes, basically anything that cannot become negative, the domain becomes  $-\infty < \log{x} <\infty$ . 
 This can be helpful, especially in return variables, if the model you are using is based on assuptions about the distribution of  $x$ . For example the assumption of normality in linear models.","Yet another reason why logarithmic transformations are useful comes into play for ratio data, due to the fact that  log(A/B) = -log(B/A) . If you plot a distribution of ratios on the raw scale, your points fall in the range  (0, Inf) . Any ratios less than 1 will be squished into a small area of the plot, and furthermore, the plot will look completely different if you flip the ratio to  (B/A)  instead of  (A/B) . If you do this on a logarithmic scale, the range is now  (-Inf, +Inf) , meaning ratios less than 1 and greater than 1 are more equally spread out. If you decide to flip the ratio, you simply flip the plot around 0, otherwise it looks exactly the same. On a log scale, it doesn't really matter if you show a ratio as  1/10 or 10/1 , which is useful when there's not an obvious choice about which it should be.",I'd say the main reason is not distributional but rather because of the non linear relationship. Logs often capture saturating relationships...,,,,60.95661408,51.49501943,60.6636147,59.2919119,54.88492922,55.77867094,,,
39960,remove special character in a List or String,machine-learning,"Regular expressions can be used to create a simple tokenizer and normalizer: 
 from __future__ import annotations
import re

def tokens(text: str) -> list(str):
    ""List all the word tokens in a text.""
    return re.findall('[\w]+', text.lower())

assert tokens(""To be, or not to be, that is the question:"") == ['to', 'be', 'or', 'not', 'to', 'be', 'that', 'is', 'the', 'question']
 
 Otherwise, use an established library like  spaCy  to generate a list of tokens.","This can be achieved by Regular Expresiions 
 import re
modified_string = re.sub(r'\W+', '', input_string) # on individual tokens 
modified_string = re.sub(r'[^a-zA-Z0-9_\s]+', '', input_string) # on sentence itself.Here I have modified RegEx to include spaces as well
 
 '\W == [^a-zA-Z0-9_], so everything except numbers,alphabets and _ would be replaced by space","I would use regex. I can only say what would work for the input sentence I can deduce from your desired output: 
 s = 'to be, or not to be: that is the question!'
 
 I simply remove all characters that are not letters (upper or lower case) or spaces. 
 import re

pattern = r'[^A-Za-z ]'
regex = re.compile(pattern)

result = regex.sub('', s).split(' ')

print(result)
 
 
 ['to', 'be', 'or', 'not', 'to', 'be', 'that', 'is', 'the', 'question'] 
 
 
 Edit 
 Based on the update comment from OP - my answer can be adjusted to work on each of the words via simple interation of the sentences: 
 cleaned_sentenced = []    # will become a list of lists

for sentence in sentences:
    temp = [regex.sub('', word) for word in sentence]
    cleaned_sentences.append(temp)
 
 This uses  regex  as defined up above.","Here is another option for you, but it should be a bit more slow than the rest of the answers. 
 import string
s = 'to be, or not to be: that is the question!'
punct_set= set(string.punctuation)#Saving punctuation into a set
s = ''.join(ch for ch in s if ch not in punct_set)#Get every character and remove punct
 
 Another way is to use the  translate  method. In python 3, a dictionary should be passed to the method.  None  maps the character that will be removed. 
 import string
s = 'to be, or not to be: that is the question!'
translation = dict.fromkeys(map(ord, string.punctuation), None)#Dictionary with punctuation to be removed
no_punct_s = s.translate(translation)","You can simply use the python regular expression library  re . It will look something like this: 
 import re

def text2word(text):
    '''Convert string of words to a list removing all special characters'''
    result = re.finall('[\w]+', text.lower())
    return result
 
 If you can log the result on the console to see the output that the function returns 
 For example: 
 string = "" To be or not to be: that is the question!""
print(text2word(string))
 
 Ouput:
 ['to', 'be', 'or', 'not', 'to', 'be', 'that', 'is', 'the', 'question']",,,,,54.16726699,59.98248474,54.51585588,62.25897025,68.23186986,,,,
39264,How does Sigmoid activation work in multi-class classification problems,machine-learning,"If your task is a kind of classification that the labels are mutually exclusive, each input just has one label, you have to use  Softmax . If the inputs of your classification task have multiple labels for an input, your classes are not mutually exclusive and you can use  Sigmoid  for each output. For the former case, you should choose the output entry with the maximum value as the output. For the latter case, for each class, you have an activation value which belongs to the last sigmoid. If each activation is more than  0.5  you can say that entry exists in the input.","softmax() will give you the probability distribution which means all output will sum to 1. While, sigmoid() will make sure the output value of neuron is between 0 to 1. 
 In case of digit classification and sigmoid(), you will have output of 10 output neurons between 0 to 1. Then, you can take biggest one of them and classify as that digit.","@bharath chandra A Softmax function will never give 3 as output. It will always output real values between 0 and 1. A Sigmoid function also gives output between 0 and 1. The difference is that in the former one, the sum of all the outputs will be equal to 1 (due to mutually exclusive nature) while in the latter case, the sum of all the outputs need not necessarily be equal to 1 (due to independent nature).","For Beginners :
You may read this  quora answer  Which explains Pros and cons of Sigmoid Activations and softmax Probability. there are 6 answers at the time of writing for inclusiveness . 
 Sigmoid vs Softmax 
 Answer Highlights  :  
 
 if you see the function of Softmax, the sum of all softmax units are supposed to be 1. In sigmoid it’s not really necessary. 
 In the binary classification both sigmoid and softmax function are the same where as in the multi-class classification we use Softmax function. 
 If you’re using one-hot encoding, then I strongly recommend to use Softmax. 
 
 What i Noticed : to the best of my knowledge >> Softmax is probability distribution for various possible classes (multi class) in our sample space. and all classes must be predefined in advance before passing anything to softmax activation layer via one-hot encoding. for example tokenization and word stemming in NLP to homogenize data. 
 For Not-beginners : 
on  the official Keras Page   softmax  documentation is given as: 
 softmax

keras.activations.softmax(x, axis=-1)

Softmax activation function.

Arguments

    x: Input tensor.
    axis: Integer, axis along which the softmax normalization is applied.

Returns

Tensor, output of softmax transformation.

Raises

    ValueError: In case dim(x) == 1.
 
 While for  Sigmoid  is given as: 
 sigmoid

keras.activations.sigmoid(x)

Sigmoid activation function.

Arguments

    x: Input tensor.

Returns

The sigmoid activation: 1 / (1 + exp(-x)).",If you want to classify multiple classes using logistic regression you would have to use multinomial logistic regression instead.,,,,,60.18471331,55.47414145,51.66311609,64.99881067,51.21632365,,,,
39100,Equivalent of numeric encoding when rows can contain multiple values,machine-learning,"The binary encoding seems to be the most natural, capturing exactly the present relationships and not introducing nonexistent relationships.  The combinations of names then are thought of as vertices of a hypercube; naturally, if you collapse this down to one dimension, you will be introducing nonexistent relationships.  Indeed, this is what happens with label encoding: Bob isn't really any closer to Alice than Dave is, but your label encoding makes that appear to be the case. 
 I've usually been happy enough with just the binary encoding, so I don't have much to back up the following, but some ideas: 
 For your case, you could still project the hypercube to one dimension; there are any number of ways to do it, and every one will add some additional information that shouldn't be there.  You could experiment and try to find one that seems most suitable for your purposes. 
 That should sound reminiscent of word embedding, and suggests another method: use a neural network to generate ""entity embeddings"", a projection from your hypercube to some smaller-dimensional space.  In this way, you uncover combinations of persons that are useful for your problem (maybe you end up with a feature that's ""at least one of Alice and Dave"", etc.).  You could also do something more transparent, and do some feature engineering by hand to pick out interesting combinations and binning to keep the number of features manageable.  For this you could consider target encoding each combination.  (Indeed, that's a very easy solution by itself, though target encoding can easily lead to overfitting if you have very many people or some rare combinations of people.) 
 Finally, you  might  be able to split a row containing (Alice, Dave) into two, one with Alice and the other with Dave.  You can now label-encode (or whatever else), and will need either a model that understands multiple rows per sample point, or a postprocessing method to deal with combining rows.","I think, it depends on how many unique values do you have and what do you want to do with this data next. Intermediate table seems good solution for now, but you're expecting overhead. So maybe it would be suitable for you to create bit masks in one-hot manner (Binary encoding).","To reduce the number of columns produced by onehot encoding one solution might be  target encoding . That is, if you have the following table : 
    Alice  Bob  Dave Other_features Target
0      1    1     0             X1      0
1      1    1     0             X2      1
2      1    1     0             X3      0
3      1    1     1             X4      1
4      1    1     1             X5      1
5      1    1     1             X6      1
6      0    0     1             X7      0
7      0    0     1             X8      0
 
 You can create a table : 
 Group               Group_Avg_target
Alice,Bob                      0.333
Alice, Bob, Dave                   1
Dave                               0
 
 Then replace the values in your dataset : 
    Group_Avg_target Other_features Target
0             0.333             X1      0
1             0.333             X2      1
2             0.333             X3      0
3                 1             X4      1
4                 1             X5      1
5                 1             X6      1
6                 0             X7      0
7                 0             X8      0
 
 That way you'll end up with one column. That mainly works if you have enough instances to have meaningfull averages by groups of people. I would not recommand using that in general as you would lose a lot of interactions betweeen people, which might be bad to explain what happened. 
 Note : to use this solution you need to make sure that there is no 'target leak', ie use the average calculated on train to encode your test dataset.","First, do the OneHotEncoder thing with your  Name  column which will give you three columns as you have stated above, Namely  Alice ,  Bob , and  Dave . 
 We have this Table after doing  OneHotEncoding  : 
     Alice  Bob  Dave
0   1      1    0     
1   1      1    1   
2   0      0    1     
 
 Now you can call  apply  function on these columns; pass  axis=1  to use the  apply  function row-wise, then convert the  dtype  to  str  and then  join  them as follows ( astype  function converts the input into the give data-type): 
 df['AliceBobDave']= df[df.columns[0:]].apply(lambda x: ','.join(x.dropna().astype(int).astype(str)),axis=1)
 
 The Above will give you output : 
      Alice  Bob  Dave  AliceBobDave
0    1      1    0     1,1,0
1    1      1    1     1,1,1
2    0      0    1     0,0,1
 
 Now run this command below to remove the  columns  not needed in the final dataframe. For ex. in this case we remove  Alice ,  Bob , and  Dave  using the  drop  function in pandas: 
 df = df.drop(['Alice','Bob','Dave'], axis = 1)
 
 After executing this command if we print  df  using  print(df) , we get: 
     AliceBobDave
0   1,1,0
1   1,1,1
2   0,0,1
 
 This is your desired output.","See my comments on @GrozaiL's post for my real thoughts on this (use some form of binary encoding, ideally one-hot columns), but if you're  really  dying for a method to assign values to subsets, why not a Godel Numbering? Assign each person a prime number and multiply them together, that'll give you one number for each subset and if you ever want to figure out which people were in the subset after the fact, you can just use the prime factorization of the assigned number to recover its membership! (Don't do this)",,,,,59.64969486,54.43473648,53.70729505,50.96437749,52.93016382,,,,
38973,Is it advisable to combine two dataset?,data,"Although generally in training a machine learning model, the more data you have the better for training generalised models, that may not be the case here. 
 Given that the two datasets were collected in completely different environments, they may have completely different distributions. In this case, training a model on the combined dataset may even reduce the performance of the model. 
 My advice would be, do some statistical analysis on each dataset independently - find the mean and variances of each of the variables for each dataset and compare them for example. If the analysis shows that the two datastes have fairly similar distributions (I’ll leave the definition of fairly similar up to you), then it should be ok to combine the two datasets to train a model on.","If you add ‘continent’ or ‘location’ as a feature for the model, then you will be able to control for potential bias while getting the results of the additional data.","Adding to what @Super_John said, if adding continents as a Feature, then you can also probably have at-least  2  more features as well, 
 
 The Latitude 
 The Longitude 
 
 Also add another temporary column to indicate the  Source  (like $1$ to $1st$ df, $2$ to $2nd$ df etc), So that we can add Colors to the  k-means 
 So now we can have a  k-means  Cluster to see whether values are overlapping or not...
(Trying to see it in an Unsupervised Way) 
 (The analogy is equivalent to the  fact that that you can cluster time(24  hours in a day) in a cyclic fashion , like plotting  $sin(x)$, $cos(X)$ and then trying to cluster them) 
 Have a look at this answer,
 Features Selection, Extraction","Yes,  usually  with ML, more data you have, better the results! Of course mixing data from different population is risky, but if it works you are on the right path. 
 Using more data helps generalise during the training of your model. So, if you are able to test your model over sample from both population and you obtain good result, you can do it.","To add to this discussion, a proper evaluation will tell you quite a bit, and can be used to present the work:  
 
 Create a test set for dataset 1.  
 Create a test set for dataset 2.  
 Train a model using only dataset 1, only dataset 2 and using a combination of dataset 1 and 2 evaluate their performances on both test sets. 
 
 If the combined model is significantly better than the separate models, you have something, and I think you can report as such in a possible publication. Of course, you will still have to motivate which machine learning model you use, your performance metric of interest, how you conduct cross-validation, ...","Before I could attempt to answer your questions, I will put across what I have understood. 
 Scenario: Two datasets with heart rate of subjects recorded in two different continents are available. 
 Aim: Find the subjects' emotions based on how much their heart rate change over time 
 Objective: Classify subjects' emotions  
 Noted: 
 
 Results are acceptable when trained and tested as separately.  
 Assume that results would improve upon combining two datasets 
 
 Questions:  
 
 Is combining the two datasets acceptable? 
 
 If the subjects of the two continents are same then there shouldn't be a problem in combining the datasets. Set of Emotions are pretty much the same across same subjects 
 
 As you are combining two somehow different datasets, will it create statistical bias? 
 
 As long as subjects of two datasets are same then combining will improve your results due to more data.  
 
 How should you report your findings in a journal paper? 
 
 You could perform hypothesis test(ANOVA) for two samples",,,,68.40317688,50,50,50,57.134159,78.47889634,,,
38543,How to statistically prove that a column in a dataframe is not needed,python,"With respect to the other answers i observed your point about having features which are strings, you must first find a way to encode them as numerical features which can then help you with PCA and the problem with the heat-map will be resolved too. Also if your problem is of regression then you can use methods such as L1-Regularization which help you in feature selection and you don't have to remove the features while pre-processing.","Maybe a principle component analysis (PCA) would be what you're looking for, identifying the components of your dataset that explain the most variance.","You could use a feature selector, Random Forests can be used as one, but first I think that you should transform those columns to a usable variable (numerical or categorical), for example, since you have dimensions, maybe create a column for height and another for width, to prevent unwanted behaviours. 
 Other straightforward methods could be Forward Selection, Backward Elimination and some more resources  here","I agree on Feature Selection. You can consult the Microsoft Learning Repository about ML with Python on  GitHub  , it provides you with some theoretical background as well as Python code to run. You could import and adapt that code for your own project.
cheers","Lots of other good ideas already, but two come to my mind: 
 
 Using a decision tree, you can demonstrate the importance of each field. 
 Another approach could be a test for multicollinerarity which will show columns that have substantially the same relationship with the target variable. In this case, you can eliminate one of the collinear fields.","All above answers are on point, but just keep in mind that whatever analysis you will do,  spurious correlations (seemingly significant) may occur , even though there is no causal link, as you claim (by knowing your domain, and basic logic, i.e.), or the link is purely technical (for example, if the variable would the rowid).  Essentially, such knowledge should be above any statistical test. Whatever statistical test you're using is only  suggesting  that with certain amount of significance there may or may not be a certain relationship, but no test can know for sure.   
 While slightly out of topic,  this discussion on statistical tests  might we a worthwhile read.",,,,50,50,54.62330394,50,52.73937804,54.34175449,,,
38540,Are there any good out-of-the-box language models for python?,python,"I also think that the first answer is incorrect for the reasons that @noob333 explained. 
 But also Bert cannot be used out of the box as a language model. Bert gives you the  p(word|context(both left and right) )  and what you want is to compute  p(word|previous tokens(only left contex)) . The author explains  here  why you cannot use it as a lm. 
 However you can adapt Bert and use it as a language model, as explained  here . 
 But you can use the open ai gpt or gpt-2 pre-tained models from the same  repo 
 Here  is how you can compute the perplexity using the gpt model. 
 import math
from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel
# Load pre-trained model (weights)
model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')
model.eval()
# Load pre-trained model tokenizer (vocabulary)
tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')

def score(sentence):
    tokenize_input = tokenizer.tokenize(sentence)
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
    loss=model(tensor_input, lm_labels=tensor_input)
    return math.exp(loss)


a=['there is a book on the desk',
                'there is a plane on the desk',
                        'there is a book in the desk']
print([score(i) for i in a])
21.31652459381952, 61.45907380241148, 26.24923942649312","I think the accepted answer is incorrect. 
 token.prob is the log-prob of the token being a particular type . I am guessing 'type' refers to something like POS-tag or type of named entity (it's not clear from spacy's documentation) and the score is a confidence measure over space of all types. 
 This is  not  the same as the probabilities assigned by a language model. A language model gives you the probability distribution over all possible tokens (not the type) saying which of them is most likely to occur next. 
 This repo has pretty nice documentation on using BERT (a state-of-the art model) with pre-trained weights for the neural network, 
 
 BERT 
 
 I think the API's don't give you perplexity directly but you should be able to get probability scores for each token  quite easily .","The  spaCy  package has many  language models , including ones trained on  Common Crawl . 
 Language model  has a specific meaning in Natural Language Processing (NlP). A language model is a probability distribution over sequences of tokens. Given a specific sequence of tokens, the model can assign a probability of that sequence appearing. SpaCy's language models include more than just a probability distribution. 
 The spaCy package needs to be installed and the language models need to be download: 
 $ pip install spacy 
$  python -m spacy download en
 
 Then the language models can used with a couple lines of Python: 
 >>> import spacy
>>> nlp = spacy.load('en')
 
 For a given model and token, there is a smoothed log probability estimate of a token's word type can be found with:  token.prob  attribute.","You can use the  lm_scorer  package to calculate the language model probabilities using GPT-2 models. 
 First install the package as: 
 pip install lm-scorer
 
 Then, you can create a scorer by specifying the model size. 
 from lm_scorer.models.auto import AutoLMScorer
scorer = AutoLMScorer.from_pretrained(""gpt2-large"")

def score(sentence):
    return scorer.sentence_score(sentence)
 
 Apply it to your text and you get back the probabilities. 
 >>> score('good luck')
8.658163769270644e-11
 
 You can also refer to a  blogpost  I had written a while back if you're looking for more details.","Here I would show how we can use transformers and the gpt model to compute the perplexity of a given sentence. 
 import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

# You can change to gpt-large or other pretrained models that you can find in Huggingface.
tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')
model = GPT2LMHeadModel.from_pretrained('distilgpt2')

def perplexity(sentence:str, stride:int=512) -> float:
    encodings = tokenizer(sentence, return_tensors='pt').input_ids
    max_length = model.config.n_positions  # 1024
    lls = []
    for i in range(0, encodings.size(1), stride):
        begin_loc = max(i + stride - max_length, 0)
        end_loc = min(i + stride, encodings.size(1))
        trg_len = end_loc - i    # may be different from stride on last loop
        input_ids = encodings[:,begin_loc:end_loc]
        target_ids = input_ids.clone()

        target_ids[:,:-trg_len] = -100
        
        with torch.no_grad():
            outputs = model(input_ids, labels=target_ids)
            log_likelihood = outputs[0] * trg_len
        
        lls.append(log_likelihood)
    ppl = torch.exp(torch.stack(lls).sum() / end_loc)
    return ppl.item()
 
 Then, you can do like this: 
 perplexity(""I love you."")","The currently accepted answer by @lads is outdated and the answer of @zijun is too complex. This is what I used (mix of the 2 answers) : 
 import math
import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')
model = GPT2LMHeadModel.from_pretrained('distilgpt2')
model.eval()

def score(sentence):
    tokenize_input = tokenizer.tokenize(sentence)
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
    loss = model(tensor_input, labels=tensor_input)[""loss""].item()
    return math.exp(loss)

a=['there is a book on the desk', 'there is a plane on the desk', 'there is a book in the desk']

print([score(i) for i in a])
# > [156.42900910254363, 289.88481709498836, 186.85249335013398]
 ```",,,,56.79599902,57.56757863,66.53508382,55.37730256,51.23638863,51.0717082,,,
38354,What more does TensorFlow offer to keras?,keras,"Deep Learning frameworks operate at 2 levels of abstraction: 
 
 Lower Level : This is where frameworks like Tensorflow, MXNet, Theano, and PyTorch sit. This is the level where mathematical operations like Generalized Matrix-Matrix multiplication and Neural Network primitives like Convolutional operations are implemented. 
 Higher Level : This is where frameworks like Keras sit. At this Level, the lower level primitives are used to implement Neural Network abstraction like Layers and models. Generally, at this level other helpful APIs like model saving and model training are also implemented. 
 
 You cannot compare Keras and TensorFlow because they sit on different levels of abstraction. I also want to take this opportunity to share my experience of using Keras: 
 
 I do not agree that Keras is only useful for basic Deep Learning work. Keras is a beautifully written API. The functional nature of the API helps you completely and gets out of your way for more exotic applications. Keras does not block access to lower level frameworks. 
 Keras results in much more readable and succinct code. 
 Keras model Serialization/Deserialization APIs, callbacks, and data streaming using Python generators is very mature. 
 Keras has been declared the official high level abstraction for TensorFlow.","If you use TensorFlow as your backend in keras, they more or less share a the same functionality. Through  keras.backend  you an access TensorFlow functions, while through  tf.keras  you have access to keras' the whole API through TensorFlow. 
 Since this is the case, I suggest you stick with keras and if you find something is missing (e.g. a metric or a loss function) you can import it through TensorFlow.","Given that TensorFlow is a more low-level library than Keras in general you would see this offers extra flexibility and improved performance (albeit relatively minor, depends mostly on how you write your code). I would say, if you are in research or developing new types of neural networks, knowledge of TensorFlow would be very useful. Outside of that, you should be fine with Keras although understanding how TensorFlow works could still be helpful if you're using it as a backend.  
 However, a while ago I read that Keras and TensorFlow will become more integrated which would make life a lot easier for you. 
 Obviously this is only my personal view, therefore I'd like to point you to some extra articles so you can do some reading of your own.
 This discussion on Kaggle  gives a great overview of arguments and when to use which.  Medium post  on this topic.","Keras as you say contains all the functionality but out of the box it only runs on CPU. By plugging in a backend such as TensorFlow or CNTK (which I personally prefer) you unlock the power of GPU which can vastly accelerate some ML workloads, particularly DL workloads. If you do not have a discrete GPU the benefits are minimal.  
 Most of the time in practice you can just set your backend and forget about it, and work entirely within Keras, even swap your backend for another and compare performance. So there is no need to learn the specifics of TF unless you want to code directly at a lower level.","Every beginner has this query. It always seems that Keras solves the basic functionalities like data input, model creation, training, evaluation in fewer lines of code.  
 But then when you start developing a ML model from scratch , you realize that you can program a lot of math into the NN , and tensorflow library provides a lot of functionalities and control making those concepts practical. The mathematical aspects of Learning can be easily visualized and made using NN made using tf.",,,,,66.06699192,77.07314426,69.85847727,59.88879242,56.85697488,,,,
38080,Interactive labeling/annotating of time series data,machine-learning,"Update: we have updated TRAINSET to include the ability to upload multiple series as well as apply multiple labels! See demo in GIF below. 
 We had this same problem again and again at Geocene, so we came up with this open-source web app called TRAINSET. You can use TRAINSET to brush labels onto time series data. You import data in a defined CSV format, then label the data, and export a labeled CSV. You can also import a pre-labeled CSV if you're really just trying to refine labels. You can use the hosted version of TRAINSET at  https://trainset.geocene.com  or you can deploy it yourself by following the readme at  https://github.com/geocene/trainset","A little bit too late to the party but it's better than never. We've released a major version update to our time-series data labeling tool called Label Studio. 
 Now it supports a variable number of channels with millions of data points in each, with zoom/pan, region labeling, and instance (single event) labeling. 
 It works with different time-series data types, for example, time may come as a float or as a strangely formatted date, has multi-user support, and multi-label classification. 
 
 Please visit  https://heartex.ai  for the commercial version and  https://labelstud.io/  for the open-source (right now needs some hand compiling)","I am currently developing a set of tools to annotate and detect patterns in time series data:  https://github.com/avenix/WDK 
 check the AnnotationApp in 1-Annotation","I also need such a tool to annotate data but did not found any suitable tool. Therefore, i wrote a small python app by myself, just abused matplotlib for this task. 
 I used  matplotlib.use('TkAgg')  and  SpanSelector  with my own  onselect(xmin, xmax)  method called for this task.
Check this code example:  https://matplotlib.org/gallery/widgets/span_selector.html","There is an open source platform for visualization called Grafana, that is a very powerful and flexible software used also for monitoring time series. They support  annotation . 
 
 That tool is pretty powerful and versatile, you can read data from a variety of data  sources . 
 Then once annotated as in the picture, you can query the Grafana annotation database to retrieve all the annotations/labels that you put thanks to the Grafana annotation  API . 
 Bonus tip 1: you can add customised tags on your annotation so that you can get additional info on your data (e.g. anomaly_A, anomaly_B, flat_normal_data).  
 Bonus tip 2: you can also show only one specific kind of anomaly still in the same platform thanks to this functionality. 
 Future improvements: extension to this powerful features are in discussion, so that it will be even more easy to annotate in presence of diagrams displaying multiple time series at ones (e.g.  anomaly of many time series ). 
 Applications: anomaly detection labelling, medical signal annotation, stock market annotation, etc.","Nova can do it interactively.  https://github.com/hcmlab/nova 
It's much more powerful than just labeling time-series data, but you can just do labeling with it. Also, I suggest you set the sample rate frequency to 1Hz. Best of Luck.","I'm using axvspan() function from matplotlib.pyplot.
Main disadvantage is a difficult configuration of text labels. 
 import matplotlib.pyplot as plt
import numpy as np
t = np.arange(0,3.14,0.01)
s = np.sin(t)
plt.axvspan(t[12], t[100], facecolor='blue', alpha=0.2)
plt.plot(t,s,color='red')",,,65.02840243,70.99033641,78.18834777,55.63187211,73.7846694,71.46016709,51.7882291,,
37435,I got the following error : 'DataFrame' object has no attribute 'data',python,"""sklearn.datasets"" is a scikit package, where it contains a method
  load_iris(). 
 
 load_iris() , by default return an object which holds  data, target  and other members in it. In order to get actual values you have to read the  data and target  content itself. 
 Whereas 'iris.csv', holds feature and target together. 
 
 FYI:  If you set  return_X_y as True  in  load_iris() , then you will directly get
  features and target. 
 
 from sklearn import datasets
data,target = datasets.load_iris(return_X_y=True)","The Iris Dataset from Sklearn is in Sklearn's  Bunch  format: 
 print(type(iris))
print(iris.keys())
 
 output: 
 <class 'sklearn.utils.Bunch'>
dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])
 
 So, that's why you can access it as: 
 x=iris.data
y=iris.target
 
 But when you read the CSV file as DataFrame as mentioned by you: 
 iris = pd.read_csv('iris.csv',header=None).iloc[:,2:4]
iris.head()
 
 output is: 
     2   3
0   petal_length    petal_width
1   1.4 0.2
2   1.4 0.2
3   1.3 0.2
4   1.5 0.2
 
 Here the column names are '1' and '2'. 
 First of all you should read the CSV file as: 
 df = pd.read_csv('iris.csv')
 
 you should not include  header=None  as your csv file includes the column names i.e. the headers. 
 So, now what you can do is something like this: 
 X = df.iloc[:, [2, 3]] # Will give you columns 2 and 3 i.e 'petal_length' and 'petal_width'
y = df.iloc[:, 4] # Label column i.e 'species'
 
 or if you want to use the column names then: 
 X = df[['petal_length', 'petal_width']]
y = df.iloc['species']
 
 Also, if you want to convert labels from string to numerical format use sklearn  LabelEncoder 
 from sklearn import preprocessing
le = preprocessing.LabelEncoder()
y = le.fit_transform(y)","If your second snippet program was run (in continuation) on the very same kernel where you ran first snippet program then you will get this error because dataset  iris  was pre-definied by you and has method  data  already built-in, provided by Scikit-Learn. 
 When working with dedicated  CSV  files,  Pandas  have different methods that you may make use of, as: 
 #To show all data(https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.all.html), use:
iris.all

#To get results that you expected, use df.columns (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.columns.html):
x = iris[iris.columns[0]]
y = iris[iris.columns[1]]
 
 Kindly confirm if your program fetched this error or separate kernels. Or else if this solution fits your requirement, you may chose to mark this as an answer for others learners to get benefited when in doubt.","When we load the iris data  directly from sklearn datasets , we don't have to worry about slicing the columns for data and target as sklearn itself would have organized the data in a manner we can use to directly to feed into the model. 
 But when we are loading from the data from csv file, we have to slice the columns as per our needs and organize it in a way so that it can be fed into in the model. When you execute the below lines after reading csv file using  read_csv  in pandas 
 x=iris.data
y=iris.target
 
 you are actually referring to the attributes of the pandas dataframe and not the actual data and target column values like in sklearn. You will have to use iris['data'], iris['target'] to access the column values if it is present in the data set.","from sklearn import datasets
iris = datasets.load_iris()
X = pd.DataFrame(iris.data)
X.columns = ['Sepal.leangth','sepal_width','petal_length','Petal_width']
Y= pd.DataFrame(iris.target)
Y.columns = ['Target']
 
 This might solve the issue!",,,,,51.89883372,51.0557217,52.6457222,54.76024848,54.28477057,,,,
37428,GraphViz not working when imported inside PydotPlus (`GraphViz's executables not found`),python,"See:  Graphviz's executables are not found (Python 3.4)   and   graphviz package doesn't add executable to PATH on windows #1666   and   Problem with graphviz #1357  - it's a reoccurring problem (for  that  program) with the PATH environment variable settings. Installing particular versions, or in a particular order, or manually adding a PATH fixes the problem. 
 It's best if the Package sets the PATH correctly  and  removes it when you uninstall the Package (so you don't get too long a PATH - which usually won't happen). Setting it manually prevents future breakage and forces it to work, but you need to manually remove the extra text if you uninstall the Package. 
 Here's the advice from those three links: 
 
 pip install graphviz 
 conda install graphviz 
 
 or 
 You need to run 
 conda install python-graphviz 
 instead of 
 pip install graphviz 
 to get these bindings, which also work with conda's Graphviz package. 
 or 
 
 Download and install graphviz-2.38.msi (use the newest version) from
 https://graphviz.gitlab.io/_pages/Download/Download_windows.html 
 Set the path variable 
 
         (a) Control Panel > System and Security > System > Advanced System Settings > Environment Variables > Path > Edit 
         (b) add 'C:\Program Files (x86)\Graphviz2.38\bin'","In my case I am able to find  graphviz  executables manually in  anaconda3\Library\bin\graphviz , but I still would get the  GraphViz's Executables not found  error. 
 I have unsuccessfully tried zhangqianyuan's suggestion as well as specific orders of module installation and using  python-graphviz  (official solution,  widely discussed here ). Only thing I didn't try was tampering with my  PATH  variable system-wide. 
 A method that worked  for me was inserting these lines in my code (before the graphviz related functions): 
 import os

os.environ['PATH'] = os.environ['PATH']+';'+os.environ['CONDA_PREFIX']+r""\Library\bin\graphviz""
 
 This is a dirty little hack, but there are some certain advantages: 
 
 PATH  changes are in effect locally and until  os  module is reloaded 
 No need to change module scripts 
 No need to change  PATH  system-wide 
 
 I am using Python 3.7, Windows 10, Anaconda. Graphviz was installed using  conda install python-graphviz , but I don't believe there's any difference in this case","If you have Anaconda, you could use Conda manager. 
 Type  Conda  at Start Panel and try install via Conda. 
 For example: 
 pip3 install graphviz","What worked for my  use case : Generating model diagrams in  Django .
But it can also be extended to generate diagrams for any other applications. 
 I installed the  GraphViz  for viewing graph from  .dot  file. Can be installed from  graphviz.org . 
 
 Create a dot file associated with the project: 
 python manage.py graph_models -a > dotfile.dot
 
 Or you could create the  .dot  files for multiple apps. Reference:  Using django-extensions to visualize the database diagram in django application , by  Thomas . 
 Now you just need to view  dotfile.dot . So where should I view that as an image? 
 
 Now open the file  gvedit.exe  (I don't know what's corresponding file in  Linux ) in the  insalled path  of the application. 

 
 For windows:  C:\Program Files (x86)\Graphviz2.38\bin\gvedit.exe . 
 
 
 Now run  gvedit.exe  and open  .dot  file created so far.","Find: C:\Users\zhangqianyuan\AppData\Local\Programs\Python\Python36\Lib\site-packages\pydotplus 
 Open  graphviz.py 
 Find line 1925 - line 1972, find the function: 
 def create(self, prog=None, format='ps'):
 
 In the function find:  
 if prog not in self.progs:
    raise InvocationException(
        'GraphViz\'s executable ""%s"" not found' % prog)

if not os.path.exists(self.progs[prog]) or \
        not os.path.isfile(self.progs[prog]):
    raise InvocationException(
        'GraphViz\'s executable ""{}"" is not'
        ' a file or doesn\'t exist'.format(self.progs[prog])
    )
 
 Between the two blocks add this(Your Graphviz's executable path): 
   self.progs[prog] = ""C:/Program Files (x86)/Graphviz2.38/bin/gvedit.exe""`
 
 After adding the result is: 
 if prog not in self.progs:
    raise InvocationException(
        'GraphViz\'s executable ""%s"" not found' % prog)

self.progs[prog] = ""C:/Program Files (x86)/Graphviz2.38/bin/gvedit.exe""

if not os.path.exists(self.progs[prog]) or \
        not os.path.isfile(self.progs[prog]):
    raise InvocationException(
        'GraphViz\'s executable ""{}"" is not'
        ' a file or doesn\'t exist'.format(self.progs[prog])
    )
 
 save the changed file then you can run it successfully. 
 you'd better save it as bmp file because png file will not work.
 picture is here","As everyone already described, it is required to install the graphviz.
However, instead of  pip , installing with  apt-get  solves the issue for me. 
 sudo apt-get install graphviz","I got the same issue, so that I just installed pydotplus independently(pip3 install pydotplus) and import pydotplus,
everything work fine.","I had the same problem and did everything suggested on this and other forums and nothing worked out. The following instructions will 100% solve this problem if you are using Windows and Python 3: 
 
 Install  pydotplus  via conda:  conda install pydotplus   
 Install  graphviz  independently  conda install python-graphviz  (if you already did these 2 steps, go straight to step 3) 
 Assuming you already have graphviz and pydotplus installed, find the  graphviz.py  file in  your   pydotplus  installation directory, in my case, it was in the following path:  C:\Users\Acevedo\Anaconda3\Lib\site-packages\pydotplus\graphviz.py 
 Open  graphviz.py  and find this block in line 606    
 # Method 3 (Windows only)
if os.sys.platform == 'win32':

    # Try and work out the equivalent of ""C:\Program Files"" on this
    # machine (might be on drive D:, or in a different language)
    if 'PROGRAMFILES' in os.environ:
        # Note, we could also use the win32api to get this
        # information, but win32api may not be installed.
        path = os.path.join(
            os.environ['PROGRAMFILES'], 'ATT', 'GraphViz', 'bin'
        )
    else:
        # Just in case, try the default...
        path = r""C:\Program Files\att\Graphviz\bin""

    progs = __find_executables(path)
 
 Comment the if/else part and hardcode the path of  your   graphviz  installation directory, inside which should be the executables ( dot.exe ,  circo.exe ,  gvedit.exe , etc.). The new code has to look like this: 
 # Method 3 (Windows only)
if os.sys.platform == 'win32':

    # Try and work out the equivalent of ""C:\Program Files"" on this
    # machine (might be on drive D:, or in a different language)
    """"""if 'PROGRAMFILES' in os.environ:
        # Note, we could also use the win32api to get this
        # information, but win32api may not be installed.
        path = os.path.join(
            os.environ['PROGRAMFILES'], 'ATT', 'GraphViz', 'bin'
        )
    else:
        # Just in case, try the default...
        path = r""C:\Program Files\att\Graphviz\bin""
        """"""
    path = r""C:\Users\Acevedo\Anaconda3\Library\bin\graphviz""

    progs = __find_executables(path)
 
 Save the  graphviz.py  file and everything should work just fine :)","Updating through running in the anaconda prompt: 
 conda install python-graphviz
 
 till version 2.38 (2019.10.16-0) did the trick for me.",72.95639546,74.42440185,56.72008443,57.03263265,62.25034649,60.51875694,63.51682014,70.04982111,59.49231057
37186,Early stopping on validation loss or on accuracy?,machine-learning,"TLDR; Monitor the loss rather than the accuracy 
 I will answer my own question since I think that the answers received missed the point and someone might have the same problem one day. 
 First, let me quickly clarify that using early stopping is perfectly normal when training neural networks (see the relevant sections in Goodfellow et al's Deep Learning book, most DL papers, and the documentation for keras' EarlyStopping callback). 
 Now, regarding the quantity to monitor: prefer the loss to the accuracy. Why? 
The loss quantify how certain the model is about a prediction (basically having a value close to 1 in the right class and close to 0 in the other classes). The accuracy merely account for the number of correct predictions. Similarly, any metrics using hard predictions rather than probabilities have the same problem. 
 Obviously, whatever metrics you end up choosing, it has to be calculated on a validation set and not a training set (otherwise, you are completely missing the point of using EarlyStopping in the first place)","In my opinion, this is subjective and problem specific.  You should use whatever is the most important factor in your mind as the driving metric, as this might make your decisions on how to alter the model better focussed. 
 Most metrics one can compute will be correlated/similar in many ways: e.g. if you use MSE for your loss, then recording MAPE (mean average percentage error) or simple $L_1$ loss, they will give you comparable loss curves. 
 For example, if you will report an  F1-score  in your report/to your boss etc. (and assuming that is what they really care about), then using that metric could make most sense. The F1-score, for example, takes  precision  and  recall  into account i.e. it describes the relationship between two more  fine-grained  metrics.  
 Bringing those things together, computing scores other than normal loss may be nice for the overview and to see how your final metric is optimised over the course of the training iterations. That relationship could perhaps give you a deeper insight into the problem, 
 It is usually best to try several options, however, as optimising for the validation loss may allow training to run for longer, which eventually may also produce a superior  F1-score . Precision and recall might sway around some local minima, producing an almost static F1-score - so you would stop training. If you had been optimising for pure loss, you might have recorded enough fluctuation in loss to allow you to train for longer.","Usually a loss function is just a surrogate one because we cannot optimize directly the metric. If the metric is representative of the task(business value the best), the value of the metric on evaluation dataset would be better than the loss on that dataset. For instance, if data imbalance is a serious problem, try PR curve.","As n1k31t4 pointed out this is rather problem specific, but I would like to suggest a few points to consider: 
 
 The loss is designed to help your model convarge, whereas a validation metric is usually what best describes the performance of the model. 
 Validation metrics are more ""stable"", as they repressent ""buisness logic"" rather than a technical tool: 
 
 
 Losses  can  often change (unless of course you're dealing with a very standard task). You may choose different losses and/or combinations of losses. The same losses can slightly change how they are aggregated or some loss related hyper-parameter change. 
 In some  uncommon  cases, the loss may even dynamically change during the training. 
 You will need to calibrate your stopping criteria each time.","I am currently training a neural network and I cannot decide which to use to implement my Early Stopping criteria: validation loss or a metrics like accuracy/f1score/auc/whatever calculated on the validation set. 
 
 If you are training a deep network, I highly recommend you not to use early stop. In deep learning, it is not very customary. Instead, you can employ other techniques like drop out for generalizing well. If you insist on that, choosing criterion depends on your task. If you have unbalanced data, you have to employ  F1  score and evaluate it on your cross-validation data. If you have balanced data, try to use accuracy on your cross-validation data. Other techniques highly depend on your task. 
 I highly encourage you to find a model which fits your data very well and employ drop out after that. This is the most customary thing people use for deep models.","I'm no expect, but i believe there is no objective answer as it is context dependent. e.g. I am trying to work out the best metric to apply to a pneumonia classifier, but I know FNRs (missing pneumonia) is WAY worse than accidentally diagnosing a healthy person who, at worst, receives useless treatement. FNRs can kill in this example. I opted for F1 in the end as i figured the harmonic mean between recll and precision is useful to improve the model as it penalises both false positives/negatives. But if I was a REAL hospital/doctor worried about being sued, I may want to reduce FNR rate as much as possible because one FN can lead to serious harm to life and business whereas a a FP ist just wasted time and money.",,,,63.32450303,57.60178244,53.16662601,59.63190254,66.2062156,50,,,
37021,"Why does adding a dropout layer improve deep/machine learning performance, given that dropout suppresses some neurons from the model?",machine-learning,"The function of dropout is to increase the robustness of the model and also to remove any simple dependencies between the neurons. 
 Neurons are only removed for a single pass forward and backward through the network - meaning their weights are synthetically set to zero for that pass, and so their errors are as well, meaning that the weights are not updated.
Dropout also works as a form of  regularisation , as it is penalising the model for its complexity, somewhat. 
 I would recommend having a read of  the Dropout section in Michael Nielsen's Deep Learning book  (freely available), which gives nice intuition and also has very helpful interactive diagrams/explanations. He explains that: 
 
 Dropout is a radically different technique for regularization. Unlike L1 and L2 regularization, dropout doesn't rely on modifying the cost function. Instead, in dropout we modify the network itself. 
 
 Here is a  nice summary article . From that article: 
 
 Some Observations: 
 
 
 
 Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. 
 
 
 
 Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less. 
 With H hidden units, each of which can be dropped, we have
2^H possible models. In testing phase, the entire network is considered and each activation is reduced by a factor p. 
 
 Example 
 Imagine I ask you to make me a cup of tea - you might always use your right hand to pour the water, your left eye to measure the level of water and then your right hand again to stir the tea with a spoon. This would mean your left hand and right eye serve little purpose. Using dropout would e.g. tie your right hand behind your back - forcing you to use your left hand. Now after making me 20 cups of tea, with either one eye or one hand taken out of action, you are better trained at using everything available. Maybe you will later be forced to make tea in a tiny kitchen, where it is only possible to use the kettle with your left arm... and after using dropout, you have experience doing that! You have become more robust to unseen data.","Another way of looking at what dropout does is that it is like a slab-and-spike prior for the coefficient for a covariate (that is some complex interaction term of the original covariates with some complicated functional transformations) in a Bayesian model. This is the interpretation proposed by Yarin Gal in his thesis (see  his list of publications ). 
 Here is a brief hand-waving argument for why this is so: 
 
 In those batches, where a neuron is eliminated, the coefficient for feature/covariate (constructed by connection in the neural network going into the neuron) is zero (spike at zero).  
 In those batches, where the neuron is present, the coefficient is unrestricted (improper flat prior = slab). 
 Averaged across all batches, you get a spike-and-slab prior. 
 
 Why would we want a slab-and-spike prior? It induces a Bayesian model averaging between a neutral network without that neuron and one with it in. In other words, it lets us express uncertainty about whether the neutral network really needs to have its full possible complexity and appropriately takes this uncertainty into account in the predictions. This addresses the major issue of neutral networks being able to overfit to data (though of course it is not the only possible way to achieve that).","Dropout does not actually removes neurons, its just that those particular neurons don't play any role (don't get activated) for the given batch of data. 
 Example -  Suppose there is a road of 8 lanes - When Trucks come, they pass through lanes 1,2,4,6,7, when Cars come, they pass through lanes 2,3,4,7,8 and when Bikes come, they pass through lanes 1,2,5,8. So regardless of any vehicle, all lanes are there, but only some of them are used. 
 Similarly, all neurons are used in whole model, but only a subset of neurons are activated for a particular batch of data. And the model is not cut down later, the model complexity remains as it is. 
 Why to use dropout? 
 As given in Deep learning book by Ian Goodfellow, 
 
 dropout is more effective than other
standard computationally inexpensive regularizers, such as weight decay, filter norm constraints and sparse activity regularization. 
 
 He also says- 
 
 One advantage of dropout is that it is very computationally cheap. 
 Another significant advantage of dropout is that it does not significantly limit the type of model or training procedure that can be used. It works well with nearly any model that uses a distributed representation and can be trained with stochastic gradient descent. This includes feedforward neural networks, probabilistic models such as restricted Boltzmann machines (Srivastava et al., 2014), and recurrent neural networks (Bayer and Osendorfer, 2014; Pascanu et al., 2014a). 
 
 This book  says- 
 
 The core idea is that introducing noise in the output values of a layer can
break up happenstance patterns that aren’t significant, which the network will start memorizing if no noise is present.","The dropout layer indiscriminately culls a specified portion of neurons, decreasing the representational capacity of the model in question. This prevents the network from fitting complex nonlinear decision boundaries(i.e. the ""noise"" in the dataset), thus preventing(or ameliorating) overfitting.","Dropout helps improving performance of a machine learning model for the following reasons: 
 
 Making Network Simpler:  It makes the network simpler hence, prevents over fitting. 
 Better than Using a Single Simple Network:  It is better than manually re-designing a simpler network because once you have designed a particular architecture, you cannot change it until the whole training process ends i.e, for all the epochs the network architecture is fixed. But in dropout the network is being simpler in various ways in each epoch. Say for 1000 epochs you are actually trying with 1000 types of simpler network's variations. 
 Learn in Many Ways:  The input and output of the network don't get changed, the only thing is changing is the mapping between them. So just imagine that, the network is learning the same thing in various different ways. So just like this network, for us the humans- whenever we think of the same problem in different ways we automatically learn to generalize it and our overall knowledge and understanding also improves and the similar thing occurs to the network. As during the dropout process in each epoch randomly some weights (connection from a neuron to another neuron of next layer) are getting cut, hence, we are forcing the network to learn using the existing connections that are still available and thus the network is learning how to analyze the same problem from different perspectives.",,,,,65.48984248,54.80905753,63.41535846,64.64665122,59.46359194,,,,
36463,Mastering NLP: Reading List,nlp,"I would suggest instead of trying to get many sources, get one source that goes through concepts first (fairly robustly), then seek out sources to refine or deepen your knowledge. One source comes from Stanford's NLP group, and is  Introduction to Information Retrieval .  The only thing I don't like about this books is that it tends to orient documents as columns (where data science has more or less agreed that they are rows), but that's a pretty trivial concern (as long as you can take a matrix transpose).  Aside from that, this book has excellent explanations, and the proper depth and breadth to be considered an exhaustive base for NLP.","[not a book but...] Definitely check out  https://github.com/sebastianruder/NLP-progress  for a self-updating list of relevant state-of-the-art literature in the field of NLP and its subfields. 
 As per the side projects that you mentioned, you might want to check e.g.  
 
 https://github.com/sebastianruder/NLP-progress/blob/master/question_answering.md 
 (possibly)  https://github.com/sebastianruder/NLP-progress/blob/master/taxonomy_learning.md 
 
 Check also recommendations that were given on SO, 
 https://stackoverflow.com/questions/2233435/machine-learning-and-natural-language-processing 
 https://stackoverflow.com/questions/212219/what-are-good-starting-points-for-someone-interested-in-natural-language-processi","Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python  is a new practical textbook that covers all the latest (2019) topics.","Collected from Different Sources 
 NLP industry is expected to grow from USD 10.2 billion in 2019 to USD 26.4 billion by 2024. Here are a few resources to get started with NLP and be a part of this highly awesome domain. 
 
 NLP course by Yandex School of Data Analytics:  https://lnkd.in/fePhndv 
 State of the Art #NLP Innovations:  https://lnkd.in/fbGkS_a 
 Deep Learning for NLP:  https://lnkd.in/fWkGCbR 
 Fast AI NLP Course:  https://lnkd.in/fPCcJAZ 
 Awesome NLP Journey:  https://lnkd.in/fk7edUt 
 Resources from Top NLP Conferences:  https://lnkd.in/fiy-rhE 
 NLP with #python:  https://lnkd.in/fBbRkd9 
 NLP with Deep Learning:  https://lnkd.in/fCvRyCi 
 Curated Resources for NLP:  https://lnkd.in/fkCcu_3 
 NLP Tutorial for #deeplearning Researchers:  https://lnkd.in/fSJ34yR 
 
 Articles 
 
 All kinds of Text Classification Models:  https://lnkd.in/fZ4_zq6 
 Open-source NLP research library, built on PyTorch:  https://lnkd.in/ft9rxyE 
 State-of-the-art NLP for TensorFlow 2.0 and PyTorch:  https://lnkd.in/fYcNwYU 
 NLP for the next decade:  https://lnkd.in/fi78EBC 
 
 
 19 Great Articles About Natural Language Processing (NLP): 
 
 Structuring Unstructured Big Data via Indexation 
 
 Your Guide to Natural Language Processing (NLP) 
 
 Comparison of Top 6 Python NLP Libraries 
 
 Text Classification & Sentiment Analysis tutorial 
 
 Deep Learning Research Review: Natural Language Processing 
 
 10 Common NLP Terms Explained for the Text Analysis Novice 
 
 Temporal Convolutional Nets Take Over from RNNs for NLP Predictions 
 
 How I used NLP (Spacy) to screen Data Science Resumes 
 
 Data Science Reveals Trump Tweets are Written by Two People 
 
 Simple introduction to Natural Language Processing  
 
 An NLP Approach to Analyzing Twitter, Trump, and Profanity 
 
 A Natural Language Processing (NLP) Approach to Data Exploration 
 
 Python NLTK Tools List for Natural Language Processing 
 
 NLP app to find great available domain names 
 
 Scaling an NLP problem without using a ton of hardware 
 
 Analyzing the structure and effectiveness of news headlines 
 
 Seven tricky sentences for NLP and text mining algorithms 
 
 Overview of Artificial Intelligence and Role of NLP 
 
 Text Classification & Sentiment Analysis tutorial 
 
 
 
 For Beginners 
 
 NLTK 
 
 Bible of NLP is  NLTK  (Natural Language Toolkit). They have a free  ebook  as well. 
 
 Books 
 
 Natural Language Processing with Python 
 
 Foundations of Statistical Natural Language Processing  - Its very important to have statistical understanding to apply NLP. 
 
 Handbook of Natural Language Processing 
 
 
 Blog Series 
 
 Dive into NLTK 
 NLPers 
 
 Youtube videos 
 
 Dan Jurafsky & Chris Manning: Natural Language Processing 
 
 Deep Learning 
 Blog 
 
 Deep Learning, NLP, and Representations 
 Word Embeddings 3 part series by Sabastian Ruder 
 
 Tutorials by Frameworks 
 
 Deep Learning for NLP with Pytorch 
 Text Processing in Keras 
 Sequence-to-Sequence Models in Tensorflow 
 Vector Representations of Words -wordtovec 
 
 MOOCs 
 Stanford CS224n: Natural Language Processing with Deep Learning 
 Python libraries 
 Spacy - Industrial-Strength Natural Language Processing","I have checked/read/watched countless sources on NLP, but at the end only two really made the difference: 
 
 The best book ever on NLP:  Speech and Language Processing  by Dan Jurafsky and James H. Martin. The authors are making all its content available for free  on their academic website . This contains 99.999% of the NLP notions needed in a whole ML career, and they keep it constantly updated. 
 Stanford University course on  Natural Language Processing with Deep Learning , by Chris Manning et al. This is a very intense course that explains everything from basics to very advanced attention RNNs. Quite challenging at some points, but it's so dense of extremely high quality content. They put all the 2019 course  on YouTube . 
 
 There's so many stuff around that I found on GitHub, books, blogs, ... you name it. But at the end what really made the difference have been these two above.",,,,,56.35953155,58.12515284,50,69.26018024,59.36415831,,,,
36450,What is the difference between Gradient Descent and Stochastic Gradient Descent?,machine-learning,"For a quick simple explanation: 
 In both gradient descent (GD) and stochastic gradient descent (SGD), you update a set of parameters in an iterative manner to minimize an error function. 
 While in GD, you have to run through ALL the samples in your training set to do a single update for a parameter in a particular iteration, in SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent. 
 Thus, if the number of training samples are large, in fact very large, then using gradient descent may take too long because in every iteration when you are updating the values of the parameters, you are running through the complete training set. On the other hand, using SGD will be faster because you use only one training sample and it starts improving itself right away from the first sample. 
 SGD often converges much faster compared to GD but the error function is not as well minimized as in the case of GD. Often in most cases, the close approximation that you get in SGD for the parameter values are enough because they reach the optimal values and keep oscillating there. 
 If you need an example of this with a practical case, check Andrew NG's notes here where he clearly shows you the steps involved in both the cases.  cs229-notes 
 Source:  Quora Thread","The inclusion of the word  stochastic  simply means the  random  samples from the training data are chosen in each run to update parameter during optimisation, within the framework of  gradient descent . 
 Doing so not only computed errors and updates weights in faster iterations (because we only process a small selection of samples in one go), it also often helps to move towards an optimum more quickly. Have a  look at the answers here , for more information as to why using stochastic minibatches for training offers advantages. 
 One perhaps downside, is that the path to the optimum (assuming it would always be the same optimum) can be much noisier. So instead of a nice smooth loss curve, showing how the error descreases in each iteration of gradient descent, you might see something like this: 
 
 We clearly see the loss decreasing over time, however there are large variations from epoch to epoch (training batch to training batch), so the curve is noisy. 
 This is simply because we compute the mean error over our stochastically/randomly selected subset, from the entire dataset, in each iteration. Some samples will produce high error, some low. So the average can vary, depending on which samples we randomly used for one iteration of gradient descent.","In Gradient Descent or Batch Gradient Descent, we use the whole training data per epoch whereas, in Stochastic Gradient Descent, we use only single training example per epoch and Mini-batch Gradient Descent lies in between of these two extremes, in which we can use a mini-batch(small portion) of training data per epoch, thumb rule for selecting the size of mini-batch is in power of 2 like 32, 64, 128 etc. 
For more details:  cs231n lecture notes","Gradient Descent  is an algorithm to minimize the  $J(\Theta)$ ! 
 Idea:  For current value of theta, calculate the  $J(\Theta)$ , then take small step in direction of negative gradient. Repeat. 
 
 Update Equation =  
 Algorithm: 
 while True:
    theta_grad = evaluate_gradient(J,corpus,theta)
    theta = theta - alpha * theta_grad
 
 But the problem is  $J(\Theta)$  is the function of all corpus in windows, so very expensive to compute. 
 Stochastic Gradient Descent  repeatedly sample the window and update after each one 
 Stochastic Gradient Descent Algorithm: 
 while True:
    window = sample_window(corpus)
    theta_grad = evaluate_gradient(J,window,theta)
    theta = theta - alpha * theta_grad
 
 Usually the sample window size is the power of 2 say 32, 64 as mini batch.","Both algorithms are quite similar. The only difference comes while iterating. In Gradient Descent, we consider all the points in calculating loss and derivative, while in Stochastic gradient descent, we use single point in loss function and its derivative randomly. Check out these two articles, both are inter-related and well explained. I hope it helps. 
 
 Gradient Descent algorithm 
 
 Stochastic Gradient Descent .","In the ""classical""  gradient method , the gradient is calculated after each batch, which is why it is also called batch gradient descent. A batch is a part of the training data, which in many cases has 32 or 64 training instances. First, the predictions for all instances in the batch are calculated and then the weights are changed by  backpropagation . This can greatly increase the computing power, especially for complex models, for example in image or speech processing. In these applications, the information is additionally relatively sparse, which means that although the data has many attributes, these also often have the value 0. 
 The  Stochastic Gradient Descent , therefore, provides the approach that the gradient is not calculated from a batch, but for each data point. That means in each iteration only one single data point is used. This reduces the used computing power enormously since the remaining batch does not have to be kept in the working memory. This is called  Stochastic Gradient Descent , because in each training step the gradient is only an approximation of the actual gradient.","I recommend taking a look to these two quick videos to understand the concepts better:
 https://youtu.be/CUh4VHpTv1w?si=SAARri4gNgc-2kxI 
 https://youtu.be/cAfS2Uq6ujc?si=ARGw8p3iLdYrw2Dh",,,68.49437586,65.98078959,73.21097083,63.66560732,83.82767687,73.66953202,50,,
36404,When to remove correlated variables,machine-learning,"You do not want to remove all correlated variables. It is only when the correlation is so strong that they do not convey extra information. This is both a function of the strength of correlation, how much data you have and whether any small difference between correlated variables tell you something about the outcome, after all.  
 The first two you can tell before you do any model, the final one not. So, it may be very reasonable to remove variables based on the combination of the first two considerations (i.e. even if the extra variables may in principle contain some useful information, you would not be able to tell given the strength of correlation and how much data you have) before you do any modelling/feature engineering. The final point can really only be assessed after doing some modelling.","Weird that nobody else mentioned  interpretability . 
 If all you are concerned with is  performance , then it makes no sense to remove two correlated variables, unless correlation=1 or -1, in which case one of the variables is redundant. 
 But if are concerned about interpretability then it might make sense to remove one of the variables, even if the correlation is mild. This is particularly true for linear models. One of the  assumptions of the linear regression  is  lack of perfect multicollinearity in the predictors. 
 If A is correlated with B, then you cannot interpret the coefficients of neither A nor B. To see why, imagine the extreme case when A=B (perfect correlation). Then, the model y=100*A+50*B is the same as the model y=5*A+10*B or y=-2000*A+4000*B. There are multiple equilibra in the possible solutions to the least square minimzation problem therefore you cannot ""trust"" neither. 
 Similar things can happen with other models. For example, if A is very correlated with B, then if the decision tree chooses A double the times as B, then you cannot say that A is more important than B. If you retrain the model, the opposite could have happened.","You should consider checking  VIF (Variance Inflation Factor). Try removing features with higher VIF. Generally, it is preferred that VIF is below 10.",It doesn't matter. But for efficiency before feature engineering.,"Determine the covariance, and do your initial work with the highest set.",,,,,74.51620318,66.37523143,53.06507963,50,50,,,,
35861,Measuring the uncertainty of predictions,classification,"Alternatively to the accepted answer, another way to estimate the uncertainty of a specific prediction is to combine the probabilities returned by the model for each class using a certain function. This is a common practice in ""Active learning"", where given a trained model you select a subset of unlabelled instances to label (to augment the initial training dataset) based on some sort of uncertainty estimation. The three most common functions used (called sampling strategies [1] in the literature) are: 
 
 Shannon entropy: you simply apply Shannon entropy to the probabilities returned by the model for each class. The highest the entropy the highest the uncertainty. 
 
 Least confident: you simply look at the highest probability returned by the model among all classes. Intuitively the certainty level is lower for a test instance with a 'low' highest probability (e.g. [.6, .35, .05] --> .6) compared to a 'high' highest probability (eg. [.9, .05, .05] --> .9). 
 
 Margin Sampling: you subtract form the highest probability the second-highest probability (e.g. [.6, .35, .05] --> .6-.35=.25). It is conceptually similar to the least confident strategy, but a bit more reliable since you're looking at the distance between two probabilities rather than a single raw value. Also, in this case, a small difference means a high uncertainty level. 
 
 
 Another more interesting way to estimate the uncertainty level for a test instance that is applicable to deep models with dropout layers is instead deep active learning [2]. Basically, by leaving dropout active while doing predictions you can bootstrap a set of different outcomes (in terms of probabilities for each class) from which you can estimate mean and variance. The variance, in this case, tells you how much the model is uncertain about that instance. 
 Anyway, consider that these are just crude approximations, using a model that specifically estimates the uncertainty of a particular prediction as suggested in the accepted answer is surely the best option.
Nevertheless, these estimations can be useful because they are potentially applicable to every model that returns probabilities (and there are also adaptations for models like SVM). 
 [1]  http://www.robotics.stanford.edu/~stong/papers/tong_thesis.pdf 
 [2]  https://arxiv.org/abs/1808.05697","In the model, you will decide how best to get uncertainty. If you used  Bayesian optimization  (that's a great package for it in Python), for example, you get a covariance matrix along with your expected values, and so inherently get an uncertainty measure. In this case, you can make predictions as to the underlying function of your data, and the (co-)variance will provide levels of uncertainty, as shown by the width of the green bands around the line below: 
 
 So the red points show where we have some sample data... notice that we have none e.g. at  X = 4  and  X = -1 , which is why we have high uncertainty; the 95% confidence interval is very large. 
 
 If you use a  standard  deep neural network TPO perform classification, for example, there is no inherent measure of uncertainty. All you really have is your  test accuracy , to let you know how well the model performs on hold-out data.  I cannot remember where it is explained, but I believe it is not actually feasible to interpret the class prediction values in terms of uncertainty. 
 For example, if you are predicting  cat  or  dog  for an image, and the two classes receive (normalized) logit values  [0.51, 0.49]  respectively, you cannot assume this means very low certainty.","Recent work (2024) has developed a reasonably simple method to extract the model uncertainty from Bayesian neural networks (NNs). The method is designed for either Monte-Carlo dropout or deep ensemble NNs, but the critical part is to be able to take samples from the posterior predictive distribution. So if you can approximate the posterior predictive distribution as described below you can use the method: 
 
 Extracting the model uncertainty is then as simple as calculating the entropy over the average of the samples (to get total uncertainty), then subtracting the average entropy for each sample (the data uncertainty): 
 
 Ref:  Thuy, Arthur, and Dries F. Benoit. ""Explainability through uncertainty: Trustworthy decision-making with neural networks."" European Journal of Operational Research 317.2 (2024): 330-340.","Though it does not exactly measure uncertainty for a classification model, you can give a look at  trust scores .","My answer is wrong, but I keep it because other people may make my mistake and the comments below my answer are valuable 
 I think you looking for 
 model.predict_prob()
 
 in python lots of models have it. and with this function, you can calculate how strong the model sure about its answer.",,,,,58.87799813,72.00779144,64.10287826,76.65894254,52.05508843,,,,
35713,"I got 100% accuracy on my test set,is there something wrong?",scikit-learn,"There may be a few reason this is happening. 
 
 First of all, check your code. 100% accuracy seems unlikely in any setting. How many testing data points do you have? How many training data points did you train your model on? You may have made a coding mistake and compared two same list. 
 Did you use different test set for testing? The high accuracy may be due to luck - try using some of the KFoldCrossValidation libraries that are widely available. 
 You can visualise your decision tree to find our what is happening. If it has 100% accuracy on the test set, does it have 100% on the training set
?","The default hyper-parameters of the  DecisionTreeClassifier  allows it to overfit your training data. 
 The default  min_samples_leaf  is  1 . The default  max_depth  is  None . This combination allows your  DecisionTreeClassifier  to grow until there is a single data point at each leaf. 
 Since you are having  $100\%$  accuracy, I would assume you have duplicates in your  train  and  test  splits. This has nothing to do with the way you split but the way you cleaned your data. 
 Can you check if you have duplicate datapoints? 
 x = [[1, 2, 3],
     [4, 5, 6],
     [1, 2, 3]]

y = [1,
     2,
     1]

initial_number_of_data_points = len(x)


def get_unique(X_matrix, y_vector):
    Xy = list(set(list(zip([tuple(x) for x in X_matrix], y_vector))))
    X_matrix = [list(l[0]) for l in Xy]
    y_vector = [l[1] for l in Xy]
    return X_matrix, y_vector


x, y = get_unique(x, y)
data_points_removed = initial_number_of_data_points - len(x)
print(""Number of duplicates removed:"", data_points_removed )
 
 If you have  duplicates  in your  train  and  test  splits, it is expected to have high accuracies.","I believe the problem you are facing is imbalance class problem. You have 99% data belongs to one class. May be the test data you have can be of that class only. Because 99% of the data belong to one class, there is high probability that your model will predict all your test data as that class. 
To deal with imbalance data you should use AUROC instead of accuracy. And you can use techniques like over sampling and under sampling to make it a balanced data set.","I had a similar issue, but I realized that I had included my target variable while predicting the test outcomes. 
 error: 
 
 predict(object = model_nb, test[,]) 
 
 void of error: 
 
 predict(object = model_nb, test[,-16]) 
 
 where the 16th column was for the dependent variable.","Please check if you used your test set for building the model. This is a common scenario, like: 
 Random Forest Classifier gives very high accuracy on test set - overfitting? 
 If that was the case, everything was making sense. Random Forest was trying not to overfit your model, while a decision tree would just memorize your data as a tree.","Agree with c zl, in my experience this doesn't sound like a stable model and points to just a random lucky cut of the data. But something that will struggle to provide similar performance on unseen data. 
 The best models are: 
 
 high accuracy on train data 
 and equally high accuracy on test data 
 and where both accuracy metrics are not more than 5~10% of each other, which probably shows model stability. The lower difference the better, I feel. 
 
 Bootstrapping and k-fold cross validation should usually provide more reliable performance numbers","As already mentioned here, DT are easy to overfit with the default parameters. So RF are usually the better choice compare to DT. Consider them as more generalized. 
 Why the accuracies are different? RF takes always random variables to be used in algorithm (for single tree) but DT takes all. So, you have a number of features (not big one I suppose) that has a very big influence on the target variable in the whole dataset. Why so? Define what are them and research deeply. 
 Now, can we say that DT is more stable for this particular task than RF? I'd say no because the situation may change. If your DT algorithm relies on 1-3 important features you can not be sure that these features will play the same significant role in the future. 
 You can use DT but keep in mind that you have to retrain the model consistently. And compare results to RF. 
 
 So, my advice: 
 
 implement feature importance (you can use RF model to get them) and use only important ones; 
 use some kind of KFold or StratifiedKFold here - it will give you better scores; 
 while you have imbalanced dataset - implement class_weight - should improve RF score; 
 implement GridSearchCV - it will also help you not to overfit (and 1000 trees are too much, tends to overfit too); 
 get train score also too all the time; 
 research important features.","Also check if one of tour predictores is a derived features from the Target variable, u can do that check-in with value_counts on ur Target and also on variables where You SEE too much similarity. You should not use derived features from Target variable as predictores.",,66.86621041,52.95516565,53.36280181,53.09630989,62.97949423,59.5531486,50.96163084,50,
34357,Why do people prefer Pandas to SQL?,python,"The real first question is why are people more productive with DataFrame abstractions than pure SQL abstractions. 
 TLDR; SQL is not geared around the (human) development and debugging process, DataFrames are. 
 The main reason is that DataFrame abstractions allow you to construct SQL statements whilst avoiding verbose and illegible nesting. The pattern of writing nested routines, commenting them out to check them, and then uncommenting them is replaced by single lines of transformation. You can naturally run things line by line in a repl (even in Spark) and view the results. 
 Consider the example, of adding a new transformed (string mangled column) to a table, then grouping by it and doing some aggregations. The SQL gets pretty ugly. Pandas can solve this but is missing some things when it comes to truly big data or in particular partitions (perhaps improved recently). 
 DataFrames should be viewed as a high-level API to SQL routines, even if with pandas they are not at all rendered to some SQL planner. 
 
 You can probably have many technical discussions around this, but I'm considering the user perspective below. 
 One simple reason why you may see a lot more questions around Pandas data manipulation as opposed to SQL is that to use SQL, by definition, means using a database, and a lot of use-cases these days quite simply require bits of data for 'one-and-done' tasks (from .csv, web api, etc.). In these cases loading, storing, manipulating and extracting from a database is not viable. 
 However, considering cases where the use-case may justify using either Pandas or SQL, you're certainly not wrong.  If you want to do many, repetitive data manipulation tasks and persist the outputs, I'd always recommend trying to go via SQL first.  From what I've seen the reason why many users, even in these cases, don't go via SQL is two-fold. 
 Firstly, the major advantage pandas has over SQL is that it's part of the wider Python universe, which means in one fell swoop I can load, clean, manipulate, and visualize my data (I can even execute SQL through Pandas...).  The other is, quite simply, that all too many users don't know the extent of SQL's capabilities.  Every beginner learns the 'extraction syntax' of SQL (SELECT, FROM, WHERE, etc.) as a means to get your data from a DB to the next place.  Some may pick up some of the more advance grouping and iteration syntax.  But after that there tends to be a pretty significant gulf in knowledge, until you get to the experts (DBA, Data Engineers, etc.). 
 tl;dr: It's often down to the use-case, convenience, or a gap in knowledge around the extent of SQL's capabilities.","As much as there is overlap in the application of these two things, this is comparing apples to oranges. 
 pandas is a data analysis toolkit implemented in Python, a general purpose programming language. SQL is a domain-specific language for querying relational data (usually in an relational database management system which SQLite, MySQL, Oracle, SQL Server, PostgreSQL etc. are examples). 
 SQL implies  
 
 working with data in an RDBMS* which may or may not be appropriate for the workload, even if it's just a small SQLite database, 
 database domain knowledge (as an end user, developer and/or administrator; the suggestion that ""SQL is faster"" I often see is a massive over-simplification), and 
 overcoming the not-insignificant learning curve in using SQL effectively, particularly in specialist applications such as data analysis (as opposed to creating simple reports of simple data). 
 
 
 * It's worth underlining the fact that SQL is so domain-specific it's becoming much less relevant to working with increasingly common alternatives to relational databases such as  NoSQL  databases. This represents a fundamental shift in how data is stored and structured, and there is really no universally common way of accessing it like the development of SQL standardisation aimed to achieve.  
 
 Python on the other hand (pandas is fairly ""pythonic"" so it holds true here) is flexible and accessible to people from various backgrounds. It can be used as a ""scripting language"", as a functional language and a fully featured OOP language. Visualisation capabilities and data source interoperability are built into pandas, but you're free to incorporate whatever Python can do into your workflow (which is most things); the scientific Python ecosystem has ballooned and includes great tools such as  Jupyter Notebook  and essential  scipy  libraries such as  matplotlib  and  numpy  (which pandas builds on). Significant elements of pandas' data analysis is  R -inspired and you won't generally find statisticians umming and ahhing about whether they use R (or possibly increasingly pandas!) over putting everything in a database and writing their analyses in SQL. 
 I'm not saying pandas is better than SQL or vice versa, but SQL is a very domain-specific tool whereas pandas is part of a giant, flexible and accessible ecosystem. I work with geospatial data systems, of which relational databases are a huge part, and SQL is a powerful and essential tool. However, pandas is an equally if not more essential part of my day-to-day toolkit and SQL is often relegated to fetching data -- perhaps with some pre-processing -- so I can do things with it in pandas.","First, pandas is not that much popular. I use both pandas and SQL. First I try to understand the task- if it can be done in SQL, I prefer SQL because it is more efficient than pandas. Try working on a large data (10,000,000 x 50). Try to do some  groupby  operation in both SQL and pandas. You will understand. 
 I use pandas where it comes handy- like splitting a column values into an array and doing some stuff on it (like choosing only some values out of that array). Now this kind of task is relatively hard to code in SQL, but pandas will ease your task.","I'm one of those people who would use (in my case) R's dplyr (the language, not necessarily the tool) in every case if I could even though I know my SQL. 
 The major benefit I see in Pandas/dplyr/data.table pipelines is that the operations are atomic and can be read top to bottom. 
 In SQL you need to parse the whole script, jumping around (what's being sumamrized, what's being joined and how - left? inner? right?, are there any filters applied?) to fully grasp what is happening. 
 In Pandas et al each step of the pipeline is self contained, it does something with the input data and returns output data, this sequential process makes it easier to reason about what's happening since there is a clearly defined state for each operation rather than just on a query level.  
 And yes you can do  WITH  statements and such but it requires much more code and is not as clear what object is being used compared to piping.","I'm fairly new to Pandas/Python but have 20+ years as a SQLServer DBA, architect, administrator, etc..  I love Pandas and I'm pushing myself to always try to make things work in Pandas before returning to my comfy, cozy SQL world. 
 Why RDBMS's are Better: 
The advantage of RDBMS's are their years of experience optimizing query speed and data read operations. What's impressive is that they can do this while simultaneously balancing the need to optimize write speed and manage highly concurrent access. Sometimes these additional overheads tilt the advantage to Pandas when it comes to simple, single-user use cases.  But even then, a seasoned DBA can tune a database to be highly optimized for read speed over write speed. DBA's can take advantage of things like optimizing data storage, strategic disk page sizing, page filling/padding, data controller and disk partitioning strategies, optimized I/O plans, in-memory data pinning, pre-defined execution plans, indexing, data compression, and many more. I get the impression from many Pandas developers that they don't understand the depth that's available there. What I think usually happens is that if  Pandas developer never has data that's big enough to need these optimizations, they don't appreciate how much time they can save you out of the box.  The RDBMS world has 30 years of experience optimizing this so if raw speed on large datasets are needed, RDBMS's can be beat. 
 Why Is Python/Pandas Better: 
That said, speed isn't everything and in many use cases isn't the driving factor. It depends on how you're using the data, whether it's shared, and whether you care about the speed of the processing. 
RDBMS's are generally more rigid in their data structures and put a burden on the developer to be more deterministic with data shapes. Pandas lets you be more loose here. Also, and this is my favorite reason, you're in a true programming language.  Programming languages give you infinitely more flexibility to apply advanced logic to the data.  Of course there's also the rich ecosystem of modules and 3rd party frameworks that SQL can't come close to. Being able to go from raw data all the way to web presentation or data visualization in one code base is VERY convenient.  It's also much more portable. You can run Python almost anywhere including public notebooks that can extend the reach of your results to get to people more quickly. Databases don't excel at this. 
 My Advice? 
If you find yourself graduating to bigger and bigger datasets you owe it to take the plunge and learn how RDBMS's can help.  I've seen million row, multi-table join, summed aggregate queries tuned from 5 minutes down to 2 seconds.  Having this understanding in your tool belt just makes you a more well rounded data scientist. You may be able to do everything in Pandas today but some day your may have an assignment where RDBMS is the best choice.","Things Pandas can do, that SQL can't do 
 
 df.describe() 
 Plotting, e.g.  df['population'].plot(kind='hist') 
 Use a dataframe directly for training machine learning algorithms 
 
 Things Pandas can do, I wasn't aware that SQL can do as well 
 
 Export to csv:  df.to_csv('foobar.csv') . This is important when you want to show something to a business owner who wants to work with Excel. And there is  df.to_excel  as well. But in SQL, you can do  SELECT a,b,a+b INTO OUTFILE '/tmp/result.txt' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\n' FROM test_table;  (thank you, vy32!)","The only thing not covered in these answers that I'd like to mention is that it also depends on how you're using SQL. Take arcpy for example. For some reason none of the arcpy.da functions have an execute many feature. This is really strange because pretty much every other python sql library does. The Where statement in the arcpy.da functions is also limited to around 120 characters. This essentially means that if you have any relatively high number of things you're trying to do with your database your only real choice is to call your chosen arcpy.da function multiple times, changing the where statement each time you do. There are a few tricks you can use to make this process go faster - you can iterate over chunks of your dataset for example - but literally every single of these tricks is much much slower than just using one arcpy.da.searchcursor to load your entire table into a pandas data frame, and then manipulating it using pandas, numpy, and, if your data is really this massive, dask. I need to emphasize here that pandas isn't just a little faster in this case. It's disgustingly faster. It's so much faster that I was literally laughing at myself for not doing it sooner. Using pandas dropped one scripts execution time down from well over an hour - I forget if this was the jump from 3.5 hours or from 1.5 hours - to literally 12 minutes. 
 One thing to note is that while I could have done this with sql it would have taken me a lot longer to learn. I would have either had to learn operations specifically for sql in Access - that's where the data for this script ended up - - sql in Access wasn't as robust as I needed it to be when I was actually looking into doing this -, or I would have had to write all my data to a sqlite3 database, manipulate it there, and then put it in Access. While this might have given me similar performance results, it would have made my script harder to modify in the future. 
 So yeah,  sometimes Pandas and is just strictly better than using the sql options you have at your disposal . Everything I would have needed to do in sql was done with a function in pandas. You can also use sql syntax with pandas if you want to. There's little reason not to use pandas and sql in tandem. 
 One more thing I want to mention about Pandas and numpy is that both of these libraries are by nature set based approaches. You can loop through dataframes and series build with these libraries, but it's really hard to modify data in these structures like that so you'll end up writing more efficient code - set based - with both of these libraries purely because it's so much easier to do. Being ""guided"" if not rail-roaded into using set based approaches is not something I've experienced with SQL.  
 One more massive thing I forgot to mention with Pandas.  Money . Pandas is a tool that a lot of Data Science jobs want you to know how to use. Pretty much every Data Science job I've looked at has paid more than database management type jobs. The only exception to this that I've noticed is in Data Engineering, but I've seen far less of those job postings. Pandas looks like it makes you more money at a glance.","I'll attempt to answer this question based on my own experience. In contrast to the other answers, I prefer  Sql  for deep learning and big-data-related things. There are numerous reasons for that. As it can be seen  here ,  
 
 Pandas provides an intuitive, powerful, and fast data analysis experience on tabular data. However, because Pandas uses only one thread of execution and requires all data to be in memory at once, it doesn’t scale well to datasets much beyond the gigabyte scale. 
 
 Sql engines usually keep the keys or special columns in data-structures like  $B ^+$  tree in order to facilitate CRUD operations. This data structure keeps the status of all data in the database. This is not pandas can do because it can't access all the data simultaneously. On the other hand, it can't do some operations even with its chunk parameter used in read_csv. As an example, you can't have direct batch operations for large datasets that your memory can't accommodate them. Any other tasks which depend on your entire dataset need extra coding. All of these can be handled in Sql without extra coding, just with a simple query. Simple Sql operations are just used without any fear about the memory.  
 Another difference is that CRUD operations in Sql can be applied distributed with different authorisation policies which are not possible in pandas.  
 It is not meant to say which is better, it all depends on your task. For large scale computation I prefer Sql and for small ones, I prefer pandas.  
 There are other things that are not in pandas which are really important for fast experience for data extraction that I'll refer to later. For now, just take a look at  here .","I think the shortest answer can be this slide from Jake Vanderplas' PyCon 2017 Keynote (and it's  a great talk  which can be a longer answer): 
 
 Thus most people don't prefer Pandas per se, they prefer (rapidly growing) Python Data Stack and Python ecosystem encompassing it. Scientist may go mostly with high-level domain packages, data engineers with ML packages, and when either of the two groups have something actionable from their interactive JupyterLab sessions, software engineers may use another packages to add HTTP API and describe deployment to move it into production. And all with the same language and ecosystem! SQL (even with all what commercial RDBMS vendors eager to sell you) is nowhere near such capability. 
 Pandas underpins some these use cases, and in turn relies on lower layers of the stack (e.g. NumPy). Could Python DBAPI and SQL be a replacement that would keep the rest intact? Parity, as such, is well covered in Pandas'  Comparison with SQL . 
 SELECT total_bill, tip, smoker, time FROM tips LIMIT 5

                    -- vs --

df[[""total_bill"", ""tip"", ""smoker"", ""time""]].head(5)
 
 or 
 SELECT day, AVG(tip), COUNT(*) FROM tips GROUP BY day

                    -- vs --

df.groupby(""day"").agg({""tip"": np.mean, ""day"": np.size})
 
 The same intent expressed in different ways (declarative DSL vs API in a high-level general purpose imperative language that mimics the DSL). From the perspective of familiarity to different roles (SQL is around for almost 50 years actually and doesn't seem to loose its relevance, and is known beyond software engineering) and expressiveness, I'd say SQL wins. 
 But if you look from a broader application maintenance and operation perspective, SQL has a number of problems it brings along: 
 
 new attack surface 
 RDBMS SQL dialect differences 
 another software system to manage in case of client-server RDBMS 
 real-world (analytical) SQL maintenance as strings (e.g. composition,
re-usability, comprehensibility) 
 ORM as a solution for above, but bringing a bag of own problems 
 
 In my opinion not dragging these problems into a fundamental component of a stack is a smart pragmatic decision. Instead you have a fast in-memory table with idiomatic (i.e. mostly counter-intuitive) high-level imperative API, which covers most of use cases. And interestingly enough there is  Blaze  which runs on top of SQL (via SQLAlchemy) and provides Pandas-like interface, which probably means that the latter is becoming the standard API for tabular data manipulation for data analysis. 
 That said it doesn't mean there aren't niche use cases in data analysis which can be solve in SQL alone. One example that I can give is ad-hoc exploratory data visualisation in tools like  sqliteviz  (in-browser SQLite with full Ploty's array of scientific visualisation) and  sqlitebrowser  (regular SQLite with simplistic visualisation capability). It's a breeze tweaking the result set (e.g. filtering, type conversion, adding computed columns, etc.) for the visualisation in SQL (not to mention how much easier are mid-complexity visualisations done in a GUI, comparing to much more idiomatic Matplotlib's API). 
 Update 
 
 There's a very interesting project from Dutch CWI, called  DuckDB . It's an open-source embedded analytical (columnar, vectorised, MVCC) database that tries to address the SQL mismatch for the most typical data science workflow. Basically to eliminate the need for re-inventing (poor man's in-memory) databases by data scientists, by removing all friction for reusing useful parts that the database field has accumulate for over than 50 years. For instance, you can completely install it with  pip install duckdb  (and it has wheels so you most likely don't need a compiler), you can save/load Pandas dataframes directly to the database, execute SQL with it on Pandas dataframes directly (i.e. zero-copy at least for numeric types), it has own query composition mechanisms and more. First 15 minutes of this talk,  DuckDB – The SQLite for Analytics , one of the authors, Mark Raasveldt, explains how DuckDB addresses the problem. 
 Practical SQL for Data Analysis -- What you can do without Pandas  gives a lot of advanced PostgreSQL examples of how to data analysis in SQL from from cleaning, sampling and splitting datasets to binning, pivots and calculating linear regression.",67.55319088,64.79077935,70.08398471,57.66967707,57.72320671,63.22762144,64.19194189,67.43038866,64.69147185
33572,When should I normalize data?,machine-learning,"It is actually depends on the algorithm you are using. 
For example, for  random forests  the ranges don't matter, since one feature is never compared in magnitude to other features. It's only the range of one feature that is split at each stage. 
 But on the other hand  SVM  or  Logistic regression  will probably do better if your features have roughly the same magnitude, unless you know apriori that some feature is much more important than others, in which case it's okay for it to have a larger magnitude.","As @Daniel Chepenko pointed out, there are models that are robust w.r.t. feature transformations (like Random Forest).
But for model which made operations on the features (like Neural Networks), usually you need to normalize data for three reasons: 
 1) Numerical stability: computers cannot represent every number, because the electronic which make them exist deals with binaries (zeros and ones). So they use a representation based on Floating Point arithmetic. In practice, this means that the numerical behavior in the range [0.0, 1.0] is not the same of the range [1'000'000.0, 1'000'001.0]. So having two features that have very different scales can lead to numerical instability, and finally to a model unable to learn anything. 
 2) Control of the gradient: imagine that you have a feature that spans in a range [-1, 1], and another one that spans in a range [-1'000'000, 1'000'000]: the weights associated to the first feature are much more sensitive to small variations, and so their gradient will become much more variable in the direction described by that feature. This can lead to other instabilities: some values of learning rate (LR) can be too small for one feature (and so the convergence will be slow) but too big for the second feature (and so you jump over the optimal values). And so, at the end of the training process you will have a sub-optimal model. 
 3) control of the variance of the data: if you have skewed features, and you don't transform them, you risk that the model will simply ignore the elements in the tail of the distributions. And in some cases, the tails are much more informative than the bulk of the distributions.","Data is usually normalized to make sure that all of your features on roughly the same scale and that the units you measure your data in do not make a difference to the model you fit in the end.  
 If you have data in the range 5-20 in the training set then in the test set your 25 will be mapped to 1.33 by the scaling (this is why the Scaler is fit to the training data, so you get a consistent mapping across training and test data). This is not a problem at all since your model doesn't really depend on your data being in [0,1].","I will try to explain it through an example. 
 Imagine that you have a problem of two attributes, temperature (Celsius) and length (mm). That problem requires the classification of the quality of long structural metal beams, based on the changes of their temperature and length during summer days. 
 By ""long"" it means they can be up to 2 meters long, that is [0-2000]mm range. Keep in mind that metal rods extend/shrink due to temperature changes. The temperature in that particular location changes between [20-35]Celsius during summer. 
 Assume that you would like to cluster the hourly samples of length and temperature using K-means clustering. Euclidean distance is usually the preferred choice to measure the distance between the cluster centers and other samples, in every iteration of the algorithm. This means that rods that have similar temperature (+-1 degree Celsius) but big difference in length (1000mm), will be in different clusters; but this might be misleading. 
 For that reason, you should scale all the dimensions in [0,1] range, so that the clustering distance is unbiased from measurement units. 
 Keep in mind that different units in engineering problems need  minmax  scaling in general, to have features that contribute to the classification outcome in a fair fashion.","MinMax scaler is not the only way to scale. There is also the  StandardScaler  which basically does: 
 $$
\begin{align}
x &\sim \mathcal{N}(\mu, \sigma)\\
x' :&= \frac{x-\mu}{\sigma}
\end{align}$$ 
 This leads to $x' \sim \mathcal{N}(0, 1)$.",,,,,50,55.37889146,57.87500917,50,50,,,,
33053,How do I compare columns in different data frames?,pandas,"If you want to check equal values on a certain column, let's say  Name , you can  merge  both DataFrames to a new one: 
 mergedStuff = pd.merge(df1, df2, on=['Name'], how='inner')
mergedStuff.head()
 
 I think this is more efficient and faster than  where  if you have a big data set.","You can double check the exact number of common and different positions between two df by using  isin  and  value_counts() . 
 Like that: 
 df['your_column_name'].isin(df2['your_column_name']).value_counts()
 
 Result: 
 
 True  = common 
 False  = different","df1.where(df1.values==df2.values).notna()
 
 True  entries show common elements. This also reveals the position of the common elements, unlike the solution with  merge .","Comparing values in two different columns 
 Using set, get unique values in each column. The intersection of these two sets will provide the unique values in both the columns. 
 Example: 
 df1 = pd.DataFrame({'c1': [1, 4, 7], 'c2': [2, 5, 1], 'c3': [3, 1, 1]})
 df2 = pd.DataFrame({'c4': [1, 4, 7], 'c2': [3, 5, 2], 'c3': [3, 7, 5]})
 set(df1['c2']).intersection(set(df2['c2']))
 
 Output:
 {2, 5} 
 
 Comparing column names of two dataframes 
 Incase you are trying to compare the column names of two dataframes: 
 If  df1  and  df2  are the two dataframes:
 set(df1.columns).intersection(set(df2.columns)) 
 This will provide the unique column names which are contained in both the dataframes. 
 Example: 
 df1 = pd.DataFrame({'c1': [1, 4, 7], 'c2': [2, 5, 1], 'c3': [3, 1, 1]})
df2 = pd.DataFrame({'c4': [1, 4, 7], 'c2': [3, 5, 2], 'c3': [3, 7, 5]})

set(df1.columns).intersection(set(df2.columns))
 
 Output:
 {'c2', 'c3'}","Note that the columns of dataframes are data series. So if you take two columns as pandas series, you may compare them just like you would do with numpy arrays.","""I'd like to check if a person in one data frame is in another one."" 
 The condition is for both name and first name be present in both dataframes and in the same row. 
 import pandas as pd
lst =[""Juan"",""Pedro"",""Carlos""]
lst2=[""Cabrera"",""Olivera"",""Paredes""]
lst3 =[""Juan"",""Pedro"",""Carlos"",""Joselo""]
lst4=[""Cabrera"",""Olivera"",""Rubianes""]
df = pd.DataFrame(list(zip(lst, lst2)),
           columns =['Name', 'First_name'])
df1 = pd.DataFrame(list(zip(lst3, lst4)),
           columns =['Name', 'First_name'])
column1 = ""Name""
column2= ""First_name""

def check_if_a_person_in_one_data_frame_is_in_another_one(df,df1,column1,column2,first_name,name):
    """"""This function check if two paired elements are present in two different dataframes.""""""
    #check that the name is in both columns
    col1=  name in list(df[column1]) and name in list(df1[column1])
    #check that first_name is in both columns
    col2 = first_name in list(df[column2]) and first_name in list(df1[column2])
    #check that both name and first_name are in the same row in the first dataframe
    try:
          both =(df[df[""First_name""]==first_name].index[0]) == (df[df[""Name""]==name].index[0])
    except:
          # if name or first_name does not exist
          pass
    #check that both name and first_name are in the same row in the second dataframe
    try:
          both =(df1[df1[""First_name""]==first_name].index[0]) == (df1[df1[""Name""]==name].index[0])
    except:
          # if name or first_name does not exist
          pass

    return col1 and col2 and both

column1 = ""Name""
column2= ""First_name"" 
first_name = ""Cabrera""
name =""Juan""  


check_if_a_person_in_one_data_frame_is_in_another_one(df,df1,column1,column2,first_name,name) 


True","You can get the whole common dataframe by using  loc  and  isin . 
 df_common = df1.loc[df1['set1'].isin(df2['set2'])]
 
 df_common  now has only the rows which are the same col value in other dataframe.","@Hermes Morales your code will fail for this: 
 lst =[""Juan"",""Pedro"",""Carlos""]
lst2=[""Cabrera"",""Paredes"", ""Olivera""]
lst3 =[""Juan"",""Pedro"",""Carlos"",""Joselo""]
lst4=[""Cabrera"",""Olivera"",""Rubianes""]
df = pd.DataFrame(list(zip(lst, lst2)),
           columns =['Name', 'First_name'])
df1 = pd.DataFrame(list(zip(lst3, lst4)),
           columns =['Name', 'First_name'])
 
 # I want to get a name and first_name which are there in df1 in a single row but not in df.
column1 = ""Name""
column2= ""First_name"" 
first_name = ""Olivera""
name =""Pedro"" 
 
 My suggestion would be to consider both the  boths  while returning the answer. 
 def check_for_both_names(df, df1, column1, column2, first_name, name):
    """"""This function check if two paired elements are present in two different dataframes.""""""
    # check that the name is in both columns
    col1 = name in list(df[column1]) and name in list(df1[column1])
    # check that first_name is in both columns
    col2 = first_name in list(df[column2]) and first_name in list(df1[column2])
    # check that both name and first_name are in the same row in the first dataframe
    try:
        both = (df[df[""First_name""] == first_name].index[0]) == (
            df[df[""Name""] == name].index[0])
    except:
        # if name or first_name does not exist
        pass
    # check that both name and first_name are in the same row in the second dataframe
    try:
        both1 = (df1[df1[""First_name""] == first_name].index[0]) == (
            df1[df1[""Name""] == name].index[0])
    except:
        # if name or first_name does not exist
        pass

    return col1 and col2 and both and both1
 
 Please correct me if I'm wrong:)","If your use case is where you have multiple data frames and you want to check whether or not the column headers are present in either of them, this piece of code might help. 
 consider you have  df1 , df2 , df3  as the dataframes, make a variable to store their columns as : 
 df1_columns = df1.columns
df2_columns = df2.columns
df3_columns = df3.columns

# optionally give the columns a name like
df1_columns.name = ""df1""
 
 Then use this function 
 # A code I prompted to generate a data frame to generate a sheet vs column presence table
# This shows if a column header is present across multiple data frames or not

import pandas as pd


def generate_presence_dataframe(*columns):
    """"""
    Generate a DataFrame to show the presence of attributes in each column.

    Args:
        *columns: Variable number of pandas DataFrame columns.

    Returns:
        A pandas DataFrame with the presence of attributes in each column.
        The column headers are based on the names of the passed columns,
        or generic names if the columns don't have names.
        The displayed columns are in the same order of the passed column arguments.
        The DataFrame is sorted based on the number of '1' values horizontally (across the rows).

    """"""
    # Step 1: Convert the column(s) to set(s)
    column_sets = [set(col) for col in columns]

    # Step 2: Create a set of all unique attributes from the column(s)
    all_attributes = sorted(list(set().union(*column_sets)))

    # Step 3: Create a dictionary to store the presence of attributes in each column
    presence_dict = {'Attributes': all_attributes}
    for i, col in enumerate(columns):
        column_name = col.name if col.name else f'Column {i+1}'
        presence_dict[column_name] = [1 if attr in col else 0 for attr in all_attributes]

    # Step 4: Create a DataFrame from the presence dictionary
    presence_df = pd.DataFrame(presence_dict)

    # Step 5: Sort the dataframe based on the number of '1' values horizontally (across the rows)
    presence_df = presence_df.iloc[presence_df.iloc[:, 1:].sum(axis=1).sort_values(ascending=False).index]

    # Reset the index
    presence_df = presence_df.reset_index(drop=True)

    return presence_df
 
 You can call it like 
 columns_presence_df = generate_presence_dataframe(df1, df2, df3)
columns_presence_df",53.48913816,55.31213562,50,62.17113576,58.40896374,56.39750052,50,53.34487878,61.96201982
33008,Is it always better to use the whole dataset to train the final model?,machine-learning,"A point that needs to be emphasized about statistical machine learning is that  there are no guarantees .  When you estimate performance using a held-out set, that is  just an estimate .  Estimates can be wrong. 
 This takes some getting used to, but it's something you're going to have to get comfortable with.  When you say ""What if the performance actually deteriorates?"", the answer is sure, that could happen.  The actual performance could be worse than you estimated/predicted.  It could also be better.  Both are possible.  That's unavoidable.  There is some inherent, irreducible uncertainty. 
 When you evaluate performance using a held-out test set, you are using data from the past to try to predict future performance.  As they say,  past performance is no guarantee of future results .  This is a fact of life that we just have to accept. 
 You can't let this immobilize you.  The fact that it's possible to do worse than you predicted is not a reason to avoid deploying to production a model trained on the data.  In particular, it's also possible to do poorly if you don't do that.  It's possible that a model trained on all the data (train+validation+test) will be worse than a model trained on just the train+validation portion.  It's also possible that it will be better.  So, rather than looking for a guarantee, we have to ask ourselves: What gives us the best chance of success?  What is most likely to be the most effective? 
 And in this case, when you want to deploy to production, the best you can do is use all the data available to you.  In terms of expected performance, using all of the data is no worse than using some of the data, and potentially better.  So,  you might as well use all of the data available to you to train the model when you build the production model.  Things can still go badly -- it's always possible to get unlucky, whenever you use statistical methods -- but this gives you the best possible chance for things to go well. 
 In particular, the standard practice is as follows: 
 
 Reserve some of your data into a held-out test set.  There is no hard-and-fast rule about what fraction to use, but for instance, you might reserve 20% for the test set and keep the remaining 80% for training & validation.  Normally, all splits should be random. 
 Next, use the training & validation data to try multiple architectures and hyperparameters, experimenting to find the best model you can.  Take the 80% retained for training and validation, and split it into a training set and a validation set, and train a model using the training set and then measure its accuracy on the validation set.  If you are using cross-validation, you will do this split many times and average the results on the validation set; if you are not, you will do a single split (e.g., a 70%/30% split of the 80%, or something like that) and evaluate performance on the validation set.  If you have many hyperparameters to try, do this once for each candidate setting of hyperparameter.  If you have many architectures to try, do this for each candidate architecture.  You can iterate on this, using what you've found so far to guide your choice of future architectures. 
 Once you're happy, you freeze the choice of architecture, hyperparameters, etc.  Now your experimentation is done.  Once you hit this point, you can never try any other options again (without obtaining a fresh new test set) -- so don't hit this point until you're sure you're ready. 
 When you're ready, then you train a model on the full training + validation set (that 80%) using the architecture and hyperparameters you selected earlier.  Then, measure its accuracy on the held-out test set.  That is your estimate/prediction for how accurate this modelling approach will be.  You get a single number here.   That number is what it is: if you're not happy with it, you can't go back to steps 1 and 2 and do more experimentation; that would be invalid. 
 Finally, for production use, you can train a model on the entire data set, training + validation + test set, and put it into production use.  Note that you never measure the accuracy of this production model, as you don't have any remaining data for doing that; you've already used all of the data.  If you want an estimate of how well it will perform, you're entitled to use the estimated accuracy from step 4 as your prediction of how well this will perform in production, as that's the best available prediction of its future performance.  As always, there are no guarantees -- that's just the best estimate possible, given the information available to us.  It's certainly possible that it could do worse than you predicted, or better than you predicted -- that's always true.","I personally haven't seen that for products going into production, but understand the logic. 
 Theoretically, the more data your deployed model has seen, the better is should generalise. So if you trained the model on the full set of data you have available, it should generalise better than a model which only saw for example train/val sets (e.g. ~ 90%) from the full data set. 
 The problem with this (and the reason we split data into train/val/test sets in the first place!) is that we want to be able to make statistical claims as to the accuracy on unseen data. As soon as we re-train a model again on  all  the data, it is no longer possible to make such claims. 
 [Edit] 
 Here is a  related question on Cross-Validated , where the accepted answer makes similar points to me and mentions other ways of doing things. 
 
 We loop over: 
 
 train a model 
 assess performance on validation set  $\rightarrow$  if satisfactory, go to step 5 
 change model 
 go to step 1 
 assess performance on test set 
 Present model with test accuracy found in step 5 
 
 Eventually, if you manage to get a great score on the test set, you can claim it generalises well.  So the question as to whether  re-training  on the full dataset will improve performance on future unseen data is not strictly something you can test. Empirical evidence of better performance in other related problem sets would be the only source or guidance at the point in time when you must make the decision. 
 A sanity check would be to test the final re-trained model again on the original test set; expecting that it scores higher than it ever did when the model only saw the train/val set, because it has actually seen the test set during training. This wouldn't make me feel 100% confident that this final model is superior in all future cases, but at least it is as good as it can be with the given data. 
 Perhaps there are more rigorous arguments against doing what you say (probably academically motivated), however it does seem appealing for practical applications!","Once you have obtained optimal hyperparamters for your model, after training and cross validating etc., in theory it is ok to train the model on the entire dataset to deploy to production. This will, in theory, generalise better. 
 HOWEVER, you can no longer make statistical / performance claims on test data since you no longer have a test dataset. 
 If you deploy a model to production using the entire training dataset, and you know the true values of the target variable of the new incoming data (i.e the data the production model is making predictions on), then you can calculate real time performance metrics as this new data is like test data (it was unseen to the model during training). From this process you could update the models hyperparameters to achieve better performance. 
 But if you knew the target values of new data, why would you train a model in the first place? 
 In general, I would say if you have enough data with enough variety, then shuffling and splitting the data 80:20 training:test should be sufficient to train a robust model and not have to worry about generalisation issues (assuming of course you regularize the model).","One of the reasons of having a data set is to avoid overfitting. If you employ cross-validation, you essentially allow the entire dataset to act as the training set, but retraining won’t let you validate whether there is sign of overfitting. I guess that either way (cross validation or retrain with the entire data set) should not dramatically change your result (from my uneducated guess), but you won’t be able to do hyperparameter tuning or validate your model performance as you don’t have a test set. Whether it ends up being better, it is hard to say, but I guess the only way to know is to do an A/B of the two models over real data over time.","Unless you're limiting yourself to a simple class of convex models/loss functions,  you're considerably better off keeping a final test split . Here's why: 
 Let's say you collect iid sample pairs from your data generating distribution, some set of (x, y). You then split this up into a training and test set, and train a model on the training set. Out of that training process you get a model instance, f(x; w). Where w denotes the model parameters. 
 Let's say you have N observations in the test set. When you validate this model on that test set you form the set of test predictions, {f(x_i, w) : i=1,2,...,N} and compare it to the set of test labels {y_i : i=1,2,...,N} using a performance metric. 
 What you're able to say using N independent observations is how you expect  that model instance , i.e. the function given a specific w, will generalize to other iid data from the same distribution. Importantly, you only really have  one observation  (that w you found) to comment on your process for determining f(x, w), i.e. the training process. You can say a little more using something like k-fold cross validation, but unless your willing to do exhaustive cross-validation (which is not really feasible in a vision or NLP context), you'll always have less data on the reliability of your training process. 
 Take a pathological example, where you draw the model parameters at random, and you don't train them at all. You obtain some model instance f(x, w_a). Despite the absurdity of your (lack of) training process, your test set performance is still indicative of how that model instance will generalize to unseen data. Those N observations are still perfectly valid to use.
Maybe you'll have gotten lucky and have landed on a pretty good w_a. However, if you combine the test and training set, then ""retrain"" the model to obtaining a w_b, you're in trouble. The results of your previous test performance amounts to basically a point estimate of how well your next random parameter draw will fare. 
 There are statistical results that you can use to comment on the reliability of the entire training process. But they require some assumptions about your model class, loss function, and your ability to find the best f(x, w) from within that class for any given set of training observation. With all that, you can get some bounds on the probability that your performance on unseen data will deviate by more that a certain amount from what you measured on the training data. However, those results do not carry over (in a useful way) to overparameterized and non-convex models like modern neural networks. 
 The pathological example above is a little over the top. But as an ML researcher and consultant, I have seen neural network training pipelines that occasionally latch on to terrible local minima, but otherwise perform great. Without a final test split, you'd have no way of being sure that hadn't happened on your final retraining. 
 More generally, in a modern machine learning context, you cannot treat the models coming out of your training process as interchangeable.  Even if they do perform similarly on a validation set . In fact, you may see considerable variation from one model to the next when using the full bag of stochastic optimization tricks. (For more details on that, check out this work on  underspecification .)",,,,,55.25502139,56.12656195,56.37196801,52.39647075,55.69200398,,,,
32306,In which epoch should i stop the training to avoid overfitting,machine-learning,"As long as your validation accuracy increases, you should keep training. I would stop when the test accuracy starts decreasing (this is known as early stopping). The general advise is always to keep the model that performs the best in your validation set. 
 Although it is right that your model overfits a little since epoch 280, it is not necessarily a bad thing provided that your validation accuracy is high. In general, most machine learning models will have higher training accuracy compared to validation accuracy, but this doesn't have to be bad. 
 In a general case, you expect your accuracy to behave in the following way. 
 
 In your case, you're before the early stopping epoch, so even if your training set accuracy is higher than your test set accuracy, it is not necessarily an issue.","""Early Stopping"" is the concept which needs to be used here. 
As mentioned in wikipedia about early stopping,  
 
 In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation. 
 
 At epoch > 280 in your graph, validation accuracy becomes lesser than training accuracy and hence it becomes a case of overfitting. In order to avoid overfitting here, training further is not recommended. However you may choose to train the model beyond the epoch where training and validation accuracy matches if the resulting validation accuracy is sufficient for the particular problem you are working on.","Keep training until your validation accuracy saturates (or starts dropping). Since the accuracy increases slowly, try to increase your learning rate parameter  eta to force the network to converge faster to the optimum weights. Be aware though, if you increase it too much though, it will become unstable.","If you're using  keras  or  tensorflow.keras , this parameter is known as  patience  in the  EarlyStopping   callback . 
 It equals the number of epochs with no validation accuracy improvement to trigger the end of the training phase. I usually set it to 2 or 3, 1 is usually too sensitive to noise.",You should also look for training error Vs testing error than training accuracy and testing accuracy.,,,,,58.43821003,66.00168621,50.9484288,53.83711185,52.62450624,,,,
31864,Share Jupyter Notebook with a non programmer,python,"There is an option to convert the notebook to HTML. If the non programmer just have to view the notebook, do that then upload it in google drive or any website or you can share it even through mail. I do this every time when I want to present using a Jupyter notebook so that it will be supported on any system.","GitHub has built in support for showing a notebook. You will just need to run the notebook yourself, then upload the file to Github like all other file types. 
 Your viewer will be able to see your notebook without any installation.",Export as HTML is probably the best option. All the graphics are embedded in the HTML file and anyone can view it in a browser pretty easily just by clicking on the file.,"There are quite a few ways depending on your needs (security, simplicity, price, etc). 
 GitHub is a good one. I found  this tutorial  helpful. 
 In college, we used  Gryd  which is pretty easy. 
 This  reddit post  recommends Binder, Coclac and Google Colab. 
 Big cloud providers like Amazon, Google, and Microsoft also offer their own ways to share, but they have some limitations (which may not apply to you) around runtime, preserving env, etc","If you want to share your Juptyer / IPython notebooks online, try using  jovian.ml  . It's a platform for sharing on collaborating on Jupyter notebooks, and it's really easy to use.  
 Step 1:  Install the Jovian python library 
 pip install jovian
 
 Step 2:  Import the library inside your Jupyter / IPython notebook  
 import jovian
 
 Step 3:  Upload the notebook to your Jovian account by running  
 jovian.commit()
 
 inside the Jupyter notebook. This will capture the Juptyer notebook (and also the Python libraries required to run it), and upload it your account, giving you shareable link. Here's an example:  https://www.jovian.ml/aakashns/jovian-tutorial 
 Viewers can also run your notebook on cloud platforms like Google Colab, BinderHub and Kaggle with a single click.",,,,,76.80787291,59.79906173,50,51.92169638,68.01988129,,,,
31617,Natural Language to SQL query,machine-learning,"If you want to tackle the problem from another perspective, with  an end to end learning , such that you don't specify ahead of time this large pipeline you've mentioned earlier, all you care about is the mapping between sentences and their corresponding SQL queries.  
 Tutorials:   
 How to talk to your database   
 Papers:   
 
 Seq2SQL: Generating Structured Queries from
Natural Language using Reinforcement Learning   salesforce 
 Neural Enquirer: Learning to Query Tables in Natural Language 
 
 Dataset:   
 A large annotated semantic parsing corpus for developing natural language interfaces. 
 Github code:   
 
 seq2sql   
 SQLNet 
 
 Also, there are commercial solutions like  nlsql","NLTK  has an excellent step by step guide on everything you need to convert human language to an SQL query using the nltk package in python. 
 It’s rudimentary, but it answers your question.","To complement Fadi's answer, the following are other useful papers on NL to SQL methods. The major difference of these methods is that they support queries that should be answered using more than one table (joining different tables), however the Salesforce paper (and their dataset) is focused on queries on one table at a time. 
 
 Learning a Semantic Neural Parser from User Feedbacks 
 An End to End Natural Language Interface for Databases 
 
 Both of these papers use the GeoQuery dataset avaialbe  here .","There are lots of works on text-to-SQL task. 
 I strongly suggest you to check  WikiSQL  and  Spider  datasets. Studies start from seq2seq + attention mech. to BERT-based solutions. Also each study points out the importance of the input representaion where you can feed all the table schema or just a column name. It's a pretty deep topic and as @PyRsquared said there is no simple answer :)","Just to complement this thread with the latest AI updates on this matter: Open AI released recently  Codex . 
 Codex is an AI trained to convert and analyze many coding languages  (main focus is python). 
 It's also  GPT-3 based .",,,,,74.17393762,70.00519527,61.426386,52.59425645,52.0518886,,,,
31259,How to properly predict data with LSTM using train/test?,neural-network,"You seek to augment the external validity of your model. 
 The most common way of doing so is by applying  k-fold cross-validation  to verify that your model generalizes well on unseen data. 
 
 In k-fold cross-validation, the original sample is randomly
  partitioned into k equal sized subsamples. Of the k subsamples, a
  single subsample is retained as the validation data for testing the
  model, and the remaining k − 1 subsamples are used as training data.
  The cross-validation process is then repeated k times, with each of
  the k subsamples used exactly once as the validation data. The k
  results can then be averaged to produce a single estimation. 
 
 This will reduce the variance of your model and will reduce its error on unseen data.","This situation is common in a generic modeling setting, not only for LSTM. 
 In the development phase, the model is built using training data, and test data is used to estimate the model quality. Post this, the model is trained on the entire data, and this updated model is used for prediction on new data points.","How you break the ""paradox"" is as follows. 
 Independently on whether you use a hold-out test set or run Cross-Validation, keep in mind that those protocols are just meant to  assess  the performance of the model.
However, after that assessment is done, you  re-train your model on all the data .  The implicit assumption is that the re-trained model will be at least as performing as the one trained on the reduced training set. 
 Therefore, in practice, this is a two-stage procedure and the issue that puzzles you does not persist.","""Test"" is often misleadingly used instead of ""validation"". Even academic literature sometimes use them interchangeably, while they are not. 
 It is correct, as you say, to discard the validity of ""test"" data if we use it during training to decide when to stop the training. 
 In such case, the correct word is ""validation"" data. Validation data is by definition the data you use to verify your model can still generalize well. A model overfitting to its training data will have its validation loss increase. 
 However, to hold more reliable assumptions about generalization, we may use a held-out, so called, ""test"" set on which we test the model, once we decided to freeze the model's parameters for good. 
 But after doing so, you may wonder: why choosing one train/validation/test configuration rather than another random one from your dataset? 
 To answer this problem and get even stronger assumptions on the generalization capabilities of our model, we can use  cross-validation  which is considered a fairly robust evaluation method in machine learning. 
 Eventually, all assumptions we make about the loss of our model on some set of data is subject to statistics, in which generally we cannot draw clear conclusions.","In general, effective learning is all about making the training error small and the gap between training and test error small. 
By test data, we mean examples that your model has never seen before. so you need development (validation) set, to fine-tune your hyperparameters such hidden cells, the number of layers, learning rate, etc. 
Split the training data into train/dev sets, be careful test set must always be generated from the same data distribution that generates your train/dev sets. 
LSTM might overfit your dataset, start with vanilla RNN, or small GRU. 
Use early stopping to stop training when the loss of the validation examples stop decreasing.",,,,,54.62340801,62.83614077,54.78115132,58.55050926,59.39960767,,,,
30881,When is precision more important over recall?,machine-learning,"For rare cancer data modeling, anything that doesn't account for false-negatives is a crime.  Recall  is a better measure than precision. 
 For YouTube recommendations, false-negatives is less of a concern.  Precision  is better here.","I can give you my real case when recall is more important: 
 We have thousands of free customers registering in our website every week. The call center team wants to call them all, but it is impossible, so they ask me to select those with good chances to be a buyer (with high temperature is how we refer to them). We don't care to call a guy that is not going to buy (so precision is not important) but for us it is very important that all of them with high temperature are always in my selection, so they don't go without buying. That means that my model needs to have a  high recall , no matter if the precision goes to hell.","I had a tough time remembering the difference between precision and recall, until I came up with this mnemonic for myself:  
 
 PREcision is to PREgnancy tests as reCALL is to CALL center. 
 
 With a pregnancy test, the test manufacturer needs to be sure that a positive result means the woman is really pregnant. People might react to a positive test by suddenly getting married or buying a house (if many consumers got false positives and suffered huge costs for no reason, the test manufacturer would lack customers). I got a false negative pregnancy test once, and it just meant it took a few more weeks before I found out I was pregnant...the truth ultimately became apPARENT. (Pun intended.) 
 Now picture a call center for insurance claims. Most fraudulent claims are phoned in on Mondays, after the fraudsters connect with collaborators and craft their made-up stories (""let's say the car was stolen"") over the weekend. What's the best thing for an insurance company to do on Mondays? Maybe they should tune to favor recall over precision. It is far better to flag more claims as positive (likely fraud) for further investigation than to miss some of the fraud and pay out cash that should have never been paid. A false positive (flagged for additional scrutiny as possibly fraud, but the customer loss was real) can likely be cleared up by assigning an experienced adjustor, who can insist on a police report, request building security video, etc. A false negative (accepting a fraudster's false claim and paying out in cash) is pure loss to the insurance company, and encourages more fraud. 
 F1 is great but understanding how the test/prediction will be used is really important, because there's always some risk of being wrong...you want to know how dire the consequences will be if wrong.","Which is more important simply depends on what the costs of each error are. 
 Precision tends to involve direct costs; the more false positives you have, the more cost per true positive you have. If your costs are low, then precision does not matter as much. For instance, if you have 1M email addresses, and it will cost $10 to send an email to all of them, it is probably not worth your time to try to identify the people most likely to respond, rather than just spamming all of them. 
 Recall, on the other hand, tends to involve opportunity costs; you are giving up opportunities every time you have a false negative. So recall is least important when the marginal value of additional correct identification is small, e.g. there are multiple opportunities, there is little difference between them, and only a limited number can be pursued. For instance, suppose you would like to buy an apple. There are 100 apples at the store, and 10 of them are bad. If you have a method of distinguishing bad apples that misses 80% of good ones, then you will identify about 18 good apples. Normally, a recall of 20% would be terrible, but if you only want 5 apples, then missing those other 72 apples does not really matter. 
 So recall is most important when: 
 
 The number of opportunities is small (if there were only 10 good apples, then you would be unlikely to find 5 good ones with a recall rate of only 20%) 
 There are significant differences between opportunities (if some apples are better than others, then a recall rate of 20% is enough to get 5 good apples, but they are not necessarily going to be the  best  apples) 
OR 
 The marginal benefit of opportunities remains high, even for a large number of opportunities. For instance, while most shoppers would not have much benefit from more than 18 good apples, the  store  would like to have more than 18 apples to sell. 
 
 Thus, precision will be more important than recall when the cost of acting is high, but the cost of not acting is low. Note that this is the cost of acting/not acting per candidate, not the ""cost of having any action at all"" versus the ""cost of not having any action at all"". In the apple example, it is the cost of buying/not buying a particular apple, not the cost of buying some apples versus the cost of not buying any apples; the cost of not buying a particular apple is low because there are lots of other apples. Since the cost of buying a bad apple is high, but the cost of passing up a particular good apple is low, precision is more important in that example. Another example would be hiring when there are a lot of similar candidates. 
 Recall is more important than precision when the cost of acting is low, but the opportunity cost of passing up on a candidate is high. There is the spam example I gave earlier (the cost of missing out on an email address is not high, but the cost of sending out an email to someone who does not respond is even lower), and another example would be identifying candidates for the flu shot: give the flu shot to someone who does not need it, and it costs a few dollars, do not give it to someone who does need it, and they could die. Because of this, health care plans will generally offer the flu shot to everyone, disregarding precision entirely.","Although in some situations recall may be more important than precision (or vice versa),  you need both to get a more interpretable assessment. 
 For instance, as noted by @SmallChess, in the medical community, a false negative is usually more disastrous than a false positive for preliminary diagnoses.  Therefore, one might consider recall to be a more important measurement.  However, you could have 100% recall yet have a useless model: if your model always outputs a positive prediction, it would have 100% recall but be completely uninformative. 
 This is why we look at multiple metrics: 
 
 precision-recall curve 
 AUROC 
 and sometimes metrics such as the  F score","Accumulation  has a great answer on how you can come up with more examples explaining the importance of precision over recall and vice versa. 
 Most of the other answers make a compelling case for the importance of recall so I thought I would give an example on the importance of precision. This is a completely hypothetical example, but it makes the case. 
 Let us say that a machine learning model is created to predict whether a certain day is a good day to launch satellites or not based on the weather. 
 
 If the model accidentally predicts that a good day to launch satellites is bad ( false negative ), we miss the chance to launch. This is not such a big deal. 
 
 However, if the model predicts that it is a good day, but it is actually a bad day to launch the satellites( false positive ) then the satellites may be destroyed and the cost of damages will be in the billions. 
 
 
 This is a case where precision is more important than recall.","Email Spam detection :This is one of the example where  Precision  is more important than  Recall . 
 Quick Recap : 
 
 Precision : This tells when you predict something positive, how many times they were actually positive. whereas, 
 Recall : This tells out of actual positive data, how many times you predicted correctly. 
 
 Having said above, in case of spam email detection, 
One should be okay if a spam email (positive case) left undetected and doesn't go to spam folder  but , if an email is good (negative), then it must not go to spam folder. i.e.  Precison is more important. (If model predicts something positive (i.e. spam), it better be spam.  else, you may miss important emails). 
 Hope it clarifies.","$ \textbf{Precision} = \frac{\text{tp}}{\text{tp}+\textbf{fp}}$  : use it when  fp  (false positive) are important (i.e. you DON'T want them, or want to minimize it). 
 Suppose you want to build a model that identify hot dogs (P) from dogs (N). Here you  don't want False Positive , because a false positive here would mean a dog (Negative) served for lunch as an hot dog (Positive), not very tasty. 
 $ \textbf{Recall} = \frac{\text{tp}}{\text{tp}+\textbf{fn}}$  : use it when  fn  (false negative) are important (i.e. you DON'T want them, or want to minimize it). 
 Suppose you want to build a model that identify zombie (P) from humans (N). Here the worst thing you can do is label a zombie as a human (label it as Negative), i.e. having a  False Negative , not good for humanity.","When we have imbalanced class and we need high true positives, precision is prefered over recall. because precision has no false negative in its formula, which can impact.",60.5009423,66.52685901,56.73119232,60.68161914,65.80891168,65.79070801,61.86361987,53.38866506,62.31544403
30860,Is it possible to cluster data according to a target?,clustering,"One approach I would try would be a supervised dimension reduction (UMAP for example  https://umap-learn.readthedocs.io/en/latest/supervised.html ) then a clustering approach (such as Hdbscan:  https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html ). This would allow you to perform clustering, in cluding a supervised dimension. Be cautious in that I have found that UMAP can 'overfit' - in the sense that it might provide clean 'groups' on training data and very different stuff on testing data.","Use  constrained clustering . 
 This allows you to set up ""must link"" and ""cannot link"" constraints. 
 Then you can cluster your data such that no cluster contains both 'churn' and 'non churn' entries bybsettingn""cannot link"" constraints. 
 I'm just not aware of any good implementations.","I have been thinking of using  shapley values  to cluster predictions. Indeed, for samples having the same prediction, the contributions of each feature leading to that prediction can be different. This information could be captured with techniques decomposing the prediction into ""contributions"" (or a proxy of contributions) such as shapley values. 
 So clustering data according to a target could be done following these three steps: 
 
 train a supervised ML model (e.g. a random forest) 
 extract the shapley values for every sample 
 cluster samples using their shapley values 
 
 A quick search on google led me to the  same idea  in  Christoph Molnar's famous book , so it comforts me in this approach.","First of all, your approaches are smart and creative but some remarks: 
 
 The question is not defined well. You talk about clustering using target which is actually paradoxical however I understand your point. The problem is that, not caring about this paradox may hurt your analysis which comes in second point. 
 According to  1 , you classify your points based on target and try to find dense subgroups there. That will not work as you are including the target in your analysis. This is the confusion made from paradoxical definition I mentioned above.  
 You have your targets, so divide your data according to them and analyze each subset of data separately, having an eye on their interaction. 
 
 Second the suggestions: 
 Subgroup Discovery 
 This is the right, solid and difficult way to find these interesting subgroups. To adopt it to your use-case you need to modify the algorithm which might make it even more difficult but is certainly worth, at least, to  have a look at . 
 Creative Way 
 Partition your data according to target. You will end up with several disjoint subsets of data. Then start a statistical analysis on the association of variables within and between subsets of data. For instance the distribution of values within a variable should be significantly different among different classes, if that variable really contributes to that target ( ANOVA ).  
 This also helps to remove variables which do not contribute to the target (by doing this  1.  you reduce the complexity of data and analysis and improve interpretability by removing them,  2.  you already found which target IS NOT contributing which is a part of your answer) 
 PS:  I just improvised. Please try it and let me know if it works or not so we can think of another solution :)","Predict target (e.g. using a Random Forest) and retrieve ""most important features"" (from feature importance analysis).
      Cluster samples with selected features (e.g. using k-means). 
 
 You must also scale based on variable importance. 
 
 However, I am afraid the clustering technique used in the 2nd step
  might not catch behaviours found in the 1st step which might explain
  churn (suppose there is a complex interaction in some trees in the RF,
  this interaction might not be cought in the k-means algorithm). 
 
 Scaling based on varimp will help with this. Actually I am not sure this is at all correct. Lets say conditional XOR based on two variables. That will divide the plane into 4 even squares where one class will be in diagonally opposite of the two squares. This does not exactly explain what is happening but it does show it. But then how to see into multidimensional space? Use hierarchical clustering diagram and color each end point by their resulting class. 
 Look into random forest ""proximity plots"". Section 15.3.3 in  elements of statistical learning .","You can train a decision tree with your features and target. Then just take the leaf of DT with the highest target rate, constrains on your features in that leaf will be your segment.",,,,54.57479987,59.75475481,62.27631582,66.13608527,54.06833499,55.86824682,,,
30697,Unsupervised image segmentation,machine-learning,"Fast answear 
 Mean Shift LSH  which is an upgrade in  $O(n)$  of the famous Mean Shift algorithm in  $O(n^2)$  well know for its image segmentation ability 
 Some explanations 
 If you desire a true  unsupervised  approach to segment images, use  clustering algorithms . The fact is that there is a lot of algorithms with different  time complexity  and  specificity . Take the most famous one, the  $K$ -Means, it is in  $O(n)$  so pretty fast but you have to specify how many cluster you want which is not what you intend by exploring an unknown image without any information about how many shapes are presents in it. Moreover even if you suppose that you know how many shape are present, we may suppose that there shapes are random which is another point where the  $K$ -Means fail because it is design to find elliptic clusters and  NOT  random shape ones. 
 At the opposite we have the Mean Shift that is able to find  automatically  the number of cluster -- which is useful when you don't know what you're looking for -- with  random shapes . 
 Of course you replace the  $K$  parameter of  $K$ -Means by others Mean Shift parameters which can be tricky to fine tuned but it doesn't exist a tool that allow you to do magic if you're not exercising to do magic. 
 An advice to image segmentation clustering 
 Transform your color space from RGB to LUV which is better for euclidean distance. 
 $K$ -Means vs Mean Shift LSH time complexity 
 
 Mean Shift :  $O(\alpha.n)$ 
 K-Means :  $O(\beta.n)$ 
 $\alpha \gt \beta$ 
 
 Mean Shift LSH is slower but it fits better with your needs. It stay still linear and is also  scalable  with the mentioned implementation. 
 PS : My profile picture is an application of the Mean Shift LSH on myself if it can help to figure out how it works.","You may need to take a look at this work submitted and accepted for CVPR 2018 :  Learning to Segment Every Thing 
 In this work, they try to segment everything, even objects not known to the network. Mask R-CNN has been used, combined with a transfer learning sub-network, they get very good results in segmenting almost everything.","Actually, your task is supervised.  Segnet  can be good architecture for your purpose which one of its implementations can be accessed  here .  SegNet learns to predict pixel-wise class labels from supervised learning. Therefore we require a dataset of input images with corresponding ground truth labels. Label images must be single channel, with each pixel labelled with its class  ... . 
 Also, take a look at  Fully Convolutional Networks  which are well suited for your task. 
 
 Based on the edits in the question, I add extra information. There are numerous methods that can be applied for this task. Basically the easiest one is to use a  background  label and classify those classes that you don't know as background by employing the mentioned architectures. By doing so you will have labels which can have overlap for background class which is a probable downside of this approach but its advantage is that in cases where your trained labels are frequently used in the inputs, you can have a relatively light version of architecture which recognizes the unknown classes.","The state-of-the-art (SOTA) for image segmentation would be Facebook's  Mask-RCNN . 
 While it is usually trained on dataset such like  COCO  or  Pascal  which feature real-life objects, you can re-trained it on a dataset of your choice, real or not. 
 Facebook provides an implementation ( Detectron ) under the Apache2 license. Give it a try!","This might be something that you are looking for. Since you ask for image segmentation and not  semantic / instance  segmentation, I presume you don't require the labelling for each segment in the image. 
 The method is called  scene-cut  which segments an image into class-agnostic regions in an unsupervised fashion. This works very well in case of indoor cluttered environment. 
 Paper link:  arxiv 
 Code:  code",,,,,59.83591646,60.71710687,51.85184698,61.35165893,75.8153148,,,,
30097,How to improve loss and avoid overfitting,deep-learning,"I am not familiar with the software you are using but keep in mind: You EXPECT accuracy to drop if you reduce over fitting. It is not a bad thing. Over-fitting is essentially ""fake accuracy"". 
 Some good approaches in general to avoid over-fitting though: Use cross-validation, normalize your features, increase size of data-set and dont just increase your data-set by copying data.","There are a few things you can do to reduce over-fitting. 
 
 Use Dropout increase its value and increase the number of training epochs 
 Increase Dataset by using Data augmentation 
 Tweak your CNN model by adding more training parameters. Reduce Fully Connected Layers. 
 Change the whole Model 
 Use Transfer Learning (Pre-Trained Models)","You can use dropout which will help in controlling the model to over train.
While using CNN,data-set plays an important role.The more data-set the more model can learn features from it.
You can divide your data-set in 3 parts.
Training ,testing and validation.","Here are few things you can try to reduce overfitting: 
 
 Use batch normalization 
 add dropout layers 
 Increase the dataset 
 Use batch size as large as possible (I think you are using 32 go with 64) 
 to generate image dataset use  flow from data 
 Use  l1 and l2 regularizes  in  conv layers 
 If dataset is big increase the layers in neural network. 
 USE callbacks   tf.keras.callbacks.ReduceLROnPlateau   here 
 
 Hope this mat help, also plot the history graph of training to have a better understanding of your model.","there some advice： 
1、use the dropout 
2、try to normalization.such as you can  optimizer = optim.SGD(model.parameters(), lr=learning_rate,weight_decay=0.1)  
3、try to change the model 
4、increase the number of the data 
sometimes the some change you have do the better effect of the ML and DL you will get",,,,,51.8989625,50,50,51.93909243,50,,,,
29893,High model accuracy vs very low validation accuarcy,python,"When a machine learning model has high training accuracy and very low validation then this case is probably known as over-fitting. The reasons for this can be as follows: 
 
 The hypothesis function you are using is too complex that your model perfectly fits the training data but fails to do on test/validation data. 
 The number of learning parameters in your model is way too big that instead of generalizing the examples , your model learns those examples and hence the model performs badly on test/validation data. 
 
 To solve the above problems a number of solutions can be tried depending on your dataset: 
 
 Use a simple cost and loss function. 
 Use regulation which helps in reducing over-fitting i.e Dropout. 
 Reduce the number of learning parameters in your model. 
 
 These are the 3 solutions that are most likely to improve the validation accuracy of your model and still if these don't work check your inputs whether they have the right shapes and sizes.",You should try to shuffle all of your data and split them to the train and test and valid set then train again.,"It seems that with validation split, validation accuracy is not working properly. Instead of using validation split in fit function of your model, try splitting your training data into train data and validate data before fit function and then feed the validation data in the feed function like this. 
 Instead of doing this 
 history=model.fit(X_train[train], y1[train], validation_split=0.30,epochs=2, batch_size=128,verbose=2)
 
 Split your train data into validation and train data by any method, and then say your validation data is  (X_val,Y_val) , then replace the above line of code with this one: 
 history=model.fit(X_train[train], y1[train], validation_data=(X_val,Y_val),epochs=2, batch_size=128,verbose=2)","I had the same condition: High  acc  and low  vad_acc . 
 It was because the parameter of Keras.model.fit, validation_split. 
 This will separate the last section of data as validation data.
Therefore, if your data was in order, your validity data will be in the same case.
Try to shuffle the training data.",It may be an imbalanced dataset problem during training though your number doesn't indicate. Try judiciously resampling with smote (oversampling) if the dataset is small or try any undersampling if the dataset is huge.,,,,,63.78504908,53.44099792,58.06773517,59.60227955,50,,,,
29542,"Fast introduction to deep learning in Python, with advanced math and some machine learning backgrounds, but not much Python experience",machine-learning,"Stanfords CS231n has a great python+numpy tutorial.
 http://cs231n.github.io/python-numpy-tutorial/ 
 As for Neural Networks and Keras + Tensorflow, I can recommend the Deep Learning specialization on Coursera. It is free for one week. If you do not want the certificates you can download all videos and stop. 
 If you want the certficates you can enroll in all courses simultaneously and finish them faster than advertised. They assume 3-6h per week for 16 weeks. If you have 6h per day, it is possible to go through a weeks content per day. Andrew Ng speaks clearly and the videos are understandable at 1.5x-2x speed. 
 As for the roadmap, i would suggest to 
 
 start with python + numpy until you have a basic understanding of array slicing and read up on unknown commands when you encounter them. 
 start a DL course of your choice but do not skip classes. That is I think it is important to know the basics even if you will only use high level frameworks later on.","Giving links some good tutorial which will help you understand concepts of deep learning and give you enough background to be able to implement simple models for your own problems. 
 https://in.udacity.com/course/deep-learning--ud730 
It's approx. duration is three months but with a little more effort you should be able to finish it in two. 
Another one is Andrew Ng course on Coursera. You should really check it out. 
Quick intro to Deep Learning:
 https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/","Okay, first I would tell you that deep learning is very easy as compared conventional machine learning so you don't have to panic with your 2-month deadline provided you have a thorough understanding of machine learning algorithms that you have mentioned above.  
 Now  here  is a course by Andrew NG that is free if you would audit it. Once you open this course just go through it you don't' even have to do the assignments just understand the concept which is very easy to grasp. 
 After you have gone through the course. Setup python environment  for windows use this .  For linux use this .  
 After that search for keras tutorials from  kerasGitHubb  page which can get you going.","If you are looking for a nice solid tutorial for Deep Learning in the domain of NLP (Natural Language Processing), then Robert Gutherie's  notebook  would be a great start. 
 It teaches you how to use vectors and how does multi-dimensional vectors look like and how to deal with them. He uses the deep-learning library of Facebook, Pytorch to explain the concepts. This library is more pythonic and less verbose than tf, so it is easier to follow. (personal opinion though). 
 After that, you can take up any library like gensim, spacy or Fasttext and put your vectors to work there. 
 A nice follow-up, also would be to go through  Pytorch's Deep Learning tutorials.  They are very nicely written and often very intuitive and the code is very easy to read (thanks to Python and pythocicity of Pytorch :D).","This would actually be more suitable as a comment, but I am lacking the reputation to do so.  
 Given your background, I recommend this page 
 https://www.python-course.eu/python3_course.php 
 It's a good introduction to Python and it also contains a part about NumPy (and I would highly recommend to get more familiar with those before doing the ML part). I would not recommend the part about machine learning because it teaches you how to program the models, but usually your job in industry is to use whatever is there, not to write things yourself.  
 For the ""normal"" machine learning the scikit-learn homepage gives good examples. This leaves deep-learning, for which I cannot recommend anything.","First of all, you CAN learn them in 2 months time, if you are going to devote everyday for this. As per your background, you don't need much of the basics of ML, you just need Python. There are many answers for the same here- 
 https://www.quora.com/What-is-the-best-way-to-learn-Python-from-scratch 
 The simple roadmap for python here would be- 
 
 Download and install python in your PC 
 Start with a course and book (related to basic python concepts and programming) and start coding. I recommend both course and book because not everything is in a book and not everything is told in the courses. 
 As soon as you start creating smaller working scripts, go to  hackerrank  and start doing exercises there. This will develop your understanding of data structures and which data structure to use in which type of problem.  
 
 This will help you in writing scripts in python. 
 For understanding deep learning, you can go to this- 
 http://deeplearning.net/tutorial/ 
 This tutorial uses  Theano  for building neural network models. It also explains many terms related to deep learning. 
 And finally (you already know), practice is what you want to learn in this short time. And your biggest positive point is that you have a PhD in Maths, so you can easily understand the mathematics behind deep learning models.",,,,54.9646997,59.03445782,60.79365946,60.03081803,63.60384691,65.59299013,,,
29480,Uploading images folder from my system into Google Colab,machine-learning,"Method 1 : 
 
 zip the file  
 Upload the zipped file, there is an Upload button under the Files Section. 
 Unzip it using the command on colab : 
!unzip level_1_test.zip  
 
 Method 2 :  
 
 upload the zip file to the google drive account. 
 The only difference is in step 2 where in place of the GUI upload option you can run the google code_snippets to upload download your zip file from the google drive to Colab account .  
 Unzip it using the command on colab : 
!unzip level_1_test.zip","The best bet would be to upload the images as a zip file to your Google drive and then access it through Google Colab (GC) 
 
 Zip the image folder 
 Upload the zip file to your Google drive 
 Turn to GC to authorise and mount your Google drive 
 from google.colab import drive
drive.mount('/content/drive')
 
 Follow the link and paste the code to your GC notebook 
 Unzip the file from GC 
 !unzip -uq ""/content/drive/My Drive/PATH_TO_ZIP"" -d ""/content/drive/My Drive/PATH_TO_OUTPUT""
 
 The files are now ready to use","You don't need to upload them if you have a download link ...( it would be faster if you can upload them all as either ways you have to do so.. So its better to upload them first and then download them in your notebook every-time you run it) 
 If you have a download link then just this 
 ! wget <Link> 
 Else upload then to your drive and then just use the following 
 from google.colab import files

uploaded = files.upload()

##files.upload returns a dictionary of the files which were uploaded. The 
##dictionary is keyed by the file name, the value is the data which was 
##uploaded.

for fn in uploaded.keys():
  print('User uploaded file ""{name}"" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn]))",I recommend you uploading a zip file containing your images to your drive and downloading the content from drive to Colab. Then you will be able to extract them. The code for uploading is  here .,"You can upload stuff to Google Drive and then download it from there on Colab. I've written some utils for that - see  this notebook . 
 As to how upload files to Google Drive, Media's suggestion is useful - upload zipped image folder.","On Colab simply use: 
 !gdown --id file_id
 
 For instance, if your Google drive share link for the file is: 
 https://drive.google.com/file/d/zz2Xs5Vriz6aF3V-Z22112yAj91c222fI1F/view?usp=sharing
 
 use: 
 !gdown --id zz2Xs5Vriz6aF3V-Z22112yAj91c222fI1F",,,,61.80598529,67.168722,65.27319788,62.56316071,72.52032012,57.94821429,,,
29006,Feature selection vs Feature extraction. Which to use when?,feature-selection,"Adding to The answer given by Toros, 
 These(see below bullets) three are quite similar but with a subtle differences-:(concise and easy to remember) 
 
 feature extraction and feature engineering : transformation of raw data into features suitable for modeling; 
 feature transformation : transformation of data to improve the accuracy of the algorithm; 
 feature selection : removing unnecessary features. 
 
 Just to add an Example of the same, 
 
 Feature Extraction and Engineering(we can extract something from them) 
 
 
 Texts(ngrams, word2vec, tf-idf etc) 
 Images(CNN'S, texts, q&a) 
 Geospatial data(lat, long etc) 
 Date and time(day, month, week, year, rolling based) 
 Time series, web, etc 
 Dimensional Reduction Techniques (PCA, SVD, Eigen-Faces etc) 
 Maybe we can use Clustering as well (DBSCAN etc) 
 .....(And Many Others) 
 
 
 Feature transformations(transforming them to make sense) 
 
 
 Normalization and changing distribution(Scaling) 
 Interactions 
 Filling in the missing values(median filling etc) 
 .....(And Many Others) 
 
 
 Feature selection(building your model on these selected features) 
 
 
 Statistical approaches 
 Selection by modeling 
 Grid search  
 Cross Validation 
 .....(And Many Others) 
 
 Hope this helps... 
 Do look at the links shared by others.
They are Quite Nice...","As Aditya said, there are 3 feature-related terms that sometimes are confused with each other. I will try and give summary explanation to each one of them: 
 
 Feature extraction:  Generation of features from data that are in a format that is difficult to analyse directly/are not directly comparable (e.g. images, time-series, etc.) In the example of a time-series, some simple features could be for example: length of time-series, period, mean value, std, etc. 
 Feature transformation:  Transformation of existing features in order to create new ones based on the old ones. A very popularly used technique for dimensionality reduction is Principal Component Analysis (pca) that uses some orthogonal transformation in order to produce a set of linearly non-correlated variables based on the initial set of variables. 
 Feature selection:  Selection of the features with the highest ""importance""/influence on the target variable, from a set of existing features. This can be done with various techniques: e.g. Linear Regression, Decision Trees, calculation of ""importance"" weights (e.g. Fisher score, ReliefF) 
 
 If the only thing you want to achieve is dimensionality reduction in an existing dataset, you can use either feature transformation or feature selection methods. But if you need to know the physical interpretation of the features you identify as ""important"" or you are trying to limit the amount of data that need to be collected for your analysis (you need all the initial set of features for feature transformation), then only feature selection can work. 
 You can find more details on  Feature Selection and Dimensionality Reduction  in the following links: 
 
 A summary of Dimension Reduction methods 
 Classification and Feature Selection: A Review 
 Relevant question and answers in Stack Overflow","I think they are 2 different things, 
 Lets start with  Feature Selection : 
 This technique is used for selecting the features which explain the most of the target variable(has a correlation with the target variable).This test is ran just before the model is applied on the data. 
 To explain it better let us go by an example: there are 10 feature and 1 target variable, 9 features explain 90% of the target variable and 10 features together explains 91% of the target variable. So the 1 variable is not making much of a difference so you tend to remove that before modelling(It is subjective to the business as well). I can also be called as Predictor Importance. 
 Now lets talk about  Feature Extraction , 
 Which is used in Unsupervised Learning,extraction of contours in images, extraction of Bi-grams from a text, extraction of phonemes from recording of spoken text. 
When you don't know anything about the data like no data dictionary, too many features which means the data is not in understandable format. Then you try applying this technique to get some features which explains the most of the data. Feature extraction involves a transformation of the features, which often is not reversible because some information is lost in the process of dimensionality reduction. 
 You can apply Feature Extraction on the given data to extract features and then apply Feature Selection with respect to the Target Variable to select the subset which can help in making a good model with good results. 
 you can go through these  Link-1 , Link-2  for better understanding. 
 we can implement them in R, Python, SPSS. 
 let me know if need any more clarification.","The two are very different: Feature Selection indeed reduces dimensions, but feature extraction adds dimensions which are computed from other features.  
 For panel or time series data, one usually has the datetime variable, and one does not want to train the dependent variable on the date itself as those do not occur in the future. So you should eliminate the datetime: feature elimination.  
 On the other hand, weekday/weekend day may be very relevant, so we need to compute the weekday status from the datetime: feature extraction.","A critical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called feature engineering,  involves: 
 • Feature selection: selecting the most useful features to train on among existing features. 
• Feature extraction: combining existing features to produce a more useful one (as we saw earlier, dimensionality reduction algorithms can help). 
• Creating new features by gathering new data 
 
 Quoting : ""A Hands on Machine Learning with SciKit-Learn, Keras & Tensorflow - Aurelien Geron""",,,,,69.34575875,71.51483056,71.70468471,66.99281387,68.16713064,,,,
28835,How to fix these vanishing gradients?,deep-learning,"In order to fix the problem of vanishing gradients, you can use  Xavier Initilization . Also, the implementation of Xavier Initialization in tensorflow can be done by following  this  thread.","You also could try to use a  Batch Norm  layer. It normalizes the outputs of previous layer, which prevents gradients from being too small - see detailed explanation  here","Edit: Definitely try Xavier initialization first, as the other answerer said. 
 In other cases, where you have to increase the gradient manually... 
 Gradient means rate of change of your loss function. If your loss function is not changing much with respect to certain weights, then changing those weights doesn't change your loss function. 
 The weights just determine the type of linear combination from the previous layer. If the loss doesn't change when you change the linear combination, then you need to amplify the effect of increasing or decreasing the linear combination. So what you want is, whenever the weights increase, you want the linear combination to increase by more, and whenever the weights decrease, you want the linear combination to decrease by more. 
 Let's say you multiplied a weight by a constant  k . Then if you increased that weight the linear combination would increase more, and if you decreased the weight then the linear combination would decrease more. So it must be that multiplying your weights by a constant  k  >  1  would increase the effects on the linear combination from changing the weights. 
 If you multiply all your weights by a constant  k , then that's the same thing as multiplying the entire linear combination by  k . So you want to multiply the linear combination by  k  before squishing it with your activation function. 
 Your new activation function then would be this: 
 
 activation = ReLU( linear combination x  k  ) 
 
 or 
 
 
 activation = 0 x (linear combination x  k ), if linear combination <= 0 
 
 activation = 1 x (linear combination x  k ), if linear combination > 0 
 
 
 
 Compare to regular ReLU, which is: 
 
 
 activation = 0 x linear combination, if linear combination <= 0 
 
 activation = 1 x linear combination, if linear combination > 0 
 
 
 
 So you can see that no matter what the case is, you want to multiply the ReLU activation by  k . Specifically, you want to multiply the ReLU activation by  k  >  1 . 
 In practical terms, this increases the slope of the right side of the ReLU function.","Activation functions 
 What activation function are you using? 
 The most common solution to the vanishing gradient issue in deep networks is to use activation functions that don't have this problem, i.e ReLU instead of sigmoid or tanh activation.","After studying about vanishing gradient problem I found some solutions including the already answered ones that are listed below:- 
1- Relu activation function 
2- Greedy layer-wise pre-training 
3- Using weight initializers 
4- using chunk like architecture as used in Resnet, SEresnet where the architecture is divided into 3 chunks and in all of the inputs are 
 
 previous chunk output 
 the initial input to NN",,,,,76.98773499,53.1232664,50.80753185,63.00369529,61.86005194,,,,
28650,Removing Categorial Features in Linear Regression,scikit-learn,"I think using Linear Regression is not a good option as,  
 
 This performs very well on numeric variables(categorical -> binary). 
 Cannot handle Missing Data(suggestible to ignore those records). 
 When you are trying to predict, By chance a category is not trained then it cannot predict. 
For example: A variable has 4 categories and in the random sample it
could pick only 3 categories, if the test has 4th category then it
will throw an error. So, we should be careful when the data is
divided between Test and Train 
 
 Now, what are the other algorithms which are available: 
 
 Random Forest(you cannot use this when any category variable has more than 52 categories, in your case it shouldn't be an issue) 
 XGBoost 
 
 Do let me know if you need any additional explanations.","Sounds like you have a lot of complex categorical variables in your model. Here's what I would do to see which ones are significant and which ones are not. For each of the 4 categorical variables, you will only need 3 binary variables to represent the options. If all 3 binary options are 0, then the fourth category is 1, so it simplifies the model a little. Here's what I would do: 
 1) Run a regression model for each categorical variable using the binary variables. You'll have 4 models in total. 
 2) Run these models with  backwards stepwise regression . You should analyze these four models to look for similarities or patterns, maybe something will jump out at you. 
 3) After all four models are run using this selection method, run a final regression using backwards stepwise selection using only the significant variables from the previous 4 runs. 
 This will leave you with a final model (and results) without cluttering the regression with all 64 iterations of the categorical variables. If the significant variables from this are too cumbersome, maybe only discuss or highlight the most significant independent variables or trim it down some other way. 
 Good luck, let us know how it goes!","Encoding categorical variables as integers is generally bad for linear regression, because the model will interpret that to mean that category 2 is twice as significant as category 1, and so on, which is not necessarily true. It isn't surprising that you got bad results. 
 A better approach is to encode your categories with dummy variables. Let's say your categorical variables are C1, C2, and C3, each taking values from 1 to 4. Then we can have twelve 0/1 dummy variables corresponding to each possible category for each categorical variable. For any input exactly four dummy variables will be 1 and the rest will be zero. 
 Your linear regression now looks like: 
 $$
\hat{y}=a_1*d_{C11}+a_2*d_{C12}+a_3*d_{C13}+a_4*d_{C14} + a_5*d_{C21}+a_6*d_{C22}+a_7*d_{C23}+a_8*d_{C24} +
a_9*d_{C31}+a_{10}*d_{C32}+a_{11}*d_{C33}+a_{12}*d_{C34} +
a_{13}*x_1+a_{14}*x_2+a_{15}*x_3+a_{16}*x_4+a_{17}*x_5
$$ 
 where $x_1$ through $x_5$ are your numerical inputs. 
 If a given input has C1=1, C2=4, and C3=3, for example, then this would reduce to: 
 $\hat{y}=a_1*1+a_8*1+a_{11}*1$ 
 It's also possible to do the same thing with 64 dummy variables for each possible combination of the categorical variables as you were doing, but in a single linear regression as above.  
 See the Wikipedia entry on  dummy variables  for more info. 
 If you are still not getting good results with linear regression, then consider using Gradient Boosting Regression Trees.","What I would do to optimise the performance of linear regression 
 
 One Hot encode the categorical features 
 Use PCA to reduce the dimensionality of the data 
 Scale the data (subtract the mean, divide by the standard deviation) 
 Train the regression model on the reduced scaled dataset 
 
 If you have enough data (>10k examples), you could even train a neural network on the data to capture the complex relationships between features which linear regression wouldn’t capture.","We have tried including all of our 8 features (categorical ones being
  encoded in integer) and doing the linear regression. 
 
 If it is not dummy encoding and your categories can not be ranged - that is wrong. For example, doing this:
[apples, bananas, strawberries] -> [0, 1, 2]
for almost all tasks will be not correct. 
 
 We have also tried taking out the categorical features and running the
  linear regression algorithm for each possible combination of our 3
  categorical features. That of course yields in a lot of regression
  runs (each category has 4 possible values; so 64 to be exact). 
 
 Also needed to be revised. If some of your combinations have very small amount of cases you can not trust the results. 
 So, 
 
 Our dataset has 8 features; 3 of them being categorical. We are
  willing to perform linear regression to fit our target data. 
 
 Do dummy encoding. If you see some strict relationship between categories you can also add one-hot dummy encoding:
category A - category B
1 - 1
2 - 0
2 - 3
1 - 1
... 
 Possibly here category A and category B are strongly correlated while both of them have category 1. Create new feature for this case. 
 In general: play with your data. Find possible relationships and add them to your model. 
 
 If I am forced to step out of linear regression, which algorithm
  should I try to apply? I would prefer to have one dataset with 8
  features instead of having 64 datasets with 5 features. There should
  be an algorithm that can capture this model. 
 
 Forests, XGBoost as mentioned earlier. For this you do not need one-hot or dummy encoding. By the way, usage of simple Decision Trees may give you beautiful pattern of relationships between categories and it's influence on target variable. 
 Try simple neural network after dummy and one-hot encoding too.","All the answers mentioned are great, but what I will do is ( a noob ) 
 
 Go with RF first to get the feature Importance. 
 Then plot the Hierarchical Clustering and plotting the  Dendograms  using Scipy to see which columns are close to each other as the model this them to be.. 
 After That, Go with  CatBoost  to give your model the final touch.. CatBoost is really very effective when you have categorical data... It will handle all of them automatically...(try it if you haven't)",,,,63.55710825,54.33572246,64.97930966,67.28683289,68.7333508,51.25729603,,,
28594,What are helpful annotation tools (if any),classification,Doccano  is an open source simpler alternative to Prodigy. Its native python via Django. I found it suitable for simple implementations.,"You can try  Prodigy  by explosion.ai, creators of spacy or  brat  an open source alternative to it. 
You may also refer to  this post  on qoura.","Label Studio is a powerful opensource with a web interface to annotate different data types. It can be  audio, text, image, video, time series  sources and mixes of them. The  conditional and nested annotations  are supported too. You write your own labeling config fitting your needs to configure the system. 
 Check it here:  https://labelstud.io/playground",I have been working with the  spaCy  extenstion on  INCEpTION  from Technische Universität Darmstadt. Seems pretty good so far.,I have just created a python library ( GitHub  -- Blog post ) to quickly create training data for spaCy NER models using ipywidgets.,,,,,50,50,60.01370841,50,50,,,,
28493,Confusion Matrix - Get Items FP/FN/TP/TN - Python,python,"Considering you have two lists y_actual and y_pred ( I assume you made a typo error on x_test and x_pred as in your code), you can pass the two lists to this function to parse them 
 def perf_measure(y_actual, y_pred):
    TP = 0
    FP = 0
    TN = 0
    FN = 0

    for i in range(len(y_pred)): 
        if y_actual[i]==y_pred[i]==1:
           TP += 1
        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:
           FP += 1
        if y_actual[i]==y_pred[i]==0:
           TN += 1
        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:
           FN += 1

return(TP, FP, TN, FN)
 
 Alternatively, if confusion matrix is a 2x2 matrix (named cm), you can use  
 TP = cm[0][0]
FP = cm[0][1]
FN = cm[1][0]
TN = cm[1][1]","Create a method that does the printing for you: 
 def print_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    print('True positive = ', cm[0][0])
    print('False positive = ', cm[0][1])
    print('False negative = ', cm[1][0])
    print('True negative = ', cm[1][1])
 
 And use it like this 
 print_confusion_matrix(x_test, x_pred)
 
 Alternatively, if you want the values return and not only printed you can do it like this: 
 def get_confusion_matrix_values(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    return(cm[0][0], cm[0][1], cm[1][0], cm[1][1])

TP, FP, FN, TN = get_confusion_matrix_values(x_test, x_pred)","In your case you can use 
 conf = confusion_matrix(x_test, x_pred)
TP = conf[0,0]
FP = conf[0,1]
TN = conf[1,0]
FN = conf[1,1]","I suggest  PyCM  lib for confusion matrix analysis. 
 Example :   
 >>> from pycm import *
>>> y_actu = [2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2] # or y_actu = numpy.array([2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2])
>>> y_pred = [0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2] # or y_pred = numpy.array([0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2])
>>> cm = ConfusionMatrix(actual_vector=y_actu, predict_vector=y_pred) # Create CM From Data
>>> cm.classes
[0, 1, 2]
>>> cm.table
{0: {0: 3, 1: 0, 2: 0}, 1: {0: 0, 1: 1, 2: 2}, 2: {0: 2, 1: 1, 2: 3}}
>>> print(cm)
Predict          0        1        2        
Actual
0                3        0        0        
1                0        1        2        
2                2        1        3        




Overall Statistics : 

95% CI                                                           (0.30439,0.86228)
Bennett_S                                                        0.375
Chi-Squared                                                      6.6
Chi-Squared DF                                                   4
Conditional Entropy                                              0.95915
Cramer_V                                                         0.5244
Cross Entropy                                                    1.59352
Gwet_AC1                                                         0.38931
Joint Entropy                                                    2.45915
KL Divergence                                                    0.09352
Kappa                                                            0.35484
Kappa 95% CI                                                     (-0.07708,0.78675)
Kappa No Prevalence                                              0.16667
Kappa Standard Error                                             0.22036
Kappa Unbiased                                                   0.34426
Lambda A                                                         0.16667
Lambda B                                                         0.42857
Mutual Information                                               0.52421
Overall_ACC                                                      0.58333
Overall_RACC                                                     0.35417
Overall_RACCU                                                    0.36458
PPV_Macro                                                        0.56667
PPV_Micro                                                        0.58333
Phi-Squared                                                      0.55
Reference Entropy                                                1.5
Response Entropy                                                 1.48336
Scott_PI                                                         0.34426
Standard Error                                                   0.14232
Strength_Of_Agreement(Altman)                                    Fair
Strength_Of_Agreement(Cicchetti)                                 Poor
Strength_Of_Agreement(Fleiss)                                    Poor
Strength_Of_Agreement(Landis and Koch)                           Fair
TPR_Macro                                                        0.61111
TPR_Micro                                                        0.58333

Class Statistics :

Classes                                                          0                       1                       2                       
ACC(Accuracy)                                                    0.83333                 0.75                    0.58333                 
BM(Informedness or bookmaker informedness)                       0.77778                 0.22222                 0.16667                 
DOR(Diagnostic odds ratio)                                       None                    4.0                     2.0                     
ERR(Error rate)                                                  0.16667                 0.25                    0.41667                 
F0.5(F0.5 score)                                                 0.65217                 0.45455                 0.57692                 
F1(F1 score - harmonic mean of precision and sensitivity)        0.75                    0.4                     0.54545                 
F2(F2 score)                                                     0.88235                 0.35714                 0.51724                 
FDR(False discovery rate)                                        0.4                     0.5                     0.4                     
FN(False negative/miss/type 2 error)                             0                       2                       3                       
FNR(Miss rate or false negative rate)                            0.0                     0.66667                 0.5                     
FOR(False omission rate)                                         0.0                     0.2                     0.42857                 
FP(False positive/type 1 error/false alarm)                      2                       1                       2                       
FPR(Fall-out or false positive rate)                             0.22222                 0.11111                 0.33333                 
G(G-measure geometric mean of precision and sensitivity)         0.7746                  0.40825                 0.54772                 
LR+(Positive likelihood ratio)                                   4.5                     3.0                     1.5                     
LR-(Negative likelihood ratio)                                   0.0                     0.75                    0.75                    
MCC(Matthews correlation coefficient)                            0.68313                 0.2582                  0.16903                 
MK(Markedness)                                                   0.6                     0.3                     0.17143                 
N(Condition negative)                                            9                       9                       6                       
NPV(Negative predictive value)                                   1.0                     0.8                     0.57143                 
P(Condition positive)                                            3                       3                       6                       
POP(Population)                                                  12                      12                      12                      
PPV(Precision or positive predictive value)                      0.6                     0.5                     0.6                     
PRE(Prevalence)                                                  0.25                    0.25                    0.5                     
RACC(Random accuracy)                                            0.10417                 0.04167                 0.20833                 
RACCU(Random accuracy unbiased)                                  0.11111                 0.0434                  0.21007                 
TN(True negative/correct rejection)                              7                       8                       4                       
TNR(Specificity or true negative rate)                           0.77778                 0.88889                 0.66667                 
TON(Test outcome negative)                                       7                       10                      7                       
TOP(Test outcome positive)                                       5                       2                       5                       
TP(True positive/hit)                                            3                       1                       3                       
TPR(Sensitivity, recall, hit rate, or true positive rate)        1.0                     0.33333                 0.5  

>>> cm.matrix()
Predict          0        1        2        
Actual
0                3        0        0        
1                0        1        2        
2                2        1        3        

>>> cm.normalized_matrix()
Predict          0              1              2              
Actual
0                1.0            0.0            0.0            
1                0.0            0.33333        0.66667        
2                0.33333        0.16667        0.5","If you are using scikit-learn you can use it like this: 
 In the binary case, we can extract true positives, etc as follows: 
 tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
 
 where  y_true  is the actual values and  y_pred  is the predicted values 
 See more details in the  documentation","tn, fp, fn, tp = confusion_matrix(x_test,x_predictions,labels).ravel()","import sklearn    
from sklearn.metrics import confusion_matrix
actual = [1, -1, 1, 1, -1, 1]
predicted = [1, 1, 1, -1, -1, 1]
confusion_matrix(actual, predicted)
 
 output would be 
 array([[1, 1],
       [1, 3]])
 
 For TP (truly predicted as positive), TN, FP, FN 
 c = confusion_matrix(actual, predicted)
TN, FP, FN, TP =  confusion_matrix = c[0][0], c[0][1], c[1][0],c[1][1]","@Srihari's answer works well but pays attention to the indention of the 'return'. Currently, it is written as follows: 
 
 
 def perf_measure(..., ...):
   for i in range(...):
     if():
     ...

return (FP, TN, ...)

               
 
 This return: ""SyntaxError: 'return' outside function "". The normal indement should be: 
 
 
 def perf_measure(..., ...):
   for i in range(...):
     if():
     ...

   return (FP, TN, ...)","Classification Task: Anamoly detection; (y=1 -> anamoly, y=0 -> not an anamoly) 
 𝑡𝑝 is the number of true positives: the ground truth label says it’s an anomaly and our algorithm correctly classified it as an anomaly. 
𝑡𝑛 is the number of true negatives: the ground truth label says it’s not an anomaly and our algorithm correctly classified it as not an anomaly.
 
𝑓𝑝 is the number of false positives: the ground truth label says it’s not an anomaly, but our algorithm incorrectly classified it as an anomaly.
 
𝑓𝑛 is the number of false negatives: the ground truth label says it’s an anomaly, but our algorithm incorrectly classified it as not being.
 
Here is a vectorized implementation. 
 def perf_measure(y_actual, y_pred):
    tp = np.sum((y_actual==1) & (y_pred==1))
    tn = np.sum((y_actual==0) & (y_pred==0))
    fp = np.sum((y_actual==0) & (y_pred==1))
    fn = np.sum((y_actual==1) & (y_pred==0))
    
    return(tp, tn, fp, fn)",54.17324326,69.48826315,56.09179413,53.00192087,58.37684153,66.61819051,70.99529317,50,50
28446,How to compare Timeseries Sequences?,time-series,"The answer to your questions depend a lot on the  nature  of the data represented in the time series. You should ask yourself some questions to better understand what might or might not work, e.g.: 
 
 Are the time sequences perfectly aligned? 
 Are two slightly shifted time series considered similar or not? 
 Are two time series with the same shape but different scale considered similar? 
 
 Normally , the answers to those questions are that series are not perfectly aligned and that variations in scale are also fine as long as the shape is similar. For these scenarios, the classical measure is  Dynamic Time Warping  (DTW). There are lower bounds for DTW that are very efficient computationally. The  research of Professor Keogh  might be interesting if you need theoretical foundation for it. 
 Also,  normally  euclidean distance and Manhattan distance are not very appropriate for time series due to their sensitivity to signal transformations (e.g. shifts), but actually they are often used in practice.","There is this very cool idea in a paper by Ryabko[1] which is not yet very well known. This is called the telescope distance $D_H$. 
 To make a valid assessment about two time series, just looking at the data is not enough. You need to compare the underlying stochastic process that generates them, i.e. you want to compare two probability distributions. And the telescope distance is precisely a metric on the space of probability distributions.  
 It goes like this (kind of formidable to the uninitiated). For a set of functions $\mathbf{H} = (\mathcal{H_1, H_2, \ldots})$, the telescope distance is defined as 
 $$D_{\mathbf{H}}(P, Q) \equiv \sum_{k=1}^{\infty} w_k \sup_{h \in \mathcal{H_k}} | E_P [h(X_1,\ldots,X_k)] - E_Q [h(Y_1, \ldots,Y_k)]|$$  
 where $P,Q$ are the distribution that generates $X$ and $Y$ respectively, and $w_k$ are some exponentially decaying weights (see the paper for details). You don't know $P$ and $Q$; all you got are the time series.  
 It turns out you could use the following empirical quantity $\hat{D}$ to estimate the true telescope distance, 
 $$
\small
\hat{D}_{\mathbf{H}}(X_{1:n}, Y_{1:m}) \equiv \sum_{k=1}^{min(m,n)} w_k \sup_{h \in  \mathcal{H_k}} \big|\frac{1}{n-k+1} \sum_{i=1}^{n-k+1} h(X_{i:i+k-1})- \frac{1}{m-k+1} \sum_{i=1}^{m-k+1} h(Y_{i:i+k-1}) \big|
$$
where $X_{1:n}$ and $Y_{1:m}$ are your observed time series. Notice the nice thing about this metric is, the two time series don't have to be equal in length.  
 Now, everything seems to be fine except that you notice you need the $h(\ldots)$? And it's not just one function, but a sequence of functions. 
 The cool idea is that these $h(\ldots)$s could be modeled as a binary-classifier well known in machine learning. For example, one could use SVM for these $h$ to discriminate between a subsequence $X$ and a subsequence of $Y$. 
 Once you've trained these binary classifiers, and there are $min(n,m)$ of them, you run them through the subsequences of the same length of $X$ and $Y$, sum them up and you're done. 
 [1]Ryabko, D., & Mary, J. (2013). A binary-classification-based metric between time-series distributions and its use in statistical and learning problems. The Journal of Machine Learning Research, 14(1), 2837-2856.","I think you are looking for the distance between two functions, which to my knowledge is a rather complex mathematical field (sorry for not being able to give a reference, but I know I read a book about it once). 
 To answere your second point: I got some decent results using dynamic time warping ( https://en.wikipedia.org/wiki/Dynamic_time_warping ). It should be available for every software kit. In python, there is a package fastdtw ( https://pypi.python.org/pypi/fastdtw ) that does a good job. 
 And I think that scaling will make a difference, no matter which method you use.","A good thing that you could try on top of Euclidian distance and DTW would be: 
 
 DBA which is a sidegrade to DTW, here is an  example   
 Telescopic distance (as suggested by horaceT)  here is a link  that one of the authors gave me, this should prove more concrete than the math only.","as ncasas mentioned ""Normally, the answers to those questions are that series are not perfectly aligned and that variations in scale are also fine as long as the shape is similar"" then DTW is good.  
 The Q is: what if the shape is not similar but the subsequence of TS1 is similar to TS2? Then you shall have a look at subsequence time series clustering. I've found this idea couple of days ago and realized there is a research paper about it already. Here you have the link:  https://arxiv.org/abs/1810.11624 
 Please remember that comparing subsequence TS is meaningfull as long as you choose right metric for clusters distance computation and it shall be based on clusters shape as per ""An Alternate Measure for Comparing
Time Series Subsequence Clusters"" paper.",,,,,51.87114519,52.51591416,50,50,52.69098101,,,,
28022,Clustering algorithm to sort filenames,classification,"This is not at all a typical clustering problem, so I doubt  any  of these algorithms will help. If you want to try clustering, you will need to do appropriate  feature extraction . Don't expect things to work on the raw data. But I guess once you have good features, the problem will already be solved. 
 Instead of trying to frame this as a clustering problem, look at it either from a  sequential  pattern view point, or even better: look at the few questions on how to  learn regexps from a set of strings .","For those interested in a solution for similar problems, I found a solution with these steps: 
 
 Splitting the filenames on ""_"", generating n strings 
 Taking the length of each string 
 Running KMeans (optimal K using Gap Statistics) 
 Taking one sample per cluster and reverse-engineering it to a generic regex, via a customized function 
 
 In practice, here is an example of 10 files: 
 
 
 File 0 splits on "" "" into groups of lengths 3, 10 and 23 respectively.
File 3 splits on "" "" into groups of lengths 11 and 13 respectively.
File 5 splits on ""_"" into groups of lengths 4, 3, 10, 5, 3 and 12 respectively. 
 Files 0, 1 and 2 belong to the same cluster and have identical naming convention. Files 3, 4, 6, 7, 8 and 9 belong to the same cluster and have identical naming convention. File 5 belongs to another cluster and has yet another naming convention.","Parse using regular expressions 
 I work a project where we get thousands of data files per day from on the order of 10 different systems. The filenames are all a jumble, and have a tendency to change over time. This is a job for regular expressions. 
 The primary thing I use to organize is functional groupings. I simply use a small set of indicators common to each file set. In my case, the server that aggregates and sends the data files to me appends its name to the filename (zs2101, or something like that). So then I search file names for this limited set of regular expressions (I am currently using 20). In your situation, it seems like the client name in the file headers is something you could search for an use for the base of your organization 
 Then, I divide files by date of generation of the data. Each file comes with a timestamp. When I read files into my archive, I find the date field in every file and convert it to a standard format, and then change the filename to reflect that standard format. Now I build a directory tree for each server name, organized by date (a folder for each year, and a sub-folder for each month, in my case).  
 My recommendation is that you use regular expressions to organize your data. These are deterministic in the sense that if you receive a strange file header or malformed filename, you know where it will end up (in my case, there is an 'Undetermined' folder that accepts everything that doesn't match a server or date regex). The problem with k-means is that if you get something you didn't plan for, it can be pretty hard to tell where the clustering algorithm will put it, leading to lost data. 
 Good luck, hope this helps.","If you don't know the patterns in advance, then it is going to be quite difficult to do the automated grouping. 
 You can, however, take the following approach 
 Step - 1  - 
 Define basic rules  (data-driven) based on which grouping can be done (like starts-with  JPM_ ,  ends-with  a number in a  specific range , file-size, source, etc) 
 Step - 2  - 
 Unless a direct match towards a group , rules should direct towards a group but with a  staging state  where a  manual verification  can be done. If there is a direct match, directly apply  step 4 . 
 Step - 3  - 
 After  manual verification , change their state to  verified .  
 Step - 4  - 
 Remember the inputs and the verified output  so that next-time when the same input comes, directly put them in the same verified group. 
 Basically, your manual verification will be your  machine-learning  and there are two advantages 
 
 In due time machine will be able to take decisions by itself. 
 Process is easy to debug, verify and port to another system.","Your problem here isn't in choosing an appropriate clustering algorithm, its defining an appropriate similarity metric. Edit distance and jaro winkler distance will cover a lot of ground for you, but you should still anticipate needing to do a fair amount of pre-processing and customization here. Also, as great as text-based metrics are, you have much more information here that can be leveraged. Your data is obviously going to have implicit clusters in it already based on documents that are contained in the same folders, and going further those folders exist in a hierarchy which also implies certain groupings. You should make sure that your clustering and/or similarity scoring incorporates these topological features in addition to any text similarity you do. 
 Your first step is going to be understanding your data more. I'd recommend constructing a tree visualization of the folder hierarchies you're going to be dealing with, taking a sample of 5-10 filenames from within each folder so you can better understand what your dealing with. From here, start trying to understand what kinds of naming conventions are in place that you can take advantage of. There are probably lots of files with dates at the beginning or end, maybe commonly occurring client names, words that are suggestive of document classifications like ""report"", ""newsletter"", ""resume"" etc. The more of these you can capture and deal with directly, the better. Next, you may start seeing some patterns that suggest ways you can further tokenize filenames. spaces, hyphens, and underscores are probably good places to start (after dealing with dates/timestamps, obviously), and CamelCasing would be worth looking out for as well. Also, different filetypes might have different naming conventions. For example, *.png files are probably more likely to have all numeric names starting with dates (i.e. someone dumped their camera to a folder). 
 If you want to just get really quick and dirty with it, something you could try would be to parse each filename into n-grams (e.g. all sequential 3-letter sequences that occur in a filename) and then score pairwise filename similarity based on the jaccard distance of the n-grams that appear in each filename.  
 Once you've figured out a couple of different approaches you want to try, you should start thinking about how to evaluate your results. Obviously you're going to start out evaluating things qualitatively, but that doesn't really help you compare the strengths/weaknesses of different approaches. One thing you could try would be to use the naming conventions learned by a particular method to try to predict whether or not randomly sampled filenames appear in the same folder or not, and score your methods based on how well the resulting classifiers perform. 
 Ultimately, your going to have to custom tailor the solution to what you see in your clients filenames. Hopefully, I gave you a few ideas to work with here.",,,,,56.93728325,55.72313061,59.85644835,50,65.18492786,,,,
28006,Data scientist vs machine learning engineer,machine-learning,"Good question. Actually there is a lot of confusion on this subject, mainly because both are quite new jobs. But if we focus on the semantics, the real meaning of the jobs become clear. 
 Beforehand is better to compare apples with apples, talking about a single subject, the Data. Machine Learning and its sub-genre (Deep Learning, etc.) are just one aspect of the Data World, together with the statistic theories, the data acquisition (DAQ), the processing (which can be non-machine learning driven), the interpretation of the results, etc. 
 So, for my explanation, I will broad the Machine Learning Engineer role to the one of Data Engineer. 
 Science is about experiment, trials and fails, theory building, phenomenological understanding.
Engineering is about work on what science already knows, perfecting it and carry to the ""real world"". 
 Think about a proxy: what is the difference between a nuclear scientist and a nuclear engineer? 
 The nuclear scientist is the one which know the science behind the atom, the interaction between them, the one which wrote the recipe which allow to get energy from the atoms. 
 The nuclear engineer is the guy charged to take the recipe of the scientist, and carry it to the real world. So it's knowledge about the atomic physics is quite limited, but he also know about materials, buildings, economics, and whatever else useful to build a proper nuclear plant. 
 Coming back to the Data world, here another example: the guys which developed Convolutional Neural Networks (Yann LeCun) is a Data Scientist, the guy which deploy the model to recognize faces in pictures is a Machine Learning Engineer. The guy responsible of the whole process, from the data acquisition to the registration of the .JPG image, is a Data Engineer. 
 So, basically, 90% of the Data Scientist today are actually Data Engineers or Machine Learning Engineers, and 90% of the positions opened as Data Scientist actually need Engineers. An easy check: in the interview, you will be asked about how many ML models you deployed in production, not on how many papers on new methods you published. 
 Instead, when you see announces about ""Machine Learning Engineer"", that means that the recruiters are well aware of the difference, and they really need someone able to put some model in production.","The terms are nebulous because they are new 
 Being in the middle of a job search in the 'data science' field, I think that there are two things going on here. First, the jobs are new, and there is no set definitions of various terms, so no commonly agreed upon matching of terms with job descriptions. Compare this to 'web developer' or 'back-end developer.' These are two similar jobs that have reasonably well agreed upon and distinct descriptions.  
 Second, a lot of people doing the job posting and initial interviews don't know that well what they are hiring for. This is particularly true in the case of small to medium sized-companies that hire recruiters to find applicants for them. It is these intermediaries that are posting the job descriptions on CareerBuilder or whatever forum. This isn't to say that many of them don't know their stuff, many of them are quite knowledgeable about the companies they represent and the requirements of the workplace. But, without well defined terms to describe different specific jobs, nebulous job titles are often the result. 
 There are three general divisions of the field 
 In my experience, there are three general divisions of the 'job space' of data science.  
 The first is the development of the mathematical and computational techniques that make data science possible. This covers things like statistical research into new machine learning methods, the implementation of these methods, and the building of computational infrastructure to employ these methods in the real world. This is the division farthest separated from the customer, and the smallest division. Much of this work is done by either academics or researchers at the big companies (Google, Facebook, etc). This is for things like developing Google's TensorFlow, IBM's SPSS neural nets, or whatever the next big graph database is going to be.  
 The second division is using the underlying tools to create application specific packages to perform whatever data analysis needs to be done. People are hired to use Python or R or whatever to build analysis capability on some set of data. A lot of this work, in my experience, involves doing the 'data laundry,' turning raw data in whatever form into something usable. Another big chunk of this work is databasing; figuring out how to store the data in a way that it can be accessed in whatever timeline you need it in. This job isn't so much taking tools, but using existing database, statistics, and graphical analysis libraries to produce some results.  
 The third division is producing analysis from the newly organized and accessible data. This is the most customer facing side, depending on your organization. You have to produce analysis that business leaders can use to make decisions. This would be the least technical of the three divisions; many jobs are hybrids between the second and third divisions at this point, since data science is in its infancy. But in the future, I strongly suspect that there will be a more clean division between these two jobs, with people win the second job needing a technical, computer science or statistics based education, and this third job needing only a general education.  
 In general, all three could describe themselves as 'data scientist', but only the first two could reasonably describe themselves as 'machine learning engineer.' 
 Conclusion 
 For the time being, you will have to find out yourself what each job entails. My current job hired me on as an 'analyst,' to do some machine learning stuff. But as we got to work, it became apparent that the company's databasing was inadequate, and now probably 90% of my time is spent working on the databases. My machine learning exposure is now just quickly running stuff through whatever scikit-learn package seems most appropriate, and shooting csv files to the third division analysts to make powerpoint presentations for the customer.  
 The field is in flux. A lot of organizations are trying to add data science decision making to their processes, but without knowing clearly what that means. Its not their fault, its pretty hard to predict the future, and the ramifications of a new technology are never very clear. Until the field is more established, many jobs themselves will be as nebulous as the terms used to describe them.","[Completely a personal opinion] 
 When the term 'Data Scientist' overtook 'Statistician', it is more towards sounding cool, rather than any major difference. Similarly, the term 'Deep Learning'. It is just neural networks (which is another Machine Learning algorithm) with a couple of more layers. No one can explain when a particular neural net can be called DL, rather than ML, cause the definition itself is fuzzy. So, is the term 'Data Scientist'. 
 However, as companies are adopting the DevOps mindset to data science, the term ML Engineer evolved.  
 What is the DevOps mindset to data science? 
 This is where you build the model, deploy it and also expected to maintain it in production. This helps in avoiding a lot of friction in software teams. 
 [PS: DevOps is a way of doing software, more like a philosophy. So, using it as a designation, again confuses me]. 
 So, ML engineers are supposed to know the nuances of systems engineering, ML, and stats (obviously).  
 A vague generalization would be Data Engineer + Data Scientist = ML Engineer. 
 Having said that, the designations in this space are becoming vague day by day, and the term 'Statistician' is becoming more and more relevant (the irony!).","Machine Learning Engineers and engineering focused Data Scientist are the same, but not all Data Scientist are engineering focused. About 5 years ago almost all Data Scientist were engineering focused, e.g, they had to write production code. Now, however, there are many Data Scientist roles that are for most part: playing in Jupyter notebook, understanding data, making pretty graphs, explaining to clients, managers, analysts... They don't do any engineering. And I believe that term Machine Learning Engineers came up to underline that this an engineering position.","It may vary from company to company, but  Data Scientist  as a designation has  been around for some time now  and is usually  meant for extracting knowledge and insights from data . 
 I have seen  data scientists  doing 
 
 Writing Image processing and image recognition algorithms, 
 Design and implement decision trees for a business use case, 
 Or simply design and implement some reports or write ETLs for data transformations. 
 
 Data science ,  however, is a super-domain of  machine learning 
 
 It employs techniques and theories drawn from many fields within the
  broad areas of mathematics, statistics, information science, and
  computer science, in particular from the subdomains of  machine
  learning, classification, cluster analysis, uncertainty
  quantification, computational science, data mining, databases, and
  visualization . 
 
 Machine learning engineer  seems to be a designation where your employer has already narrowed down to the  
 
 Approach, 
 Tools,  
 and a rough model (of what to deliver)   
 
 to extract knowledge or insights from data using machine learning and your  work will be to design and implement machine learning algorithms to deliver the same .","TL;DR: It depends on who is asking. 
 The answer to this question depends largely on the expectations, knowledge, and experience of whomever is asking. An analogous question with just as fuzzy of an answer is: 
 
 What is the difference between a software developer, a software
  engineer, and a computer scientist? 
 
 To some people, particularly people who study or teach computer science and software engineering, there is a large and defined difference between these fields. But to the average HR worker, technical recruiter, or manager, these are all just ""Computer People"". 
 I love  this quote by Vincent Granville , emphasis mine: 
 
 Earlier in my career (circa 1990) I worked on image remote sensing
  technology, among other things to identify patterns (or shapes or
  features, for instance lakes) in satellite images and to perform image
  segmentation:  at that time my research was labeled as computational
  statistics, but the people doing the exact same thing in the computer
  science department next door in my home university, called their
  research artificial intelligence. Today, it would be called data
  science  or artificial intelligence, the sub-domains being signal
  processing, computer vision or IoT.","I don't disagree with any of the answers given. However, I do think that there is a role of Data Scientist that is being glossed over in virtually all of the answers here. Most of these answers say something to the effect of, ""Well, an engineer just writes and deploys the model . . . "". Hold on a sec - there's  A LOT  of work in those two steps! 
 My core  definition of a Data Scientist is someone that applies the scientific method to working with data.  So I'm constantly thinking of hypostheses, designing tests, collecting my data and executing those tests, checking my cross validation results, trying new approaches, transforming my data, etc, etc. That's essentially what goes into ""just writes and deploys the model"" in a professional setting.  
 So, for your answer, I think ""the devil is in the details"" because you can't just gloss over some of these steps/terms. Also, if you are job hunting, you should be careful because ""data engineer"" and ""data scientist"" can have woefully different pay scales - you do not want to be a data scientist on a data engineer salary!  
 I always put myself out there as a data scientist, I tell companies that I work on predictive models (not just analytical) and that I'm not an Excel jockey - I write in programming languages (R, Python, etc). If you can find a position that let's you do both of those, then you're on your way to being a data scientist.","Machine Learning is more specific and in the field you will need to master the following: 
 
 Features vs Labels 
 Test data vs Training data 
 Feature Normalization 
 Common data structures (arrays of arrays) 
 Feature Selection","A data scientist’s position these days has become much more generalized and broad-based to the degree that it could fully supersede Machine Learning. And yet, there are cases where a data scientist does not perform data analysis on the data itself. 
 A data scientist’s roles can be multifarious. Manipulating, processing, and querying large volumes of concurrent data is now an incredibly skilled task in the age of ‘Big Data’ technologies. Thus, a data scientist’s main function may be, for example, to run and manage an architecture to absorb a wide range of data from various sources. 
 Statistics differ greatly in terms of wages and future growth! But one thing is fore certain that t he number of positions for data scientists well outruns the number of jobs for engineers in machine learning. 
 And it can be said confidently that these jobs will certainly not go anywhere, at least for the next 20 years, as the amount of data and its complexities will continue to increase significantly more. 
 Have a look at the following machine learning vs data scientist comparison. 
 A lot of Data Scientists’ job posting appeared and was flooding the market in the previous years. The same for the Machine Learning Engineer position is happening, it’s a fairly new one and is slowly evolving in areas where we have data specialists.",78.09771337,57.47805691,66.94157272,81.0533613,68.39670611,53.89651542,69.00557633,56.71219285,78.76417544
27767,Opening a 20GB file for analysis with pandas,python,"If it's a csv file and you do not need to access all of the data at once when training your algorithm, you can read it in chunks. The  pandas.read_csv  method allows you to read a file in chunks like this:  
 import pandas as pd
for chunk in pd.read_csv(<filepath>, chunksize=<your_chunksize_here>)
    do_processing()
    train_algorithm()
 
 Here is the method's  documentation","There are two possibilities: either you  need  to have all your data in memory for processing (e.g. your machine learning algorithm would want to consume all of it at once), or you can do without it (e.g. your algorithm only needs samples of rows or columns at once). 
 In the first case, you'll need to  solve a memory problem . Increase your memory size, rent a high-memory cloud machine, use inplace operations, provide information about the type of data you are reading in, make sure to delete all unused variables and collect garbage, etc.  
 It is very probable that 32GB of RAM would not be enough for Pandas to handle your data. Note that the integer ""1"" is just one byte when stored as text but 8 bytes when represented as  int64  (which is the default when Pandas reads it in from text). You can make the same example with a floating point number ""1.0"" which expands from a 3-byte string to an 8-byte  float64  by default. You may win some space by letting Pandas know precisely which types to use for each column and forcing the smallest possible representations, but we did not even start speaking of Python's data structure overhead here, which may add an extra pointer or two here or there easily, and pointers are 8 bytes each on a 64-bit machine. 
 To summarize: no, 32GB RAM is probably not enough for Pandas to handle a 20GB file. 
 In the second case (which is more realistic and probably applies to you), you need to solve a  data management problem . Indeed, having to load all of the data when you really only need parts of it for processing, may be a sign of bad data management. There are multiple options here: 
 
 Use an SQL database. If you can, it is nearly always the first choice and a decently comfortable solution. 20GB sounds like the size most SQL databases would handle well without the need to go distributed even on a (higher-end) laptop. You'll be able to index columns, do basic aggregations via SQL, and get the needed subsamples into Pandas for more complex processing using a simple  pd.read_sql . Moving the data to a database will also provide you with an opportunity to think about the  actual  data types and sizes of your columns. 
 If your data is mostly numeric (i.e. arrays or tensors), you may consider holding it in a HDF5 format (see  PyTables ), which lets you conveniently read only the necessary slices of huge arrays from disk. Basic  numpy.save and numpy.load  achieve the same effect via memory-mapping the arrays on disk as well. For GIS and related raster data there are  dedicated   databases , which might not connect to pandas as directly as SQL, but should also let you do slices and queries reasonably conveniently. 
 Pandas does not support such ""partial"" memory-mapping of HDF5 or numpy arrays, as far as I know. If you still want a kind of a ""pure-pandas"" solution, you can try to work around by ""sharding"": either storing the  columns  of your huge table separately (e.g. in separate files or in separate ""tables"" of a single HDF5 file) and only loading the necessary ones on-demand, or storing the  chunks of rows  separately. However, you'd then need to implement the logic for loading the necessary chunks, thus reinventing the bicycles already imlpemented in most SQL databases, so perhaps option 1 would still be easier here.  If your data comes in a CSV, though, you can process it in chunks by specifying the  chunksize  parameter to  pd.read_csv .","I just had this issue a few days ago! Not sure if this helps in your specific case since you aren't providing so many details, but my situation was to work offline on a 'large' dataset. The data was obtained as 20GB gzipped CSV files from energy meters, time series data at several seconds intervals.  
 File IO:  
 data_root = r""/media/usr/USB STICK""
fname = r""meters001-050-timestamps.csv.gz""
this_file = os.path.join(data_root,fname)
assert os.path.exists(this_file), this_file
this_file
 
 Create a chunk iterator directly over the gzip file (do not unzip!) 
 cols_to_keep = [0,1,2,3,7]
column_names = ['METERID','TSTAMP','ENERGY','POWER_ALL','ENERGY_OUT',]
parse_dates = ['TSTAMP']
dtype={'METERID': np.int32, 
       'ENERGY': np.int32,
       'POWER_ALL': np.int32,
       'ENERGY_OUT': np.int32,
      }
df_iterator = pd.read_csv(this_file, 
                        skiprows=0, 
                        compression='gzip',
                        chunksize=1000000, 
                        usecols=cols_to_keep,
                        delimiter="";"",
                        header=None,
                        names = column_names,
                      dtype=dtype,
                     parse_dates=parse_dates,
                     index_col=1,
                     )
 
 Iterate over the chunks 
 new_df = pd.DataFrame()
count = 0
for df in df_iterator:
    chunk_df_15min = df.resample('15T').first()
    #chunk_df_30min = df.resample('30T').first()
    #chunk_df_hourly = df.resample('H').first()
    this_df = chunk_df_15min
    this_df = this_df.pipe(lambda x: x[x.METERID == 1])
    #print(""chunk"",i)
    new_df = pd.concat([new_df,chunk_df_15min])
    print(""chunk"",count, len(chunk_df_15min), 'rows added')
    #print(""chunk"",i, len(temp_df),'rows added')
    #break
    count += 1
 
 Inside the chunk loop, I am doing some filtering and re-sampling on time. Doing this I reduced the size from 20GB to a few hundred MB HDF5 for further offline data exploration.","In my experience, initializing  read_csv()  with parameter  low_memory=False  tends to help when reading in large files. I don't think you have mentioned the file type you are reading in, so I am not sure how applicable this is to your situation though.","If your file is a CSV then you can simply do it in Chunk by Chunk. You can just simply do: 
 import pandas as pd
for chunk in pd.read_csv(FileName, chunksize=ChunkSizeHere)
(Do your processing and training here)",,,,,58.37409657,57.34457307,54.9268947,57.42361866,56.44562172,,,,
26789,Should I use regex or machine learning?,machine-learning,"I would say depends on the requirements and how much effort you want to give the task.  
 
 Using regex is definitely easier but in the some time you will not be able to cover everything specially if the text you have is not structured as you expect all the time. So in this case you will be updating you regex patterns each time you miss something ... 
 If you go for machine learning approach then you will need data to train your models on it ( a lot of this data). Time for training, and enhancing the quality of your model. Hopefully you will get something good. 
 
 As a conclusion, I think, if you can cover the requirement with regex go for it. If regex will not be a good solution then start thinking in machine learning solutions.","If you foresee to build something for a big public, definitely you cannot use regular expressions. There is no way you can write a regular expression that can span the variance that a class of documents (email or PDF) can have. 
 Even if you are happy with a regex that can handle efficiently a (small) percentage of the possible documents, and then amend it from time to time, it will take so much more time with respect to find the training data and train a ML algorithm to do it. 
 Only if you have to parse some kind of standardized document (PDF or email), you can think of using some regex parser. 
 That's the reason why, in general, you need to use some ML technique to reach your goal.","You receive an email from a friend that says, ""let's have lunch next Tuesday"" and your email program detects it and asks if you want to save a new calendar entry for ""lunch on Tuesday"". 
 
 What you describe is called  Information extraction  and is a big field of NLP (Natural Language Processing). You are looking for  temporal expression identification . You can have a look at the  Stanford Temporal Tagger: SUTime  to get a ""live"" demo. From what I see  here  it is a regex-based rule system. 
 To give you an impression how powerful rule-based systems can be:  
 
 Weizenbaum’s own secretary reportedly asked Weizenbaum to leave the room so that she and ELIZA could have a real conversation. Weizenbaum was surprised by this, later writing, “I had not realized… that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.” 
 
 Source:  Wikipedia: Eliza 
 See also 
 
 Apple Data Detectors 
 TimeML: Robust specification of event and temporal expressions in text","In scale, unless you are expecting to receive only a particular format, it is machine learning. For the first task, you should first parse the text and then scan it, probably with a Named Entity Recognition (NER) system to extract the information you are after. Having a NER system would work, as you can manually code different types of features that will greatly improve the performance. If you just want to perform candidate matching, then standard bag-of-word approaches would perform decently.  
 For the second case, things are similar. You can rely in some syntactic analysis of the sentence to obtain the invitation and the time/day of the proposal. This again can be coupled with NER systems. Lately, for both tasks neural networks yielded promising approaches. But, in any case, you need labeled data, which can be cumbersome to obtain.  
 On the other hand, regex can be great ways to go with, especially if you can predict/adapt to the variability of the incoming data. In any case, they can be used to create your first training data.","If in case 1 there would be one template for organizing information on CVs then you can go for regex, but in order to have really helpful tool for real world CV you have to train ML solution. Just type in google ""cv example"" and you will see that people use different words and have different order of information and different formatting (formatting could be eliminated from problem scope). You can use regex but it will be enormous and I doubt that you will outperform some NER solution. 
 In case 2 it's common chat bot case so it's question of whether on not use ML to implement chat bot (spoiler - use ML)",Not every Tuesday is lunch day even if you are talking about some lunch on some Tuesday. It should be a NLP system using NER to provide the best prediction.,"Ive been able to use a set of regex rules that feed a scoring system to profile Pubmed abstracts. For example, any instance of 'increased risk', 'increased association', etc., adds to an 'association' counter. Similarly, 'reduced risk' subtracts from the same counter.  
 I can also identify and extract specifics such as statistical data measures (eg p-values), gene names, sample population/patient characteristics, etc. 
 Of course this approach is not valid for complex grammar but it lends itself to the predictable and formulaic format that abstracts are written in, and in particular to the discipline that I am interested in, ie molecular biology.    
 I was surprised at how useful such a simple methodology can be for profiling an abstract.  
 The key is in developing the regex rules. For my application, ie extracting data from Pubmed abstracts, I was essentially modelling my internal process of what I look for when scanning abstracts to determine relevance.  
 Over time I found I was adding new phrases to the regex 'library' to capture instances where the rule system encountered text it did not profile correctly.",,,71.77087112,62.51655153,52.79219639,55.18321362,63.30413276,50.94472718,58.68588291,,
26723,How would you visualize data that comes in the millions of records?,visualization,Holoviews visual library can handle very large data  http://holoviews.org/   http://holoviews.org/user_guide/Large_Data.html,"Plotting millions of entries through histograms, pie charts, doughnut charts, tree maps, area charts, bar charts, choropleths (and so on - and on and on) does not pose any challenge. You can only find it very slow and annoying if you were to use scatter plots/violin plots, or visualise as a very large graph.","I recommend you using  PCA . It finds directions which data is highly distributed in. Using this procedure, the components __ new features __ will be in descending order for the eigenvalues. Each eigenvalue that has greater value than the next eigenvalues, will have much information than them. After using  PCA  you can use its first three principal components for plotting. Each of the new features is a linear combination of the previous features. Using e.g. first three principal components will have so much information which will be representative of your data. In cases that data is not correlated, the preceding statement may not be always true but in your case that you have so many features, based on experience, you definitely have so many correlated features. For more information take a look at  here  and  here  which may help you.","Sampling is a totally good option, especially if the size of your data is bogging down the tool you're using to plot it.  
 If that's not the problem, a common issue is that plotting opaque markers will show you where data is located, but will disguise density information. For example, imagine a situation where every pixel of your plotting area is associated with at least ome observation (i.e. you have a uniformly colored plot) but one pixel is actually associated with 99% of your data. A good technique for situations like this is to try to visualize the density of the data. A simple approach is to add transparency to your markers (often by adjusting the ""alpha"" parameter), or you can model the density more directly with binning (e.g. a histogram or hexgrid) or with a kernel density estimate.  
 If you have discrete data, overplotting will likely be an issue but density might give you weird results. A good way to address this is to ""jitter"" your data by adding noise to one or more plotting dimensions to force your data to spread out more. 
 If you have time series data, you can resample to a coarser resolution: e.g., if you have a data point for every millisecond, your data will probably be easier to visualize if you aggregate by hour, day, or week. 
 Similarly, you can summarize the data by plotting a model. Plot $X$ vs $E[Y|X]$ instead of $X$ vs $Y$and throw in some error bands for good measure.  
 All that said: just try plotting it first and see what happens. Your visualization tool might do some stuff under the hood to render at least some of this manual effort unnecessary.","If you must plot raw values, use a random sampling strategy or some form of decimation (every nth value). Otherwise, computing and plotting summary statistics will be orders of magnitude faster. While lossy, careful attention to variability around the metrics will help you understand the form of the raw data",,,,,56.70008113,52.030359,51.01199534,55.6318717,50.73999141,,,,
26597,How to set the number of neurons and layers in neural networks,machine-learning,"The consideration of the number of neurons for each layer and number of layers in  fully connected networks  depends on the feature space of the problem. For illustrating what happens in the two dimensional cases in order to depict, I use 2-d space. I have used images from the works of  a scientist . For understanding other nets like  CNN  I recommend you taking a look at  here . 
 Suppose you have just a single neuron, in this case after learning the parameters of the network you will have a linear decision boundary which can separate the space to two individual classes.  
 
 
 Suppose that you are asked to separate the following data. You will need  d1  which specifies the upper decision boundary and somehow is doing  AND  operation to determine whether the input data is on the left side of it or on the right side. Line  d2  is doing another  AND  operation which investigates whether the input data is upper than  d2  or not. In this case  d1  is trying to understand whether the input is on the left side of line to classify the input as  circle , also  d2  is trying to figure out whether the input is on the right side of the line to classify the input as  circle . Now we need another  AND  operation to wrap up the results of the two lines which are constructed after training their parameters. If the input is on the left side of  d1  and on the right side of  d2 , it should be classified as  circle . 
 
 Now suppose that you have the following problem and you are asked to separate the classes. In this case the justification is exactly like the above's. 
 
 For the following data: 
 
 the decision boundary is not convex and is more complex than the previous boundaries. First you have to have a sub-net which finds the inner circles. Then you have to have another sub-net which finds the inner rectangular decision boundary which decides the inputs which are inside of the rectangle are not circle and if they are outside, they are circle. After these, you have to wrap up the results and say if the input data is inside the bigger rectangle and outside of the inner rectangle, it should be classified as  circle . You need another  AND  operation for this purpose. The network would be like this: 
 
 
 Suppose that you are asked to find the following  circled  decision boundary. 
 
 In this case your network would be like the following network which was referred to but with much more neurons in the first hidden layer.","Very good question, as there doesn't exist an exact answer to this question yet. This is an active field of research. 
 Ultimately, the architecture of your network is related to the dimensionality of your data. Since neural networks are universal approximators, as long as your network is big enough, it has the ability to fit your data. 
 The only way to truly know which architecture works best is to try all of them, and then pick the best one. But of course, with neural networks, it is quite difficult as each model takes quite some time to train. What some people do is first train a model that ""too big"" on purpose, and then prune it by removing weights that do not contribute much to the network. 
 What if my network is ""too big"" 
 If your network is too big, it might either overfit or struggle to converge. Intuitively, what happens is that your network is trying to explain your data in a more complicated way than it should. It's like trying to answer a question that could be answered with one sentence with a 10-page essay. It might be hard to structure such a long answer, and there may be a lot of unnecessary facts thrown in. ( see this question ) 
 What if my network is ""too small"" 
 On the other hand, if your network is too small, it will underfit your data and therefore. It would be like answering with one sentence when you should have written a 10-page essay. As good as your answer might be, you will be missing some of the relevant facts.  
 Estimating the size of the network 
 If you know the dimensionality of your data, you can tell whether your network is big enough. To estimate the dimensionality of your data, you could try computing its rank. This is a core idea in how people are trying to estimate the size of networks. 
 However, it is not as simple. Indeed, if your network needs to be 64-dimensional, do you build a single hidden layer of size 64 or two layers of size 8? Here, I am going to give you some intuition as to what would happen in either case. 
 Going deeper 
 Going deep means adding more hidden layers. What it does is that it allows the network to compute more complex features. In Convolutional Neural Networks, for instance, it has been shown often that the first few layers represent ""low-level"" features such as edges, and the last layers represent ""high-level"" features such as faces, body parts etc. 
 You typically need to go deep if your data is very unstructured (like an image) and needs to be processed quite a bit before useful information can be extracted from it. 
 Going wider 
 Going deeper means creating more complex features, and going ""wider"" simply means creating more of these features. It might be that your problem can be explained by very simple features but there needs to be many of them. Usually, layers are becoming narrower towards the end of the network for the simple reason that complex features carry more information than simple ones, and therefore you don't need as many.","Short Answer: It is very related to the dimensions of your data and the type of the application. 
 Choosing the right number of layers can only be achievable with practice.  There is no general answer to this question yet . By choosing a network architecture, you constrain your space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data. In a DeepNN each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the problem at hand, this information can never be recovered by later layers. This is usually referred as "" Information Bottleneck "". 
 Information Bottleneck is a double-edged sword: 
 1) If you use a few number of layers/neurones, then the model will just learn a few useful representations/features of your data and lose some important ones, because the capacity of middle layers are very limited ( underfitting ). 
 2) If you use a big number of layers/neurones, then the model will learn  too much representations/features that are specific to the training data and don’t generalize to data in the real-world and outside of your training set ( overfitting ).  
 Useful links for examples and more finding: 
 [1]  https://livebook.manning.com#!/book/deep-learning-with-python/chapter-3/point-1130-232-232-0 
 [2]  https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/","Working with neural networks since two years ago, this is a problem I always have each time I wan't to model a new system. The best approach I've found is the following: 
 
 Look for similar problems that have also been modeled with feed-forward networks and study their architectures. 
 Begin with that configuration, train the data set and evaluate the test set. 
 Perform  pruning  in your architecture and compare results in the data set with the previous results. If the accuracy of your model is not affected so you can infer that the original model is overfitting the data. 
 Otherwise, try adding more degrees of freedom (i.e. more layers). 
 
 The general approach is to try different architectures, compare results and take the best configuration. Experience gives you more intuition in the first architecture guess.","Adding to the previous answers, there are approaches where the topology of the neural network emerges endogenously, as part of the training. Most prominently, you have Neuroevolution of Augmenting Topologies (NEAT) where you start with a basic network without hidden layers and then use a genetic algorithm to ""complexify"" the network structure. NEAT is implemented in many ML frameworks. Here is a pretty accessible article on an implementation to learn Mario:  CrAIg: Using Neural Networks to learn Mario","1.) The optimal number of neurons in each layer depends on your function you try to approximate. For one function, there might be a perfect number of neurons in one layer. But for another fuction, this number  might be different. 
 2.) According to the Universal approximation theorem, a neural network with only one hidden layer can approximate any function (under mild conditions), in the limit of increasing the number of neurons. 
 3.) In practice, a good strategy is to consider the number of neurons per layer as a hyperparameter. A recent study showed that optimizing these hyperparameters should not be done independently. Instead, one can perform exhaustive grid search on a small neural network, to find the best hyperparameters, and then scale the entire neural network ( EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks ).",,,,55.79746958,57.64557419,58.54576236,54.8793338,57.88403401,74.5888288,,,
26555,"ValueError: shapes (1,10) and (2,) not aligned: 10 (dim 1) != 2 (dim 0)",machine-learning,"# The confusion occurs due to the two different forms of statsmodels predict() method.
# This is just a consequence of the way the statsmodels folks designed the api.
# Both forms of the predict() method demonstrated and explained below.

import statsmodels.api as sm
import numpy as np
import matplotlib.pyplot as plt

# random normal x, y with y = x + 10
x = np.random.randn(100)
y = x + np.random.randn(100) + 10

fig, ax = plt.subplots(figsize=(8, 4))
ax.scatter(x, y, alpha=0.6, color='blue')
# will add the regression line to the plot and display below.

# Plot a linear regression line through the points in the scatter plot, above.
# Using statsmodels.api.OLS(Y, X).fit().
# To include a regression constant, one must use sm.add_constant() to add a column of '1s'
# to the X matrix. Basically, this tells statsmodels to calculate a constant for the regression line.
#
# FYI, the sklearn.linear_model.LinearRegression model includes a fit_intercept parameter
# and does not require the X matrix to have a column of ones.
x_matrix = sm.add_constant(x)
model = sm.OLS(y, x_matrix)
# regression_results is an object: statsmodels.regression.linear_model.RegressionResults.
regression_results = model.fit()
# nice summary
# print(regression_results.summary())

#
# There are two forms of the predict() method:
# There is sm.OLS(y, x).predict(). This predict() method has no knowledge of the fitted coefficients
# produced by model.fit(), above. This is just the way the scipy developers decided to implement
# the linear model.
# The fitted coefficients for the linear model are in RegressionResults and RegressionResults
# has its own predict().
# If you use model.predict(), you need to pass the coefficients. This form of the predict() method
# basically calculates y = ax + b where you pass the coefficients, a and b. This is why the error
# occurs.
#
# Use RegressionResults.predict() which acts as you expect, except that you  still must add
# a column of ones to the x-values used to predict y. This is because the original model
# was fit with a regression constant (intercept). Generally linear models are fit with an intercept
# unless there is some problem-specific reason not to.
#
x_pred = np.linspace(x.min(), x.max(), 50)
# put the X matrix in 'standard' form, i.e. with a column of ones.
x_matrix = sm.add_constant(x_pred)
y_pred = regression_results.predict(x_matrix)

# Line from RegressionResults.predict() in 'black'.
ax.plot(x_pred, y_pred, color='black')
# one more graphic to add before display below.

# So why is model.predict() provided? The calculation for a linear model is a trivial
# linear numpy calculation. For more complex models, this will not be the case
# and model.predict() can be useful.
# Here is how to use it.

coef = regression_results.params[1]     # get the fitted model coefficients
const = regression_results.params[0]

# These two lines of code produce the same results, as expected.
# y_pred = coef * x_pred + const
y_pred = model.predict(params=[const, coef], exog=x_matrix)

# Line from model.predict() in 'purple' overlays the 'black' line from above.
ax.plot(x_pred, y_pred, color='purple')


plt.show()
plt.close()","X_ne1 = X_test[:,3]
y_pred2 = regressor_OLS.predict(X_ne1)","You also need to drop the columns that corresponded to the one you dropped while building a more optimized regressor. 
 X_new = X_test[:, [0,3]] 
y2_pred = regressor_OLS.predict(X_new)
 
 Also you will need to use the predict on your test set which is not clear in your question.","You don't need to take columns from X as you have already defined X_opt. 
Also you shouldn't use 3 as you have just 2 columns. 
First you need to split the dataset into X_opt_train and X_opt_test and y_train and y_test. Then you fit the dataset to X_opt_train and y_train. And then you predict: 
y_pred = regressor_OLS.predict(X_opt_test) 
 At least this works for me.I had the same error","I'm writing the following code and there is an error I get when running:
import numpy as np
import matplotlib.pyplot as plt 
 Define the sample frequency and number of samples 
 fs = 1 # Hz
N = 128 
 Generate a sinusoidal signal with two frequencies 
 #f1 = 50
#f2 = 150
t = np.linspace(0, (N-1)/fs, N)
#x = np.sin(2 np.pi f1 t) + np.sin(2 np.pi f2 t)
x = np.loadtxt(r'C:\Anghel Cernescu_DELL\Fatigue book\Fatigue load histories\Center Notched Plate GKN\Time history.txt')
print(x) 
 Compute the FFT of the signal 
 X = np.abs(np.fft.fft(x)) 
 Normalize the FFT results 
 X = X / N 
 Compute the ASD 
 ASD = X**2 
 Determine the frequency range 
 f = np.linspace(0, fs/2, N//2) 
 Generate a random phase spectrum 
 phase_spectrum = np.random.uniform(0, 2*np.pi, N//2)
print(phase_spectrum) 
 Create the complex spectrum 
 complex_spectrum = ASD.dot(np.exp(1j * phase_spectrum)) 
 Inverse FFT to obtain the time-domain signal 
 x_reconstructed = np.fft.ifft(complex_spectrum) 
 Plot the original and reconstructed signals 
 plt.plot(t, x, label='Original Signal')
plt.plot(t, x_reconstructed, label='Reconstructed Signal')
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')
plt.title('Signal Generation from ASD')
plt.legend()
plt.show() 
 The error is:
ValueError: shapes (128,) and (64,) not aligned: 128 (dim 0) != 64 (dim 0) 
 Can someone give me a solution? 
 Thanks,
Anghel",,,,,50,50,50,50,56.62588126,,,,
26449,Beginner math books for Machine Learning,machine-learning,"Although you need book, I recommend the following courses respectively for understanding statistics which are used for machine learning and other tasks in data science. They are free. 
 
 Learn Statistics - Intro to Statistics Course 
 Intro to Descriptive Statistics 
 Inferential Statistics: Learn Statistical Analysis 
 
 If I want to recommend a book, I would recommend the following book which is free under  CC license . It has nice examples and is so much practical; moreover, there are lots of codes in it which help you feel statistics in real world examples.  
 
 Think Python by Allen B. Downey 
 Python Data Science Handbook 
 
 Also the following link may help:  
 
 From Google Itself Good And Concise","Introduction to Linear Algebra   is a good starting point. Make sure you are good with probability theory, linear algebra, and statistics. A very in depth knowledge may not be necessary, but having a good knowledge is required.","Before doing my master in Analytics, I was suggested by my seniors to go through these couple of books to know more about Machine Learning and Statistics. 
 Namely: 
 
 Discovering statistics with SPSS/R - Andy Field   
 R Beginner and R for Everyone 
 Predictive Analytics - The Power to Predict Who Will Click, Buy, Lie or Die 
 Data Science for Business  and many more 
 
 If you cannot find these books online, do let me know will share the link, I have them on my drive. These books helped me in understanding the basics of stats with examples explained in layman terms. 
 If you are looking for some online courses, let me know can suggest you couple of good courses(most of them are free).","I cannot tell from your question how adept you are at mathematics or where your learning stops. I'll assume since you are a computer software engineer that you're familiar with algebra, geometry, and perhaps some calculus. 
 I'd recommend you start your learning by reading up on statistics and understanding concepts like descriptives, exploratory data analysis, correlation, distributions, and so on. I see that you prefer books rather than videos, so I'll meet you half way and provide you with a few books that are online, as well as a book or two that you can buy in print. 
 First, I'd recommend  Penn State's online graduate course curriculum in statistics . You can explore each of their courses using the menu on the left. Once you select a course, scroll down on the course's webpage and click on the link that reads ""online course notes"". The course notes for these courses are much more than notes and read like full books. They are very instructive. Also, check out  Penn State's online undergraduate course curriculum in statistics , too, in case you find something in the graduate coursework that is too advanced and want a ""simpler"" explanation. 
 Second, review the  Handbook of Biological Statistics  by John H. McDonald. Don't let the title fool you; this book is an excellent primer on statistics and data analysis that is applicable to any domain. 
 Third, review  The Little Handbook of Statistics  by Gerard Dallal. Again, don't let the title fool you; this book is another gem that walks you through some important statistics fundamentals. 
 Fourth, check out the book  Think Stats  by Allen Downey. There's a free version online of an earlier edition; the most recent edition you'll have to buy. It's worth it though, especially if you work in Python. In this book, the author teaches you statistics and data analysis using Python to analyze real-world (toy) datasets. This is a really great book to work through. 
 Lastly, check out  Data Science from Scratch  by Joel Grus. This book focuses more on data analysis (instead of statistics fundamentals) and places a greater emphasis on machine learning and modeling. It uses Python (and the Python data science stack) to walk you through analyzing and conducting predictive analytics on real-world (toy) datasets. Another great book to work through.","Keep in mind that while I have a Masters in Applied Statistics, I'm going to give you a very simple answer: take a course on probabilities.  
 Most of the modern ML programming frameworks take a large majority of the math out of data science; you really just won't need it in most scenarios. But you will always need the ability to understand your results and the majority of results are expressed in probabilities. If I was new to data science I would take a (brief) course on probabilities, seek to understand what proportions and percentages really mean and then I would work to know a framework (like Tensorflow) really, really well. If you can do that, you can write some really interesting algorithms and not have to be obsessive about the math.",,,,,56.19093555,50,59.34196634,57.73173026,53.76270123,,,,
26351,Feature Scaling of Training Set and Test Set,machine-learning,"You do feature scaling for accelerating learning process. Features may have different scales. One maybe from 1 to 10 and one may be from -100 to 1000. Using normalization, you make the scale of them the same as each other, helps accelerate the learning process. You should find the mean and variance for each feature separately on your training data. then during training and testing each feature should be reduced by the corresponding mean and be divided by the corresponding standard deviation. So yes, for each feature during testing and training you have to provide same values for mean and std which are obtained using training data. I suggest you taking a look at  here .","The main reason to use statistics computed on only the training set is to avoid leaking information from the test set. 
 If this is not a concern, then it is perfectly OK to use statistics from the entire data set. 
 See  here  for further discussion.","With scaling (or Z-transformation), you need a mean and a variance, which should come from total data.   
 What's more, if your model is going to be used on future coming data, then this mean and variance need to be applied to new data as well, as oppose to based on new data's mean or variance. 
 One important assumption in modelling is that feature / pattern in training would be the same or similar in testing set. This is the base that we can use history data to predict future. 
 Hence this requires consistency in mapping values to another using one transformation function.  Different sets of mean and variance, would lead to different transformation.","The argument of avoiding information leaking into the new data by using training data statistics (mean, standard deviation) for normalization seems counter-intuitive because inserting training mean and standard deviation into the new data IS information leak from training data to new data.  
 Statistically, this procedure ""forces"" the membership of the new data to the training population which should not be necessarily true in all cases: prediction is also a test of stationarity of underlying processes. 
 For example, statistical t-tests for mean differences use each group's statistics therefore being able to detect (mean) differences between groups; this would be difficult (the test would have a lower power) if combined (or, only one of the group's) statistics would be used instead. 
 I think the current practice tries to keep the prediction error low by ""bleeding"" information from the training set into the new set.","Before answering or sharing my opinion I, will like to extend the question beyond just training and testing to include prediction, that too real time prediction and not batch mode prediction. 
 Recommended sequence of steps during modeling. 
 
 Divide the sample data in training and validation set. 
 Scale training data. 
 Using same factor as training data (example mean and variance of training data) scale test data. 
 
 For in production prediction in real time use the above stored value to scale the feature.
Update these values during model retraining.
Refer  here  for detail discussion.",,,,,61.97443281,63.02505177,55.66797042,55.91821661,61.46151605,,,,
26180,L2 loss vs. mean squared loss,loss-function,"Function  $L_2(x):=\left \|x \right \|_2$  is a norm, it is not a loss by itself. It is called a ""loss"" when it is used in a loss function to measure a distance between two vectors,  $\left \| y_1 - y_2 \right \|^2_2$ , or to measure the size of a vector,  $\left \| \theta \right \|^2_2$ . This goes with a loss minimization that tries to bring these quantities to the ""least"" possible value. 
 These are some illustrations: 
 
 $L_p$  norm:  $L_p(x) := \left \|x \right \|_p = (\sum_{i=1}^{D} |x_i|^p)^{1/p}$ , 
where  $D$  is the dimension of vector  $x$ , 
 Squared error:  $\mbox{SE}(A, \theta) =\sum_{n=1}^{N} \left \| y_n - f_{\theta}(x_n) \right \|^2_2$ , 
where  $A=\{(x_n, y_n)_{n=1}^{N}\}$  is a set of data points, and  $f_{\theta}(x_n)$  is model's estimation of  $y_n$ , 
 Mean squared error:  $\mbox{MSE}(A, \theta) =\mbox{SE}(A, \theta)/N$ ,    
 Least squares optimization:  $\theta^*=\mbox{argmin}_{\theta} \mbox{MSE}(A, \theta)$$=\mbox{argmin}_{\theta} \mbox{SE}(A, \theta)$ ,   
 Ridge loss:  $\mbox{R}(A, \theta, \lambda) = \mbox{MSE}(A, \theta) + \lambda\left \| \theta \right \|^2_2$ 
 Ridge optimization (regression):  $\theta^*=\mbox{argmin}_{\theta} \mbox{R}(A, \theta, \lambda)$ . 
 
 In all of the above examples,  $L_2$  norm can be replaced with  $L_1$  norm or  $L_\infty$  norm, etc.. However the names ""squared error"", ""least squares"",
 and ""Ridge"" are reserved for  $L_2$  norm. For example for  $L_1$ , ""squared error"" becomes ""absolute error"": 
 
 Absolute error:  $\mbox{AE}(A, \theta) =\sum_{n=1}^{N} \left \| y_n - f_{\theta}(x_n) \right \|_1$ ,","They are different: 
 L2 =  $\sqrt{\sum_{i=1}^{N}(y_i-y_{i}^{pred})^2}$ 
 MSE =  $\frac{\sum_{i=1}^{N}(y_i-y_{i}^{pred})^2}{N}$ 
 There are  sum  and  square root  for L2-Norm, but  sum  and  mean  for MSE! 
 We can check it by following code: 
 import numpy as np
from sklearn.metrics import mean_squared_error

y = np.array(range(10, 20))  # array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])
y_pred = np.array(range(10))  # array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
np.linalg.norm(y_pred - y, ord=2)  # L2-Nomr: 31.622776601683793
mean_squared_error(y_pred, y)  # MSE: 100.0","To be precise, L2 norm of the error vector is a  root  mean-squared error, 
up to a constant factor. Hence the  squared  L2-norm notation  $\|e\|^2_2$ , commonly found in loss functions. 
 However,  $L_p$ -norm losses should not be confused with regularizes. For instance, a combination of the L2 error with the L2 norm of the weights (both squared, of course) gives you a well known ridge regression loss, while a combination of L2 error + L1 norm of the weights gives rise to a Lasso regression.","Belter  is right, but, as observed by  Toonia , we can see that:
 $$L_2 = \sqrt{N \times MSE}= \sqrt{\sum_{i=1}^{N}(y_i-y_{i}^{pred})^2}$$","By the theory of Riemann integration,
 \begin{align*}
\int_a^b |f(x)-g(x)|^2dx &= \lim_{n \to \infty} \sum_{k=1}^n |f(x_k)-g(x_k)|^2 \Delta x\\
                    &= \lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n |f(x_k) - g(x_k)|^2  \\
                    & \approx \frac{1}{n} \sum_{k=1}^n |f(x_k) - g(x_k)|^2 
\end{align*} 
for  $n$  sufficiently large. You can recognize the LHS as originating from the  $L2$  norm while the RHS, MSE. If working on function spaces and point-wise evaluation of functions are considered then MSE essentially approximates squared  $L2$  norm, for the difference. MSE on the other hand, is the squared norm modulo the dimension in finite dimensions. i.e.,
 $$
||y - \hat{y}||_2^2 =  \sum_{k=1}^n |y_k - \hat{y}_k|^2\\
\text{MSE} = \frac{1}{n} ||\cdot||_2^2
$$ 
The difference, if there is one, is measure-theoretic.",an L2 optimization and MSE optimization are equivalent,"I think for computation purpose we are using L2 norms. Because if we use MSE we have to use ""for loop"" and this will take more computation. But, on the other hand, we can use N2 norms by using matrix and this saves more computation for any programing language considering if we have a huge data.
Overall, I think both are doing the same thing. Please correct me if I am wrong!",,,55.50114203,59.3983036,63.81147196,50,52.62536159,50,50,,
25180,Parameter Tuning by Cross Validation for Random Forest,scikit-learn,"Are you looking at the accuracy on your validation set, rather than your training set? (you should be). Are you making sure your gap between training and validation accuracy is low? (you should be!) Is there enough data to warrant 3-fold cross-validation, or should you do 10-fold (and use more data for training)?   
 In general, random search (where you sample randomly from the parameter space) will get you a good result faster than grid search.  It has the added benefit that you can specify how many models you want to build, as each model's parameters are sampled independently and not constrained to cover an entire space like grid search.","It may be that your coarse scale is too large (though it is a good idea to begin). If for example 150 depth would be the best solution but 500 is better than 100 then it will give you 500 as a parameter result.
Have you tried with shorter intervals between values for this parameter ?
What does it give if you try differently?","max_depth of 500 cannot be right, if I do not mistake it implies about 2^500 leafs, which is not feasible.  
 You probably have some bugs in your code. Post your code if possible.  
 Also, are you sure you don’t mix up max_depth and number of trees?","500 can be right. It does  not  imply 2^500 leafs, some leafs can stop earlier. How many observations do you have?  
 In random forest you could use the out-of-bag predictions for tuning. That would make your tuning algorithm faster. 
 Max_depth = 500 does not have to be too much. The default of random forest in R is to have the maximum depth of the trees, so that is ok. You should validate your final parameter settings via cross-validation (you then have a nested cross-validation), then you could see if there was some problem in the tuning process.","Running this now, and updating to import  GridSearchCV  from  model_selection , the code is: 
 import sklearn  
import sklearn.ensemble  
import sklearn.metrics  
from sklearn.datasets import fetch_20newsgroups  
# from sklearn.grid_search import GridSearchCV
from sklearn.model_selection import GridSearchCV

categories = ['sci.med', 'soc.religion.christian']  
newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, 
remove=('headers', 'footers', 'quotes'))  
newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, 
remove=('headers', 'footers', 'quotes'))  
class_names = ['medicine', 'christian']  

vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)  
train_vectors = vectorizer.fit_transform(newsgroups_train.data)  
test_vectors = vectorizer.transform(newsgroups_test.data)  

rf = sklearn.ensemble.RandomForestClassifier(max_features='sqrt')  

param_grid = {  
           ""n_estimators"" : [10, 100, 1000],  
           ""max_depth"" : [5, 100, 500],  
           ""min_samples_leaf"" : [1, 20, 40]}  

CV_rf = GridSearchCV(estimator=rf, param_grid=param_grid)  
CV_rf.fit(train_vectors, newsgroups_train.target)  
print(CV_rf.best_params_)  

 
 Which outputs: 
 {'max_depth': 100, 'min_samples_leaf': 1, 'n_estimators': 1000}",,,,,58.6345599,53.4395874,50,75.27852621,50,,,,
25053,Best practical algorithm for sentence similarity,nlp,"Cosine Similarity for Vector Space  could be you answer. 
 Or you could calculate the eigenvector of each sentences. But the Problem is, what is similarity? 
 ""This is a tree"",
""This is not a tree"" 
 If you want to check the semantic meaning of the sentence you will need a wordvector dataset. With the wordvector dataset you will able to check the relationship between words. Example: (King - Man + woman = Queen) 
 Siraj Raval has a good  python notebook  for creating wordvector datasets.","One approach you could try is averaging word vectors generated by word embedding algorithms (word2vec, glove, etc). These algorithms create a vector for each word and the cosine similarity among them represents semantic similarity among the words. In the case of the average vectors among the sentences. A good starting point for knowing more about these methods is this paper:  How Well Sentence Embeddings Capture Meaning . It discusses some sentence embedding methods. I also suggest you look into  Unsupervised Learning of Sentence Embeddings
using Compositional n-Gram Features  the authors claim their approach beat state of the art methods. Also they provide the code and some usage instructions in this  github repo .","bert-as-service  offers just that solution. 
 To answer your question, implementing it yourself from zero would be quite hard as BERT is not a trivial NN, but with this solution you can just plug it in into your algo that uses sentence similarity.","You should check out  this . fuzzywuzzy is an awesome library for string/text matching that gives a number between 0 to 100 based on how similar two sentences are. It uses Levenshtein Distance to calculate the differences between sequences in a simple-to-use package. Also, check out  this  blog post for a detailed explanation of how fuzzywuzzy does the job. This blog is also written by the fuzzywuzzy author",This blog  has the solution for short text similarity. They mainly use the BERT neural network model to find similarities between sentences.,"Depending on the representation of your sentences, you have different similarity metrics available. Some might be more suited to the representation you are using than others. 
 One of the most popular metrics is the  cosine distance . 
 However, you have other available in the literature, such as: 
 
 Jaccard similarity 
 Sørensen–Dice coefficient 
 Tversky index 
 
 You can experiment with these alternatives and see what works best for your use-case.",Python now has sent2vec library:  https://pypi.org/project/sent2vec/,Google has a model called  universal sentence encoder  using which you can find the embedding of the sentences. Doing cosine similarity between the embeddings can help in finding the similarity between sentences.,,55.35709199,58.03501525,62.60437849,53.33667625,57.52320975,56.5739131,50,60.99733799,
24637,Cleaning time series data,data-cleaning,"I think you have a few options: 
 
 If you have a pre-set rule to exclude outliers, such as a hard-threshold at 100 which you know the data shouldn't exceed, then something as simple as  x = [e for e in x if e < 100]  will do. 
 If you have a parametric belief, such as any observation that falls beyond so many standard deviations from mean, or quartiles, are outliers; then you can implement the other answers that have been mentioned. 
 Else, you can go for a clustering approach. Here I believe your first shot should be a k-means clustering. This is super easy to build and interpret. See my code below. 
 x = [0,14,0,6,102,0,0]
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=2).fit(np.array(x).reshape(-1, 1)) 
 #First cluster:
np.array(x)[np.where(kmeans.labels_ == 0)] 
 #Second cluster (outliers):
np.array(x)[np.where(kmeans.labels_ == 1)] 
 K-means is known to be sensitive to outliers, hence a more robust method such as MeanShift, which you tried, is a good rival to k-means. I would run both, and stick with the result that makes better sense to me. 
 
 Hope this helps!","One solution is using  mean  and  variance  to detect outlires in your time-series. For example: 
 >> data=np.array([0,0,102,6,0,14,0])
>> c = 1
>> abs(data - np.mean(data)) < c * np.std(data)
Output: array([ True,  True, False,  True,  True,  True,  True], dtype=bool)
>> clean_data= data[abs(data - np.mean(data)) < c * np.std(data)]
Output: array([ 0,  0,  6,  0, 14,  0])
 
 you can play with  c  based on your requirement. 
 Moreover, instead of using mean and variance of all the data, you can use this method for each section of your time-series separately (e.g. every 30 days). Because there might be different behavior in different time-intervals.","Here is what I am using: 
 import numpy as np
from sklearn.cluster import MeanShift, estimate_bandwidth

x = [0,14,0,6,102,0,0]

X = list(zip(x,np.zeros(len(x))))
bandwidth = estimate_bandwidth(X, quantile=0.2)
ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)
ms.fit(X)
labels = ms.labels_
cluster_centers = ms.cluster_centers_

labels_unique = np.unique(labels)
n_clusters_ = len(labels_unique)

X = np.array(X)
for k in range(n_clusters_):
    my_members = labels == k
    print(k, X[my_members, 0])
 
 Source:  http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html","I would use the Interquartile range ( $IQR$ ), where the outliers are the values larger than  $Q3+1.5 \times IQR$ , and the values less than  $Q1-1.5 \times IQR$ , where  $Q1$  and  $Q3$  are the first and third quartiles, respectively.  Here  is a good example.","Usually, everyone is trying to remove the outlier which is there in data. Instead, you can replace those outliers with Median or Mean, which can give you better results and trend analysis. 
 Some references:  Replacing outlier with median ,  Remove outlier from data frame 
 In my project, have replaced outliers with the median, and it gave better results.",,,,,50.40788636,62.34939707,50,52.72191442,50.95331479,,,,
24534,Does gradient descent always converge to an optimum?,machine-learning,"Gradient Descent is an algorithm which is designed to find the optimal points, but these optimal points are not necessarily global. And yes if it happens that it diverges from a local location it may converge to another optimal point but its probability is not too much. The reason is that the step size might be too large that prompts it recede one optimal point and the probability that it oscillates is much more than convergence. 
 About gradient descent there are two main perspectives, machine learning era and deep learning era. During machine learning era it was considered that gradient descent will find the local/global optimum but in deep learning era where the dimension of input features are too much it is shown in practice that the probability that all of the features be located in there optimal value at a single point is not too much and rather seeing to have optimal locations in cost functions, most of the time saddle points are observed. This is one of the reasons that training with lots of data and training epochs cause the deep learning models outperform other algorithms. So if you train your model, it will find a detour or will find its way to go downhill and do not stuck in saddle points, but you have to have appropriate step sizes. 
 For more intuitions I suggest you referring  here  and  here .","Asides from the points you mentioned (convergence to non-global minimums, and large step sizes possibly leading to non-convergent algorithms), ""inflection ranges"" might be a problem too. 
 Consider the following ""recliner chair"" type of function. 
 
 Obviously, this can be constructed so that there is a range in the middle where the gradient is the 0 vector. In this range, the algorithm can be stuck indefinitely.  Inflection points  are usually not considered local extrema.","Conjugate gradient is not guaranteed to reach a global optimum or a local optimum! There are points where the gradient is very small, that are not optima (inflection points, saddle points). Gradient Descent could converge to a point $x = 0$ for the function $f(x) = x^3$.","[Note 5 April 2019: A new version of the paper has been updated on arXiv with many new results. We introduce also backtracking versions of Momentum and NAG, and prove convergence under the same assumptions as for Backtracking Gradient Descent. 
 Source codes are available on GitHub at the  link . 
 We improved the algorithms for applying to DNN, and obtain better performance than state-of-the-art algorithms such as MMT, NAG, Adam, Adamax, Adagrad,... 
 The most special feature of our algorithms are that they are automatic, you do not need to do manual fine-tuning of learning rates as common practice. Our automatic fine-tuning is different in nature from Adam, Adamax, Adagrad,... and so on. More details are in the paper. 
 ] 
 Based on very recent results: In my joint work in  this paper 
 We showed that  backtracking  gradient descent, when applied to an  arbitrary C^1 function   $f$ , with only a  countable  number of critical points, will always either converge to a critical point or diverge to infinity. This condition is satisfied for a generic function, for example for all Morse functions. We also showed that in a sense it is very rare for the limit point to be a saddle point. So if all of your critical points are non-degenerate, then in a certain sense the limit points are all minimums. [Please see also references in the cited paper for the known results in  the case of the standard gradient descent.] 
 Based on the above, we proposed a new method in deep learning which is on par with current state-of-the-art methods and does not need manual fine-tuning of the learning rates. (In a  nutshell , the idea is that you run backtracking gradient descent a certain amount of time, until you see that the learning rates, which change with each iteration, become stabilise. We expect this stabilisation, in particular at a critical point which is C^2 and is non-degenerate, because of the convergence result I mentioned above. At that point, you switch to the standard gradient descent method. Please see the cited paper for more detail. This method can also be applied to other optimal algorithms.) 
 P.S. Regarding your original question about the standard gradient descent method, to my knowledge only in the case where the derivative of the map is globally Lipschitz and the learning rate is small enough that the standard gradient descent method is proven to converge. [If these conditions are not satisfied, there are simple  counter-examples  showing that no convergence result is possible, see the cited paper for some.] In the paper cited above, we argued that in the long run the backtracking gradient descent method will become the standard gradient descent method, which gives an explanation why the standard gradient descent method usually works well in practice. 
 [Addendum: 30 March 2021] Inspired by the comment by Dole (which I replied below), it may be worthy to mention that a recent variant of Backtracking GD arXiv:1911.04221 can be shown to, for a cost function which either has countably many critical points or satisfies the Lojasiewicz' gradient inequality, either diverge to infinity or converge to a Single Limit point which is not a saddle point (hence, roughly speaking, converges to a local minimum). 
 [Remark: 30 March 2021] In view of comments by Dole below, I would like to emphasise that the results mentioned here are for iterative optimisation algorithms (being practically useful), and not for flow methods (such as gradient flows). Also, the assumptions in the results here are very general, no (strong-) convexity or compact sub level or C^{1,1}_L and so on needed. When reading results in a book/paper, it is helpful to check the assumptions of the results to see whether they are practically useful or mostly cliche.","Gradient Descent need not always converge at global minimum. 
 It all depends on following conditions; 
 
 The function must be convex function.
What is convex function? 
 
 If the line segment between any two points on the graph of the function lies above or on the graph then it is convex function. 
 example is given below: 
 
 
 Less Learning rate(alpha), which means the step size
The alpha must be less and must change according the gradient.
 
 
 If the alpha is high the step oscillates and global minimum is not guaranteed. 
 
 
 Global minimum is not guaranteed with concave function.
We will see by an example 
 
 
 In the above example if we take initial point at local maximum it can converge towards local minimum or global minimum. So, It does not guarantee glabal minimum",,,,,63.23418627,57.90577201,76.28349655,69.66171212,58.26741678,,,,
24511,Why should the data be shuffled for machine learning tasks,machine-learning,"Based on  What should we do when a question posted on DataScience is a duplicate of a question posted on CrossValidated? , I am reposting my answer to the same question asked on CrossValidated ( https://stats.stackexchange.com/a/311318/89653 ). 
 Note: throughout this answer I refer to minimization of training loss and I do not discuss stopping criteria such as validation loss. The choice of stopping criteria does not affect the process/concepts described below. 
 The process of training a neural network is to find the minimum value of a loss function $ℒ_X(W)$, where $W$ represents a matrix (or several matrices) of weights between neurons and $X$ represents the training dataset. I use a subscript for $X$ to indicate that our minimization of $ℒ$ occurs only over the weights $W$ (that is, we are looking for $W$ such that $ℒ$ is minimized) while $X$ is fixed. 
 Now, if we assume that we have $P$ elements in $W$ (that is, there are $P$ weights in the network), $ℒ$ is a surface in a $P+1$-dimensional space. To give a visual analogue, imagine that we have only two neuron weights ($P=2$). Then $ℒ$ has an easy geometric interpretation: it is a surface in a 3-dimensional space. This arises from the fact that for any given matrices of weights $W$, the loss function can be evaluated on $X$ and that value becomes the elevation of the surface. 
 But there is the problem of non-convexity; the surface I described will have numerous local minima, and therefore gradient descent algorithms are susceptible to becoming ""stuck"" in those minima while a deeper/lower/better solution may lie nearby. This is likely to occur if $X$ is unchanged over all training iterations, because the surface is fixed for a given $X$; all its features are static, including its various minima. 
 A solution to this is mini-batch training combined with shuffling. By shuffling the rows and training on only a subset of them during a given iteration, $X$ changes with  every  iteration, and it is actually quite possible that no two iterations over the entire sequence of training iterations and epochs will be performed on the exact same $X$. The effect is that the solver can easily ""bounce"" out of a local minimum. Imagine that the solver is stuck in a local minimum at iteration $i$ with training mini-batch $X_i$. This local minimum corresponds to $ℒ$ evaluated at a particular value of weights; we'll call it $ℒ_{X_i}(W_i)$. On the next iteration the shape of our loss surface actually changes because we are using $X_{i+1}$, that is, $ℒ_{X_{i+1}}(W_i)$ may take on a very different value from $ℒ_{X_i}(W_i)$ and it is quite possible that it does not correspond to a local minimum! We can now compute a gradient update and continue with training. To be clear: the shape of $ℒ_{X_{i+1}}$ will -- in general -- be different from that of $ℒ_{X_{i}}$. Note that here I am referring to the loss function $ℒ$ evaluated on a training set $X$; it is a complete surface defined over all possible values of $W$, rather than the evaluation of that loss (which is just a scalar) for a specific value of $W$. Note also that if mini-batches are used without shuffling there is still a degree of ""diversification"" of loss surfaces, but there will be a finite (and relatively small) number of unique error surfaces seen by the solver (specifically, it will see the same exact set of mini-batches -- and therefore loss surfaces -- during each epoch). 
 One thing I deliberately avoided was a discussion of mini-batch sizes, because there are a million opinions on this and it has significant practical implications (greater parallelization can be achieved with larger batches). However, I believe the following is worth mentioning. Because $ℒ$ is evaluated by computing a value for each row of $X$ (and summing or taking the average; i.e., a commutative operator) for a given set of weight matrices $W$, the arrangement of the rows of $X$  has no effect  when using full-batch gradient descent (that is, when each batch is the full $X$, and iterations and epochs are the same thing).","Shuffling data serves the purpose of reducing variance and making sure that models remain general and overfit less. 
 The obvious case where you'd shuffle your data is if your data is sorted by their class/target. Here, you will want to shuffle to make sure that your training/test/validation sets are representative of the overall distribution of the data. 
 For batch gradient descent, the same logic applies. The idea behind batch gradient descent is that by calculating the gradient on a single batch, you will usually get a fairly good estimate of the ""true"" gradient. That way, you save computation time by not having to calculate the ""true"" gradient over the entire dataset every time. 
 You want to shuffle your data after each epoch because you will always have the risk to create batches that are not representative of the overall dataset, and therefore, your estimate of the gradient will be off. Shuffling your data after each epoch ensures that you will not be ""stuck"" with too many bad batches. 
 In regular stochastic gradient descent, when each batch has size 1, you still want to shuffle your data after each epoch to keep your learning general. Indeed, if data point 17 is always used after data point 16, its own gradient will be biased with whatever updates data point 16 is making on the model. By shuffling your data, you ensure that each data point creates an ""independent"" change on the model, without being biased by the same points before them.","Suppose data is sorted in a specified order. For example a data set which is sorted base on their class. So, if you select data for training, validation, and test without considering this subject, you will select each class for different tasks, and it will fail the process.  
 Hence, to impede these kind of problems, a simple solution is shuffling the data to get different sets of training, validation, and test data. 
 About the mini-batch, answers to  this post  can be a solution to your question.","Because  $ℒ$  is evaluated by computing a value for each row of  $X$  (and summing or taking the average; i.e., a commutative operator) for a given set of weight matrices  $W$ , the arrangement of the rows of  $X$  has no effect when using full-batch gradient descent 
 
 Complementing @Josh's answer, I would like to add that, for the same reason, shuffling needs to be done before batching. Otherwise, you are getting the same finite number of surfaces.","We need to shuffle only for minibatch/SGD, no need for batch gradient descent. 
 If not shuffling data, the data can be sorted or similar data points will lie next to each other, which leads to slow convergence: 
 
 Similar samples will produce similar surfaces (1 surface for the loss function for 1 sample) -> gradient will points to similar directions but this direction rarely points to the minimum-> it may drive the gradient very far from the minimum 
 “Best direction”: the average of all gradient of all surfaces (batch gradient descent) which points directly to the minum 
 “Minibatch direction”: average of a variety of directions will point closer to the minimum, although non of them points to the minimum 
 “1-sample direction”: point farther to the minimum compared to the minibatch 
 
 I drew the plot of the L-2 loss function for linear regression for  y=2x   here","When we train a machine learning model, we give it a lot of data to learn from. If we give the data to the model in the same order every time, the model may end up memorizing that order instead of learning the general patterns in the data. This means that the model may be good at predicting things in that specific order, but not so good at predicting things in other orders or on new data. 
 Shuffling the data means that we randomize the order in which the data is presented to the model during training. This helps the model to learn the general patterns in the data, rather than memorizing the specific order. This can improve the model's accuracy and ability to predict things on new data. 
 So shuffling data is just a way to make sure that the model learns the general patterns in the data, and not just the order in which it was presented.","For best accuracy of the model, it's always recommended that training data should have all flavours of data. 
 Shuffling of training data helps us in achieving this target.","By shuffling the rows and training on only a subset of them during a given iteration, 𝑋 changes with every iteration, and it is actually quite possible that no two iterations over the entire sequence of training iterations and epochs will be performed on the exact same 𝑋","Since the SGD algorithm selects the subset of instances randomly, it is quite possible that it may take few instances many numbers of time per epoch, which may bring the cost function to a global minima. 
 If training instances are shuffled then the chances of selecting repeating instances is much less. 
 Source of inforamation: handson machine learning with scikit learn keras and tensorflow.",52.15328377,59.94023753,55.98201408,53.04171941,54.17986641,62.41590736,66.16153415,53.36092129,56.33945123
24452,"In supervised learning, why is it bad to have correlated features?",machine-learning,"Correlated features in general don't improve models (although it depends on the specifics of the problem like the number of variables and the degree of correlation), but they affect specific models in different ways and to varying extents: 
 
 For linear models (e.g., linear regression or logistic regression),  multicolinearity  can yield  solutions that are wildly varying and possibly numerically unstable . 
 Random forests can be good at detecting interactions between different features, but highly correlated features can mask these interactions. 
 
 More generally, this can be viewed as a special case of  Occam's razor . A simpler model is preferable, and, in some sense, a model with fewer features is simpler. The concept of  minimum description length  makes this more precise.","(Assuming you are talking about supervised learning) 
 Correlated features will not always worsen your model, but they will not always improve it either. 
 There are three main reasons why you would remove correlated features: 
 
 Make the learning algorithm faster 
 
 Due to the curse of dimensionality, less features usually mean high improvement in terms of speed. 
 If speed is not an issue, perhaps don't remove these features right away (see next point) 
 
 Decrease harmful bias 
 
 The keyword being harmful. If you have correlated features but they are also correlated to the target, you want to keep them. You can view features as hints to make a good guess, if you have two hints that are essentially the same, but they are good hints, it may be wise to keep them. 
 Some algorithms like Naive Bayes actually directly benefit from ""positive"" correlated features. And others like random forest may indirectly benefit from them. 
 Imagine having 3 features A, B, and C. A and B are highly correlated to the target and to each other, and C isn't at all. If you sample out of the 3 features, you have 2/3 chance to get a ""good"" feature, whereas if you remove B for instance, this chance drops to 1/2 
 Of course, if the features that are correlated are not super informative in the first place, the algorithm may not suffer much. 
 So moral of the story, removing these features might be necessary due to speed, but remember that you might make your algorithm worse in the process. Also, some algorithms like decision trees have feature selection embedded in them. 
 A good way to deal with this is to use a wrapper method for feature selection. It will remove redundant features only if they do not contribute directly to the performance. If they are useful like in naive bayes, they will be kept. (Though remember that wrapper methods are expensive and may lead to overfitting) 
 
 Interpretability of your model 
 
 If your model needs to be interpretable, you might be forced to make it simpler. Make sure to also remember Occam's razor. If your model is not ""that much"" worse with less features, then you should probably use less features.","Why is Multicollinearity a Potential Problem? 
 A key goal of regression analysis is to isolate the relationship between each independent variable and the dependent variable. The interpretation of a regression coefficient is that it represents the mean change in the dependent variable for each 1 unit change in an independent variable when you hold all of the other independent variables constant. That last portion is crucial for our further discussion about multicollinearity. 
 The idea is that you can change the value of one independent variable and not the others. However, when independent variables are correlated, it indicates that changes in one variable are associated with shifts in another variable. The stronger the correlation, the more difficult it is to change one variable without changing another. It becomes difficult for the model to estimate the relationship between each independent variable and the dependent variable independently because the independent variables tend to change in unison. 
 What Problems Do Multicollinearity Cause? 
 Multicollinearity causes the following two basic types of problems: 
 
 The coefficient estimates can swing wildly based on which other 
independent variables are in the model. The coefficients become very 
sensitive to small changes in the model. 
 Multicollinearity reduces the precision of the estimate coefficients,
which weakens the statistical power of your regression model. You
might not be able to trust the p-values to identify independent
variables that are statistically significant. 
 
 Imagine you fit a regression model and the coefficient values, and even the signs, change dramatically depending on the specific variables that you include in the model. It’s a disconcerting feeling when slightly different models lead to very different conclusions. You don’t feel like you know the actual effect of each variable! 
 Now, throw in the fact that you can’t necessarily trust the p-values to select the independent variables to include in the model. This problem makes it difficult both to specify the correct model and to justify the model if many of your p-values are not statistically significant. 
 As the severity of the multicollinearity increases so do these problematic effects.  However, these issues affect only those independent variables that are correlated. You can have a model with severe multicollinearity and yet some variables in the model can be completely unaffected. 
 Do I Have to Fix Multicollinearity? 
 Multicollinearity makes it hard to interpret your coefficients, and it reduces the power of your model to identify independent variables that are statistically significant. These are definitely serious problems. However, the good news is that you don’t always have to find a way to fix multicollinearity!  
 The need to reduce multicollinearity depends on its severity and your primary goal for your regression model. Keep the following three points in mind: 
 
 The severity of the problems increases with the degree of the
multicollinearity. Therefore, if you have only moderate
multicollinearity, you may not need to resolve it. 
 Multicollinearity affects only the specific independent variables
that are correlated. Therefore, if multicollinearity is not present
for the independent variables that you are particularly interested
in, you may not need to resolve it. Suppose your model contains the
experimental variables of interest and some control variables. If
high multicollinearity exists for the control variables but not the
experimental variables, then you can interpret the experimental
variables without problems. 
 Multicollinearity affects the coefficients and p-values, but it does
not influence the predictions, precision of the predictions, and the
goodness-of-fit statistics . If your primary goal is to make
predictions, and you don’t need to understand the role of each
independent variable, you don’t need to reduce severe
multicollinearity. (Reference: ""The fact that some or all predictor
variables are correlated among themselves does not, in general,
inhibit our ability to obtain a good fit nor does it tend to affect
inferences about mean responses or predictions of new observations.""
— Applied Linear Statistical Models, p289, 4th Edition.) 
 
 Source:  Multicollinearity in Regression Analysis: Problems, Detection, and Solutions by Jim Frost","In perspective of storing data in databases, storing correlated features is somehow similar to storing redundant information which it may cause wasting of storage and also it may cause inconsistent data after updating or editing tuples.  
 If we add so much correlated features to the model we may cause the model to consider unnecessary features and we may have curse of high dimensionality problem ,  I guess this is the reason for worsening the constructed model. 
 In the context of machine learning we usually use  PCA  to reduce the dimension of input patterns. This approach considers removing correlated features by someway (using  SVD ) and is an unsupervised approach. This is done to achieve the following purposes: 
 
 Compression 
 Speeding up learning algorithms 
 Visualizing data 
 Dealing with curse of high dimensionality 
 
 Although this may not seem okay but I have seen people that use removing correlated features in order to avoid overfitting but I don't think it is a good practice. For more information I highly recommend you to see  here . 
 Another reason is that in deep learning models, like  MLPs  if you add correlated features, you just add unnecessary information which adds more calculations and parameters to the model.","Sometimes correlated features -- and the duplication of information that provides -- does not hurt a predictive system. Consider an ensemble of decision trees, each of which considers a sample of rows and a sample of columns. If two columns are highly correlated, there's a chance that one of them won't be selected in a particular tree's column sample, and that tree will depend on the remaining column. Correlated features mean you can reduce overfitting (through column sampling) without giving up too much predictive quality.","Making a decision should be done on the minimum necessary variables to do so.  This is, as mentioned above, the formalization of Occam's razor with minimum description length above.  I like that one.   
 I would tend to characterize this phenomena in something like a  HDDT  to mean the most efficient tree that makes no spurious decision based on available data, and avoiding all instances of decisions that may otherwise have been made on multiple data points without understanding that they were correlated.","The answer to this question depends greatly upon the purpose of the model. In inference, highly correlated features are a well-known problem. For example, two features highly correlated with each other and with y, might both come out as insignificant in an inference model, potentially missing an important explanatory signal. Therefore, in inference it is generally recommended to thin them out. 
 If your supervised learning is for prediction, the answer - counter to conventional wisdom - is usually the opposite. The only reason to remove highly correlated features is storage and speed concerns. Other than that, what matters about features is whether they contribute to prediction, and whether their data quality is sufficient.  
 Noise-dominated features will tend to be less correlated with other features, than features correlated with y. Hence, as mentioned above in the example by Valentin, thinning out the latter will increase the proportion of the former. 
 In particular, methods like random forests and KNN treat all features equally, so thinning out correlated features directly reduces their signal-to-noise ratio. 
 Methods that auto-select features like single trees, ""pure"" lasso, or neural networks, might be less affected. But even then, other than longer computing time, there is rarely anything to lose prediction-wise from keeping correlated features in the mix.",,,63.69518214,69.26097585,51.73215492,68.50854684,64.26907664,52.49154662,73.65927645,,
24403,Prediction interval around LSTM time series forecast,machine-learning,"Directly, this is not possible. However, if you model it in a different way you can get out confidence intervals. You could instead of a normal regression approach it as estimating a continuous probability distribution. By doing this for every step you can plot your distribution. Ways to do this are Kernel Mixture Networks ( https://janvdvegt.github.io/2017/06/07/Kernel-Mixture-Networks.html , disclosure, my blog) or Density Mixture Networks ( http://www.cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.7-MixDensityNetworks.pdf ), the first uses kernels as base and estimates a mixture over these Kernels and the second one estimates a mixture of distributions, including the parameters of each of the distributions. You use the log likelihood for training the model. 
 Another option for modeling the uncertainty is to use dropout during training and then also during inference. You do this multiple times and every time you get a sample from your posterior. You don't get distributions, only samples, but it's the easiest to implement and it works very well. 
 In your case you have to think about the way you generate t+2 up to t+10. Depending on your current setup you might have to sample from the previous time step and feed that for the next one. That doesn't work very well with the first approach, nor with the second. If you have 10 outputs per time step (t+1 up to t+10) then all of these approaches are more clean but a bit less intuitive.","Conformal Prediction  as a buzz word might be interesting for you because it works under many conditions - in particular it does not need normal distributed error and it works for almost any machine learning model.  
 Two nice introductions are given by 
 Scott Locklin  and  Henrik Linusson .","I am going to diverge a little bit and argue that calculation confidence interval is in practice is usually not a valuable thing to do. The reason is there is always a whole bunch of assumptions you need to make. Even for the simplest linear regression, you need to have 
 
 Linear relationship.  
 Multivariate normality.  
 No or little multicollinearity.  
 No auto-correlation.  
 Homoscedasticity. 
 
 A much more pragmatic approach is to do a Monte Carlo simulation. If you already know or willing to make of assumption around the distribution of your input variables, take a whole bunch of sample and feed it to you LSTM, now you can empirically calculate your ""confidence interval"".","Yes, you can. The only thing you need to change is the loss function. Implement the loss function used in quantile regression and integrate it. Also, you want to take a look at how you evaluate those intervals. For that, I would use ICP, MIL and RMIL metrics.","You can easily quantify uncertainty for time series models using conformal prediction 
 https://github.com/valeman/awesome-conformal-prediction 
 Here is an article about doing it with classical models 
 https://medium.com/@valeman/conformal-prediction-forecasting-with-nixtlas-statsforecast-cc39b9e30b36 
 But you can also do it easily using Nixtla's ecosystem for machine and deep learning models  https://github.com/Nixtla/",,,,,53.26125289,51.4259294,57.53198187,54.07717788,57.88248325,,,,
24319,Are there free cloud services to train machine learning models?,machine-learning,"There are no  unlimited  free services*, but some have starting credit or free offers on initial signup. Here are some suggested to date: 
 
 AWS: If specifically deep learning on a large data set, then probably AWS is out - their free offer does not cover machines with enough processing power to tackle deep learning projects.  
 Google Cloud might do, the starting credit offer is good enough to do a little deep learning (for maybe a couple of weeks), although they have signup and tax restrictions.  
 Azure have a free tier with limited processing and storage options. 
 
 Most free offerings appear to follow the ""Freemium"" model - give you limited service that you can learn to use and maybe like. However not enough to use heavily (for e.g. training an image recogniser or NLP model from scratch) unless you are willing to pay. 
 This best advice is to shop around for a best starting offer and best price. A review of services is not suitable here, as it will get out of date quickly and not a good use of Stack Exchange. But you can find  similar questions on Quora  and other sites - your best bet is to do a web search for ""cloud compute services for deep learning"" or similar and expect to spend some time comparing notes. A few specialist deep learning services have popped up recently such as  Nimbix  or  FloydHub , and there are also the big players such as Azure, AWS, Google Cloud. 
 You won't find anything completely free and unencumbered, and if you want to do this routinely and have time to build and maintain hardware then it is cheaper to buy your own equipment in the long run - at least at a personal level. 
 To decide whether to pay for cloud or build your own, then consider a typical price for a cloud machine suitable for performing deep learning at around \$1 per hour (prices do vary a lot though, and it is worth shopping around, if only to find a spec that matches your problem). There may be additional fees for storage and data transfer. Compare that to pre-built deep learning machines costing from \$2000, or  building your own for \$1000  - such machines might not be 100% comparable, but if you are working by yourself then the payback point is going to be after only a few months use. Although don't forget the electricity costs - a powerful machine can draw 0.5kW whilst being heavily used, so this adds up to more than you might expect. 
 The advantages of cloud computing are that someone else does the maintenance work and takes on the risk of hardware failure. These are valuable services, and priced accordingly.  
 
 * But see  Jay Speidall's answer  about Google's colab service, which appears to be free to use, but may have some T&C limitations which may affect you (for instance I doubt they will be happy for you to run content production of Deep Dream or Style Transfer on it)","I want to add one more resource,  Google Colaboratory . It's a free cloud iPython notebook and gives you free usage of a GPU . I'm not sure of the exact limitations just yet, but it appears you get 12 hours of GPU time per instance and can do this multiple times per month.  
 This looks like a great resource for students and other non-professionals, especially for smaller jobs that you can run in half a day. It essentially saves you up to $10 per training session, which is a pretty significant resource for machine learning research in my opinion. I seriously hope it doesn't get abused.","Check out  Crestle .(Free one hour GPU compute time) 
 Google's colab 
 Floyd-hub 
 Paperspace (not free but it's easy to use referrals and earn 15$ compute time..) 
 Azure ($200) 
 deepcognition.ai  (2 hours on different GPU enabled machines) 
 IBM Cloud 
 
 For a more updated list you can star this github repo:
  Cloud GPUs","Yes, with limitations. Google Cloud Compute gives you 300 dollars worth of free credit signing up, and Microsoft Azure gives you 200 dollars (but their GPU time is a bit cheaper, so it's almost the same).  
 This gets you a lot of GPU time, and will get you started while you weigh your options.","Microsoft's  Azure Machine Learning Studio  has an ""always free"" tier, subject to  certain limitations , including 
 
 100 modules per experiment (a ""module"" in Azure-speak is any discrete operation such as ""load data"" or ""train model"", so you can do quite a bit with 100 of them) 
 10Gb of storage 
 1 hour per experiment 
 No parallel execution on multiple nodes 
 
 Training will continue to run while you are not connected, to answer your second question. You can set up your experiments via the web interface or  on the command line .",It seems that  Intel  lets users use its AI  DevCloud  for free for thirty days I guess.  Here  is the instructions.,,,,66.90226675,58.34447032,60.1610849,54.70422432,56.35046865,53.96554751,,,
24093,How to clone Python working environment on another machine?,python,"First of all this is a Python/Anaconda question and should probably be asked in a different stack exchange subsite. 
 
 As for the question itself - you can export your Anaconda environment using: 
 conda env export > environment.yml
 
 And recreate it using: 
 conda env create -f environment.yml
 
 Please note that as others suggested - you should use virtual environments which allows you to create a certain environment that is separated from that of your machine and manage it more easily. 
 To create a virtual environment in Anaconda you can use: 
 conda create -n yourenvname python=x.x anaconda
 
 which you activate using: 
 source activate yourenvname","TLDR 
 Use Conda Pack 
install using conda or pip:    
 conda-forge:   
 conda install -c conda-forge conda-pack
 
 PyPI:   
 pip install conda-pack
 
 Then for:   
 Backing up:   
 # Pack environment my_env into my_env.tar.gz
$ conda pack -n my_env

# Pack environment my_env into out_name.tar.gz
$ conda pack -n my_env -o out_name.tar.gz

# Pack environment located at an explicit path into my_env.tar.gz
$ conda pack -p /explicit/path/to/my_env
 
 Restoring:   
 And to restore it on the other machine(s): 
 # Unpack environment into directory `my_env`
 $ mkdir -p my_env
$  tar -xzf my_env.tar.gz -C my_env

# Use Python without activating or fixing the prefixes. Most Python
# libraries will work fine, but things that require prefix cleanups
# will fail.
$ ./my_env/bin/python

# Activate the environment. This adds `my_env/bin` to your path
$ source my_env/bin/activate

# Run Python from in the environment
(my_env) $ python

# Cleanup prefixes from in the active environment.
# Note that this command can also be run without activating the environment
# as long as some version of Python is already installed on the machine.
(my_env) $ conda-unpack
 
 a bit of an explanation :  
 If you plan on getting an exact copy of your current environment and then move it to another machine with the same platform and OS, without redownloading all packages again from Internet (good for offline machines/behind firewalls). All other previous methods require internet connection. so in case you dont have access to internet, then you can use  conda pack .  
 Conda Pack 
 
 Conda-pack is a command line tool that archives a conda environment,
  which includes all the binaries of the packages installed in the
  environment. This is useful when you want to reproduce an environment
  with limited or no internet access. All the previous methods download
  packages from their respective repositories to create an environment.
  Keep in mind that conda-pack is both platform and operating system
  specific and that the target computer must have the same platform and
  OS as the source computer. 
 To install conda-pack, make sure you are in the root or base
  environment so that it is available in sub-environments. Conda-pack is
  available at conda-forge or PyPI. 
 
 for future updates check the  ref","First export environment configuration of your current conda environment using: 
 conda-env  export -n your_env_name > your_env_name.yml
 
 example: 
 conda-env  export -n base> base.yml
 
 After running above command their should be yml configuration file in your current directory which contain information of your conda environment 
 To create new environment using yml configuration file run: 
 conda-env create -n new_env -f=\path\to\base.yml 
 
 example: 
 conda-env create -n venv -f=base.yml
 
 In case the above one does not work (due to the various issues of conda itself), it's always worth a try with the following variation: 
 conda-env create --name new_env --file \path\to\base.yml","If your program is mostly Python, you could rely solely on virtual environments. 
 Create virtual environments to isolate your dependencies rather than using the system libraries. Then use virtual environment tools to duplicate your environments. 
 In the working virtualenv, create a file with the version of each installed Python library : 
 pip freeze > requirements.txt
 
 In the new virtualenv, ask  pip  to install those libraries with the same version : 
 pip install -r requirements.txt
 
 This makes sure you get the same lib versions on both machines. And since requirements.txt is tracked by your VCS, you can always recreate the environment of an old version of your code. 
 Of course, if you need a database, a production web server, etc. you end up with a few more steps and you can't rely on virtualenv to ensure both environments match. This is where Docker steps in (See  Pieter21's answer ).","Look into 'containers', e.g. Docker ( https://www.docker.com/what-container ), a more lightweight alternative to virtualization. 
 It will require some time investment but in the end will provide many benefits. 
 From the link, where I marked your specific need in  bold italic : 
 Package software into standardized units for development, shipment and deployment 
 A container image is a lightweight, stand-alone, executable package of a piece of software that includes everything needed to run it: code, runtime, system tools, system libraries, settings. Available for both Linux and Windows based apps,  containerized software will always run the same, regardless of the environment . Containers isolate software from its surroundings, for example differences between development and staging environments and help reduce conflicts between teams running different software on the same infrastructure.","A wrap up of the existing ways to create an environment based on another one: 
 
 Cloning an environment : 
 
 From an existing environment: 
 $ conda create --name NEW_ENV_NAME --clone ORIG_ENV_NAME 
 
 From an exported environment file on the same machine: 
  $ conda create --name ENV_NAME —-file FILE_NAME.yml 
 
 From an exported environment file on a different machine: 
 
 
 $ conda env export > ENV_NAME.yml
$ conda env create -f ENV_NAME.yml","From the very end of  this documentation page : 
 Save packages for future use: 
 conda list --export > package-list.txt
 
 Reinstall packages from an export file: 
 conda create -n myenv --file package-list.txt","One-liner: 
 conda create --clone source_env --name destination_env","to clone a miniconda/anaconda environment for Windows users: 
 
 install miniconda/anaconda with  ""all users""  option. if you install with your current user, the path will be set during the compile of the exe files and you are lost 
 
 using conda-pack is not bad but for me zipping the complete ~/<anaconda/miniconda> path and unzipping it on the target machine was working properly too. 
 
 
 
 btw. do not forget to copy the dlls libcrypto-xxx.dll and libssl-xxx.dll from the folder ~/Library/bin to the folder ~/DLLs",56.50513293,55.50168659,51.9277706,56.10343855,52.00049248,56.51119206,50,55.61503999,54.73939625
23969,Sentence similarity prediction,python,"Your problem can be solved with Word2vec as well as Doc2vec. Doc2vec would give better results because it takes sentences into account while training the model.   
 Doc2vec solution 
You can train your doc2vec model following this  link . You may want to perform some pre-processing steps like removing all  stop words  (words like ""the"", ""an"", etc. that don't add much meaning to the sentence). Once you trained your model, you can find the similar sentences using following code.   
 import gensim  

model = gensim.models.Doc2Vec.load('saved_doc2vec_model')  

new_sentence = ""I opened a new mailbox"".split("" "")  
model.docvecs.most_similar(positive=[model.infer_vector(new_sentence)],topn=5)
 
 Results: 
 [('TRAIN_29670', 0.6352514028549194),
 ('TRAIN_678', 0.6344441771507263),
 ('TRAIN_12792', 0.6202734708786011),
 ('TRAIN_12062', 0.6163255572319031),
 ('TRAIN_9710', 0.6056315898895264)]
 
 The above results are list of tuples for  (label,cosine_similarity_score) . You can map outputs to sentences by doing  train[29670] .   
 Please note that the above approach will only give good results if your doc2vec model contains embeddings for words found in the new sentence. If you try to get similarity for some gibberish sentence like  sdsf sdf f sdf sdfsdffg , it will give you few results, but those might not be the actual similar sentences as your trained model may haven't seen these gibberish words while training the model. So try to train your model on as many sentences as possible to incorporate as many words for better results.  
 Word2vec Solution 
If you are using word2vec, you need to calculate the average vector for all words in every sentence and use cosine similarity between vectors.   
 def avg_sentence_vector(words, model, num_features, index2word_set):
    #function to average all words vectors in a given paragraph
    featureVec = np.zeros((num_features,), dtype=""float32"")
    nwords = 0

    for word in words:
        if word in index2word_set:
            nwords = nwords+1
            featureVec = np.add(featureVec, model[word])

    if nwords>0:
        featureVec = np.divide(featureVec, nwords)
    return featureVec
 
 Calculate Similarity  
 from sklearn.metrics.pairwise import cosine_similarity

#get average vector for sentence 1
sentence_1 = ""this is sentence number one""
sentence_1_avg_vector = avg_sentence_vector(sentence_1.split(), model=word2vec_model, num_features=100)

#get average vector for sentence 2
sentence_2 = ""this is sentence number two""
sentence_2_avg_vector = avg_sentence_vector(sentence_2.split(), model=word2vec_model, num_features=100)

sen1_sen2_similarity =  cosine_similarity(sentence_1_avg_vector,sentence_2_avg_vector)","Word Mover’s Distance (WMD)  is an algorithm for finding the distance between sentences. WMD is based on word embeddings (e.g., word2vec) which encode the semantic meaning of words into dense vectors. 
 
 The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to ""travel"" to reach the embedded words of another document. 
 
 For example: 
 
Source:  ""From Word Embeddings To Document Distances"" Paper 
 The  gensim package  has a  WMD implementation . 
 For your problem, you would compare the inputted sentence to all other sentences and return the sentence that has lowest WMD.","You can try an easy solution using  sklearn  and it's going to work fine. 
 
 Use  tfidfvectorizer  to get a vector representation of each text 
 Fit  the vectorizer with your data, removing  stop-words. 
 Transform  the new entry with the vectorizer previously trained 
 Compute the  cosine similarity  between this representation and each representation of the elements in your data set. 
 
 If you have a hugh dataset you can cluster it (for example using KMeans from scikit learn) after obtaining the representation, and before predicting on new data.  
 This code perform all these steps. You can check it on my github  repo . 
 from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
import numpy

texts = [""This first text talks about houses and dogs"",
        ""This is about airplanes and airlines"",
        ""This is about dogs and houses too, but also about trees"",
        ""Trees and dogs are main characters in this story"",
        ""This story is about batman and superman fighting each other"", 
        ""Nothing better than another story talking about airplanes, airlines and birds"",
        ""Superman defeats batman in the last round""]

# vectorization of the texts
vectorizer = TfidfVectorizer(stop_words=""english"")
X = vectorizer.fit_transform(texts)
# used words (axis in our multi-dimensional space)
words = vectorizer.get_feature_names()
print(""words"", words)


n_clusters=3
number_of_seeds_to_try=10
max_iter = 300
number_of_process=2 # seads are distributed
model = KMeans(n_clusters=n_clusters, max_iter=max_iter, n_init=number_of_seeds_to_try, n_jobs=number_of_process).fit(X)

labels = model.labels_
# indices of preferible words in each cluster
ordered_words = model.cluster_centers_.argsort()[:, ::-1]

print(""centers:"", model.cluster_centers_)
print(""labels"", labels)
print(""intertia:"", model.inertia_)

texts_per_cluster = numpy.zeros(n_clusters)
for i_cluster in range(n_clusters):
    for label in labels:
        if label==i_cluster:
            texts_per_cluster[i_cluster] +=1 

print(""Top words per cluster:"")
for i_cluster in range(n_clusters):
    print(""Cluster:"", i_cluster, ""texts:"", int(texts_per_cluster[i_cluster])),
    for term in ordered_words[i_cluster, :10]:
        print(""\t""+words[term])

print(""\n"")
print(""Prediction"")

text_to_predict = ""Why batman was defeated  by superman so easy?""
Y = vectorizer.transform([text_to_predict])
predicted_cluster = model.predict(Y)[0]
texts_per_cluster[predicted_cluster]+=1

print(text_to_predict)
print(""Cluster:"", predicted_cluster, ""texts:"", int(texts_per_cluster[predicted_cluster])),
for term in ordered_words[predicted_cluster, :10]:
print(""\t""+words[term])","There is some recent work based on Variational Auto-Encoder in RNN models. Generating Sentences from a Continuous Space , with pytorch implementations:  github code . 
they managed to compress the semantic, syntactic global feature of a sentence into some latent space expressed maybe with some finite 10 to 30 independent random variables (factorized distribution). 
the novel idea in this work, they interpolate between two sentences. and the results were quite amazing.","One can use this Python 3 library to compute sentence similarity:  https://github.com/UKPLab/sentence-transformers 
 Code example from  https://www.sbert.net/docs/usage/semantic_textual_similarity.html : 
 # pip install -U sentence-transformers
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('paraphrase-MiniLM-L12-v2')

# Two lists of sentences
sentences1 = ['The cat sits outside',
             'A man is playing guitar',
             'The new movie is awesome']

sentences2 = ['The dog plays in the garden',
              'A woman watches TV',
              'The new movie is so great']

#Compute embedding for both lists
embeddings1 = model.encode(sentences1, convert_to_tensor=True)
embeddings2 = model.encode(sentences2, convert_to_tensor=True)

#Compute cosine-similarits
cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)

#Output the pairs with their score
for i in range(len(sentences1)):
    print(""{} \t\t {} \t\t Score: {:.4f}"".format(sentences1[i], sentences2[i], cosine_scores[i][i]))
 
 The library contains the state-of-the-art sentence embedding models. 
 See  https://stackoverflow.com/a/68728666/395857  to perform sentence clustering.","The generalized solution consists of the following steps - 
 
 Featurization or word embeddings of a sentence.  
 Applying a similarity metric among sentences.  
 
 For 1. word2vec is the best choice but if you don't want to use word2vec, you can make some approximations to it. One ways is to make a co-occurrence matrix of words from your trained sentences followed by applying  TSVD  on it. Coccurance matrix of  $nXn$  dimensionality when converted into  $nXd$  dimensionality, makes for word vectors of  $d$  dimensions.  
 Once you get word embedding of each word, you can apply any of the similarity metrics like cosine similarity, etc. on each sentence to measure similarity with others.",,,,61.34734707,55.01093212,52.58941929,57.10554246,65.12289716,58.93023484,,,
23894,issue with oneHotEncoding,python,"There is an easy way to use one hot encoding in pandas and you can read about it in the following link: 
 https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html 
 The  get_dummies  method is easy and optimized for use in pandas Data Frames. 
 Good luck!","It may be a late answer, but I got the same problem and below is the solution 
 # Don't use categorical_features=[10] in encoder init
from sklearn.preprocessing import OneHotEncoder
onehotencoder=OneHotEncoder()
Y= onehotencoder.fit_transform(X[:,[10]]).toarray()","You need two steps: 
 
 Use  LabelEncoder  to encode your string variables to integers 
 Then use  OneHotEncoder  on your integer variables","You could just use a  LabelBinarizer . Label binarizer will skip the two step process(converting string to integer and then integer to float) as mentioned by  DontDivideByZero . 
 from sklearn.preprocessing import labelBinarizer
encoder = LabelBinarizer()
Y = encoder.fit_transform(X)
 
 This way you will convert the entire  X  matrix, but later you can quite easily extract  Y[10]  which is the one hot encoded matrix that you are looking for.","I was also facing the same issue. I tried every possible way to do and got into why it wasn't happening. 
Actually, earlier OneHotEncoding needed numerical value first (earlier we couldn't directly encode string type data to numerical using OneHotEncoding, so first we used to apply LabelEncoding first) and then we used to apply OneHotEncoding. 
 But Now, OneHotEncoding could directly work with String data types also, but here we need data to be of type -either a DATAFRAME or a 2D Array (Not Object type) example [50,1]. 
 Now you can do so, this way: 
 from sklearn.preprocessing import OneHotEncoder
    onehotencoder = OneHotEncoder()
    X_new_enc= onehotencoder.fit_transform(X[:,[3]]).toarray()  #[String_Column Index]
 
 OR you rather use get_dummies directly (pandas based) 
 X= pd.get_dummies(X)
 
 Feel free to ask any doubts over this.",,,,,50,74.72039852,60.59012893,50,76.61435358,,,,
23591,Clustering algorithms for high dimensional binary sparse data,machine-learning,"How many ethnic groups did you identify? 
 If I had to visualize your problem, I'd determine the key influencers for each of the ethnic groups in a Naive Bayes like approach.
These genes (gene combinations?!) (including their values) may strongly correlate to some ethnic group, while not (or inverse) correlate to another.  
 Place them on top of a pyramid graph. Place bars to the left and right for the correlation values.","'Clustering different ethnic groups for visualization' seems more like you are trying to do supervised dimensionality reduction since you already know the target variables in this case. 
 Since you will be using it for classification later, I assume you already know the number of ethnic groups. This can be done using Linear Discriminant Analysis (LDA). Check out this post:  https://stats.stackexchange.com/questions/161362/supervised-dimensionality-reduction","Procedure-1  : 
 I think it would be better if you can try combining some geners, it is most likely that some follow similar trend, once you identify them try combining them. 
 You can use some Dimensionality reduction, then you can make more sense out of the data, as of now even if you give directly also it might take time for the model to understand and give some useful results. 
 Once you get the outcome of Dimensionality reduction you can directly apply multi class classification algorithms like  SVM ,  RF  and many more. 
 Procedure-2 : 
 Another thing which you can try is, You can concatenate all the features(Gener's) into 1 single feature and try try understanding and see if does makes any sense/ get some good insights(Exploratory Analysis). 
 Do let me know if you have questions. 
 SVM : Support Vector Machine 
 RF : Random Forest","I suspect the number of ethnic groups is large and you are given a large enough sample of random people from different ethnic background to work with. So I propose the following: 
 Rather than using clustering (unsupervised segmentation) you could use an existing less granular ethnic grouping. Let's say your unique ethnic groups in your raw dataset is like that in  https://en.wikipedia.org/wiki/List_of_contemporary_ethnic_groups  
then you could use a high level grouping with smaller groups such as that in
  https://www.google.com/search?q=ethnic+grouping+in+the+world&rlz=1C1CHBF_enUS810US810&oq=ethnic+grouping+in+the+world&aqs=chrome..69i57.10445j0j8&sourceid=chrome&ie=UTF-8  
for the purpose of understanding the high level group profiles: for example by analysing the descending rank of frequency count of gene features that are more prevalent in each group. Technically you can do the same using the original more granular ethnic groups. 
 Normally you cannot apply traditional principal component analysis on the gene features since they are categorical with values 0 or 1, but you could apply a more appropriate method that does not require continuous variables inputs, such us the method used in this R package:  https://cran.r-project.org/web/packages/FactoMineR/index.html   
 Clustering and recommendation in one shot: 
 You could also try explicit collaborative filtering which requires data to be in the format of user by item, in the following way: 
 
 since the gene features are all binary you could use your ethnic group numeric id as the rating but you have to convert it to numeric from 1 to N distinct ethnic groups 
 define person as the user dimension and the id of each gene feature as the item dimension 
 re-organize the data as [Person, genes,group], where genes=[1,2,...,N_genes] and group=[1,2,...,N_ethnic] keeping rows where gene feature=1 only in this format, the zero value assumed where the combination does not exist, separate the data in training and validation datasets 
 Apply Alternative Least Square ( https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html ) on the training data then use the validation data to validate how well ALS predicts the ethnic group, may use the number of elements in the smaller ethnic grouping above as the an initial number of components in the ALS process 
 if the number of correct predictions of each ethnic group given gene features for all persons is reasonably higher than incorrect predictions then use the model to predict ethnic group membership, you can also look at  the mix of gene features that are characteristic to each ethnic group from this result.","If you know the ground truth of data, the ethnic here. You can visualize your binary cluster as follow. Compute prototypes of each cluster using majority vote per feature which has a linear complexity in number of observations and in number of features. Then visualize each binary prototype as a binary grid of size $100\times100$ for your $10000$ features. Select two of your favorite colors and enjoy. You will see if centroids are overlapping with others when they share same color at same pixels.
If you desire to cluster your data rapidly i will advice you to start with $K$-$Modes$ which is the binary equivalent of $K$-$Means$, both are in $O(n)$, set $K$ accordingly to your number of ethnic and once you have clusters apply again majority vote to extract prototypes, visualize them and observe if it has similitude with ground truth. You can find an easy to use version of the algorithm  here  with a practical bootstrap example, with visualization, on  this  SparkNotebook.","You already know to which cluster each person belongs, so you need to run a clustering algorithm that makes this prediction for you. Your question is about data exploration: You're trying to understand your data. Your actual problem is a supervised (multi-class) classification problem, and clustering algorithms are not suited for that, because they are unsupervised. 
 I would recommend to do two things: First, reduce the dimensionality to be able to visualize. Second, calculate metrics on the original high-dimensional dataset to gain more understanding. 
 To visualize the data, I recommend  to use t-SNE to visualize in two dimensions and color with the ethnic group . This will give you an idea if your data forms clusters in the 10k-dimensional space. 
 Then, if you want to improve your feeling or intuition about your data further, by thinking about it in terms of clusters in the 10k-dimensional space, then you can calculate cluster metrics such as the  Silhouette score , cluster compactness (average distance to the centre), or display the distance between clusters in a  heatmap . You can merge two clusters by giving them the same label, and see how your results change. 
 I can't anticipate the results that you may get, so it could be very enlightening, meaning that you can tell that certain clusters are very compact, others very extensive, some are very similar to others and so on. But perhaps, using the above methods, you cannot make sense of your data at all. If that happens, then I would say it's time to stop thinking about your data as points in ""gene space"", with differences between people indicating a ""distance"", etc. In this case, it can be that the mapping from genes to ethnic grouping is more complex (non-linear) than a spatial clustering, so you need to use a classification algorithm that is capable of encoding this non-linearity. 
 Deep learning doesn't have many prerequisites but one of them is that it can only encode continuous functions. Neural networks also require numerical, real-valued input features. Since your problem has binary data and there is no reason to think that the gene to ethnicity mapping is a smooth function, perhaps algorithms based on decision trees are a good place to start. 
 Good luck! :-)",,,,50,53.10015008,53.36547133,52.05592589,59.41931015,65.88954275,,,
23376,How to get the number of syllables in a word?,nlp,"You can try another Python library called  Pyphen . It's easy to use and supports a lot of languages. 
 import pyphen
dic = pyphen.Pyphen(lang='en')
print dic.inserted('Rohit')
>>'Ro-hit'","I was facing the exact same issue, this is what I did: 
Catch the key error you get when the word is not found in cmu's dictionary as below: 
 from nltk.corpus import cmudict
d = cmudict.dict()

def nsyl(word):
    try:
        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]]
    except KeyError:
        #if word not found in cmudict
        return syllables(word)
 
 Call the below syllables function 
 def syllables(word):
    #referred from stackoverflow.com/questions/14541303/count-the-number-of-syllables-in-a-word
    count = 0
    vowels = 'aeiouy'
    word = word.lower()
    if word[0] in vowels:
        count +=1
    for index in range(1,len(word)):
        if word[index] in vowels and word[index-1] not in vowels:
            count +=1
    if word.endswith('e'):
        count -= 1
    if word.endswith('le'):
        count += 1
    if count == 0:
        count += 1
    return count","Below is how I did 
 def countsyllables(pron):
 return len([char for phone in pron for char in phone if char[-1].isdigit() ])
from nltk.corpus import cmudict
from nltk.corpus import brown
cmudict_dict=cmudict.dict()
sw = stopwords.words('english')
bwns=[w.lower() for w in brown.words() if w.lower() not in sw ]
missingw=[]
syllablecnt=[]
for  w in bwns:
  try:
    syllablecnt.append(countsyllables(cmudict_dict[w]))
  except:
    missingw.append(w)
    continue  
# below is approximate count of syllable in the text brown, there are many missing words too    
sum(syllablecnt)","Like you, I wasn't thrilled with the quality of syllable counting functions I could find online, so here's my take: 
 import re

VOWEL_RUNS = re.compile(""[aeiouy]+"", flags=re.I)
EXCEPTIONS = re.compile(
    # fixes trailing e issues:
    # smite, scared
    ""[^aeiou]e[sd]? $|""
    # fixes adverbs:
    # nicely
    + ""[^e]ely$ "",
    flags=re.I
)
ADDITIONAL = re.compile(
    # fixes incorrect subtractions from exceptions:
    # smile, scarred, raises, fated
    ""[^aeioulr][lr]e[sd]? $|[csgz]es$ |[td]ed $|""
    # fixes miscellaneous issues:
    # flying, piano, video, prism, fire, evaluate
    + "".y[aeiou]|ia(?!n$ )|eo|ism $|[^aeiou]ire$ |[^gq]ua"",
    flags=re.I
)

def count_syllables(word):
    vowel_runs = len(VOWEL_RUNS.findall(word))
    exceptions = len(EXCEPTIONS.findall(word))
    additional = len(ADDITIONAL.findall(word))
    return max(1, vowel_runs - exceptions + additional)
 
 We avoid looping in pure Python; at the same time, these regexes should be easy to understand. 
 This performs better than the various snippets floating around online that I've found (including Pyphen and Syllapy's fallback). It gets over 90% of cmudict correct (and I find its mistakes quite understandable). 
 cd = nltk.corpus.cmudict.dict()
sum(
    1 for word, pron in cd.items()
    if count_syllables(word) in (sum(1 for p in x if p[-1].isdigit()) for x in pron)
) / len(cd)
# 0.9073751569397757
 
 For comparison, Pyphen is at 53.8% and the syllables function in the other answer is at 83.7%. 
 Here are some common words it gets wrong: 
 from collections import Counter
for word, _ in Counter(nltk.corpus.brown.words()).most_common(1000):
    word = word.lower()
    if word in cd and count_syllables(word) not in (sum(1 for p in x if p[-1].isdigit()) for x in cd[word]):
        print(word)","I've tried a lot of automatic methods and data based methods and none really get all of them right. If the word is in the dictionary, its a sure thing to get the right number of syllables. Failing that we can try an automatic method. In your case of Rohit works to say it has 2 syllables. Comes has 1, karate has 3, readier has 3, Siberia has 4, insouciance has 4, pineapple has 3, strawberries has 3, snozzberries has 3. So it seems to be fairly comprehensive. If this function gets something wrong leave a comment. 
 from nltk.corpus import cmudict
import syllapy

d = cmudict.dict()    
def syllable_count(word):
    try:
        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]
    except KeyError:
        #if word not found in cmudict
        return syllapy.count(word)","https://github.com/repp/big-phoney  seems to work great if you patch the pull request @  https://github.com/repp/big-phoney/pull/8/commits/580d5a582e445510d28a6270aa16453ed868151e 
 It uses a Dict and when out of dictionary it uses a tensorflow model to predict phones & syllables that guess with more training it could be more accurate.
Only EN currently though and seems a bit stagnant but maybe should get some love...",,,,50,69.30178626,53.84594744,65.68400149,65.0189766,55.99912609,,,
23159,"In softmax classifier, why use exp function to do normalization?",machine-learning,"It is more than just numerical. A quick reminder of the softmax:
$$
P(y=j | x) = \frac{e^{x_j}}{\sum_{k=1}^K e^{x_k}}
$$ 
 Where $x$ is an input vector with length equal to the number of classes $K$. The softmax function has 3 very nice properties: 1. it normalizes your data (outputs a proper probability distribution), 2. is differentiable, and 3. it uses the exp you mentioned. A few important points: 
 
 The loss function is not directly related to softmax. You can use standard normalization and still use cross-entropy. 
 A ""hardmax"" function (i.e. argmax) is not differentiable. The softmax gives at least a minimal amount of probability to all elements in the output vector, and so is nicely differentiable, hence the term ""soft"" in softmax. 
 Now I get to your question. The $e$ in softmax is the natural exponential function.  Before  we normalize, we transform $x$ as in the graph of $e^x$: 
 
 
 If $x$ is 0 then $y=1$, if $x$ is 1, then $y=2.7$, and if $x$ is 2, now $y=7$! A huge step! This is what's called a non-linear transformation of our unnormalized log scores. The interesting property of the exponential function combined with the normalization in the softmax is that high scores in $x$ become much more probable than low scores.  
 An example . Say $K=4$, and your log score $x$ is vector $[2, 4, 2, 1]$. The simple argmax function outputs: 
 $$
[0, 1, 0, 0]
$$ 
 The argmax is the goal, but it's not differentiable and we can't train our model with it :( A simple normalization, which is differentiable, outputs  the following probabilities: 
 $$
[0.2222, 0.4444, 0.2222, 0.1111]
$$ 
 That's really far from the argmax! :( Whereas the softmax outputs:
$$
[0.1025, 0.7573, 0.1025, 0.0377]
$$ 
 That's much closer to the argmax! Because we use the natural exponential, we hugely increase the probability of the biggest score and decrease the probability of the lower scores when compared with standard normalization. Hence the ""max"" in softmax.","This question is very interesting. I do not know the exact reason but I think the following reason could be used to explain the usage of the exponential function. This post is inspired by statistical mechanics and the principle of maximum entropy. 
 I will explain this by using an example with  $N$  images, which are constituted by  $n_1$  images from the class  $\mathcal{C}_1$ ,  $n_2$  images from the class  $\mathcal{C}_2$ , ..., and  $n_K$  images from the class  $\mathcal{C}_K$ . Then we assume that our neural network was able to apply a nonlinear transform on our images, such that we can assign an 'energy level'  $E_k$  to all the classes. We assume that this energy is on a nonlinear scale which allows us to linearly separate the images. 
 The mean energy  $\bar{E}$  is related to the other energies  $E_k$  by the following relationship
 \begin{equation}
N\bar{E} = \sum_{k=1}^{K} n_k E_k.\qquad (*)
\label{eq:mean_energy}
\end{equation} 
 At the same time, we see that the total amount of images can be calculated as the following sum 
 \begin{equation}
    N = \sum_{k=1}^{K}n_k.\qquad (**)
    \label{eq:conservation_of_particles}
\end{equation} 
 The main idea of the maximum entropy principle is that the number of the images in the corresponding classes is distributed in such a way that that the number of possible combinations of for a given energy distribution is maximized. To put it more simply the system will not very likeli go into a state in which we only have class  $n_1$  it will also not go into a state in which we have the same number of images in each class. But why is this so? If all the images were in one class the system would have very low entropy. The second case would also be a very unnatural situation. It is more likely that we will have more images with moderate energy and fewer images with very high and very low energy. 
 The entropy increases with the number of combinations in which we can split the  $N$  images into the  $n_1$ ,  $n_2$ , ...,  $n_K$  image classes with corresponding energy. This number of combinations is given by the multinomial coefficient 
 \begin{equation}
\begin{pmatrix}
N!\\
n_1!,n_2!,\ldots,n_K!\\
\end{pmatrix}=\dfrac{N!}{\prod_{k=1}^K n_k!}.
\end{equation} 
 We will try to maximize this number assuming that we have infinitely many images  $N\to \infty$ . But his maximization has also equality constraints  $(*)$  and  $(**)$ . This type of optimization is called constrained optimization. We can solve this problem analytically by using the method of Lagrange multipliers. We introduce the Lagrange multipliers  $\beta$  and  $\alpha$  for the equality constraints and we introduce the Lagrange Function  $\mathcal{L}\left(n_1,n_2,\ldots,n_k;\alpha, \beta \right)$ . 
 \begin{equation}
    \mathcal{L}\left(n_1,n_2,\ldots,n_k;\alpha, \beta \right) = \dfrac{N!}{\prod_{k=1}^{K}n_k!}+\beta\left[\sum_{k=1}^Kn_k E_k - N\bar{E}\right]+\alpha\left[N-\sum_{k=1}^{K} n_k\right]
\end{equation} 
 As we assumed  $N\to \infty$  we can also assume  $n_k \to \infty$  and use the Stirling approximation for the factorial 
 \begin{equation}
    \ln n! = n\ln n - n + \mathcal{O}(\ln n).
\end{equation} 
 Note that this approximation (the first two terms) is only asymptotic it does not mean that this approximation will converge to  $\ln n!$  for  $n\to \infty$ . 
 The partial derivative of the Lagrange function with respect  $n_\tilde{k}$  will result in 
 $$\dfrac{\partial \mathcal{L}}{\partial n_\tilde{k}}=-\ln n_\tilde{k}-1-\alpha+\beta E_\tilde{k}.$$ 
 If we set this partial derivative to zero we can find 
 $$n_\tilde{k}=\dfrac{\exp(\beta E_\tilde{k})}{\exp(1+\alpha)}. \qquad (***)$$ 
 If we put this back into  $(**)$  we can obtain 
 $$\exp(1+\alpha)=\dfrac{1}{N}\sum_{k=1}^K\exp(\beta E_k).$$ 
 If we put this back into  $(***)$  we get something that should remind us of the softmax function 
 $$n_\tilde{k}=\dfrac{\exp(\beta E_\tilde{k})}{\dfrac{1}{N}\sum_{k=1}^K\exp(\beta E_k)}.$$ 
 If we define  $n_\tilde{k}/N$  as the probability of class  $\mathcal{C}_\tilde{k}$  by  $p_\tilde{k}$  we will obtain something that is really similar to the softmax function 
 $$p_\tilde{k}=\dfrac{\exp(\beta E_\tilde{k})}{\sum_{k=1}^K\exp(\beta E_k)}.$$ 
 Hence, this shows us that the softmax function is the function that is maximizing the entropy in the distribution of images. From this point, it makes sense to use this as the distribution of images. If we set  $\beta E_\tilde{k}=\boldsymbol{w}^T_k\boldsymbol{x}$  we exactly get the definition of the softmax function for the  $k^{\text{th}}$  output.","In addition to vega's explanation, 
 let's define generic softmax:
 $$P(y=j | x) = \frac{\psi^{x_j}}{\sum_{k=1}^K \psi^{x_k}}$$ 
where  $\psi$  is a constant >= 1 
 if  $\psi=1$ , then you are pretty far from argmax as @vega mentioned. 
 Let's now assume  $\psi=100$ , now you are pretty close to the argmax but you also have a really small numbers for negative values and big numbers for positives. This numbers  overflows  the  float point arithmetic limit  easily(for example maximum limit of numpy float64 is  $10^{308}$ ). In addition to that, even if the selection is  $\psi=e$  which is much smaller than  $100$ , frameworks should implement a more stable version of softmax (multiplying both numerator and denominator with constant  $C$ ) since results become to small to be able to express with such precision. 
 So, you want to pick a constant big enough to approximate argmax well, and also small enough to express these big and small numbers in calculations. 
 And of course,  $e$  also has pretty nice derivative.","Apart from the great answers mentioned here, one should also think about shift-invariance of softmax (or for that matter any exponential functions). Consider logits output from a classifier network (3 classes)  [a, b, c] . Then the probability distribution will remain invariant even if it had been  [a+x, b+x, c+x] . 
 For example, if we consider  e^e^x - e  for normalization (as was mentioned in one of the comments). We miss out on this nice property. Now you may ask why is shift-invariance desired versus say scale invariance of logits. I find shift-invariance as an intuitively better design choice since and do not know a more theoretically backed reasoning or if it exists,  
 Secondly, as for using any other exponents on softmax other than  e , it is being used. Look into learning classification with temperature and is a common technique in machine learning. So yes the softmax outputs may not correspond to probabilities and temperature scaling is used to calibrate these probabilities (where temperature may be learnt or treated as a hyperparameter)","In addition to above answers, the softmax should have the following properties (from  source ) 
 
 Monotonically increasing: larger inputs must give larger outputs. 
 Non-negative outputs: Probability values must be non-negative. 
 The outputs should sum to one: Probability values must sum to one 
 
 Here the non-negativity property is ensured by the  exponential function .",,,,,67.33605281,56.77656266,52.42847225,57.17056297,52.60771109,,,,
22828,Clustering with cosine similarity,machine-learning,"First, every clustering algorithm is using some sort of distance metric. Which is actually important, because every metric has its own properties and is suitable for different kind of problems. 
 You said you have cosine similarity between your records, so this is actually a distance matrix. You can use this matrix as an input into some clustering algorithm. 
 Now, I'd suggest to start with hierarchical clustering - it does not require defined number of clusters and you can either input data and select a distance, or input a distance matrix (where you calculated the distance in some way). 
 Note that the hierarchical clustering is expensive to calculate, so if you have a lot of data, you can start with just sample.","I'd use sklearn's Hierarchical clustering 
 from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from scipy.cluster import  hierarchy

#Vectorizing
X = CountVectorizer().fit_transform(docs)
X = TfidfTransformer().fit_transform(X)
#Clustering
X = X.todense()
threshold = 0.1
Z = hierarchy.linkage(X,""average"", metric=""cosine"")
C = hierarchy.fcluster(Z, threshold, criterion=""distance"")
 
 C  is your clustering of the documents  docs . 
 You can use other metrics instead of  cosine , and use a different threshold than  0.1","DBSCAN  can  trivially be implemented with a similarity measure instead of a distance. You just need to change the <= epsilon into a >= epsilon. 
 HAC also works just fine with similarities (at least single-link, complete-link, UPGMA, WPGMA - don't use Ward), if you swap ""min"" and ""max"" (you want to merge with maximum similarity rather than minimum distance). 
 If you are lazy, you can also just transform your similarity into a distance. If you have a fixed maximum, dist=max-sim will often do.","I think the  clustMixType  package might give you better results/insights. 
 By using  this package  you can use combination of Categorical and Numeric Data directly, it doesn’t need any kind of hot encoding. 
 You just need to feed in the data and it automatically segregates into Categorical and Numeric Data, if you find any issues at the time of segregation you can use functions like  as.factor(to convert to a categorical)  and  as.numeric(to convert to a Numeric field) . 
 You can calculate  Lambda(mean Distance value)  before hand and fed in as an input to the algorithm. 
 If you don’t know the optimal number of clusters, you can use  WSS(within Sum of Squares) ,  plot(elbow chart)  to decide the optimal number of clusters.","All clustering methods use a distance metric of some sort. And remember that distance is essentially a dissimilarity measure. So if you normalize your similarity betwen 0 and 1, your distance is simply 1-similarity 
 As for algorithms that do not require a number of clusters to be specified, there are of course hierarchical clustering techniques, which essentially build a tree like structure that you can ""cut"" wherever you please (you can use some perfomance metrics to do that automatically) 
 X-means is a version of K-means which tries a certain number of K and picks the one that maximizes some evaluation function. 
 Mean shift also ""finds"" a natural number of clusters but is sensible to other parameters such as the bandwith for instance.",,,,,64.94293199,63.62342282,55.85186014,53.14773874,59.84015708,,,,
22795,Do Clustering algorithms need feature scaling in the pre-processing stage?,machine-learning,"Clustering algorithms are certainly effected by the feature scaling.  
 Example:  
 Let's say that you have two features: 
 
 weight (in Lbs) 
 height (in Feet) 
 
 ... and we are using these to predict whether a person needs a 'S' or 'L' size shirt. 
 We are using weight+height for that, and in our trained set let's say we have two people already in clusters:  
 
 Adam (175Lbs+5.9ft) in 'L' 
 Lucy (115Lbs+5.2ft) in 'S'.  
 
 We have a new person - Alan (140Lbs+6.1ft.), and your clustering algo will put it in the cluster which is nearest.  So, if we don't scale the features here, the height is not having much effect and Alan will be allotted in 'S' cluster. 
 So, we need to scale it. Scikit Learn provides many functions for scaling. One you can use is  sklearn.preprocessing.MinMaxScaler .","Yes. Clustering algorithms such as K-means do need feature scaling before they are fed to the algo. Since, clustering techniques use  Euclidean Distance  to form the cohorts, it will be wise e.g to scale the variables having heights in meters and weights in KGs before calculating the distance.","In fact, most clustering algorithms are even  highly sensitive to scaling . Rescaling the data can completely ruin the results. 
 Bad scaling also appears to be a key reason why people fail with finding meaningful clusters. It is just very easy to do badly. 
 By no means rely on automatic scaling. It must fit your task and data. Preprocessing is an art, and will require most of the work. 
 Non-continuous variables are big issue. While you can ""hack"" data into binary encodings and then pretend they are suitable, the discreteness poses a major issue for the algorithms. For example, many points have the same distance. And the  mean  of such a variable doesn't make a lot of semantic sense anymore. The squared deviation (as used by k-means) is even worse. Results may often be better if you ignore such variables when clustering. 
 Same goes for bad attributes, such as identifiers, sequence numbers, etc.","Scaling affects Clustering Results in a way that depends by the metric used (Euclidean Distance, Squared Euclidean Distance, Manhattan Distance, …)  
 In general when you are mixing features which have different physical measurements units, you can think of a Linear Transformation (i.e. Offset + Scale Factor) to transform them into a common space  
 You can also think to try learning this transformation, using some more complicated model than a linear one (if you think your problem requires it)","Feature scaling will certainly effect clustering results. Exactly what scaling to use is an open question however, since clustering is really an exploratory procedure rather than something with a ground truth you can check against. Ultimately you want to use your knowledge of the data to determine how to relatively scale features. In practice clustering is going to work best with numeric features, so if you have a mix of feature types you may want to look at embedding methods such as GLRM as a preprocessing step.","There are different clustering algorithms. Without knowing every one well, I would assume it may vary. 
 One very popular clustering algorithm is  k-means  and this one  usually needs scaling  since  
 
 ""K-means clustering is ""isotropic"" in all directions of space and
  therefore tends to produce more or less round (rather than elongated)
  clusters. In this situation  leaving variances unequal is equivalent
  to putting more weight on variables with smaller variance , so
  clusters will tend to be separated along variables with greater
  variance."" 
 
 For more see:  https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering",,,,62.45793957,69.2724749,57.8911224,53.71607635,61.84491379,61.17510078,,,
22494,Convolutional neural network overfitting. Dropout not helping,deep-learning,"Ok, so after a lot of experimentation I have managed to get some results/insights. 
 In the first place, everything being equal, smaller batches in the training set help a lot in order to increase the general performance  of the network, as a negative side, the training process is muuuuuch slower. 
 Second point, data is important, nothing new here but as I learned while fighting this problem, more data always seems to help a bit. 
 Third point, dropout is useful in large networks with lots of data and lots of iterations, in my network I applied dropout on the final fully connected layers only, convolution layers did not get dropout applied. 
 Fourth point (and this is something I am learning over and over): neural networds take A LOT to train, even on good GPUs (I trained this network on  floydhub, which uses quite expensive NVIDIA cards), so  PATIENCE is key . 
 Final conclusion: Batch sizes are more important that one might think, apparently it is easier to hit a local minimum when batches are larger. 
 The code I wrote is available as a  python notebook , I think it is decently documented.","There are several possible solutions for your Problem. 
 
 Use Dropout in the earlier layers (convolutional layers) too. 
 Your network seems somehow quite big for such an ""easy"" task; try to reduce it. The big architectures are also trained on much bigger datasets. 
 
 If you want to keep your ""big"" architecture try: 
 
 Image augmentation in order to virtually increase your training data 
 Try adversarial training. It sometimes helps.","I suggest you analyze the learning plots of your validation accuracy as Neil Slater suggested. Then, if the validation accuracy drops try to reduce the size of your network (seems too deep), add dropout to the CONV layers and BatchNormalization after each layer. It can help get rid of overfitting and increase the test accuracy.","One thing that hasn't been mentioned yet and that you can consider for the future: you can still increase your dropout at the fully connected layers. 
 I read a paper once that used 90% dropout rate. Although it had many many nodes (2048 if i recall correctly), I have tried this myself on layers with fewer nodes and it was very helpful in some cases. 
 I just looked up which paper it was. I can't recall which paper I just remembered but I found these that also had some success with 90% dropout rates. 
 
 Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., &
Fei-Fei, L. (2014). Large-scale video classification with
convolutional neural networks. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition (pp. 1725-1732). 
 Simonyan, K., & Zisserman, A. (2014). Two-stream convolutional
networks for action recognition in videos. In Advances in neural
information processing systems (pp. 568-576). 
 Varol, G., Laptev, I., & Schmid, C. (2017). Long-term temporal
convolutions for action recognition. IEEE transactions on pattern
analysis and machine intelligence.","I had this problem too.  After dinking with it for hours, by chance I decided to shuffle the data before feeding it into the system and voila, it started working.  It took me a bit to figure out that it was the shuffling that did the trick!  Hope this saves somebody from frustration!",,,,,59.82681529,55.83935704,56.62255514,63.51776289,50,,,,
22335,Why are Machine Learning models called black boxes?,machine-learning,"The  black box  thing has nothing to do with the level of expertise of the audience (as long as the audience is human), but with the  explainability  of the function modelled by the machine learning algorithm. 
 In logistic regression, there is a very simple relationship between inputs and outputs. You can sometimes understand why a certain sample was incorrectly catalogued (e.g. because the value of certain component of the input vector was too low). 
 The same applies to decision trees: you can  follow  the logic applied by the tree and understand why a certain element was assigned to one class or the other. 
 However, deep neural networks are the paradigmatic example of black box algorithms. No one, not even the most expert person in the world grasp the function that is actually modeled by training a neural network. An insight about this can be provided by  adversarial examples : some slight (and unnoticeable by a human) change in a training sample can lead the network to think that it belongs to a totally different label. There are some techniques to create adversarial examples, and some techniques to improve robustness against them. But given that no one actually knows all the relevant properties of the function being modeled by the network, it is always possible to find a novel way to create them. 
 Humans are also black boxes and  we are also sensible to adversarial examples .","While I agree on  ncasas answer  in most points (+1), I beg to differ on some: 
 
 Decision Trees can be used as black box models, too. In fact, I'd say in most cases they are used as black-box models. If you have 10,000 features and a tree of depth of 50 you cannot reasonably expect a human to understand it. 
 Neural Networks can be understood. There are many analyzation techniques (see  chapter 2.5 of my master thesis  for some which are aimed at improving the model). Especially occlusion analysis (Figure 2.10), Filter visualization (Figure 2.11). Also the  Why Should I Trust You?  paper ( my notes ). 
 
 Explaining the prediction of a black-box model by fancy occlusion analysis (from ""Why should I trust you?""):
 
 I would like to point out  The Mythos of Model Interpretability . It formulates some ideas about interpretability in a concise way. 
 Your question 
 
 Why are Machine Learning models called black boxes? 
 
 How people use it : Because they do not model the problem in a way which allows humans to directly say what happens for any given input. 
 Personal thoughts 
 I don't think this notion of a ""black box model"" makes much sense. For example, think of weather forecasting. You cannot expect any human to say which weather will be predicted if he is only given the data. Yet most people would not say that physical weather models are black box models. So where is the difference? Is it only the fact that one model was generated using data and the other one was generated using insights into physics? 
 When people speak of black box models they usually say it as if it is a bad thing. But humans are black box models, too. The critical difference I see here is that the class of errors humans make is easier to predict for humans. Hence it is a training problem (adverserial examples on the NN side) and an education problem (teaching humans how NNs work). 
 How the term 'black-box model' should be used : An approach which makes more sense to me is to call the problem a ""black box problem"", similar to what  user144410  (+1) writes. Hence any model which only treats the problem as a black box - hence something you can put input in and get output out - is a black box model. Models which have insights (not only assume!) about the problem are not black-box models. The insight part is tricky. Every model makes restrictions on the possible function which it can model (yes, I know about the universal approximation problem. As long as you use a fixed-size NN it doesn't apply). I would say something is an insight into the problem if you know something about the relationship of input and output without poking the problem (without looking at data). 
 What follows from this: 
 
 Neural Networks can be non-blackbox (whitebox?) 
 Logistic Regression can be a black-box model. 
 It's more about the problem and your insights about it, less about the model.","It comes down to model interpretability and explainability. Given the output of a simpler model, it is possible to identify exactly how each input contributes to model output, but that gets more difficult as models get more complex. For example with regression you can point to the coefficients, with a decision tree you can identify the splits. And with this information, you could derive rules to explain model behaviour.  
 However, as the number of model parameters increases, it becomes increasingly difficult to explain exactly what combinations of input lead to the final model output, or derive rules from the model's behaviour. Say in the financial industry when the COO comes over and asks 'so, why did your high frequency trading algo break the economy', he doesn't want to hear how it was built, just why it sent him bankrupt. It will be possible to state how the model was constructed, but it might not be possible to explain what combinations of factors that the model received as input led to the output, and that’s why people are talking about black boxes.","Black box models refer to any mathematical models whose equations are chosen to be as general and flexible as possible without relying on any physical/scientific laws. 
 Grey box models are mathematical models where part of the equations (mathematical function) comes from physical known laws but the remaining part is assumed general function to compensate for the unexplained part. 
 White box models are mathematical models completely built on physical laws and understanding of the system, like for example mechanical motion laws (model of aircraft ..etc) 
 See  here","ML engineers don't know what goes on inside a neural net 
 Sorry to contradict you, but it's true. They know how neural networks learn, but they do not know what any given neural network has learned. The logic learned by neural networks is notoriously inscrutable. 
 The point of using machine learning is usually to learn the rules that a programmer or domain expert would not think of. This is inherently difficult to figure out. 
 It's analogous to a conventional computer program written with one letter variable names, no comments, no obvious structure, using obscure mathematics, and all by someone who is now dead. You can step through it in a debugger, but it is still far from clear how it works. 
 Rarely, someone does take the trouble to figure out what a neural network does. For example, the  min-conflicts algorithm  was discovered by analyzing a neural network trained on the  N-queens problem . But it's a lot of work.","A black box, as you may know, refers to a function where you know the signature of the inputs and outputs, but can't know how it determines the outputs from the inputs. 
 The use of the term is being buzz-worded incorrectly in this case.  It may be beyond the writer/author's willingness or capacity to know and understand ML models, but that does not mean it is beyond the willingness or capacities of others.  The engineers that create each ML model know exactly how it works and can pull up the decision tree at will and walk it.  Just because someone may be too lazy or it may take a while to do so does not mean the information is not readily available for consumption. 
 ML models are not black boxes, they are clear boxes that are just really big.","In the blog posting cited in the question, the discussion is about the fact that the experts who develop machine learning models in finance can't explain to their customers (financiers with no training in machine learning) how the model makes the decisions that it does.   
 This brings out a distinction between models that are black boxes because of information that is truly secret (e.g. the coefficients are encoded in a tamper proof FPGA) and models that are open (in the sense that the coefficients are known) but not comprehensible  to a particular audience .  
 This latter kind of ""black box"" is problematic because customers want to reassure themselves that the model you've constructed has ""face validity.""  With other types of models such as Logistic Regression, it's relatively easy to look at the coefficients and check that they have the expected plus or minus signs- even a mathematically illiterate MBA can understand that.","Machine Learning can be rightly considered Black boxes, solutions for the XOR problem using neural networks can be modelled but as the number of inputs grow, so does the complexity and dimensions.  If it is too complex to understand and explain, then it is a black box, whether or not we can calculate the results or not 
 We can only perceive them upto 3 dimensions but this is sufficient because we can extrapolate this upto higher dimensions using the 3d model as a point of reference.  We can imagine local minimums, as well as parts of datasets that are partially learnt. 
 I have toyed with the idea for a while and so I produced animations of neural networks at work and improved my understanding of neural networks.  I have produced animations with 1 and 2 hidden layers (3rd is mostly done) and how they learn data.  
 The animation is slow and the top right animation showing the upper layers is worth watching, you can speed the animations on Youtube if you like, significant changes can be seen on the top right animation with the Blue and Red Mesh at 3:20 Orange and Red mesh at 6 mins and the Blue, Orange and Red mesh at 8:20.  The directions of the weight changes are obviously in the bottom left animation 
 https://www.youtube.com/watch?v=UhQJbFDtcoc","I think the  black box  concept as used in this way originates from black box  testing  in software and hardware Quality Assurance. It is when you either choose not to / or even can't look into and see the inner working of what you are testing. It could be for a reason that it would be  
 
 impractical or impossible to peek into it (it is in a sealed environment and we simply can't look into it) - But it might as well be 
 because there is a larger chance of writing crappy tests if one can see the inside. Larger risk of (with or without intent) ""writing tests designed to pass"". 
 
 Writing the test to fit the thing that is being tested, lowering the chances of actually finding anything. 
 It would be  perfectly possible  for a skilled signal engineer to peek into the inner workings of a neural network and check which features are being selected for in a particular training sequence.",62.80620146,74.43950042,58.16064643,59.51882879,53.56182291,62.69841121,70.31684535,59.08052261,59.36109258
21902,How duplicated items can be deleted from dataframe in pandas,pandas,"The best way would be to use  drop_duplicates() . If you have a larger DataFrame and only want those two columns checked, set subset equal to the combined columns you want checked. 
 df = df.drop_duplicates()
 
 or 
 df = df.drop_duplicates(subset=['userid', 'itemid'])","To avoid reassignment, use (inplace = True) 
 df.drop_duplicates(inplace=True)
 
 This is same as  
 df = df.drop_duplicates()","df.groupby(df.index).first() 
 
 worked for me.","Use  drop_duplicates()  of  pandas : 
 import pandas as pd    
df = pd.DataFrame({'userid':[1,1,1,1, 2,2,2],
                   'itemid':[1,1,3,4, 1,2,3] })
print(df)
print()
print(df.drop_duplicates())
 
 Consider that drop won't change the df itself and just pass a new data frame which has dropped the specified row(s). If you want to change the  df  itself set  inside  parameter to  True .","Two methods are needed - 1)  sort_values  2)  drop_duplicates 
 Below is a sample code 
 import pandas as pd
df = pd.DataFrame({'userid':[1,1,1,1, 2,2,2],
                   'itemid':[1,1,3,4, 1,2,3] }
df.sort_values(by = [""userid"", ""itemid], ascending = True).drop_duplicates(subset = [""itemid""], keep=""last"")","Using the following code: 
 df = df.drop_duplicates(['userid','itemid'])",,,,62.37549127,58.77767879,50,63.47421805,60.32856924,57.50141839,,,
21734,Keras -- Transfer learning -- changing Input tensor shape,keras,"You can do this by creating a new VGG16 model instance with the new input shape  new_shape  and copying over all the layer weights. The code is roughly 
 new_model = VGG16(weights=None, input_shape=new_shape, include_top=False)
for new_layer, layer in zip(new_model.layers[1:], model.layers[1:]):
    new_layer.set_weights(layer.get_weights())","This should be pretty easy with  kerassurgeon .  First you need to install the library; depending on if you are using Keras through TensorFlow (with tf 2.0 and up) or Keras as a separate library, it needs to be installed in different ways. 
 For Keras in TF:  pip install tfkerassurgeon  ( https://github.com/Raukk/tf-keras-surgeon ).
For standalone Keras:  pip install kerassurgeon  ( https://github.com/BenWhetton/keras-surgeon ) 
 To replace the input (example with TF 2.0; currently untested code): 
 from tensorflow import keras  # or import keras for standalone version
from tensorflow.keras.layers import Input

model = load_model('path/to/my/trained/model.h5')
new_input = Input(shape=(540, 960, 3), name='image_input')

# or kerassurgeon for standalone Keras
from tfkerassurgeon import delete_layer, insert_layer

model = delete_layer(model.layers[0])
# inserts before layer 0
model = insert_layer(model.layers[0], new_input)","This how I change the input size in Keras model. I have two CNN models, one with input size  $[None, None, 3]$  while the other has input size  $[512,512,3]$ . Both models have the same weights. By using  set_weights(model.get_weights()) , the weights of model 1 can be transferred to model 2. 
 inputs = Input((None, None, 3))
.....
model = Model(inputs=[inputs], outputs=[outputs])
model.compile(optimizer='adam', loss='mean_squared_error')
model.load_weights('my_model_name.h5')

inputs2 = Input((512, 512, 3))
....
model2 = Model(inputs=[inputs2], outputs=[outputs])
model2.compile(optimizer='adam', loss='mean_squared_error')
model2.set_weights(model.get_weights())","The output width and height of the output dimensions of the VGGnet are a fixed portion of the input width and height because the only layers that change those dimensions are the pooling layers. The number of channels in the output is fixed to the number of filters in the last convolutional layer. The flatten layer will reshape this to get one dimension with the shape: 
 ((input_width * x) * (input_height * x) * channels) 
 where x is some decimal < 1.  
 The main point is that the shape of the input to the Dense layers is dependent on width and height of the input to the entire model. The shape input to the dense layer cannot change as this would mean adding or removing nodes from the neural network. 
 One way to avoid this is to use a global pooling layer rather than a flatten layer (usually GlobalAveragePooling2D) this will find the average per channel causing the shape of the input to the Dense layers to just be  (channels,)  which is not dependant on the input shape to the whole model. 
 Once this is done none of layers in the network are dependent on the width and height of the input so the input layer can be changed with something like 
 input_layer = InputLayer(input_shape=(480, 720, 3), name=""input_1"")
model.layers[0] = input_layer","@gebbissimo answer worked for me in TF2 with just small adaptations that I share below in a single function: 
 def change_input_size(model,h,w,ch=3):
   model._layers[0]._batch_input_shape = (None,h,w,ch)
   new_model = keras.models.model_from_json(model.to_json())
   new_model.summary()
   for layer,new_layer in zip(model.layers,new_model.layers):
      new_layer.set_weights(layer.get_weights())
   return new_model","Here is another solution, not specific to the VGG model.  
 Note, that the weights of the dense layer cannot be copied (and will thus be newly initialized). This makes sense, because the shape of the weights differs in the old and the new model. 
 import keras
import numpy as np

def get_model():
    old_input_shape = (20, 20, 3)
    model = keras.models.Sequential()
    model.add(keras.layers.Conv2D(9, (3, 3), padding=""same"", input_shape=old_input_shape))
    model.add(keras.layers.MaxPooling2D((2, 2)))
    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(1, activation=""sigmoid""))
    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0001), metrics=['acc'], )
    model.summary()
    return model

def change_model(model, new_input_shape=(None, 40, 40, 3)):
    # replace input shape of first layer
    model._layers[1].batch_input_shape = new_input_shape

    # feel free to modify additional parameters of other layers, for example...
    model._layers[2].pool_size = (8, 8)
    model._layers[2].strides = (8, 8)

    # rebuild model architecture by exporting and importing via json
    new_model = keras.models.model_from_json(model.to_json())
    new_model.summary()

    # copy weights from old model to new one
    for layer in new_model.layers:
        try:
            layer.set_weights(model.get_layer(name=layer.name).get_weights())
        except:
            print(""Could not transfer weights for layer {}"".format(layer.name))

    # test new model on a random input image
    X = np.random.rand(10, 40, 40, 3)
    y_pred = new_model.predict(X)
    print(y_pred)

    return new_model

if __name__ == '__main__':
    model = get_model()
    new_model = change_model(model)",,,,54.43310377,57.12677292,56.08204856,57.15248268,54.35223359,56.98831316,,,
20296,Cross-entropy loss explanation,machine-learning,"The  cross entropy  formula takes in two distributions,  $p(x)$ , the true distribution, and  $q(x)$ , the estimated distribution, defined over the discrete variable  $x$  and is given by  
 $$H(p,q) = -\sum_{\forall x} p(x) \log(q(x))$$ 
 For a neural network, the calculation is independent of the following: 
 
 What kind of layer was used. 
 What kind of activation was used - although many activations will not be compatible with the calculation because their outputs are not interpretable as probabilities (i.e., their outputs are negative, greater than 1, or do not sum to 1). Softmax is often used for multiclass classification because it guarantees a well-behaved probability distribution function. 
 
 For a neural network, you will usually see the equation written in a form where  $\mathbf{y}$  is the ground truth vector and  $\mathbf{\hat{y}}$  (or some other value taken direct from the last layer output) is the estimate. For a single example, it would look like this: 
 $$L = - \mathbf{y} \cdot \log(\mathbf{\hat{y}})$$ 
 where  $\cdot$  is the inner product. 
 Your example ground truth  $\mathbf{y}$  gives all probability to the first value, and the other values are zero, so we can ignore them, and just use the matching term from your estimates  $\mathbf{\hat{y}}$ 
 $L = -(1\times log(0.1) + 0 \times \log(0.5) + ...)$ 
 $L = - log(0.1) \approx 2.303$ 
 An important point from comments 
 
 That means, the loss would be same no matter if the predictions are  $[0.1, 0.5, 0.1, 0.1, 0.2]$  or  $[0.1, 0.6, 0.1, 0.1, 0.1]$ ?  
 
 Yes, this is a key feature of multiclass logloss, it rewards/penalises probabilities of correct classes only. The value is independent of how the remaining probability is split between incorrect classes. 
 You will often see this equation averaged over all examples as a  cost  function. It is not always strictly adhered to in descriptions, but usually a  loss  function is lower level and describes how a single instance or component determines an error value, whilst a  cost  function is higher level and describes how a complete system is evaluated for optimisation. A cost function based on multiclass log loss for data set of size  $N$  might look like this: 
 $$J = - \frac{1}{N}\left(\sum_{i=1}^{N} \mathbf{y_i} \cdot \log(\mathbf{\hat{y}_i})\right)$$ 
 Many implementations will require your ground truth values to be one-hot encoded (with a single true class), because that allows for some extra optimisation. However, in principle the cross entropy loss can be calculated - and optimised - when this is not the case.","The answer from Neil is correct. However I think its important to point out that while the  loss  does not depend on the distribution between the incorrect classes (only the distribution between the correct class and the rest), the  gradient  of this loss function does effect the incorrect classes differently depending on how wrong they are. So when you use cross-ent in machine learning you will change weights differently for [0.1 0.5 0.1 0.1 0.2] and [0.1 0.6 0.1 0.1 0.1].  This is because the score of the correct class is normalized by the scores of all the other classes to turn it into a probability.","Let's start with understanding entropy in information theory: Suppose you want to communicate a string of alphabets ""aaaaaaaa"". You could easily do that as 8*""a"". Now take another string ""jteikfqa"". Is there a compressed way of communicating this string? There isn't is there. We can say that the entropy of the 2nd string is more as, to communicate it, we need more ""bits"" of information. 
 This analogy applies to probabilities as well. If you have a set of items, fruits for example, the binary encoding of those fruits would be  $log_2(n)$  where n is the number of fruits. For 8 fruits you need 3 bits, and so on. Another way of looking at this is that given the probability of someone selecting a fruit at random is 1/8, the uncertainty reduction if a fruit is selected is  $-\log_{2}(1/8)$  which is 3. More specifically, 
 $$-\sum_{i=1}^{8}\frac{1}{8}\log_{2}(\frac{1}{8}) = 3$$ 
This entropy tells us about the uncertainty involved with certain probability distributions; the more uncertainty/variation in a probability distribution, the larger the entropy (e.g. for 1024 fruits, it would be 10). 
 In ""cross""-entropy, as the name suggests, we focus on the number of bits required to explain the difference in two different probability distributions. The best case scenario is that both distributions are identical, in which case the least amount of bits are required i.e. simple entropy. In mathematical terms, 
 $$H(\bf{y},\bf{\hat{y}}) = -\sum_{i}\bf{y}_i\log_{e}(\bf{\hat{y}}_i)$$ 
 Where  $\bf{\hat{y}}$  is the predicted probability vector (Softmax output), and  $\bf{y}$  is the ground-truth vector( e.g. one-hot). The reason we use natural log is because it is easy to differentiate (ref. calculating gradients) and the reason we do not take the log of ground truth vector is because it contains a lot of 0's which simplifies the summation. 
 Bottom line: In layman's terms, one could think of cross-entropy as the distance between two probability distributions in terms of the amount of information (bits) needed to explain that distance. It is a neat way of defining a loss that goes down as the probability distributions get closer to one another.","Let's see how the gradient of the loss behaves... We have the cross-entropy as a loss function, which is given by 
 $$
H(p,q) = -\sum_{i=1}^n p(x_i) \log(q(x_i)) = -(p(x_1)\log(q(x_1)) + \ldots + p(x_n)\log(q(x_n)) 
$$ 
 Going from here.. we would like to know the derivative with respect to some $x_i$:
$$
\frac{\partial}{\partial x_i} H(p,q) = -\frac{\partial}{\partial x_i} p(x_i)\log(q(x_i)).
$$
Since all the other terms are cancelled due to the differentiation. We can take this equation one step further to
$$
\frac{\partial}{\partial x_i} H(p,q)  = -p(x_i)\frac{1}{q(x_i)}\frac{\partial q(x_i)}{\partial x_i}.
$$ 
 From this we can see that we are still only penalizing the true classes (for which there is value for $p(x_i)$). Otherwise we just have a gradient of zero. 
 I do wonder how to software packages deal with a predicted value of 0, while the true value was larger than zero... Since we are dividing by zero in that case.","I would like to add a couple of dimensions to the above answers: 
 true label = [1 0 0 0 0]
predicted = [0.1 0.5 0.1 0.1 0.2]
 
 cross-entropy(CE) boils down to taking the log of the lone +ve prediction. So CE = -ln(0.1) which is = 2.3. 
 This means that the -ve predictions dont have a role to play in calculating CE. This is by intention. 
 On a rare occasion, it may be needed to make the -ve voices count. This can be done by treating the above sample as a series of binary predictions. So: 
 true labels = [1,0], [0,1], [0,1], [0,1], [0,1]
predicted = [0.1, 0.9], [.5, .5], [.1, .9], [.1, .9], [.2, .8]
 
 Now we proceed to compute 5 different cross entropies - one for each of the above 5 true label/predicted combo and sum them up. Then: 
 CE = -[ ln(.1) + ln(0.5) + ln(0.9) + ln(0.9) + ln(0.8)] = 3.4 
 The CE has a different scale but continues to be a measure of the difference between the expected and predicted values.  The only difference is that in this scheme, the -ve values are also penalized/rewarded along with the +ve values. 
 All frameworks by default use the first definition of CE and this is the right approach in 99% of the cases. However if your problem is such that you are going to use the output probabilities (both +ve and -ves) instead of using the max() to predict just the 1 +ve label, then you may want to consider this version of CE. 
 The last situation could be a multi-label one. What if multiple classes 'could' be present in a single sample - something like - 
 true label = [1 0 0 0 1]
and predicted is = [0.1 0.5 0.1 0.1 0.9]
 
 By definition, CE measures the difference between 2 probability distributions. But the above two lists are not probability distributions. Probability distributions should always add up to 1. How do we handle this? 
 Solution: Firstly, in multi-label problem there are more than a single '1' in the output. So. we should get rid of the softmax and bring in sigmoids - one each for every neuron in the last layer (note that number of neurons = num of classes). Secondly, we use the above approach to calculate loss - wherein we break the expected and predicted values into 5 individual probability distributions of: 
 true labels = [1,0], [0,1], [0,1], [0,1], [1,0]
predicted = [.1, .9], [.5, .5], [.1, .9], [.1, .9], [.9, .1]
 
 Now just like before, we proceed to take the cross entropy of the above 5 true labels and the 5 predicted probability distributions and sum them up. Then: 
 CE = -[ ln(.1) + ln(0.5) + ln(0.9) + ln(0.9) + ln(0.9)] = 3.3 
 Occasionally, the number of classes may be very high - say a 1000 and there may be only couple of them present in each sample. So the true label is something like: [1,0,0,0,0,0,1,0,0,0..... 990 zeroes ]. The predicted could be something like: [.8, .1, .1, .1, .1, .1, .8, .1, .1, .1..... 990 0.1's ] 
 In this case the CE = 
 - [ ln(.8) + ln(.8) for the 2 +ve classes and 998 * ln(0.9) for the 998 -ve classes]

= 0.44 (for the +ve classes) +  105 (for the negative classes)
 
 You can see how the -ve classes are beginning to create a nuisance when calculating the loss. The voice of the +ve samples (which may be all that we care about) is getting drowned out. What do we do? We can't use categorical CE (the version where only +ve samples are considered in calculation). This is because, we are forced to break up the probability distributions into multiple binary probability distributions because otherwise it would not be a probability distribution in the first place. Once we break it into multiple binary probability distributions, we have no choice but to use binary CE and this of course gives weightage to -ve classes. 
 One option is to drown the voice of the -ve classes by a multiplier. Facebook did that and much more in a paper they came up with in 2018 and you can refer to focal loss for more details 
 For a more in-depth treatment of this subject, you can refer to:  https://towardsdatascience.com/cross-entropy-classification-losses-no-math-few-stories-lots-of-intuition-d56f8c7f06b0","The problem is that the probabilities are coming from a 'complicated' function that incorporates the other outputs into the given value. The outcomes are inter-connected, so this way we are not deriving regarding to the actual outcome, but by all the inputs of the last activation function (softmax), for each and every outcome. 
 I have found a very nice description  here  where the author shows that the actual derivative is  $p_i - y_i$ . 
 Other neat description can be found  here . 
 I think that using a simple sigmoid as a last activation layer would lead to the approved answer, but using softmax indicates different answer.",,,,58.16180869,54.14013262,60.29665733,57.44408546,56.87218653,50,,,
20208,Is Excel sufficient for data science?,programming,"First of all check out  this post . It has many reasons why Excel is inferior to other solutions, regarding data science tasks. Excel also can't handle large datasets (hundreds of thousands of records - not to mention anything in the vicinity of  Big Data ), image and sound data. 
 Excel is good for simple tasks concerning spreadsheets; it emphasizes more on  presentation  and  ease of use , while having minimal support for actually analysing the data. Unless all you want to do is to calculate simple statistical measures (mean, average, etc) or building a  very simple  model (e.g. linear regression), Excel is inefficient. That being said, 99% the work a company has to deal with concerning data is simple enough to be manageable through Excel. 
 However  Data Science  mainly deals with regression, classification and complex models that excel isn't equipped to handle! If your students want to have a look at data science you need to teach them a tool that will be useful to them (R, python, etc.). These languages also have libraries with tons of built in models to ""play with"". 
 Another really huge reason I would go with the latter options is that they are  open source . I personally feel that open source software should be preferred from an educational standpoint to proprietary solutions (this is also why I suggest python and R over Matlab)!","I just got done with a Masters in Business Analytics and was faced with the same problem you are describing. Luckily I am a technical person and was able to teach myself R and Python, but I was stuck teaching the rest of the class how to use R and Python. The classes I had that used R/Python were handicapped by the lack of technical understanding by the students and so too much time was spent covering how to just open R/Python. The classes that went the other route were underwhelming and not very practical. I wanted to do for a class project something that ended up not being able to be done in Excel because of its limitations but the teacher wouldn't accept any other tools.  
 It may not be something you can do right away but I would highly recommend that you try and get the department to require a programming course prior to taking your course. Data Science and Business Analytics IMHO should be cross discipline degree paths that require a good bit of Computer Science, but until the programs mature and the university system gets better it might not happen for a while.","I think you need to be teaching them a popular Data Science language like Python or R. Excel is not going to help them in a real job, and isn't practical for data science purposes.
I would probably say Python would be most valuable to them in the long run, and with packages like scikit-learn your regressions and classifications can be demonstrated in very few lines of code which they can read and understand more easily. It is not always easy to understand what R is doing by just reading it. 
 Another word of advice: Don't waste time forcing your students to set up an IDE and download the necessary packages, if you use python create a virtual environment for them with all the necessary packages, and set up an IDE like pycharm(they can get this and most other IDEs under a student/academic license) where then can develop and run their code through UI rather than console which they may find daunting and confusing. If you go down the R route then make sure you have an IDE like RStudio set up for them and make sure all of the includes and package installs are either included in your example code or fully described.","How do I convince myself and the students that Excel is insufficient
  for a serious business student studying data science 
 
 Create in R a huge data.frame (couple mln rows and hundreds of columns), save it as .xlsx.  
 Show them the time difference in loading it with R, and in Excel on the same machine. Compare basic statistics operations between the two on the same dataset, even plots. 
 Point no. 2-4 on yout list can be done in Excel too, just A LOT more painfully, show them a couple of example of how much simple (and faster) is filtering with  dplyr , compared to basic Excel, again on a huge dataset this would highlight the difference. 
 Bonus point if you can come up with a dataset that crashes your PC with Excel going. 
 Also, I'd enphatize the ""free-to-use"" part of R (or Python). For example, compared to SAS, if you simply want to try one solution (ie some kind of cluster), you load the library, and give it a try, no need to pay more, just for trying.  
 To me that's the beauty of it, you can try for free whatever you need, and often that's key in DS, imagine if you'd have to pay for each library you install.","Excel and Data Science - sounds really strange to me. Maybe Excel and 'Data Analysis'. 
 Anyways, I think a good compromise between Excel and R is: KNIME ( http://www.knime.org/knime-analytics-platform ). It's free on the desktop and much easier to get started. You can import / export to Excel but also use R, Python or Java if the ~ 1.000 nodes miss some functionality that you need. Since the workflows are visually created, it's also much easier to show them to someone who doesn't know any programming languages - which is quite an advantage in some companies.","I think the problem is that you are trying to convince your students that by taking your class, they can do data science similar to the level of modern data science, i.e., fancy stuff like image processing, face recognition. You hear this saying most of the time, ""by taking this class, you will..."" What you need to teach them is the love for data and the courage to look through a bunch of data, messing around with them to hopefully make some sense out of them. The moment they can do that, you can call them data scientists and you should feel proud of yourself for now having a new generation of data scientists. After that, if they are very serious about data science, they can go on taking other intense courses that deal with math, statistics, and computer science (programming experience like you said).
I was in the situation similar to your students. I had no CS background but wanted to break into data science and AI by taking some online classes with fancy promises. I ended up wasting tons of money yet found myself in immense frustration (oh, I need to take this class to know this algorithm, oh they are talking about neural networks now so I have to sign up for the other class, etc.)
TL;DR. Tools just account for 1% of the problem you have. With your background, you should have no problem in figuring out the above tasks in Excel in a week.",,,,69.09345204,56.14382107,57.40321761,63.17465155,65.28662352,65.45692994,,,
20139,Gradients for bias terms in backpropagation,python,"I would like to explain the meaning of  db2=np.sum(dz2,axis=0,keepdims=True)  as it also confused me once and it didn't get answered. 
 The derivative of  L  (loss) w.r.t.  b  is the  upstream derivative  multiplied with the  local derivate :
 $$
\frac{ \partial L}{\partial \mathbf{b}} = \frac{ \partial L}{\partial Z} \frac{ \partial Z}{\partial \mathbf{b}}
$$ 
 If we have multiple samples  Z  and  L  are both matrices. b is still a vector. 
 The local derivative is simply a  vector of ones :
 $$
\frac{ \partial Z}{\partial \mathbf{b}} = \frac{\partial}{\partial \mathbf{b}} W \times X + \mathbf{b} = \mathbf{1}
$$ 
 That means our complete derivative is a matrix multiplication, that looks as follows  (e.g. 2 samples with 3 outputs) :
 $$
\frac{\partial L}{\partial Z} \times \mathbf{1} = 
\begin{bmatrix}
. &.  &. \\ 
. &.  &. 
\end{bmatrix}
\begin{bmatrix}
1\\ 
1\\
1\\
\end{bmatrix}
$$ 
 Note that this is  the sum of the rows .  
 And that's where  db2=np.sum(dz2, axis=0, keepdims=True)  comes from. It is simply an abbreviation for the matrix multiplication of the local and the upstream derivatives.","The bias term is very simple, which is why you often don't see it calculated. In fact 
 db2 = dz2 
 So your update rules for bias on a  single item  are: 
 b2 += -alpha * dz2 
 and 
 b1 += -alpha * dz1 
 In terms of the maths, if your loss is $J$, and you know $\frac{\partial J}{\partial z_i}$ for a given neuron $i$ which has bias term $b_i$ . . . 
 $$\frac{\partial J}{\partial b_i} = \frac{\partial J}{\partial z_i} \frac{\partial z_i}{\partial b_i}$$ 
 and  
 $$\frac{\partial z_i}{\partial b_i} = 1$$ 
 because $z_i = (\text{something unaffected by } b_i) + b_i$ 
 
 It looks like the code you copied uses the form 
 db2=np.sum(dz2,axis=0,keepdims=True)
 
 because the network is designed to process examples in (mini-)batches, and you therefore have gradients calculated for more than one example at a time. The sum is squashing the results down to a single update. This would be easier to confirm if you also showed update code for weights.","first, you must correct your formula for the gradient of the sigmoid function.  
 The first derivative of sigmoid function is:  (1−σ(x))σ(x) 
 Your formula for dz2 will become:  dz2 = (1-h2)*h2 * dh2 
 You must use the output of the sigmoid function for  σ(x)  not the gradient. 
 You must sum the gradient for the bias as this gradient comes from many single inputs (the number of inputs = batch size). Thus, we must accumulate them to update the biases of layer 2. However, for the gradients come to layer 1, since they come from many nodes of layer 2, you have to sum all the gradient for updating the biases and weights in layer 1. This case is different from the sum of biases in layer 2.  
 My implement for two fully-connected layers with the activation functions are sigmoid functions: 
 lr = 1e-3
f = lambda x: 1.0/(1.0 + np.exp(-x))
# pass through layer 1
out_l1 = np.dot(x, W_1) + b_1

out_s1 = f(out_l1)

# pass through layer 2
out_l2 = np.dot(x, W_2) + b_2

out_s2 = f(out_l2)

loss = get_loss(out_s2, y)

# BACKWARD PASS
grad = out_s2 - y

d_h2 = (1 - out_s2) * out_s2 * grad

# Accumulate the gradient come from all examples
d_W2 = out_s1.T.dot(d_h2)
d_b2 = np.sum(d_h2, axis=0, keepdims=True)

# sum of gradient come out from prev node:
grad_1 = np.sum(d_W2.T, axis=0, keepdims=True)
d_h1 = (1 - out_l1) * out_l1 * grad_1

d_W1 = x.T.dot(d_h1)
d_b1 = np.sum(d_h1, axis=0, keepdims=True)

W_1 -= d_W1 * lr
b_1 -= d_b1 * lr

W_2 -= d_W2 * lr
b_2 -= d_b2 * lr","I'm going to elaborate a bit more on oezguensi's answer and see if we can resolve the problems with the dimensions which seem a bit off. 
 In the post you had 2 samples with 3 outputs (classes). In that case,  $\frac{\partial L}{\partial Z}$  should have dimensions (3,2):  https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-matrix  . 
 On the other hand  $\mathbf{b}$  should be a (2,1) matrix of bias vectors, since you broadcast the bias twice, once per sample. Each one of these bias vectors has 3 components corresponding to the biases of each linear combination. 
 Let's type it out. Assume each data point has  $D = 3$  features:
So we have:
 $S = 2$  samples (batch size).
 $D = 3$  features.
 $M = 3$  classes (outputs). 
 We have:  $X_{S\times D} = 
\begin{bmatrix} 
    x_{11} & x_{12} & x_{13} & x_{14}\\
    x_{21} & x_{22} & x_{23} & x_{24}\\
\end{bmatrix}$ 
and:  $W_{D\times M} = \begin{bmatrix} 
    w_{11} & w_{12} & w_{13} \\
    w_{21} & w_{22} & w_{23} \\
    w_{31} & w_{32} & w_{33} \\
    w_{41} & w_{42} & w_{43} \\
\end{bmatrix}$ 
 Then we have:
 $Z_{S\times M} = W \times X =
\begin{bmatrix} 
    z_{11} & z_{12} & z_{13} \\
    z_{21} & z_{22} & z_{23} \\
\end{bmatrix}$ 
 For each row of the  $Z$  matrix we have a bias vector  $\overrightarrow{b}_{M \times 1} = (b_1, b_2, b_3)$  when we broadcast we get the bias matrix  $\mathbf{b} = \begin{bmatrix} 
    \overrightarrow{b} \\
    \overrightarrow{b} \\
\end{bmatrix}$ 
 Let's write the derivatives (using the Wikipedia formula above):
 $(\partial{L}/\partial{Z})_{M \times S} = \begin{bmatrix} 
    \partial{L}/\partial{z_{11}} & \partial{L}/\partial{z_{21}}  \\
    \partial{L}/\partial{z_{12}} & \partial{L}/\partial{z_{22}}  \\
    \partial{L}/\partial{z_{13}} & \partial{L}/\partial{z_{23}}   \\
\end{bmatrix}$ 
 The bias:
 $(\partial{Z}/\partial\overrightarrow{b})_{S \times 1} = \begin{bmatrix} 
    1 \\
    1 \\
\end{bmatrix}$ 
 The derivative of the upstream with respect to the bias vector:
 $$\frac{\partial L}{\partial \overrightarrow{b}}  = \frac{\partial L}{\partial Z}\frac{\partial Z}{\partial \overrightarrow{b}}$$ 
 Has shape  $M\times 1$  and is the sum along the  columns  of the   $(\partial{L}/\partial{Z})_{M \times S}$  matrix.
Each entry of this matrix gives you the downstream gradient of the entries of  $\overrightarrow{b}$ . 
 But it's important to note that it is common to give the upstream derivative matrix as its transpose, with shape  $S \times M$ , that is: batch size as rows and classes as columns. In this case, you sum along the rows of the transpose. 
 So just keep an eye on the shape of the upstream gradient to find out which direction to sum.","Instead of use ...
 \begin{bmatrix}
1\\
\vdots\\
1 
\end{bmatrix} 
to express all derivative of loss on bias equal to 1. 
 Try to think of  $\frac{\partial L}{\partial b}$  as: 
 \begin{bmatrix}
1       & \cdots  & 1\\
\vdots  & \cdots  & 1\\
1       & \cdots  & 1
\end{bmatrix} 
 See dL/db at each dim. is a summing of examples at that dim.",,,,,50,59.78943408,59.51232119,53.90368876,51.93290059,,,,
20118,Advantages of pandas dataframe to regular relational database,pandas,"I think the premise of your question has a problem. Pandas is not a ""datastore"" in the way an RDBMS is. Pandas is a Python library for manipulating data that will fit in memory. Disadvantages: 
 
 Pandas does not persist data. It even has a (slow) function called TO_SQL that will persist your pandas data frame to an RDBMS table. 
 Pandas will only handle results that fit in memory, which is easy to fill. You can either use dask to work around that, or you can work on the data in the RDBMS (which uses all sorts of tricks like temp space) to operate on data that exceeds RAM.","In addition to the accepted answer: 
 Relational databases have a large number of bytes of per-row overhead (example:  this question ), which is used for bookkeeping, telling nulls from not nulls, ensuring standards such as  ACID . Every time you read/write a column, not only the few bytes representing the value of this column will be read, but also these bookkeeping bytes will be accessed and possibly updated. 
 In contrast, pandas (also R data.table) is more like an in-memory column store. One column is just an array of values and you are able to use fast numpy vectorized operations / list apprehensions that only access values that you really need. Just that for tables with few primitive columns makes relational databases multiple times slower for many data science use cases.","From the pandas ( Main Page ) 
 
 Python Data Analysis Library¶ 
 pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. 
 
 While pandas can certainly access data via SQL, or from several other data storage methods, its primary purpose is to make it easier when using Python to do data analysis. 
 To that end pandas has various methods available that allow some  relational algebra  operations that can be compared to SQL. 
 Also Pandas provides easy access to  NumPy , which 
 
 is the fundamental package for scientific computing with Python. It contains among other things: 
 
 a powerful N-dimensional array object 
 sophisticated (broadcasting) functions 
 tools for integrating C/C++ and Fortran code 
 useful linear algebra, Fourier transform, and random number capabilities","Pandas is an in-memory data storage tool. This allows you to do very rapid calculations over large amounts of data very quickly. 
 SQL (usually) persistently stores data and is a database. It is also possible to run an in-memory SQL db which may be faster than using pandas, such as SQLite.","SQL allows you to persist and do many different relation transactions and always have it readily available for multiple different uses. Essentially one source of truth or place to go. There is over head for sure. However, some analyses can be very complicated and require significant amount of set based operations which can turn even a small data set into a large one very quickly. I have had data processes that have over 2000 queries that process terabytes in less than 5 mins and can score billions of records for a predictive model at the end and python and numpy scored a fraction of the dataset in 10x time as a relational data store and serve it up to a presentation layer. 
 An additional point, if doing this in the cloud make sure you have a dynamical instance that can scale its memory.  With SQL it is all about having disc and enough compute to get it done in a timely fashion. 
 I see many ways that they can work in synergy. Many data science jobs are what Pandas was designed to do. Some data science jobs are what RDBs were designed to do. Use both in balance. 
 It is all about the right tool to do the right job.",,,,,56.24984921,59.35423427,55.81643391,56.06355852,52.61900751,,,,
20075,When would one use Manhattan distance as opposed to Euclidean distance?,machine-learning,"According to  this interesting paper , Manhattan distance (L1 norm) may be preferable to Euclidean distance (L2 norm) for the case of high dimensional data. 
 The authors of the paper even go a step further and suggest to use Lk norm distances, with a fractional value of k, for very high dimensional data in order to improve the results of distance-based algorithms, like clustering.","I found something which might be  intuition  about this problem in  Hands-On Machine Learning with Scikit-Learn and TensorFlow 
 
 Both the RMSE and the MAE are ways to measure the distance between two
  vectors: the vector of predictions and the vector of target values.
  Various distance measures, or norms, are possible: 
 
 Computing the root of a sum of squares (RMSE) corresponds to the Euclidian norm: it is the notion of distance you are familiar with. It
  is also called the ℓ2 norm(...) 
 Computing the sum of absolutes (MAE) corresponds to the ℓ1 norm,(...). It is sometimes called the Manhattan norm because it
  measures the distance between two points in a city if you can only
  travel along orthogonal city blocks. 
 More generally, (... )ℓ 0 just gives the number of non-zero elements in the vector, and ℓ∞ gives the maximum absolute value in the vector. 
 The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to
  outliers than the MAE. But when  outliers are exponentially rare (like
  in a bell-shaped curve), the RMSE performs very well and is generally
  preferred.","I can suggest a couple ideas, from  wikipedia . 
 
 If you want to place less emphasis on outliers, manhattan distance will try to reduce all errors equally since the gradient has constant magnitude. 
 If your noise is distributed Laplacian, the MLE is found by minimizing the manhattan estimate.","The use of Manhattan distance depends a lot on the kind of co-ordinate system that your dataset is using. While Euclidean distance gives the shortest or minimum distance between two points, Manhattan has specific implementations.  
 For example, if we were to use a Chess dataset, the use of Manhattan distance is more appropriate than Euclidean distance. Another use would be when are interested in knowing the distance between houses which are few blocks apart.  
 Also, you might want to consider Manhattan distance if the input variables are not similar in type (such as age, gender, height, etc.). Due to the curse of dimensionality, we know that Euclidean distance becomes a poor choice as the number of dimensions increases. 
 So in a nutshell: Manhattan distance generally works only if the points are arranged in the form of a grid and the problem which we are working on gives more priority to the distance between the points only along with the grids, but not the geometric distance.","Manhattan distance uses the L1 Norm which encourages sparsity wrt the L2 norm. Therefore if you want to 0 out components which have poor predictive power, then they are a great choice. Whereas the L2 norm would waste time computing small deviations which wouldn’t even affect your answer.","Manhattan Distance is the L1 norm form (L1 norm is the sum of the magnitude of vectors in space), while Euclidean Distance is L2 Norm form (The L2 norm calculates the distance of the vector coordinate from the origin of the vector space.",,,,65.79746106,55.91054363,63.57977955,81.93435592,60.18617586,65.50439823,,,
20071,How do I load FastText pretrained model with Gensim?,nlp,"Here's the link for the methods available for fasttext implementation in gensim  fasttext.py 
 from gensim.models.wrappers import FastText

model = FastText.load_fasttext_format('wiki.simple')

print(model.most_similar('teacher'))
# Output = [('headteacher', 0.8075869083404541), ('schoolteacher', 0.7955552339553833), ('teachers', 0.733420729637146), ('teaches', 0.6839243173599243), ('meacher', 0.6825737357139587), ('teach', 0.6285147070884705), ('taught', 0.6244685649871826), ('teaching', 0.6199781894683838), ('schoolmaster', 0.6037642955780029), ('lessons', 0.5812176465988159)]

print(model.similarity('teacher', 'teaches'))
# Output = 0.683924396754","For  .bin  use:  load_fasttext_format()  (this typically contains full model with parameters, ngrams, etc). 
 For  .vec  use:  load_word2vec_format  (this contains ONLY word-vectors -> no ngrams + you can't update an model). 
 Note :: If you are facing issues with the memory or you are not able to load .bin models, then check the  pyfasttext  model for the same. 
 Credits : Ivan Menshikh (Gensim Maintainer)","Update 04/2020 
 load_fasttext_format()  is now deprecated, the updated way is to load the models is with  gensim.models.fasttext.load_facebook_model()  or  gensim.models.fasttext.load_facebook_vectors()  for binaries and vecs respectively. 
 For example: 
 from gensim.models.fasttext import load_facebook_model

wv = load_facebook_model('<path_to.bin.gz>')","I really wanted to use gensim, but ultimately found that using the native  fasttext  library worked out better for me. The following code you can copy/paste into google colab and will work, out of the box: 
 pip install fasttext 
 import fasttext.util
fasttext.util.download_model('en', if_exists='ignore')  # English
ft = fasttext.load_model('cc.en.300.bin')
 
 Works for out of vocab words too: 
 ft.get_word_vector(""another"")
ft.get_word_vector(""dkjeri37id20hnd"")","The FastText binary format (which is what it looks like you're trying to load) isn't compatible with Gensim's  word2vec  format; the former contains additional information about subword units, which  word2vec  doesn't make use of. 
 There's some discussion of the issue (and a workaround), on the FastText Github page. In short, you'll have to load the text format (available at  https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md ). 
 Once you've loaded the text format, you can use Gensim to save it in binary format, which will dramatically reduce the model size, and speed up future loading. 
 https://github.com/facebookresearch/fastText/issues/171#issuecomment-294295302","Let’s use a pre-trained model rather than training our own word embeddings.
For this, you can download pre-trained vectors from  here .
Each line of this file contains a word and it’s a corresponding n-dimensional vector. We will create a dictionary using this file for mapping each word to its vector representation. 
 from gensim.models import FastText
def load_fasttext():
        print('loading word embeddings...')
        embeddings_index = {}
        f = open('../input/fasttext/wiki.simple.vec',encoding='utf-8')
        for line in tqdm(f):
        values = line.strip().rsplit(' ')
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
        f.close()
        print('found %s word vectors' % len(embeddings_index))
    
        return embeddings_index

embeddings_index=load_fastext()
 
 
 Let’s check the embedding for a word, 
 
 embeddings_index['london'].shape
 
 Here’s a bit more info, from a blog post I wrote for my company, on  FastText  and other document classification methods (for smaller datasets)",,,,70.41190672,65.71890736,73.3607303,65.23900507,64.23117597,59.86491032,,,
19858,Intro to Machine Learning,machine-learning,"Andrew Ng's course on Machine Learning on Coursera  is terrific. I'm doing it right now, and he has wonderfully gauged student understanding. He helps you along and doesn't skip lots of steps. It apparently is  not  his goal to convince you of how smart he is - he's more concerned with explaining everything clearly. Highly recommended!","Try  Datacamp 
 It's a fantastic site with video tutorials and an online editor so you don't need to bother messing around installing an IDE and the right packages and getting them all to work, you can just work in your browser! They have lectures from industry leaders and professors, each short course is a nice bitesize chunk of 4 hours estimated including the interactive exercises. 
 The best part is that because each course is only 4 hours long, you can pick and choose which ones are relevant to you, such as diving straight into machine learning if you already have a good python or R background, and finding out how to properly clean and prepare your data, or engineer new features. 
 Good luck!","Analytics Vidya is of the best sites that contribute to Machine Learning.
Provides tutorials , tutorial links and also solutions for challanges","Apart from Andrew Ng's Machine Learning course and Datacamp I will also add the 10 course specialization - Data Science Specialization by John Hopkins University offered on Coursera. It covers pretty much everything from importing and cleansing of data, to the application of Machine Learning algorithms and showcasing your results.  
 Regarding your project, the first step is ensure what you want to do, what is the problem statement or the right question. Second step, get the data. Stock market data is mostly open for use. Third step, feature selection and engineering which is essentially selecting or building features which will help in determining relationship and finding structure/pattern. There are many machine learning algorithms which you can use. Time series analysis is used mostly for this problem domain but since it is little complex you can try out different classification techniques like kNN, Naive Bayes, SVM to determine if the stock will be good or bad.  
 Choose a data science tool where you can implement your algorithm. It could be either R, Python, SAS, MATLAB to name a few.  
 All the best.",Since you mentioned about in  stock market predictions . I suggest you to checkout the  Machine Learning for trading course at Udacity.,,,,,56.8943164,54.05325896,59.3282658,57.93300878,59.72122824,,,,
19802,Best practices to store Python machine learning models,python,"You may have a look at  nexus  or  dvc  or  datmo . 
 There was recently a  presentation  at at meetup in berlin,  zalandos AI data engineering  meetup.","I would like to suggest 2 more approaches. 
 
 Store them in document storage (eg. mongoDB)  - this method is recommended when your model files are less then 16Mb (or the joblib shards are), then you can store model as binary data. in addition, some ML libraries support model export and import in json (eg. LightGBM), which makes it a perfect candidate for storage in document storage.  Advantages : easy tracking of model generation and easy access,  Disadvantages : things will get messy if model object is too large. 
 Store your model on object storage (eg. Amazon S3)  - this method is good if your models are very large, in this case you get unlimited storage and fairly easy API, you pay more, that is for sure.  Advantages : Unlimited space and ability to store arbitrary file formats.  Disadvantages : cost, and the fact that to do it right you'll need to develop your own tracking system. 
 
 good luck!","I faced this problem (and still face it today) for many years. I really thing that, if you don't provide detailed requirements, you can't expect a serious answer. I explain myself with examples of my work: 
 
 I regularly try multiple variations of the same model to find what parameters work best. It takes several day to train one single model which produces some output that is later used for evaluation. To do so, I make a simple NumPy dump of the model since it is easy to share it between servers, or colleagues. You should avoid pickle since it stores much more (instances of class, libraries...) than just the parameters learned by your model. Importing the model on another machine might not work if the python environment slightly differs.  
 When pushing a model in production, I need 1) a version of the model that I can load fast in case of a server breakdown (typically a binary format, storing only what is necessary such as weights of a neural network) and 2) a way to keep the model in-RAM to quickly deal with the API requests.  
 
 For two different purposes, I need three different formats. Then, more generally speaking, the choice of the format depends on the tools you use. For example, if you work with TensorFlow, you might be interested in their  TensorFlow Serving  system","I have faced this issue for many years, as a data scientist or an ML engg. we have to create a ton of models before coming to a conclusion. Effectively storing all the model's profiles, parameters, and features used is a pain specially if you have multiple notebooks. 
 
 modellogger is a python package that can help you to organise the stuffs and create a full blown summary with dynamic stats on the  call of an function. 
 pip install modellogger 
 
 follow the 3 steps easy  documentation  and voila you have it.","I had also run into this problem several times, so I've created an  open source modelstore Python library  which seeks to tackle the problem of simplifying the best practices around versioning, storing, and downloading models. 
 As others have pointed out, the best practices around this area are still forming. The whole area is fairly straightforward if you are saving  one  model, but starts becoming complicated as you need to store many models or newly trained versions of existing models. I've seen teams save models into shared drives, Cloud Storage/s3 buckets or git repos, manually or using ad hoc scripts. 
 To unify this, there are options like  MLFlow's artifact storage  which is great if you can set up and maintain a tracking server. Or, as others have mentioned, there are tools like DVC. 
 The  modelstore  library unifies the versioning and saving of an ML model into a single  upload()  command, and also provides a  download()  function to get that model back from storage. Here is (broadly) what it looks like: 
 from modelstore import ModelStore

modelstore = ModelStore.from_aws_s3(os.environ[""AWS_BUCKET_NAME""])

model = train() # Replace with your code

# Here's an sklearn example - the library currently supports 9 different ML frameworks
modelstore.sklearn.upload(""my-model"", model)
 
 The  upload()  command will create a tar archive containing your model and some meta-data about it, and upload it to a specific path in your storage.",,,,,50,56.34596427,57.74717256,54.23736421,59.06488843,,,,
19748,Using Machine Learning to create marketing segments,machine-learning,"Wow, what a great question! This is a critical task for anyone who works with data in a marketing function.  
 First , I'll address your question about pairing the prediction to email. Your email is serving as a unique identifier. When you split your data set, you'll have to keep the email and suppress it from the prediction, and use it to match/compare to the validation data.  
 If these are new predictions, you could create a new column in your data that contains the ""predicted"" value. Specifics on this process would be better suited for another community, probably StackOverflow. 
 Second , I'd like to talk about what you're trying to achieve. 
 I think there are two questions ways you could approach this with the work you've already done: 
 
 Identify who is  most likely to subscribe , and focus your efforts on creating nurturing campaigns to ""warm"" these prospects. The final goal is to get a subscription. 
 Here you're going to be using those ""events"" that you mentioned. What exactly do you have? in the past. I've just dumped web activity data and started working with that. You'll have to engineer a lot of features to capture information that your web activity won't report on (these could be # of logins, # of checkouts, time since last visit, this is where you need to get creative). 
 Take this data and try to make some predicte who is likely to subscribe. In some sales industries, this is refereed to as a lead score. 
 Since you're using R, you could try using  XGBoost  - https://cran.r-project.org/web/packages/xgboost/vignettes/xgboostPresentation.html . Since you're trying to predict a binary variable, you could use their decision tree. 
 This is going to take a while, so don't get frustrated if you don't this to production in the first week (or month). Here's some reading: 
 https://datastories.com/gallery/predictive-user-scoring-example 
 https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/ 
 https://rdatascientist.wordpress.com/2015/01/26/predictive-lead-scoring-using-r-first-of-a-two-part-blog-series/ 
 Identify which  types  of customers you want to acquire, and focus on building campaigns to engage these types of customers. The final goal here is to acquire quality customers. 
 This is more marketing and strategizing than it is machine learning. Don't get me wrong, there  is  maching learning in identifying the clusters. Here's a great example I used:  http://www.kimberlycoffey.com/blog/2016/8/k-means-clustering-for-customer-segmentation . 
 Once you identify what the different segments and what behavior they exhibit, you have to team up with the other stakeholders and try to decide what kind of customers do you want? Whats the benefit is getting a customer who makes one large purchase a year vs. a customer who makes five smaller purchases a year? You'll have to take your time identifying customers/prospects who are the most valuable, and plan a strategy around that.","It would be a good idea to create some visualizations of your data before choosing any particular method. Start with this and you might see some useful trends! 
 For starters, I would probably make a binary classifier from logistic regression. Feed a fraction of the data you have for training and see how it performs on the rest. This is one of the simplest approaches and could work very well.","You might want to extract certain features from an email address: 
 Like provider, name etc. Only email address meta infromation *might be too though. 
 If you do not want to use an email address but just map it you can simply use a map of entries to email addresses.","It would make sense if you cluster your Data and check each Cluster's RFM (Recency, Frequency, Monetary) value or the ratio of Subscriptions. Then depending on your budget target the best Clusters and check their email addresses domains or locations (from IP). Then it comes upon discussion and Optimisation on your Marketing plan.","You don't provide a whole lot of detail about the different events triggered but I would imagine that the  arules package  in R would be great at identifying some patterns to examine.   
 Using the apriori algorithm in this way in the domain of marketing is often called a market basket analysis.  It can be quite powerful to highlight relationships and can often give you a start of point for more discoveries.",,,,,55.78461647,51.81867374,50.82126229,52.49004762,55.34633068,,,,
19077,Difference between machine learning and artificial intelligence,machine-learning,"The subject areas Artifical Intelligence and Machine Learning (plus Data Science) are loosely defined, such that it is hard to make strict statements about how they relate. In the general case, it seems that there are parts that overlap, but that they are quite far from being ""the same subject with two different names"" as suggested in the question. 
 The term  Artifical Intelligence  has many possible meanings and interpretations - which version to refer varies by time and by the source using it. Textbooks on artificial intelligence will often cover topics such as search algorithms, logical deduction and other things which are clearly not machine learning as it is practised today. 
 For instance, we could take it to refer to  Artificial General Intelligence  (or ""hard AI""), and it should be clear in this case that at least some form of learning algorithm(s) would be required to meet the goals of AGI. However, it is far less clear how much of AGI can be solved by combining machine learning into complex structures. 
 The term  Machine Learning  has a few different working definitions, but this is a popular one: 
 
 A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. 
 
 This is far more tightly defined than Artificial Intelligence, but still has a lot of scope.  
 The trend to conflate AI and ML appears to be a media and marketing issue, not a technical one. I suspect this is in part due to advances in the last 5-10 years in neural networks. Neural network models have made strong progress, especially in signal processing of images, video, audio. There is also an analogy with biological brains which can be compelling - especially when the subject matter is simplified for consumption by mainstream media.  
 It is worth mentioning Data Science too. Like Artificial Intelligence, the term is somewhat fuzzily defined. Also like AI, Data Science has more to it than just Machine Learning. To Data Science practitioners, ML is part of a toolkit to achieve goals - for some people it is a large part of what they do, for others it is just one part of a wider scope (actually training and refining a ML model might take only a small fraction of a professional data scientist, analyst or statistician's time). I think it is reasonable to state that Artificial Intelligence and Data Science relate to Machine Learning in a similar way.","Machine learning  in layman terms is an algorithm that allows machines to identify patterns in data and then develop a model which can be used to predict unseen data. 
 Artificial Intelligence  is the ability of machines to make intelligent decisions which are equal to or better than their human counterpart. 
 Difference between the two : 
 A.I. is a very broad field of computer intelligence in which machine learning is one of the ways it gains the intelligence to predict outcomes. But AI also contains robotics, speech synthesis, computer vision and others. 
 So if I were to draw a Venn diagram of artificial intelligence then machine learning would be a subset.","Deep Learning is a subset of Machine Learning which is a subset of Artificial Intelligence. Machine learning is a particular approach for AI but not the only one. Symbolic Logic, Bayersian Statistics are a few examples of AI approaches which do not use any kind of machine learning algorithms.","ML, by Tom M. Mitchell: 
 
 A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. 
 
 AI, but not ML: 
 
 SLAM 
 Path finding :  Bellman–Ford algorithm ,  A* search algorithm ,  Dijkstra's algorithm 
 Markov chains 
 cellular automata 
 
 Thank you,  Servan Grüninger , for your help. 
 See also:  How does machine learning relate to artificial intelligence?","A good example of AI, but not machine learning is evolutionary computation. Here instead of learning from experience (as in Tom M. Mitchell's definition) we have genotype changing in each generation of computer program version, measured by its performance at task (phenotype expression in environment). 
 As  Melanie Mitchell  puts it: 
 '...from the earliest days computers were applied... to modeling the brain, mimicking human learning, and
simulating biological evolution... The first has grown into the field of neural networks, the second into machine learning, and the
third into what is now called ""evolutionary computation,""...'
Although, now neural networks are mostly considered as part of machine learning .","As the great Tom Mitchell has said in his book ""Machine Learning is the ability to learn without being explicitly programmed."" 
 Machine learning algorithms are widely employed and are encountered daily. Examples are automatic recommendations when buying a product or voice recognition software that adapts to your voice. 
 AI is any technology that enables a system to demonstrate human-like intelligence. 
 ""If we plug several photos of cats doing different things or in different places into a computer, but all the photos are still tagged as cats, then the computer will learn from each photo it is shown,” said Kamelia Aryafar, Ph.D., director of machine learning at  Overstock . “Eventually, it will recognize that the cat is the common denominator in each set of data, in turn helping the computer learn to identify cats.” 
 When a machine can tell the difference between objects and make a choice to discard or accept them, based on understood criteria, AI is born. In fact, any time a decision is being made by a machine, that is artificial intelligence and has gone beyond mere machine learning.","Let's take the  total Turing test  as an example. A computer is often said to be intelligent if it can pass the total Turing test. 
 A computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer. The total Turing Test also includes a video signal so that the interrogator can test the subject's perceptual abilities, as well as the opportunity for the interrogator to pass physical objects ""through the hatch."" 
 To pass the total Turing Test, the computer would need to possess the
following capabilities: 
 
 natural language processing  to enable it to communicate successfully
in English; 
 knowledge representation  to store what it knows or hears; 
 automated reasoning  to use the stored information to answer questions
and to draw new conclusions; 
 robotics  to manipulate objects and move about; 
 computer vision  to perceive objects, and 
 machine learning  to adapt to new circumstances and to detect and
extrapolate patterns. 
 
 As you may already see,  Machine Learning is a subset of Artificial Intelligence that concerns with the ability of an intelligent agent to  learn .","Artificial intelligence : program that can sense, reason, act and adapt. 
 Machine learning : algorithms whose performance improve as they are exposed to more data over time.",,66.39915662,68.34303918,65.56204842,57.55583349,57.46775921,63.76785951,56.68570568,64.64926261,
18956,Different number of features in train vs test,python,"Even though @Jekaterina Kokatjuhha's answer is accepted, I completely disagree with what it suggests. You should  never  make use of your test set when creating your pipeline. Technically, you don't know what your test set is until your pipeline has been completed. 
 Your original approach is the correct one. 
 To make the process less complex, I'd recommend using  scikit-learn's OneHotEncoder  instead of  pd.get_dummies . When you fit the encoder on your training data, it will keep track of which dummies to create. In your case, this is all the cabins in your training set. This means that when you apply the encoder on your test data, it will create the same amount of columns as for your training data. If your test data contains a cabin that doesn't exist in your training data, it will simply be ignored. You also have the option to throw an error by setting the  handle_unknown  argument to  ""error""","You could concatenate your train and test datasets, crete dummy variables and then separate them dataset. 
 Something like this: 
 train_objs_num = len(train)
dataset = pd.concat(objs=[train, test], axis=0)
dataset = pd.get_dummies(dataset)
train = copy.copy(dataset[:train_objs_num])
test = copy.copy(dataset[train_objs_num:])","As mentioned by @Valentin Calomme the right approach is to use one hot encoding method of scikit learn. The reason merging training and testing data is not beneficial this method fails in case of when you put model into production & then suddenly mapping changes & ultimately leads to failure of the model. 
 Follow below implementation for end to end implementation of that method: 
 import pandas as pd
import pickle
from sklearn.preprocessing import OneHotEncoding

df_trn = pd.read_csv('train.csv')

# You can use other methods to remove nan records
df_trn.dropna(axis=0, inplace=True)

# Always perform following step on training records
cabin_onehot = OneHotEncoder()
cabin_arr = cabin_onehot.fit_transform(df_trn.loc[:, ['Cabin']]).toarray()

# Save this onehot encoding object for reuse purpose
with open('cb.pkl', 'wb') as f:
    pickle.dump(cabin_onehot, f)

# Write code to merge df_trn & cabin_arr
...

# Now let's apply same technique on test records
df_test = df.read_csv('test.csv')

# Load it before using it & same thing we do we when use in production/testing environment
with open('cb.pkl', 'rb') as f:
    cabin_onehot = pickle.load(f)
cabin_arr_test = cabin_onehot.transform(df_test.loc[:, ['Cabin']]).toarray()
 
 In this way you will not face issue of different dimension in test records.","Hope this works.
If the models trained are GLM,DT,RF you can extract the train data column names using the below syntax 
 train_data<-attr(model$terms, 'term.labels')

  df<-as.data.frame(train_data)

  df<-as.data.frame(do.call(rbind,df))

  names(df) <- df[1,]

  df <- df[-1,]
 
 Now,convert categorical columns to dummy variables in the test dataset.
The below line of code will make the train and test data column names same and set train_data column names to 0 which are not present in the test dataset. 
 test_data[setdiff(names(df), names(test_data))] <- 0","You would prefer to concatenate data first and then convert in dummies followed by again splitting them in Training and Testing dataset as suggested by @Jekaterina Kokatjuhha.  
 Though the option of the merging of the dataset could be problem-specific, you may also read the article  here  for getting more understanding about having different observations within Train and Test Dataset.","Different categories in the train and test set is a massive problem that ideally won't occur if you do the train test split properly.  
 Consider using something called as Stratified Shuffle Split and then one-hot-encode the data before modelling further. Stratified Shuffle Split essentially preserves the percentage of the categorical features in each column and then makes the split. This leads to well-balanced Training and Testing data-sets. 
 Regards,",,,,54.80539233,56.84895022,54.05281027,56.78953193,59.92990578,60.86144116,,,
18258,How to force weights to be non-negative in Linear regression,python,"What you are looking for, is the  Non-negative least square regression .
It is a simple optimization problem in quadratic programming where your constraint is that all the coefficients(a.k.a weights) should be positive. 
 Having said that, there is  no standard implementation  of Non-negative least squares in Scikit-Learn.  The pull request is still open . 
 But, looks like  Scipy has implemented the same .  
 PS:  I haven't tried the scipy version. I found it solely by googling around.","I use a workaround with Lasso on Scikit Learn (It is definitely not the best way to do things but it works well). Lasso has a parameter  positive  which can be set to  True  and force the coefficients to be positive. Further, setting the Regularization coefficient  alpha  to lie close to 0 makes the Lasso mimic Linear Regression with no regularization. Here's the code: 
 from sklearn.linear_model import Lasso
lin = Lasso(alpha=0.0001,precompute=True,max_iter=1000,
            positive=True, random_state=9999, selection='random')
lin.fit(X,y)","As of version 0.24, scikit-learn  LinearRegression  includes a parameter  positive , which does exactly that; from the  docs : 
 
 positive :  bool, default=False 
 When set to  True , forces the coefficients to be positive. This option is only supported for dense arrays. 
 New in version 0.24.","There are is a constrained least squares method  scipy.optimize.lsq_linear . Another option is to use an  optimizing solver for Python . Here is one of the options (Gekko) that I maintain that includes coefficient constraints. 
 # Constrained Multiple Linear Regression
import numpy as np
nd = 100 # number of data sets
nc = 5   # number of inputs
x = np.random.rand(nd,nc)
y = np.random.rand(nd)

from gekko import GEKKO
m = GEKKO(remote=False); m.options.IMODE=2
c  = m.Array(m.FV,nc+1)
for ci in c:
    ci.STATUS=1
    ci.LOWER=0
xd = m.Array(m.Param,nc)
for i in range(nc):
    xd[i].value = x[:,i]
yd = m.Param(y); yp = m.Var()
s =  m.sum([c[i]*xd[i] for i in range(nc)])
m.Equation(yp==s+c[-1])
m.Minimize((yd-yp)**2)
m.solve(disp=True)
a = [c[i].value[0] for i in range(nc+1)]
print('Solve time: ' + str(m.options.SOLVETIME))
print('Coefficients: ' + str(a))
 
 It uses the nonlinear solver  IPOPT  to solve the problem. It is a good option for problems that aren't too large because there is some waisted computational effort on calculating exact 1st and 2nd derivatives for possible nonlinear functions. It may be faster for larger problems with the  APOPT  solver with  m.options.SOLVER=1 .","Here is an example of why you would want to do it (and approximately how). 
 I have 3 predictive models of housing prices: linear, gradient boosting, neural network. 
 I want to blend them into a weighted average and find the best weights. 
 I run linear regression, and I get a solution with weights like -3.1, 2.5, 1.5, and some intercept. 
 So what I do instead of using sklearn is: 
 blendlasso = LassoCV(alphas=np.logspace(-6, -3, 7),
                     max_iter=100000,
                     cv=5,
                     fit_intercept=False,
                     positive=True)
 
 And I get positive weights that sum (very close) to 1. In my example, I want the alpha that works best out-of-sample so I use LassoCV with cross-validation. 
 The sklearn docs state that you shouldn't set alpha to 0 for numerical reasons, however, you can also use straight Lasso() and set the alpha parameter as low as you can get away with to get a reasonable answer.",,,,,68.06562258,56.73130066,52.51309213,53.8108083,58.54383853,,,,
17769,How to fill missing value based on other columns in Pandas dataframe?,pandas,"Assuming three columns of your dataframe is  a ,  b  and  c . This is what you want: 
 df['c'] = df.apply(
    lambda row: row['a']*row['b'] if np.isnan(row['c']) else row['c'],
    axis=1
)
 
 Full code: 
 df = pd.DataFrame(
    np.array([[1, 2, 3], [4, 5, np.nan], [7, 8, 9], [3, 2, np.nan], [5, 6, np.nan]]), 
    columns=['a', 'b', 'c']
)
df['c'] = df.apply(
    lambda row: row['a']*row['b'] if np.isnan(row['c']) else row['c'],
    axis=1
)","What about using the  fillna()  method of the dataframe? 
 df['C'].fillna(df.A * df.B)","Another option: 
 df.loc[(pd.isnull(df.C)), 'C'] = df.A * df.B","Assuming that the three columns in your dataframe are  a ,  b  and  c . Then you can do the required operation like this: 
 values = df['a'] * df['b']
df['c'] = values.where(df['c'] == np.nan, others=df['c'])","for i in df[""a""]:
if i%2==0:
    df[""c""].fillna(value=df[""a""]*df[""b""])
else:
    df[""c""].fillna(value=df[""a""]+df[""b""])
 
 A simple solution for beginners.","In the first case you can simply use  fillna : 
 df['c'] = df.c.fillna(df.a * df.b)
 
 In the second case you need to create a temporary column: 
 df['temp'] = np.where(df.a % 2 == 0, df.a * df.b, df.a + df.b)
df['c'] = df.c.fillna(df.temp)
df.drop('temp', axis=1, inplace=True)",,,,53.12734044,53.59924779,50,57.32799145,53.00264761,51.3757649,,,
17759,Encoding features like month and hour as categorial or numeric?,machine-learning,"Have you considered adding the (sine, cosine) transformation of the time of day variable? This will ensure that the 0 and 23 hour for example are close to each other, thus allowing the cyclical nature of the variable to shine through.  
 ( More Info )","The answer depends on the kind of relationships that you want to represent between the time feature, and the target variable. 
 If you encode time as numeric, then you are imposing certain restrictions on the model. For a linear regression model, the effect of time is now monotonic, either the target will increase or decrease with time. For decision trees, time values close to each other will be grouped together. 
 Encoding time as categorical gives the model more flexibility, but in some cases, the model may not have enough data to learn well. One technique that may be useful is to group time values together into some number of sets, and use the set as a categorical attribute. 
 Some example groupings: 
 
 For month, group into quarters or seasons, depending upon the use case. Eg: Jan-Mar, Apr-Jun, etc. 
 For hour-of-day, group into time-of-day buckets: morning, evening, etc, 
 For day-of-week, group into weekday, weekend. 
 
 Each of the above can also be used directly as a categorical attribute as well, given enough data. Further, groupings can also be discovered by data analysis, to complement a domain knowledge based approach.","I recommend using numerical features.  Using categorical features essentially means that you don't consider distance between two categories as relevant (e.g. category 1 is as close to category 2 as it is to category 3). This is definitely not the case for hours or months. 
 However, the issue that you raise is that you want to represent hours and months in a manner where 12 is as close to 11 as it is to 1. In order to achieve that, I recommend going with what was suggested in the comments and using a sine/cosine function before using the hours/months as numerical features.","It depends on which algorithm you're using. 
 If you're using tree-based algorithms like random forest, just pass this question. Categorical encoding isn't necessary for tree-based algorithms. 
 For other algorithms like neural network, I suggest trying both method(continuous & categorical). The effect differs between different situations.","To rephrase the  answer provided by @raghu . One major difference between categorical and numerical features is whether the magnitude of the numbers are comparable, i.e., is 2019 bigger than 2018, or December(12) bigger than March (3)? Not really. While there is a sequential order in these numbers, their magnitude is not comparable. Thus, transforming into a categorical value may make more sense.","Because of all the data you have is well defined I would suggest you a categorical encoding, which is also easier to apply.",,,,53.11748514,54.62024435,68.64565328,52.643194,53.17982315,53.56629818,,,
17675,What are some of the best practices for sharing data and models with colleagues?,machine-learning,"You can try using  dvc , which stands for data version control.
 https://dvc.org/","Honestly once you get to something serious (big or evolving) the main problem is about sharing and updating data. Once a solution is devised for data, it is not really hard to adapt it to models. 
 Depending on the volume and the usage, the data can be stored, exchanged and acessed in a wide range of solutions. It might be old historical/external solution with associated langages (SAS), internal data bases on linux server with a 'lab' interface, clouds solutions or just csv file for tinier data sets. Once this is arranged and the solution to access those data is devised is it rather easy to adapt the solution to models. Sometimes it demand a bit of involvment to handle the rights properly but it shouldn't be too much of a hassle if the architecture is set with that goal in mind. 
 So, regarding models, depending on the context, the solution might be sharing SAS programs on a shared workspace, pickled python models on a linux server or simply Excel files trough mail. More recently, new 'tools' appeared and result in new solution : share experiments
trough notebooks (Jupyter notebooks or R Markdown) and use Git for versionning models.","Listening to the podcast Partially Derivative Episode  ""Data Science On The Silicon Beach""  the host interviews Maksim Percherskiy, Chief Data Officer for the City of San Diego. 
 Talking about the stack he uses for the City of San Diego: (08:50)  ""The way we move data around ... we use  Airflow  [...] and Airflow is just Python.""  Percherskiy continues characterizing the data sharing problem in the context of city government.","For big files, I use cloud storage (Google, Amazon, Microsoft, or whichever ecosystem your company's on), with folders named after the issue/project ticket name/number. These services support file versioning, by the way. Small files I just attach to the ticket. If have to share something small and transient with a handful of people I can use email or our corporate chat application.","To store and share the data amongst colleagues, cloud storage is the option we use (s3, google storage) where you can just have a folder structure to store all your datasets. While there is no specific way to share models, it totally depends on the model type, one thing that's used is making a binary of model (pickle in python) and share that file which you can also encrypt in case you are floating around sensitive data.  
 In case it is an unsupervised learning model you can directly share the codebase.","I'd say this question is much broader, than simple sharing files. How do you perform research with team in agile fashion? Sadly, there's just a few solutions on the market. As mentioned before, most of the people hack it using already available services. Some time ago I've stumbled upon  neptune.ml . It looks nice, though is quite pricey. Most of the time I'm doing something similar. I try to stick to  git-flow  convention and I have separate folder in repo named  research , next to vanilla  git-flow  branches. Also keep your data in separate repo, so you know against which data version you ran your experiments.",,,,51.34158472,56.54421168,53.65971555,51.55386875,65.4407877,52.22943311,,,
17578,Using TensorFlow with Intel GPU,tensorflow,"At this moment, the answer is  no . Tensorflow uses CUDA which means only NVIDIA GPUs are supported. 
 For OpenCL support, you can track the progress  here . 
 BTW, Intel/AMD CPUs are supported. 
 
 The default version of Tensorflow doesn't work with Intel and AMD GPUs, but there are ways to get Tensorflow to work with Intel/AMD GPUs: 
 
 For Intel GPUs, follow this  tutorial from Microsoft . 
 For AMD GPUs, use this  tutorial .","You might want to check out  https://github.com/benoitsteiner/tensorflow-opencl/  which is a fork of Tensorflow with OpenCL support.
If your OS is supported by the fork and you are able to properly install it in your system then you can run Keras on top of it. 
 Note however that integrated GPUs in general do not offer a lot of calculating power, roughly your GPU will be around 125 GFLOPS ( https://en.wikipedia.org/wiki/List_of_Intel_graphics_processing_units#Notes ), that is your CPU is most likely more powerful. For example, comparing your GPU with a Radeon RX 480 or a GeForce GTX 1080 Ti, they are respectively almost 50 and 100 times more powerful.",Keras is an abstraction layer for tensorflow/ theano. You need a nvidia card but tensorflow as well as theano can be used with CPU support only. Instructions can be found on their websites.,There is a document about Intel Optimization for TensorFlow. You can find it  here .,"Adding this bit of info for people around.. 
 Tensorflow can be now activated on Intel-gpus as well.. 
 For this, just create a new environment on anaconda , and do  pip install intel-tensorflow 
 Now, when the needed libararies are installed,
we can do sanity test by a sample program. 
 import tensorflow as tf
import os

def get_mkl_enabled_flag():

    mkl_enabled = False
    major_version = int(tf.__version__.split(""."")[0])
    minor_version = int(tf.__version__.split(""."")[1])
    if major_version >= 2:
        if minor_version < 5:
            from tensorflow.python import _pywrap_util_port
        else:
            from tensorflow.python.util import _pywrap_util_port
            onednn_enabled = int(os.environ.get('TF_ENABLE_ONEDNN_OPTS', '0'))
        mkl_enabled = _pywrap_util_port.IsMklEnabled() or (onednn_enabled == 1)
    else:
        mkl_enabled = tf.pywrap_tensorflow.IsMklEnabled()
    return mkl_enabled

print (""We are using Tensorflow version"", tf.__version__)
print(""MKL enabled :"", get_mkl_enabled_flag())
 
 It should return  MKL enabled : True 
 Note - These optimisations are brought through  (Intel MKL) Intel oneAPI Math Kernel Library 
 More Info","The top answer is out of date. Intel GPUs that support DirectX 12, which include Intel UHD (which won't give you much of a speedup) and the new Intel ARC GPUs (which will give you a speedup in the range of recent Nvidia gaming GPUs) are now natively supported in Tensorflow, since at least version 2.10. 
 This link is to TF 2.10 which mentions it, but when TF releases a new version, this link will point to newer documentation. 
 https://www.tensorflow.org/install/gpu_plugins","There is a  tensorflow-directml  that currently work with Python 3.5-3.7. 
 The  github page  and the  roadmap wiki  seem to indicate it is under active development.",Try Intel extension for Tensorflow  https://intel.github.io/intel-extension-for-tensorflow/latest/docs/install/install_for_xpu.html#,,71.97603195,59.60719896,57.13596254,72.00606948,60.97574691,73.74476631,53.75219476,73.28123081,
17540,make seaborn heatmap bigger,visualization,"I found out how to increase the size of my plot with the following code... 
 plt.subplots(figsize=(20,15))
sns.heatmap(corr)","This would also work. 
 plt.figure(figsize=(20,15))
ax=subplot(111)
sns.heatmap(corr,ax=ax)","plt.figure(figsize=(20,15))
 
 plt  is not always defined, I can use seaborn without  plt . 
 To use the above line you need to also import plt like: 
 from matplotlib import plt","The basic idea is to increase the default figure size in your plotting tool. You need to import matplotlib and set either default figure size or just the current figure size to a bigger one. Also, seaborn is built on top of matplotlib. You need to install and import matplitlib to make the best use of seaborn library.","This will also work and allows for scale to be parameterized. Figure size can even be adjusted after plotting. 
 fig = plt.gcf()  # or by other means, like plt.subplots
figsize = fig.get_size_inches()
fig.set_size_inches(figsize * 1.5)  # scale current size by 1.5
 
 fig.set_size_inches 
 fig.get_size_inches","import seaborn as sn
import matplotlib.pyplot as plt
%matplotlib inline
plt.figure(figsize=(10,7))
sn.heatmap(cm,annot=True)
 
 
 You can increase Heatmap size by using  plt.figure(figsize=(10,7))",,,,57.64935502,58.21996534,54.78632365,62.91817504,50,64.19863656,,,
17294,NLP: What are some popular packages for multi-word tokenization?,nlp,"The multiword tokenizer 'nltk.tokenize.mwe' basically merges a string already divided into tokens, based on a lexicon, from what I understood from the API documentation. 
 One thing you can do is tokenize and tag all words with it's associated part-of-speech (PoS) tag, and then define regular expressions based on the PoS-tags to extract interesting key-phrases.  
 For instance, an example adapted from the  NLTK Book Chapter 7  and   this blog post : 
 def extract_phrases(my_tree, phrase):
   my_phrases = []
   if my_tree.label() == phrase:
      my_phrases.append(my_tree.copy(True))

   for child in my_tree:
       if type(child) is nltk.Tree:
            list_of_phrases = extract_phrases(child, phrase)
            if len(list_of_phrases) > 0:
                my_phrases.extend(list_of_phrases)

    return my_phrases



def main():
    sentences = [""The little yellow dog barked at the cat"",
                 ""He studies Information Technology""]

    grammar = ""NP: {<DT>?<JJ>*<NN>|<NNP>*}""
    cp = nltk.RegexpParser(grammar)

    for x in sentences:
        sentence = pos_tag(tokenize.word_tokenize(x))
        tree = cp.parse(sentence)
        print ""\nNoun phrases:""
        list_of_noun_phrases = extract_phrases(tree, 'NP')
        for phrase in list_of_noun_phrases:
            print phrase, ""_"".join([x[0] for x in phrase.leaves()])
 
 You defined a grammar based on regex over PoS-tags: 
 grammar = ""NP: {<DT>?<JJ>*<NN>|<NNP>*}""
cp = nltk.RegexpParser(grammar)
 
 Then you applied it to a tokenized and tagged sentence, generating a Tree: 
 sentence = pos_tag(tokenize.word_tokenize(x))
tree = cp.parse(sentence)
 
 Then you use  extract_phrases(my_tree, phrase)  to recursively parse the Tree and extract sub-trees labeled as NP. The example above would extract the following noun-phrases: 
 Noun phrases:
(NP The/DT little/JJ yellow/JJ dog/NN) The_little_yellow_dog
(NP the/DT cat/NN) the_cat

Noun phrases:
(NP Information/NNP Technology/NNP) Information_Technology
 
 There is a great blog post by Burton DeWilde about many more ways to extract interesting keyphrases:  Intro to Automatic Keyphrase Extraction","For your problem i think gensim can be very useful, what can be implemented with Gensim library is phrase detection. It is similar to n-gram, but instead of getting all the n-gram by sliding the window, it detects frequently used phrases and stick them together. It statistically walks through the text corpus and identifies the common side-by-side occuring words. 
Following is the way it calculates the best suitable multi word tokens.
 
 Following is the code to use it. it calculates the two word tokens. 
 from gensim.models.phrases import Phrases, Phraser

tokenized_train = [t.split() for t in x_train]
phrases = Phrases(tokenized_train)
bigram = Phraser(phrases)
 
 and this is how you would use it 
 
 Notice the word ""new_york"" which is concatenated, since in the corpus, statistical evidence of both ""new"" and ""york"" words coming together was significant.  
 Moreover, you can go upto n-grams for this not just bi-grams.
Here is the  article  which explains it in detail.","The tokenization process shouldn't be changed even when you are interested in multi words. After all, the words are still the basic tokens. What you should do it to find a way to combine the proper words into term. 
 A simple way to do so is to look for term in which the probability of the term is higher than that of the independent tokens. 
For example P(""White house"") > P(""White"")*P(""House"")
Choosing the proper values of need lift, number of occurrences and term classification can be deduce if you have a dataset of terms form the domain.
If you don't have such a domain then requirement at least 10 occurrences and and a lift of at least 2 (usually it is much higher since each token probability is low) will work quite well. 
 In your case can can also extract terms by combining contexts relevant to your domain (e.g., ""studied X"", ""practiced Y""). 
 Again, you can build complex and elegant models for that but usually, looking for the few next words after the context indicators will be very beneficial.","This  extension of Stanford CoreNLP  to capture MultiWord Expressions(MWE's) worked like a charm for one of my tasks. For Python users, gear up to write some connector code or hack the Java code.","Use  Stanford CoreNLP  library for multi word tokenization. I found it when I was working on similar task and it worked pretty well! 
 Updated: You can use Stanford  CoreNLP pipeline which includes multi word tokenization model. Link of demo for training neural networks with your own data is  here","gensim is one of the best to do nlp tasks on text data, 
 Gensim python library",,,,55.74050035,63.99221709,62.15204104,50,74.50331019,54.17965246,,,
17258,Is there a way to measure correlation between two similar datasets?,correlation,I would take a look at  Canonical correlation Analysis .,"I see a lot of people post this similar question on StackExchange, and the truth is that there is no methodology to compare if data set A looks like set B. You can compare summary statistics, such as means, deviations, min/max, but there's no magical formula to say that data set A looks like B, especially if they are varying data sets by rows and columns. 
 I work at one of the largest credit score/fraud analytics companies in the US. Our models utilize large number of variables. When my team gets a request for a report, we have to look at each individual variable to inspect that the variables are populated as they should be with respect to the context of the client. This is very time consuming, but necessary. Some tasks do not have magical formulas to get around inspecting and digging deep into the data. However, any good data analyst should understand this already. 
 Given your situation, I believe you should identify key statistics of interest to your data/problems. You may also want to look at what distributions look like graphically, as well as how variables relate to others. If for data set A,  Temp  and  Ozone  are positively correlated, and if B is generated through the same source (or similar stochastic process), then B's  Temp  and  Ozone  should also exhibit a similar relationship. 
 My I will illustrate my point via this example: 
 data(""airquality"")
head(airquality)
dim(airquality)

set.seed(123)
indices <- sample(x = 1:153, size = 70, replace = FALSE) ## randomly select 70 obs

A = airquality[indices,]
B = airquality[-indices,]


summary(A$Temp) ## compare quantiles

summary(B$Temp)

plot(A)
plot(B)

plot(density(A$Temp), main = ""Density of Temperature"")
plot(density(B$Temp), main = ""Density of Temperature"")


plot(x = A$Temp, y = A$Ozone, type = ""p"", main = ""Ozone ~ Temp"",
     xlim = c(50, 100), ylim = c(0, 180))
lines(lowess(x = A$Temp, y = A$Ozone), col = ""blue"")
 
 
 plot(x = B$Temp, y = B$Ozone, type = ""p"", main = ""Ozone ~ Temp"",
     xlim = c(50, 100), ylim = c(0, 180))
lines(lowess(x = B$Temp, y = B$Ozone), col = ""blue"")
 
   
 cor(x = A$Temp, y = A$Ozone, method = ""spearman"", use = ""complete.obs"") ## [1] 0.8285805

cor(x = B$Temp, y = B$Ozone, method = ""spearman"", use = ""complete.obs"") ## [1] 0.6924934","Well, if your samples are collections of points, I would separate this in two steps: 
 
 Calculate distances between inner points: choose how to calculate the distance between (1,2,3) and (2,1,3), for instance. Here, depending on the nature of your problem, you could go for something akin to the  euclidean distance  or if you only care about the orientation of the points, something like the  cosine similarity . 
 Summarize all the distances as a single number: depending on your problem, you could get its average, its median or some other quantity. The main idea is to reduce all the numbers to a single one.",If you are interested in the 1-Dimensional distributions you could use a test (like a Kolmogorov-Smirnov test). I would naively expect that while this cant tell you if data is similar it can tell you if it is not. Or you create multidimensional histograms and calculate a Chi2 similar quantity. Obviously this can run into some problems if the parameter space is rather sparsely filled.,"I would think your datasets as ""Clusters"" and there are some distance metrics for clusters.  
 https://stats.stackexchange.com/questions/270951/distance-between-2-clusters",,,,,55.11492757,50.98175884,51.39338291,52.20940251,52.06493994,,,,
17157,Do modern R and/or Python libraries make SQL obsolete?,python,"R and SQL are two completely different beasts. SQL is a language that you can use to query data that is stored in databases as you already experienced. The benefits of SQL versus R lays mostly in the fact of the database server (MS SQL, Oracle, PostgreSQL, MySQL, etc.). 
 Most, if not all, modern database servers permit multiple users to query data from the same data source and insert, update and delete data in the same tables all while ensuring that the data remains consistent. This is essential for say recording a bank transaction. Can you imagine running a bank on R? That's where database servers come in. They ensure ACID properties of procedures run on the database. ACID stands for Atomicity, concurrency, isolation and durability (see  ACID description on wikipedia ). R is a single user platform where everything happens in memory. So, if your computer stops working halfway in a big operation, your data will not be stored. You are also the only person who can access the data. To be clear, R is not considered an alternative for database servers and/or SQL. 
 Another main advantage of database servers is that a good database design will ensure that you can query your database fast by performing query optimization. To achieve this database servers keep track of the design of a table. See for a full discussion of this topic the  wiki page . R cannot perform query optimization. Poor database design,  can lead to slow execution of your queries. Database servers can also perform optimization over queries that query multiple tables if foreign keys are properly used in the database design. 
 The SQL language has a very different syntax and I share your experience that it is shorter to write data munging steps using the data table or dplyr syntax. However, sometimes your data is too big for R or you need to store the results in the database as part of a periodic batch job, which will require to code your logic in SQL. 
 In my experience there are particular use cases for SQL and R/Python. SQL is great for storing business critical data and for allowing multiple people to access, modify, insert and delete data in a centralized environment. For any one-off data munging R and Python are great. If your data munging needs to be periodically executed, you will need to port your R/Python script to SQL.","These aren't even comparable, really.  SQL is a language meant for accessing data, R is a language meant for working with data.  
 SQL isn't an effective tool for munging because it it's difficult to see intermediate steps and when it throws errors, it isn't likely to address the form/quality/structure of your data.  
 My workflow is typically: 
 
 Get raw data from SQL query (in R) 
 Build munging routine 
 If possible, re-write SQL query to accomplish munging I accomplished in R 
 
 Also realize that not all consumers of data use R, but many still interface their platform of choice with data using SQL.","library(dbplyr)  has the correct approach: write everything in R (using the tidyverse) and let the library just-in-time ""compile"" the R code to low-level SQL.  
 Since not all munging is translatable, another approach is the one taken by SQL Server: let R code snippets be invoked from SQL ""select"" commands.","The 1., 2., 3. approach mentioned by HEITZ is in my experience possible extending with an alternative for 3. where you write your data from R (data.table) back into MySQL. 
 So full steps are MySQL->data.table->MySQL 
 If you ensure you use data.table syntax where you don't copy the DT its also RAM-friendly.","In a word  NO  .   SQL  is a powerful concise and flexible way to describe and summarize structured semi structured and even unstructured data - when an appropriate interpreter layer is placed atop it. By the way  sql  is considered a nearly must-have for data scientists. 
 SQL  is a  concise and powerful way to perform its core operations of: 
 
 projections ( select  ..) 
 filtering (  where  ..) 
 grouping / filtering  (  group by  and  having ) 
 basic aggregations (  count ,  sum  ,  avg  ..) 
 joins 
 
 The real power comes when combining results using  inline views  . When I need to do that I will use one of  sqldf ,  pandasql ,  pysparkSql / sparkSql  or a direct rdbms connection.  Writing the same in the most concise manner possible with  data.table  (much better than  data.frame ) or  datatable  (better than  pandas ) is still more clunky,  much  more clunky or nearly impossible depending on the complexity of the queries attempted. 
 For data  munging :  that is a different story: some operations are easily expressed in sql and some not so much.  When however you incorporate  UDF s there is a wider latitude of what can be achieved.  My current task includes a number of  UDF s to do such things as custom  intersection  operations, custom  aggregations , and custom  scoring methods .","I have the exact same problem as OP except I have a lot of experience in SQL. SQL is appropriate periodically, but our database is very regular: it grows less than 6000 rows of data per year! I would guess the absolute total of all available disaggregated rows from the beginning of history is less than 700000. Overrelying on SQL to ask questions of small data is like passing around a magic eightball. Depending on the script you'll draw wildly different results for the same question. 
 Applying tidy principles to SQL is critical, and minimizing it's use for pre-analysis is key. Yes R is preferred once you run SQL to get broad data, but if you don't know R, you can build what you want in Power BI for free and accelerate Queries there in PowerQuery. 
 SQL is hammer, but data cleaning isn't a nail. 
 Obviously for truly big data operations, the initial tables necessitate complex timesuck queries. You need to know how to code that, but as a data scientist I need to be careful to reduce the reliance on SQL and incorporate tidy principles without reinventing the wheel.",,,,59.68173029,60.53083806,64.20229329,50,57.50176332,62.60188892,,,
17146,"Does ensemble (bagging, boosting, stacking, etc) always at least increase performance?",ensemble-modeling,"Under Ensemble you can use Majority Votes, Average, Weights etc to get the final outcome from Ensemble model. To understand it better you can go through this  Link , explained well by Alexander. 
 Now, let us consider that you have 3 models which has an accuracy of 65-70%. Now by stacking these 3 models there is very high chance that you models accuracy would increase. In another scenario you have 3 models model-1: 95%, model-2: 55%, model-3: 45% accuracy, then if you stack them then there is a very good chance it can worsen the result. 
 Conclusion, it all depends on the individual models performance, Ensemble performs well when you combine moderately performing models. 
 Technically there is no proof saying that this method is suitable for this scenario but trail and error might help you to get good results. It is subjective to the business scenario. Similarly, for bagging and boosting. 
 In my experience with Bagging when the model accuracy is bad, I tried using bagging to fit the data better but EOD training accuracy(20% to 10% approx) was decreased but test accuracy was worsened(11% to 20% approx). So, you have to decide which suits your business problem better and take it forward.","The short answer is no. 
 I have worked on several projects that evaluated an ensemble of several classifiers versus the classifies themselves.  In some cases the precision and recall was better with the ensemble, but more often, it was not.  That's not to say it's not worth investigating.  But sometimes, there is one model that does a reasonable job of classifying data, but it can get drowned out in an ensemble.  Perhaps a weighted ensemble might improve the results, but its not a clear cut approach to improving the performance. 
 In practice, I would try several models and then try an ensemble of the models.  If the ensemble is the best, however you define best, go with it.  But sometimes it is easier to just pick the best base model and then figure out how to tune that model.","As you said, you cannot prove mathematically that esembling increases performance, but it generally does. That's reason why gradient boosting and random forests are so popular in kaggle competitions, because they outperform what a decision tree can learn in many ways. 
 As a curiosity, even Neural Networks can be used as ""weak"" learners, as can be seen in  https://arxiv.org/abs/1704.00109 . So, ensembling is a very powerful technique that can be applied in many areas of machine learning. The main problem is that ensembles are not easily interpretable, being way more black-boxy than its weak learners.","If your individual classifiers are better than random guessing , i.e their error rate is less than 0.5 , then ensemble of these classifiers will lead to an increase in performance , i.e , a drop in error rate.  
 You can refer to  Combining Different Models for Ensemble Learning  chapter in  Sebastian Raschka - Python Machine Learning  Book for a mathematical understanding of the same.","In practice, when it comes to stacking, almost all the time (I explain the reasons why in:  Why does stacking work ? ) but a summary could be the following: 
 The reason is that, if you have one model which is good, and all the others are bad, your ""second stage"" model would put all the weights on the best model and possibly ignore the others. 
 Of course, it does not always happens like this and I have seen cases where more feature extraction was better than stacking models (the number of classes was particularly high though). 
 The mathematical making stacking is so effective are: 
 
 Convexity of the loss (penalty) functions. If they are convex (as it often happens), the Jensen's inequality gives a good intuition why the mean of the predictions of various models reduces the error 
 More general decision boundaries. The graphs below show the decision boundaries of two models, and the stacked version of these two models: 
 
 
 
 
 As you can see, the last graph seem to appear to a more general class of functions than the first two graphs, allowing to learn more general boundaries",,,,,62.08257965,59.16558574,60.21948586,60.08745547,55.90703702,,,,
17099,Adding Features To Time Series Model LSTM,machine-learning,"For RNNs (e.g., LSTMs and GRUs), the layer input  is  a list of timesteps, and each timestep  is  a feature tensor. That means that you could have a input tensor like this (in Pythonic notation): 
 # Input tensor to RNN
[
    # Timestep 1
    [ temperature_in_paris, value_of_nasdaq, unemployment_rate ],
    # Timestep 2
    [ temperature_in_paris, value_of_nasdaq, unemployment_rate ],
    # Timestep 3
    [ temperature_in_paris, value_of_nasdaq, unemployment_rate ],
    ...
]
 
 So absolutely, you can have multiple features at each timestep. In my mind, weather is a time series feature: where I live, it happens to be a function of time. So it would be quite reasonable to encode weather information as one of your features in each timestep (with an appropriate encoding, like cloudy=0, sunny=1, etc.). 
 If you have non-time-series data, then it doesn't really make sense to pass it through the LSTM, though. Maybe the LSTM will work anyway, but even if it does, it will probably come at the cost of higher loss / lower accuracy per training time. 
 Alternatively, you can introduce this sort of ""extra"" information into your model outside of the LSTM by means of additional layers. You might have a data flow like this: 
 TIME_SERIES_INPUT ------> LSTM -------\
                                       *---> MERGE ---> [more processing]
AUXILIARY_INPUTS --> [do something] --/
 
 So you would merge your auxiliary inputs into the LSTM outputs, and continue your network from there. Now your model is simply multi-input. 
 For example, let's say that in your particular application, you only keep the last output of the LSTM output sequence. Let's say that it is a vector of length 10. You auxiliary input might be your encoded weather (so a scalar). Your merge layer could simply append the auxiliary weather information onto the end of the LSTM output vector to produce a single vector of length 11. But you don't  need  to just keep the last LSTM output timestep: if the LSTM outputted 100 timesteps, each with a 10-vector of features, you could still tack on your auxiliary weather information, resulting in 100 timesteps, each consisting of a vector of 11 datapoints. 
 The Keras documentation on its  functional API  has a good overview of this. 
 In other cases, as @horaceT points out, you may want to condition the LSTM on non-temporal data. For example, predict the weather tomorrow, given location. In this case, here are three suggestions, each with positive/negatives: 
 
 Have the first timestep contain your conditioning data, since it will effectively ""set"" the internal/hidden state of your RNN. Frankly, I would  not  do this, for a bunch of reasons: your conditioning data needs to be the same shape as the rest of your features, makes it harder to create stateful RNNs (in terms of being really careful to track how you feed data into the network), the network may ""forget"" the conditioning data with enough time (e.g., long training sequences, or long prediction sequences), etc. 
 Include the data as part of the temporal data itself. So each feature vector at a particular timestep includes ""mostly"" time-series data, but then has the conditioning data appended to the end of each feature vector. Will the network learn to recognize this? Probably, but even then, you are creating a harder learning task by polluting the sequence data with non-sequential information. So I would also  discourage  this. 
 Probably the  best  approach would be to directly affect the hidden state of the RNN at time zero. This is the approach taken by  Karpathy and Fei-Fei  and by  Vinyals et al . This is how it works: 
 
 For each training sample, take your condition variables $\vec{x}$. 
 Transform/reshape your condition variables with an affine transformation to get it into the right shape as the internal state of the RNN: $\vec{v} = \mathbf{W} \vec{x} + \vec{b}$ (these $\mathbf{W}$ and $\vec{b}$ are trainable weights). You can obtain it with a Dense layer in keras. 
 For the very first timestep, add $\vec{v}$ to the hidden state of the RNN when calculating its value. 
 
 This approach is the most ""theoretically"" correct, since it properly conditions the RNN on your non-temporal inputs, naturally solves the shape problem, and also avoids polluting your inputs timesteps with additional, non-temporal information. The downside is that this approach often requires graph-level control of your architecture, so if you are using a higher-level abstraction like Keras, you will find it hard to implement unless you add your own layer type.","Based on all the good answers of this thread, I wrote a library to condition on auxiliary inputs. It abstracts all the complexity and has been designed to be as user-friendly as possible: 
 https://github.com/philipperemy/cond_rnn/  (tensorflow) 
 Hope it helps!","There is a function in keras LSTM  reset_states(states) . 
 However the parameter states is the concatination of two states, hidden state h and cell state. 
 States = [h, c]
 
 it would be interesting to know if you should initialize  h  or  c  according to the approaches in the above mentioned papers.","Adam's answer does seem to make the most sense, however, I am not sure about the second statement ""Polluting sequential data with non-sequential information"".  
 So recently I trained a character-level LSTM model, in which I just appended a non-sequential feature in the end of the sequential features. The model learned how to differentiate that pretty well.  
 The question if the model will perform better had I done it Adam's way, is still to be tested. But for people who don't want to go the extra mile, appending non-sequential features to sequential ones works just fine.","This is probably not the most efficient way, but the static variables could be repeated to timeseries length using  tf.tile() .",,,,,58.67179336,50,52.06671642,54.13036003,50,,,,
16904,GBM vs XGBOOST? Key differences?,machine-learning,"Quote from the author of  xgboost : 
 
 Both xgboost and gbm follows the principle of gradient boosting.  There are however, the difference in modeling details. Specifically,  xgboost used a more regularized model formalization to control over-fitting, which gives it better performance. 
 We have updated a comprehensive tutorial on introduction to the model, which you might want to take a look at.  Introduction to Boosted Trees 
 The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost. For model, it might be more suitable to be called as regularized gradient boosting. 
 
 Edit: There's a detailed  guide  of xgboost which shows more differences. 
 References 
 https://www.quora.com/What-is-the-difference-between-the-R-gbm-gradient-boosting-machine-and-xgboost-extreme-gradient-boosting 
 https://xgboost.readthedocs.io/en/latest/tutorials/model.html","In addition to the answer given by Icyblade, the developers of xgboost have made a number of important performance enhancements to different parts of the implementation which make a big difference in speed and memory utilization: 
 
 Use of sparse matrices with sparsity aware algorithms 
 Improved data structures for better processor cache utilization which makes it faster. 
 Better support for multicore processing which reduces overall training time. 
 
 In my experience when using GBM and xgboost while training large datasets (5 million+ records), I've experienced significantly reduced memory utilization (in R) for the same dataset and found it easier to use multiple cores to reduce training time.","One very important difference is  xgboost  has implemented  DART, the dropout regularization for regression trees .   
 References 
 Rashmi, K. V., & Gilad-Bachrach, R. (2015). Dart: Dropouts meet multiple additive regression trees. arXiv preprint arXiv:1505.01866.","I think the difference between the gradient boosting and the Xgboost is in xgboost the algorithm focuses on the computational power, by parallelizing the tree formation which one can see in  this blog . 
 Gradient boosting only focuses on the variance but not the trade off between bias where as the xg boost can also focus on the regularization factor.","XGBoost implementation is buggy.   Crashed silently when training on GPU on v 082  . It happened to me as well on  v 0.90 , so the issue has not been addressed so far, and the ""fix"" provided in GitHub didn't work for me. 
 LGBM 2.3.1  works like a charm out of the box, though installing it requires a little more effort. So far no issues training on GPU. 
 About  XGBoost  being "" so fast "", you should take a look at  these benchmarks .",,,,,65.10154415,59.32252185,54.31493294,57.41303049,57.1401485,,,,
16843,Perceptron learning rate,neural-network,"I agree with  Dawny33 , choosing learning rate only scales w. 
 While training of Perceptron we are trying to determine minima and choosing of learning rate helps us determine how fast we can reach that minima. If we choose larger value of learning rate then we might overshoot that minima and smaller values of learning rate might take long time for convergence. 
 It is okay in case of Perceptron to neglect learning rate because Perceptron algorithm guarantees to find a solution (if one exists) in an upperbound number of steps, in other implementations it is not the case so learning rate becomes a necessity in them.  
 It might be useful in Perceptron algorithm to have learning rate but it's not a necessity.","With regard to the single-layered perceptron (e.g. as described in  wikipedia ), for every initial weights vector $w_0$ and training rate $\eta>0$, you could instead choose $w_0'=\frac{w_0}{\eta}$ and $\eta'=1$. 
 For the same training set, training a perceptron with $w_0,\eta$ would be identical to training with $w_0',\eta'$, in the sense that: 
 
 Both perceptrons would make exactly the same mistakes. 
 After every mistake, each perceptron would update $w$ such that it would define the same hyperplane as the other perceptron. 
 Both perceptrons would make the same amount of mistakes until convergence. 
 
 (For a partial proof and code example, see  here .) 
 
Thus, in case $w_0=0$, the learning rate doesn't matter at all, and in case $w_0\not=0$, the learning rate also doesn't matter, except that it determines where the perceptron starts looking for an appropriate $w$. 
 So although tuning the learning rate might help to speed up the convergence in many other learning algorithms, it doesn't help in the case of the simple version of single-layered perceptron.","The choice of learning rate m does not matter because it just changes
  the scaling of w. 
 
 I agree that it is just the scaling of  w  which is done by the learning rate. 
 Having said that, as I have explained in  this answer , the magnitude of learning rate does play a part in the accuracy of the perceptron.","Some of the answers on this page are misleading. In the perceptron algorithm, the weight vector is a linear combination of the examples on which an error was made, and if you have a constant learning rate, the magnitude of the learning rate simply scales the length of the weight vector. The decision boundary depends on the direction of the weight vector, not the magnitude, so assuming you feed examples into the algorithm in the same order (and you have a positive learning rate) you will obtain the same exact decision boundary regardless of the learning rate.  
 The talk of ""overshooting the minima"" does not apply here, because there are an infinite number of weight vectors with different magnitudes which are all equivalent, and therefore an infinite number of minima. 
The whole beauty of the perceptron algorithm is its simplicity, which makes it less sensitive to hyperparameters like learning rate than, for instance, neural networks. The answer above citing an infinite learning rate is more of an edge case than an informative example - any machine learning algorithm will break if you start setting things to infinity. 
 That being said, it was recently pointed out to me that more complex implementations of learning rates, such as AdaGrad (which maintains a separate learning rate for each feature) can indeed speed up convergence. 
 Long story short, unless you are using something significantly more complex than a single constant learning rate for your perceptron, trying to tune the learning rate will not be useful.","To clarify (for people like myself who are learning from scratch and need basic explanations), what Wikipedia means (if you look through the source) is that the learning rate does not affect eventual convergence, assuming the learning rate is between 0 and 1. A learning rate too large (example: consider an infinite learning rate where the weight vector immediately becomes the training case) can fail to converge to a solution. 
 The learning rate can, however, affect the speed at which you reach convergence (as mentioned in the other answers).",,,,,84.9605298,78.1519011,83.99558213,81.50296186,77.46627702,,,,
16807,"Why mini batch size is better than one single ""batch"" with all training data?",machine-learning,"The key advantage of using minibatch as opposed to the full dataset goes back to the fundamental idea of stochastic gradient descent 1 . 
 In batch gradient descent, you compute the gradient over the entire dataset, averaging over potentially a vast amount of information. It takes lots of memory to do that. But the real handicap is the batch gradient trajectory land you in a bad spot (saddle point). 
 In pure SGD, on the other hand, you update your parameters by adding (minus sign) the gradient computed on a  single  instance of the dataset. Since it's based on one random data point, it's very noisy and may go off in a direction far from the batch gradient. However, the noisiness is exactly what you want in non-convex optimization, because it helps you escape from saddle points or local minima(Theorem 6 in [2]). The disadvantage is it's terribly inefficient and you need to loop over the entire dataset many times to find a good solution. 
 The minibatch methodology is a compromise that injects enough noise to each gradient update, while achieving a relative speedy convergence. 
 1  Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186). Physica-Verlag HD. 
 [2] Ge, R., Huang, F., Jin, C., & Yuan, Y. (2015, June). Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition. In COLT (pp. 797-842). 
 EDIT : 
 I just saw this comment on Yann LeCun's facebook, which gives a fresh perspective on this question (sorry don't know how to link to fb.) 
 
 Training with large minibatches is bad for your health.
More importantly, it's bad for your test error.
Friends dont let friends use minibatches larger than 32.
Let's face it: the  only  people have switched to minibatch sizes larger than one since 2012 is because GPUs are inefficient for batch sizes smaller than 32. That's a terrible reason. It just means our hardware sucks. 
 
 He cited this paper which has just been posted on arXiv few days ago (Apr 2018), which is worth reading, 
 Dominic Masters, Carlo Luschi,  Revisiting Small Batch Training for Deep Neural Networks , arXiv:1804.07612v1 
 From the abstract, 
 
 While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance ... 
 The best performance has been consistently obtained for mini-batch sizes between m=2 and m=32, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.","Memory is  not  really the reason for doing this, because you  could  just accumulate your gradients as you iterate through the dataset, and apply them at the end, but still in SGD you apply them at every step. 
 Reasons that SGD is used so widely are: 
 1) Efficiency.  Typically, especially early in training, the parameter-gradients for different subsets of the data will tend to point in the same direction. So gradients evaluated on 1/100th of the data will point roughly in the same general direction as on the full dataset, but only require 1/100 the computation.  Since convergence on a highly-nonlinear deep network typically requires thousands or millions of iterations no matter how good your gradients are, it makes sense to do many updates based on cheap estimates of the gradient rather than few updates based on good ones. 
 2) Optimization: Noisy updates may allow you to bounce out of bad local optima (though I don't have a source that shows that this matters in practice). 
 3) Generalization.  It seems (see  Zhang et al: Theory of Deep Learning III: Generalization Properties of SGD ) that SGD actually helps generalization by finding ""flat"" minima on the training set, which are more likely to also be minima on the test set.  Intuitively, we can think of SGD as a sort of  Bagging  - by computing our parameters based on many minibatches of the data, we reenforce rules that generalize across minibatches, and cancel rules that don't, thereby making us less prone to overfitting to the training set.","Unless I'm mistaken, the batch size is the number of training instances let seen by the model during a training iteration 
 
 Correct (although I would call it ""weight update step"") 
 
 and epoch is a full turn when each of the training instances have been seen by the model 
 
 Correct 
 
 If so, I cannot see the advantage of iterate over an almost insignificant subset of the training instances several times in contrast with applying a ""max batch"" by expose all the available training instances in each turn to the model (assuming, of course, enough the memory). What is the advantage of this approach? 
 
 Well, pretty much that. You usually don't have enough memory. Lets say we are talking about image classification. ImageNet is a wildly popular dataset. For quite a while, VGG-16D was one of the most popular mod.els. It needs calculcate 15 245 800 floats (in the feature maps) for one 224x224 image. This means about 61MB per image. This is just a rough lower bound on how much memory you need during training for each image.  ImageNet contains several thousand (I think about 1.2 million?) images. While you might have that much main memory, you certainly do not have that much GPU memory. I've seen GPU speeding up things to about 21x. So you definitely want to use the GPU. 
 Also: The time for one mini-batch is much lower. So the question is: Would you rather do n update steps with mini-batch per hour on a GPU or m update steps with batch without GPU, where n >> m.","Aside from the other answers I think it's worth pointing out that there are two quantities which are distinct but often coupled: 
 
 The number of inputs used to compute the gradient of the parameters in each step. 
 
 As others have pointed out, the gradient with respect to a minibatch is an approximation of the true gradient.  The larger the minibatch, the better the approximation. 
 
 The number of inputs collected into an array and computed ""at the same time"" 
 
 The trade off here is purely about performance (memory/cycles). 
 These quantities are typically the same, i.e. the minibatch size, but in principle they can be decoupled.","Mini-Batch Gradient Descent(MBGD) : 
 
 Pros :
 
 It's good at a large dataset while  BGD  is not. 
 It's good at online learning while  BGD  is not. *Online learning is the way which a model incrementally learns from a stream of dataset in real-time. 
 It doesn't need the repreparation of a whole dataset if you want to update a model while  BGD  needs. 
 It can more easily escape local minima or saddle points than  BGD . 
 
 
 Cons :
 
 The computation is less stable than  BGD . 
 It's less strong in noise(noisy data) than  BGD . 
 It gets a less accurate value than  BGD .",,,,,68.0956279,51.57965626,62.71204223,51.3521193,54.31542365,,,,
16729,Why neural networks models do not allow for multiplication of inputs?,neural-network,"Using a low-level library, such as Theano or TensorFlow, it is likely that you can construct new schemes for reducing tensors (maybe via some learnable weight vector etc). In TensorFlow, you also get automated gradient calculations, so you should be able to still define a cost function and use existing optimisers, without needing to analyse the new design yourself or re-write back propagation formulae. 
 The universal approximation theorem  essentially states that in order to have a network that  could  learn a specific function, you don't need anything more than one hidden layer using the standard matrix multiplying 
plus non-linear activation function. So, invention and use of new architectures needs some reason. The reason is usually that these additions improve the speed or scope of what can be learned, they generalise better from training data, or they model something from a problem domain really well. 
 No doubt there have been explorations of all sorts of variations to the standard NN model over time. The ones that have become popular have all proven themselves on some task or other, and often have associated papers demonstrating their usefulness.  
 For example Convolutional Neural Networks have proven themselves good at image-based tasks. The design of a 2-dimensional CNN layer has a logical match to how pixels in an image relate to each other locally - defining edges, textures etc, so the architecture in the model nicely matches to some problem domains. 
 If you can find a good match from your vector multiplication model to a specific problem domain, then that's an indication that it may be worth implementing in order to test that theory. If you just want to explore other NN structures to see whether they work, you can still do so, but without a specific target you will be searching for a problem to solve (unless by chance you stumble upon something generally useful that has been overlooked before). 
 To address the question in the title: 
 
 Why neural networks models do not allow for multiplication of inputs? 
 
 Some low-level ones do (e.g. TensorFlow). However, this is not an architecture that has proven itself useful, so authors of higher-level libraries (e.g. Keras) have no reason to implement or support it. There is the possibility that this situation is an oversight, and the idea is generally useful. In which case, once someone can show this you will find it would get support across actively-developed libraries pretty quickly since it seems straightforward to implement.","The ""universal approximation"" of a standard neural network is not universal. The small letters tell that the target function must be bounded, and that you may need a really wide hidden layer to get a big value range, when the output of each unit is bounded to e.g. (-1,1). Then comes the issue of efficient learning, because ability to represent does not mean ability to learn efficiently. Exploring other types of neurons can be useful. 
 A concrete application of multiplicative neurons could be to have an initial layer that generates an arbitrary number of compounded features from the inputs, generalizing beyond a fixed set of polynomial terms with non-negative integer exponents below a given value. The next layers could consist of standard additive neurons, giving linear combinations of the features. 
 Multiplicative neurons have been explored. Also neurons that interpolate (at least approximately) between addition and multiplication: 
 
 https://arxiv.org/abs/1503.05724 
 https://arxiv.org/abs/1808.00508 
 
 The last one, NALU from Deepmind, apparently is a great success, and exists in community implementations for several NN frameworks. It can be plugged in for a linear layer, although costs more to compute, and has more parameters to train. 
 The logarithm of a product is the sum of logarithms of the factors.  So one way to make a multiplicative neuron layer for  positive  inputs, is to apply a logarithm to the inputs, then run them through a linear layer, then exponentiate the output.  The weights will correspond to exponents. 
 Update: See  this answer .","This is the standard definition of matrix multiplication. Summing those values I mean. You don't have to make it work like that, you can in fact multiply them if you like. Simply do a series of scalar multiplications instead: 
 # Input vector
X = tf.placeholder(""float32"", (-1, 30)) # so a 1x30 input vector

# ""Layer 1""
w_l1 = []
for n in range(50):
  w_l1.append(tf.Variable(tf.random_normal((1,))))

l1 = []
for n in range(50):
  l1.append(X * w_l1[n])

l1_out = l1[0]
for n in range(1, 50):
  l1_out *= l1[n]
 
 Is this close to what you mean ? 
 More generally, I think most of what you want to achieve can be done with the reduce_prod method in tensorflow, and theano has an analogous function. I think it isn't done in practice because it doesn't form a ring over Tensors.","This  paper states (verbatim): 
 
 ... multiplication and division can be computed by depth-3 ANNs","Why can't the input be a vector of all those values, so that I can
  decide to multiply them if I want, or apply another function from the
  vector space to the reals, and then apply the activation function on
  that real value? 
 
 You  CAN  implement that in  Keras , using a custom  Lambda  layer, see  this . 
 So assume that you have a feedforward neural network with 2 layers, which are fully connected. In Keras this means that you have 2  Dense  layers sequentially. Each node of each layer performs a sum of its inputs and thereafter applies the activation function. If you wish to multiply the outputs of the first layer and then feed their products to the next Dense layer, you just need to insert a Lambda layer in between. The new Lambda layer shall multiply -or do whatever you want with- the outputs of the first layer and then feed the result in the other Dense layer. 
 
 Any implementation I have seen does not allow this, and if I wanted to
  do it, it seem I would have to implement my whole neural network
  architecture by myself without the possibility of using Theano, Keras
  or other libraries 
 
 Essentially, you just need to write  only  your custom Lambda layer and  model.add()  it in between any two layers of interest :)",,,,,60.73371498,54.97393095,54.0256507,53.1691015,53.78519595,,,,
16422,Machine Learning vs Deep Learning,machine-learning,"As a research area, Deep Learning is really just a sub-field of Machine Learning as Machine Learning is a sub-field of Artificial Intelligence. 
 1) Unsupervised Feature Learning 
 Conceptually, the first main difference between "" traditional "" (or "" shallow "") Machine Learning and Deep Learning is Unsupervised Feature Learning. 
 As you already know, successfully training a "" traditional "" Machine Learning model  (ex: SVM, XGBoost...)  is only possible after suitable pre-processing and judicious feature extraction to select meaningful information from the data.
That is, good feature vectors contain features distinctive between data points with different labels and consistent among data points with the same label.
Feature Engineering is thus the process of manual feature selection from experts. This is a very important but tedious taks to perform!  
 Unsupervised Feature Learning is a process where the model itself selects features automatically through training.
The topology of a Neural Network organized in layers connected to each other have the nice property of mapping a low-level representation of the data to a higher-level representation. Through training, the network can thus "" decide "" what part of the data matters and what part of the data doesn't. This is particularly interesting in Computer Vision or Natural Language Processing where it is quite hard to manually select or engineer robust features. 
 
 (picture credits: Tony Beltramelli) 
 As an example, let's assume we want to classify cat pictures. Using a Deep Neural Net, we can feed in the raw pixel values that will be mapped to a set of weights by the first layer, then these weights will be mapped to other weights by the second layer, until the last layer allows some weights to be mapped to numbers representing your problem. (ex: in this case the probability of the picture containing a cat) 
 Even though Deep Neural Networks can perform Unsupervised Feature Learning, it doesn't prevent you from doing Feature Engineering yourself to better represent your problem.  Unsupervised Feature Learning, Feature Extraction, and Feature Engineering are not mutually exclusive! 
 Sources: 
 
 http://deeplearning.stanford.edu/tutorial/ 
 https://arxiv.org/abs/1404.7828 
 https://arxiv.org/abs/1512.05616  (Chapter 2, Section 2) 
 
 2) Linear Separability 
 Deep Neural Networks can solve some non-linearly separable problems by bending the feature space such that features become linearly separable.
Once again, this is possible thanks to the network topology organized in layers mapping inputs to new representations of the data. 
 
 (picture credits: Christopher Olah) 
 Sources:
 http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ 
 3) Statistical Invariance 
 Lastly, Deep Neural Networks are surpassing traditional Machine Learning algorithms in some domains because some architectures are showcasing Statistical Invariance (ex: Spacial Statistical Invariance with Convolutional Neural Networks and Temporal Statistical Invariance with Recurrent Neural Networks) 
 Check this Udacity video for more details:
 https://www.youtube.com/watch?v=5PH2Vot-tD4","In addition to what Himanshu Rai said, Deep learning is a subfield which involves the use of neural networks.These neural networks try to learn the underlying distribution by modifying the weights between the layers.
Now, consider the case of image recognition using deep learning:
a neural network model is divided among layers, these layers are connected by links called weights, as the training process begins, these layers adjust the weights such that each layer tries to detect some feature and help the next layer for its processing.The key point to note is we don't explicitly tell the layer to learn to detect edges, or eyes, nose or faces.The model learns to do that itself.Unlike classical machine learning models.","Inspired by Einstein, ""“If you can't explain it to a six year old, you don't understand it yourself.” 
 All of the above answers are very well explained but if one is looking for an easy to remember, abstract difference, here is the best one I know: 
 
 The key difference is Machine Learning only digests data, while Deep Learning can generate and enhance data. It is not only predictive but also generative. 
 
 Source.  Of course there is much more to it but for beginners it can get way too confusing.","While both machine learning and deep learning involve training models to make predictions or decisions based on data, deep learning excels at automatically learning hierarchical representations of complex patterns or features from raw data. 
 This can be best illustrated with an MNIST example. Imagine we are trying to classify images of handwritten digits. In traditional machine learning, we might need to manually select relevant features, like the curvature of lines or the presence of loops. However, deep learning algorithms can automatically learn these features by stacking multiple layers of interconnected neurons. Each layer extracts progressively higher-level features, such as edges, corners, and shapes, leading to a more accurate representation of the input data. 
 This ability to automatically learn hierarchical representations makes deep learning particularly powerful for tasks like image recognition, natural language processing, and speech recognition. It eliminates the need for manual feature engineering, making it more scalable and adaptable to a wide range of problems. 
 So, while both machine learning and deep learning involve training models, deep learning's ability to learn complex features automatically gives it an edge in handling intricate and unstructured data, making it a popular choice for solving challenging problems in various domains.","Okay, think of it like this. In machine learning algirithms, such as linear regression or random forest you give the algorithms a set of features and the target and then it tries to minimize the cost function, so no it doesnt learn any new features, it just learns the weights. Now when you come to deep learning, you have atleast one, (almost always more) hidden layer with a set number of units, these are the features that are being talked about. So a deep learning algorithm doesnt just learn the sets of weights, in that process it also learns the values for hidden units which are complex high level features of the trivial data that you have given. Hence while practicing vanilla machine learning a lot of expertise lies in your ability to engineer features because the algorithm isnt learning any by itself. I hope I answered your question.",,,,,62.74646284,62.05493164,59.8440707,74.22265543,67.47465102,,,,
16342,Unbalanced multiclass data with XGBoost,classification,"scale_pos_weight  is used for binary classification as you stated. It is a more generalized solution to handle imbalanced classes. A good approach when assigning a value to  scale_pos_weight  is: 
 sum(negative instances) / sum(positive instances)
 
 For your specific case, there is another option in order to weight individual data points and take their weights into account while working with the booster, and let the optimization happen regarding their weights so that each point is represented equally. You just need to simply use: 
 xgboost.DMatrix(..., weight = *weight array for individual weights*)
 
 You can define the weights as you like and by doing so, you can even handle imbalances within classes as well as imbalances across different classes.","This answer  by @KeremT is correct. I provide an example for those who still have problems with the exact implementation. 
 weight  parameter in XGBoost is per instance not per class. Therefore, we need to assign the weight of each class to its instances, which is the same thing. 
 For example, if we have three imbalanced classes with ratios 
 class A = 10%
class B = 30%
class C = 60%
 
 Their weights would be (dividing the smallest class by others) 
 class A = 1.000
class B = 0.333
class C = 0.167
 
 Then, if training data is 
 index   class
0       A
1       A
2       B
3       C
4       B
 
 we build the  weight  vector as follows: 
 index   class    weight
0       A        1.000
1       A        1.000
2       B        0.333
3       C        0.167
4       B        0.333","For sklearn version < 0.19 
 Just assign each entry of your train data its class weight. First get the class weights with  class_weight.compute_class_weight  of sklearn then assign each row of the train data its appropriate weight. 
 I assume here that the train data has the column  class  containing the class number. I assumed also that there are  nb_classes  that are from 1 to  nb_classes . 
 from sklearn.utils import class_weight
classes_weights = list(class_weight.compute_class_weight('balanced',
                                             np.unique(train_df['class']),
                                             train_df['class']))

weights = np.ones(y_train.shape[0], dtype = 'float')
for i, val in enumerate(y_train):
    weights[i] = classes_weights[val-1]

xgb_classifier.fit(X, y, sample_weight=weights)
 
 Update for sklearn version >= 0.19 
 There is simpler solution 
 from sklearn.utils import class_weight
classes_weights = class_weight.compute_sample_weight(
    class_weight='balanced',
    y=train_df['class']
)

xgb_classifier.fit(X, y, sample_weight=classes_weights)","Everyone stumbles upon this question when dealing with unbalanced multiclass classification problem using XGBoost in R. I did too! 
 I was looking for an example to better understand how to apply it. Invested almost an hour to find the link mentioned below. For all those who are looking for an example,  here  goes. 
 Thanks  wacax","The parameter  scale_pos_weight  works for two classes (binary classification). 
 The parameter  weight  goes into the xgb.DMatrix function can be used for three or more classes. The weights can be computed like this: 
 weights = total_samples / (n_classes * class_samples * 1.0)",I wonder if using scale_pos_weight and weights in case of binary classification will bring comparable result.,,,,52.45517901,52.46333346,50.50497617,65.43566148,50,50,,,
16318,What is the benefit of splitting tfrecord file into shards?,python,"Splitting TFRecord files into shards helps you shuffle large datasets that won't fit into memory. 
 Imagine you have millions of training examples saved on disk and you want to repeatedly run them through a training process. Furthermore, suppose that for each repetition of the training data (i.e. each epoch) you want to load the data in a completely random order. 
 One approach is to have one file per training example and generate a list of all filenames. Then at the beginning of each epoch you shuffle the list of filenames and load the individual files. The problem with this approach is that you are loading millions of files from random locations on your disk. This can be slow especially on a hard disk drive. Even a RAID 0 array will not help with speed if you are loading millions of small files from random locations. The problem gets even worse if you are accessing the files over a network connection. 
 Another approach is to read the training examples in sequence from one large TFRecord file and shuffle the examples in memory using a shuffle buffer. However, the shuffle buffer typically cannot be larger than the DDR memory available to your CPU. And if the shuffle buffer is significantly smaller than your dataset, then it may not adequately shuffle the data. The data may be ""locally"" shuffled but not ""globally"" shuffled. That is, examples from the beginning of the dataset may not be shuffled with examples from the end of the dataset. 
 A good solution is to use a balanced combination of the above two approaches by splitting your dataset into multiple TFRecord files (called shards). During each epoch you can shuffle the shard filenames to obtain global shuffling and use a shuffle buffer to obtain local shuffling. A good balance will make the shards large enough to prevent disk speed issues but will keep the shards small enough to allow for adequately shuffling by a shuffle buffer. 
 Here are the exact steps: 
 
 Randomly place all training examples into multiple TFRecord files (shards). 
 At the beginning of each epoch, shuffle the list of shard filenames. 
 Read training examples from the shards and pass the examples through a shuffle buffer. Typically, the shuffle buffer should be larger than the shard size to ensure good shuffling across shards. 
 Pass the shuffled examples into your training process.","In researching the benefits of splitting into multiple files, the only  reasonable answer  came from one of the Google folks. 
 They said performance gains are negligible, but I agree that splitting files can help, especially if you want to transfer the dataset to another location. 
 Keep in mind that now you don't need to shuffle before saving, because (currently) recommended method to read TFRecords uses  tf.data.TFRecordDataset  which implements very useful  .shuffle()  method.","For those still wondering: it's so that you can shuffle your data. With your TFrecords in one file, you can't shuffle the order. This is typically necessary with SGD.  
 However, with shards, you can shuffle the order of the shards which allows you to approximate shuffling the data as if you had access to the individual TFRecords. This is clearly better than nothing, and clearly the more shards you have the better this approximation. 
 The alternative is pre-shuffle your data via duplicating it or don't use TFRecords at all.","Splitting a TFRecords file into multiple shards has essentially 3 advantages: 
 
 Easier to shuffle . As others have pointed out, it makes it easy to shuffle the data at a coarse level (before using a shuffle buffer). 
 Faster to download . If the files are spread across multiple servers, downloading several files from different servers in parallel will optimize bandwidth usage (rather than downloading one file from a single server). This can improve performance significantly compared to downloading the data from a single server. 
 Simpler to manipulate . It's easier to deal with 10,000 files of 100MB each rather than with a single 1TB file. Huge files can be a pain to handle: in particular, transfers are much more likely to fail. It's also harder to manipulate subsets of the data when it's all in a single file.","In case you would like to use a TPU, you need a GCS bucket. You can increase the amount of throughput tremendously by having multiple files, so you can have multiple streams and the TPU sit less in ideal",,,,,67.53187623,62.0827005,66.08374754,65.09780757,52.39501963,,,,
16119,The model performance vary between different train-test split?,machine-learning,"What you are experiencing is not a problem, but rather an inherent attribute of all classifiers. The performance of a classifier depends on the training set, therefore the performance will vary with different training sets. 
 To find the best parameters for a specific classifier you will therefore want to vary training and test split (such as in crossvalidation) and choose the parameter set which achieves the best average accuracy or AUC.   
 Finally you will want to test the trained classifier on another dataset - evaluation set - which has not been part of the dataset used in crossvalidation.","While training, your model will not have the same output when you train with different parts of the dataset.  Cross validation is used to help negate this, by rotating the training and validation sets and training more. 
 Your dataset most likely has high variance, given the large jump in accuracy based on different validation sets.  This means that the data is spread out, and can result in overfitting the model.  You can imagine an overfitted model like this: 
 
 The green line represents the overfitted model. 
 Common techniques to reduce overfitting in random forests is k-fold Cross Validation, with k being between 5 and 10, and growing a larger forest.","Most answers fail to address the following problem: even if you split your data into train and test, and perform k-fold cross validation on the training data to obtain the best model, your model's performance on the test data will depend on the initial ""split"" of training and test data. I can see only three solutions to this: 
 
 Do not split the data into training and test and instead do k-fold cross validation on the full data set. 
 Select the test data in some non-random way, for example, the last  $K$  observations in the data set, if the data had collection timestamps, the logic being that you would like to maximise your performance on your latest observations. 
 Use  full data set with Cross Validation, but do a random shuffle split of the data as part of the cross validation","Great answers but the answer also depends on the model usage. A small change in the training data row slice produces a large change in a validation set performance lowers my confidence that this is a good model. When using cross validation, I look at the variance of the performance to gauge if there was a lucky split or general instability. I do not ever blindly take the largest average of the cross validation performance. 
 When I encounter a model with a significant variance measured how you described, I may need to declare a problem if the model will be used in certain business contexts.  
 I may not be able to help the business understand the impact of the model, the best way to use the model in certain situations, the expected performance of the model over the long-term (some models are used once to score and some are scored millions of times over months or longer) and how to monitor the performance over the lifecycle. 
 In these cases, I may need to start from scratch - get more data (rows or columns), feature engineering, redefine my target (broader, narrower or different to achieve a similar business result), look for a different algorithm, or change how we approach this business problem. Or accept the risk, watch it closely, and be prepared to act accordingly. 
 Once again, dependent on the usage of the model if and how big a problem this may be. And how large a variance in model performance is also business context sensitive.",I don't think there is no definite answer for this issue. But my impression is that you need to know some robustness of your model (selection) in the sense that the (selected) model performances are decent and stable regardless of train/test splits.,,,,,67.64142546,54.80634053,63.57870687,58.41057239,68.97905711,,,,
16060,"What is the difference between ""equivariant to translation"" and ""invariant to translation""",neural-network,"Equivariance and invariance  are sometimes used interchangeably in common speech. They have ancient roots in maths and physics. As pointed out by  @Xi'an , you can find previous uses (anterior to Convolutional Neural Networks) in the statistical literature, for instance on the notions of the  invariant estimator  and especially the  Pitman estimator . 
 However, I would like to mention that  it would be better if both terms keep separate meaning , as the prefix  "" in- ""  in  invariant  is privative (meaning ""no variance"" at all), while  "" equi- ""  in  equivariant  refers to ""varying in a similar or equivalent proportion"".  In other words, one  in-  does not vary, the other  equi-  does . 
 Let us start from simple image features, and suppose that image  $I$  has a unique maximum  $m$  at spatial pixel location  $(x_m,y_m)$ , which is here the main classification feature.  In other words: an image and all its translations are ""the same"" .
An interesting property of classifiers is their ability to classify in the same manner some distorted versions  $I'$  of  $I$ , for instance translations by all vectors   $(u,v)$ .  
 The maximum value   $m'$  of  $I'$  is  invariant :  $m'=m$ : the value is the same. While its location will be at  $(x'_m,y'_m)=(x_m-u,y_m-v)$ , and is  equivariant , meaning that  is varies ""equally"" with the distortion .  
 The precise formulations given (in mathematical terms) for equivariance depend on the class of objects and transformations one considers: translation, rotation, scale, shear, shift, etc. So I prefer here to focus on the notion that is most often used in practice (I accept the blame from a theoretical stand-point). 
 Here, translations by vectors   $(u,v)$  of the image (or some more generic actions) can be equipped with a structure of composition, like that  of a group  $G$  (here the group of translations). One specific  $g$  denotes a specific element of the translation group ( translational symmetry ). A function or feature  $f$  is invariant under the group of actions  $G$  if for all images in a class, and for any  $g$ , 
 $$f(g(I)) = f(I)\,.$$   
 In other words: if you change the image by action  $g$ , the values for feature or function  $f$  are the same. 
 It becomes equivariant if there exists another mathematical  structure or action   (often a group again)  $G'$  that reflects 
 the
transformations  (from  $G$ ) in  $I$   in a meaningful way . In other words, such that for each  $g$ , you have some (unique?)  $g' \in G'$  such that  
 $$f(g(I)) = g'(f(I))\,.$$   
 In the above example on the group of translations,  $g$  and  $g'$  are the same (and hence  $G'=G$ ): an integer translation of the image reflects as the exact same translation of the maximum location. This is sometimes refered to as ""same-equivariance"". 
 Another common definition is: 
 $$f(g(I)) = g(f(I))\,.$$   
 I however used potentially different  $G$  and  $G'$  because sometimes  $f(.)$  and  $g(.)$  do not lie in the same domain. This happens for instance in multivariate statistics (see e.g.  Equivariance and invariance properties of multivariate quantile and related functions, and the role of standardisation ).
But here, the uniqueness of the mapping between  $g$  and  $g'$  allows to get back to the original transformation  $g$ .  
 Often, people use the term invariance because the equivariance concept is unknown, or everybody else uses invariance, and equivariance would seem more pedantic.  
 For the record, other related notions (esp. in maths and physics) are termed  covariance ,  contravariance , differential  invariance .  
 In addition, translation-invariance, as least approximate, or in envelope, has been a quest for several signal and image processing tools. Notably, multi-rate (filter-banks) and multi-scale (wavelets or pyramids) transformations have  been design in the past 25 years, for instance under the hood of shift-invariant, cycle-spinning, stationary, complex, dual-tree wavelet transforms (for a review on 2D wavelets,  A panorama on multiscale geometric representations ). The wavelets can absorb a few discrete scale variations. All theses (approximate) invariances  often come with the price of redundancy in the number of transformed coefficients.
But they are more likely to yield shift-invariant, or shift-equivariant features.","The terms are different: 
 
 Equivariant to translation  means that a translation of input features results in an equivalent translation of outputs. So if your pattern 0,3,2,0,0 on the input results in 0,1,0,0 in the output, then the pattern 0,0,3,2,0 might lead to 0,0,1,0 
 Invariant to translation  means that a translation of input features doe not change the outputs at all. So if your pattern 0,3,2,0,0 on the input results in 0,1,0 in the output, then the pattern 0,0,3,2,0 would also lead to 0,1,0 
 
 For feature maps in convolutional networks to be useful, they typically need both properties in some balance. The equivariance allows the network to generalise edge, texture, shape detection in different locations. The invariance allows precise location of the detected features to matter less. These are two complementary types of generalisation for many image processing tasks.","Complementary  to the previous answers - an image often says more than a thousand formulas. 
 
 Source:  AMMI Seminar - Geometric Deep Learning and Reinforcement Learning (2021)","Just adding my 2 cents 
 Regarding an image classification task solved with a typical CNN Architecture consisting of a Backend (Convolutions + NL + possibly Spatial Pooling) which performs Representation Learning and of a Frontend (e.g. Fully Connected Layers, MLP) which solves the specific task, in this case image classification, the idea is to build a function $ f : I \rightarrow L $ able to map from the Spatial Domain $ I $ (Input Image) to the Semantic Domain $ L $ (Label Set) in a 2 step process which is  
 
 Backend (Representation Learning) : $ f : I \rightarrow \mathcal{L} $ maps the Input to the Latent Semantic Space  
 Frontend (Task Specific Solver) : $ f : \mathcal{L} \rightarrow L $ maps from the Latent Semantic Space to the Final Label Space   
 
 and it is performed using the following properties   
 
 spatial equivariance, regarding ConvLayer (Spatial 2D Convolution+NonLin e.g. ReLU) as a shift in the Layer Input produces a shift in the Layer Output (Note: it is about the Layer, not the single Convolution Operator) 
 spatial invariance, regarding the Pooling Operator (e.g. Max Pooling passes over the max value in its receptive field regardless of its spatial position) 
 
 The closer to the input layer, the closer to the purely spatial domain $ I $ and the more important the spatial equivariance property which allows to build spatially equivariant hierarchical (increasingly) semantic representation  
 The closer to the frontend, the closer to the latent purely semantic domain $ \mathcal{L} $ and the more important the spatial invariance as the specific meaning of the image is desired to be independent from the spatial positions of the features  
 Using fully connected layers in the frontend makes the classifier sensitive to feature position at some extent, depending on the backend structure : the deeper it is and the more the translation invariant operator (Pooling) used  
 It has been shown in  Quantifying Translation-Invariance in Convolutional Neural Networks  that to improve the CNN Classifier Translation Invariance, instead of acting on the inductive bias (architecture hence depth, pooling, …) it's more effective to act on the dataset bias (data augmentation)","These answers are kind of unintuitive so let me add my take from the mathematics point of view (should hopefully be clearer). 
 First recall the following concepts from Elementary Group Theory: 
 If  $G$  is a group with identity  $e$  and  $X$  is a set then the left action  $\alpha$  of  $G$  on  $X$  is a function  $\alpha: G \times X \to X$  that satisfies: two properties: 
 
 for  $x \in X$ , we have  $\alpha(e,x) = x$ 
 $\alpha(g, \alpha(h, x)) = \alpha(gh, x)$  for all  $g,h \in G$ 
 
 Intuitively we can say then a group action  $\alpha$  of  $G$  on  $X$  is a group homomorphism from  $G \to Sym(X)$  of all bijections from  $X$  to itself 
 Then recall: 
 
 A G-set is a set with continuous group action  $\mu: G \times X \to X$  where  $X$  has the discrete topology 
 
 Then we get to the good stuff: 
 Equivariance: 
 
 a form of symmetry for functions from one space with symmetry to another.
More formally: 
 
 A function is called \underline{equivariant} when it's domain and co-domain are acted on by the same symmetry group and when the function commutes with the action of the group.
In essence: Let  $G$  be a group, let  $X,Y$  be G-sets, then  $f: X \to Y$  is equivariant if  $f(g \cdot x) = g \cdot f(x)$  for all  $g \in G$  and  $x \in X$ 
 
 In dumbed down terms: equivariant just means unaffected by a specific group action 
 
 
 Now note that: Equivariance generalizes invariance 
 Invariance: When a mathematical object is unchanged after operations or transformations of a certain type are applied to the object e.x. determinant and eigenvectors are invariant under change of basis 
 Citations: 
 [1]  https://en.wikipedia.org/wiki/Equivariant_map 
 [2]  https://en.wikipedia.org/wiki/Group_action 
 [3]  https://ncatlab.org/nlab/show/G-set#:~:text=Given%20a%20topological%20group%20G,of%20the%20discrete%20group%20G . 
 [4]  https://en.wikipedia.org/wiki/Invariant_(mathematics)",,,,,75.90247949,68.87626289,50,62.38405957,60.1515218,,,,
15989,Micro Average vs Macro average Performance in a Multiclass classification setting,multiclass-classification,"Micro- and macro-averages (for whatever metric) will compute slightly different things, and thus their interpretation differs. A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes).  
 To illustrate why, take for example precision $Pr=\frac{TP}{(TP+FP)}$. Let's imagine you have a  One-vs-All  (there is only one correct class output per example) multi-class classification system with four classes and the following numbers when tested: 
 
 Class A: 1 TP and 1 FP 
 Class B: 10 TP and 90 FP 
 Class C: 1 TP and 1 FP 
 Class D: 1 TP and 1 FP 
 
 You can see easily that $Pr_A = Pr_C = Pr_D = 0.5$, whereas $Pr_B=0.1$. 
 
 A macro-average will then compute: $Pr=\frac{0.5+0.1+0.5+0.5}{4}=0.4$ 
 A micro-average will compute: $Pr=\frac{1+10+1+1}{2+100+2+2}=0.123$ 
 
 These are quite different values for precision. Intuitively, in the macro-average the ""good"" precision (0.5) of classes A, C and D is contributing to maintain a ""decent"" overall precision (0.4). While this is technically true (across classes, the average precision is 0.4), it is a bit misleading, since a large number of examples are not properly classified. These examples predominantly correspond to class B, so they only contribute 1/4 towards the average in spite of constituting 94.3% of your test data. The micro-average will adequately capture this class imbalance, and bring the overall precision average down to 0.123 (more in line with the precision of the dominating class B (0.1)). 
 For computational reasons, it may sometimes be more convenient to compute class averages and then macro-average them. If class imbalance is known to be an issue, there are several ways around it. One is to report not only the macro-average, but also its standard deviation (for 3 or more classes). Another is to compute a weighted macro-average, in which each class contribution to the average is weighted by the relative number of examples available for it. In the above scenario, we obtain: 
 $Pr_{macro-mean}={0.25·0.5+0.25·0.1+0.25·0.5+0.25·0.5}=0.4$
$Pr_{macro-stdev}=0.173$ 
 $Pr_{macro-weighted}={0.0189·0.5+0.943·0.1+0.0189·0.5+0.0189·0.5}={0.009+0.094+0.009+0.009}=0.123$ 
 The large standard deviation (0.173) already tells us that the 0.4 average does not stem from a uniform precision among classes, but it might be just easier to compute the weighted macro-average, which in essence is another way of computing the micro-average.","This is the  Original Post . 
 
 In Micro-average method, you sum up the individual true positives, false positives, and false negatives of the system for different sets and the apply them to get the statistics. 
 Tricky, but I found this very interesting. There are two methods by which you can get such average statistic of information retrieval and classification. 
 1. Micro-average Method 
 In Micro-average method, you sum up the individual true positives, false positives, and false negatives of the system for different sets and the apply them to get the statistics. For example, for a set of data, the system's 
 True positive (TP1)  = 12
False positive (FP1) = 9
False negative (FN1) = 3
 
 Then precision (P1) and recall (R1) will be  $57.14 \%=\frac {TP1}{TP1+FP1}$  and  $80\%=\frac {TP1}{TP1+FN1}$ 
 and for a different set of data, the system's 
 True positive (TP2)  = 50
False positive (FP2) = 23
False negative (FN2) = 9
 
 Then precision (P2) and recall (R2) will be 68.49 and 84.75 
 Now, the average precision and recall of the system using the Micro-average method is 
 $\text{Micro-average of precision} = \frac{TP1+TP2}{TP1+TP2+FP1+FP2} = \frac{12+50}{12+50+9+23} = 65.96$ 
 $\text{Micro-average of recall} = \frac{TP1+TP2}{TP1+TP2+FN1+FN2} = \frac{12+50}{12+50+3+9} = 83.78$ 
 The Micro-average F-Score will be simply the harmonic mean of these two figures. 
 2. Macro-average Method 
 The method is straight forward. Just take the average of the precision and recall of the system on different sets. For example, the macro-average precision and recall of the system for the given example is 
 $\text{Macro-average precision} = \frac{P1+P2}{2} = \frac{57.14+68.49}{2} = 62.82$ 
 $\text{Macro-average recall} = \frac{R1+R2}{2} = \frac{80+84.75}{2} = 82.25$ 
 The Macro-average F-Score will be simply the harmonic mean of these two figures. 
 Suitability
Macro-average method can be used when you want to know how the system performs overall across the sets of data. You should not come up with any specific decision with this average. 
 On the other hand, micro-average can be a useful measure when your dataset varies in size.","In a multi-class setting micro-averaged precision and recall are always the same. 
 $$
P = \frac{\sum_c TP_c}{\sum_c TP_c + \sum_c FP_c}\\
R = \frac{\sum_c TP_c}{\sum_c TP_c + \sum_c FN_c}
$$
where c is the class label. 
 Since in a multi-class setting you count  all  false instances it turns out that
$$
\sum_c FP_c = \sum_c FN_c
$$ 
 Hence P = R. In other words, every single False Prediction will be a False Positive for a class, and every Single Negative will be a False Negative for a class. If you treat a binary classification case as a bi-class classification and compute the micro-averaged precision and recall they will be same.  
 The answer given by Rahul is in the case of averaging binary precision and recall from multiple dataset. In which case the micro-averaged precision and recall are different.","Assume that we are classifying an email into one of the three groups: urgent, normal and spam. We compare the predicts with the ground truth labels, then we get the following confusion matrix and the recall and precision for each class.
 
 But how can we derive a single metric that tells us how well the system is doing? There are two methods to combie these values: 
 
 
 In  macroaveraging , we compute the performance for each class, and then average over classes. In  microaveraging , we collect the decisions for all classes into a single confusion matrix, and then compute precision and
recall from that table. The above figure shows the confusion matrix for each class separately, and shows the computation of microaveraged and macroaveraged precision. 
 
 What are the advantages and disadvantages of the two methods? 
 
 As the figure shows, a microaverage is dominated by the more frequent class (in this case spam), since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important. 
 
 
 In your case, since 67.28% of the data fall in class label 1, I guess that class label 1 dominates the microaverage and the performance of that class is better than other classes. If all classes are equally important the macroaverage is fairer. 
 Reference：  Speech and Language Processing","That's how it should be. I had the same result for my research. It seemed weird at first. But precision and recall should be the same while micro-averaging the result of multi-class single-label classifier. This is because if you consider a misclassification c1=c2 (where c1 and c2 are 2 different classes), the misclassification is a false positive (fp) with respect to c2 and false negative (fn) with respect to c1. If you sum the fn and fp for all classes, you get the same number because you are counting each misclassification as fp with respect to one class and fn with respect to another class.","Multiclass Averaging 
 Introduction 
 I refer you to the  original article  for more details. 
 Sklearn  documentation  defines the average briefly: 
 
 'macro' : Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. 
 
 
 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives. 
 
 Macro averaging 
 Macro averaging reduces your multiclass predictions down to multiple sets of binary predictions, calculates the corresponding metric for each of the binary cases, and then averages the results together. As an example, consider precision for the binary case.  $P =\dfrac{TP}{TP+FP}$ 
 In the multiclass case, macro averaging reduces the problem to multiple one-vs-all comparisons. The precision is calculated for each “relevant” column. This process is repeated for the other levels. The results are then averaged together. 
 The formula representation looks like this. For k classes: 
 $ P_{macro} = \dfrac{P_a + P_b + ... + P_n}{k}$ 
 Note that in macro averaging, all classes get equal weight when contributing their portion of the precision value to the total. This might not be a realistic calculation when you have a large amount of class imbalance. In that case, a weighted macro average might make more sense 
 Micro averaging 
 Micro averaging treats the entire set of data as an aggregate result, and calculates 1 metric rather than k metrics that get averaged together. 
 For precision, this works by calculating all of the true positive results for each class and using that as the numerator, and then calculating all of the true positive and false positive results for each class, and using that as the denominator. 
 The formula representation looks like this. For k classes: 
 $ P_{micro} = \dfrac{TP_a + TP_b + ... + TP_n}{\left(TP_a  + TP_b + ... + TP_n\right) + ( FP_a + FP_b + ... + FP_n)}$ 
 In this case, rather than each class having equal weight, each observation gets equal weight. This gives the classes with the most observations more power.","The advantage of using the Macro F1 Score is that it gives equal weight to all data points. 
 For example:
Let's think of it as the F1 micro takes the Sum of all the Recall and Presession of different labels independently, so when we have class imbalance like: 
 T1 = 90% , T2 = 80% , T3=5 
 then F1 Micro gives equal weight to all the class and is not affected by the deviations in the distribution of the class log the Log loss it penalizes small deviations in the class.","I think the reason why macro average is lower than micro average is well explained by pythiest's answer (dominating class has better predictions and so the micro average increase). 
 But the fact that micro average is equal for Precision, Recall and F1 score is because micro averaging these metrics results in overall Accuracy (as micro avg considers all classes as positive). Note that if Precision and Recall are equal then F1 score is just equal to precision/recall.  
 As for the question if the ""weighted macro-average"" always going to equal the ""micro average""? I did some experiments with different no. of classes and different class imbalance and it turns out that this is not necessary true. 
 These statements are made with assumption that we are considering all the classes of same dataset (in contrast to Rahul Reddy Vemireddy's answer)",,77.40776943,73.99084508,62.64646864,51.82962553,56.75884735,75.81761072,56.43178182,77.02120685,
15984,Can we train a neural network to tell if an object is present or not in an Image?,deep-learning,"A standard computer vision and deep learning dataset for this problem was developed by the Canadian Institute for Advanced Research (CIFAR). The CIFAR-10 dataset consists of 60,000 photos divided into 10 classes (hence the name CIFAR-10). Classes include common objects such as airplanes, automobiles, birds, cats and so on. The dataset is split in a standard way, where 50,000 images are used for training a model and the remaining 10,000 for evaluating its performance. Other datasets are MNIST, CIFAR-100 STL-10, SVHN, ILSVRC2012 etc 
 State of the art results are achieved using very large  Convolutional Neural networks . You can learn about state of the are results on CIFAR-10 on  Rodrigo Benenson’s webpage . Model performance is reported in classification accuracy, with very good performance above 90%. 
 Here is a sample  tutorial  on convolutional neural network with caffe and python","Yes.  Simply remove the softmax function after the final layer or replace it with a Sigmoid, and your vanilla classification neural network becomes an object presence detection network. This transforms the problem from classification to multi-classification or regression, and multiple classes can be detected in an image in parallel without the requirement to predict only one or a fixed number of classes.",This problem has been researched in several projects. An interesting overview of articles on the topic can be found  here . For a crash course in image recognition and neural nets see  this  interesting post. I hope this helps you to get started.,"yes you can train but before that, you have to train network to identify the object you want. It means first you need to input dataset to the network with samples.  Let's take an example. If you want to identify chair object from the image then make the dataset of chairs having different designs. So based on that if you give any new chair image as an input to check it will identify that object as a chair from the trained dataset.","I feel awkward for asking this question because all I had to do was add a Threshold to the prediction i.e. after training the model to recognise the objects in the picture ( I used faster-rcnn to find multiple objects). I just had to make a threshold such that, only if the model is at least 50% sure that the object is a car then a car exists in the picture.",,,,,57.45670038,59.31931929,54.3030428,61.80468095,57.84355739,,,,
15903,Why do convolutional neural networks work?,machine-learning,"Actually I guess the question is a bit broad! Anyway.  
 Understanding Convolution Nets 
 What is learned in  ConvNets  tries to minimize the cost function to categorize the inputs correctly in classification tasks. All parameter changing and learned filters are in order to achieve the mentioned goal.  
 Learned Features in Different Layers 
 They try to reduce the cost by learning low level, sometimes meaningless, features like horizontal and vertical lines in their first layers and then stacking them to make abstract shapes, which often have meaning, in their last layers. For illustrating this fig. 1, which has been used from  here , can be considered. The input is the bus and the gird shows the activations after passing the input through different filters in the first layer. As it can be seen the red frame which is the activation of a filter, which its parameters have been learned, has been activated for relatively horizontal edges. The blue frame has been activated for relatively vertical edges. It is possible that  ConvNets  learn unknown filters that are useful and we, as e.g. computer vision practitioners, have not discovered that they may be useful. The best part of these nets is that they try to find appropriate filters by their own and don't use our limited discovered filters. They learn filters to reduce the amount of cost function. As mentioned these filters are not necessarily known.  
 
 In deeper layers, the features learned in previous layers come together and make shapes which often have meaning. In  this paper  it has been discussed that these layers may have activations which are meaningful to us or the concepts which have meaning to us, as human beings, may be distributed among other activations. In fig. 2 the green frame shows the activatins of a filter in the fifth layer of a  ConvNet . This filter cares about the faces. Suppose that the red one cares about hair. These have meaning. As it can be seen there are other activations that have been activated right in the position of typical faces in the input, the green frame is one of them; The blue frame is another example of these. Accordingly, abstraction of shapes can be learned by a filter or numerous filters. In other words, each concept, like face and its components, can be distributed among the filters. In cases where the concepts are distributed among different layers, if someone look at each of them, they may be sophisticated. The information is distributed among them and for understanding that information all of those filters and their activations have to be considered although they may seem so much complicated. 
 
 CNNs  should not be considered as black boxes at all.  Zeiler et all  in  this amazing paper  have discussed the  development of better models is reduced to trial and error  if you don't have understanding of what is done inside these nets. This paper tries to visualize the feature maps in  ConvNets . 
 Capability to Handle Different Transformations to Generalize 
 ConvNets  use  pooling  layers not only to reduce the number of parameters but also to have the capability to be insensitive to the exact position of each feature. Also the use of them enables the layers to learn different features which means first layers learn simple low level features like edges or arcs, and deeper layers learn more complicated features like eyes or eyebrows.  Max Pooling  e.g. tries to investigate whether a special feature exists in a special region or not. The idea of  pooling  layers is so useful but it is just capable to handle transition among other transformations. Although filters in different layers try to find different patterns, e.g. a rotated face is learned using different layers than a usual face,  CNNs  by there own do not have any layer to handle other transformations. To illustrate this suppose that you want to learn simple faces without any rotation with a minimal net. In this case your model may do that perfectly. suppose that you are asked to learn all kind of faces with arbitrary face rotation. In this case your model has to be much more bigger than the previous learned net. The reason is that there have to be filters to learn these rotations in the input. Unfortunately these are not all transformations. Your input may also be distorted too. These cases made  Max Jaderberg et all  angry. They composed  this  paper to deal with these problems in order to settle down our anger as theirs. 
 Convolutional Neural Networks Do Work 
 Finally after referring to these points, they work because they try to find patterns in the input data. They stack them to make abstract concepts by there convolution layers. They try to find out whether the input data has each of these concepts or not in there dense layers to figure out which class the input data belongs to.  
 I add some links which are helpful: 
 
 Understanding convolution operation 
 Understanding arithmetic behind  ConvNets 
 Useful toolboxes for tracking what happens in these nets","ConvNets work because they exploit feature locality. They do it at different granularities, therefore being able to model  hierarchically  higher level features. They are translation invariant thanks to pooling units. They are not rotation-invariant  per se , but they usually converge to filters that are  rotated versions of the same filters , hence supporting rotated inputs. 
 I know of no other neural architecture that profits from feature locality in the same sense as ConvNets do.","Convolutional neural networks work because it's a good extension from the standard deep-learning algorithm. 
 Given unlimited resources and money, there is no need for convolutional because the standard algorithm will also work. However, convolutional is more efficient because it  reduces the number of parameters . The reduction is possible because it takes advantage of feature locality, exactly what @ncasas writes.","One should never forget the other components in a typical ConvNet. The convolution filters pick out the spatial invariant features, like edges and circles. These features are quantified in a pooling layer which follows the C layer. Finally, they are fed into (usually) multiple fully connected layers (fc). Credit must be given to these fully connected layers which are nothing more than what you find in any ordinary MLP.","Building on  ncasas  and  horaceT  answers, ConvNets are very efficient because:   
 
 They are invavriant to geometrical transformations and learn features that get increasingly complicated and detailed, hence being powerful hierarchical  feature extractors  thanks to the convolutional layers. 
 They combine the extracted features and aggregate them in a non linear fashion to predict the output and therefore being  robust classifiers  thanks to the fully connected layers. 
 
 If you want to learn more about convnets and the different blocks building them as well as the intuitions that underly, here's a  post  I recently wrote on my personal blog that goes through the details.","Is it known why convolutional neural networks always end up learning increasingly sophisticated features as we go up the layers? 
 This is pure mathematics. A neural network, at the end of the day, is a big mathematical function. And the deeper the network, the bigger the function it represents. And by bigger, I obviously mean high-dimensional. The features learned are more sophisticated because they are the results of more sophisticated functions. 
 What caused them create such a stack of features 
 Interestingly enough, conventional neural networks were inspired by our own, actually cat's, biology.  Hubel and Wiesel  conducted experiments on the visual cortex of cats, and they realized that light was perceived by stacks of optic fields. This is what inspired convolutional layer and a deeper architecture.",,,,54.18278917,52.20573109,72.07317781,52.17287507,52.08202256,64.5574912,,,
15812,Check similarity between time series,python,"There are many ways in which you can compute a distance between time series, and the method to use will depend on your data. 
 As stated by other answers,  Dynamic Time Warping  may be the way to go. However, this method assumes that there may be a non-linear warp between different parts of the time series. 
 If you are not expecting warping or delays in the signal, something as simple as Euclidean distance may be a better way to go. Of course, you should apply Euclidean distance only after you applied some preprocessing (for instance, amplitude scaling). 
 Take a look to  this presentation , that introduces the pros and cons of these methods and discuss preprocessing in more detail:","Sounds like a job for  Dynamic Time Warping , there are implementations in Python and R.","First, you should define what you mean with similarity and a corresponding metric to measure it. The second step is to apply the metric to (A, D), (B, D), etc. Then, the set that gives the smallest value is the one with the highest similarity. If you find that your metric does not what you want, simply change it until it meets your requirements. You need to be clear about what you mean with ""similarity"" and how to measure it though. 
 One possibility is to interpret the sets of parameters as a point in a N-dimensional space and calculate the average euclidean distance.","I wrote this  tutorial  a while back to precisely provide guidance on these issues.
It covers four ways to quantify similarity (synchrony) between time series data using Pearson correlation, time-lagged cross correlation, dynamic time warping (as mentioned earlier), and instantaneous phase synchrony. What you choose to use will depend on how you define similarity and the characteristics of your data.","In addition to the provided methods, I have found  Pearson Correlation Coefficient  (also mentioned in  this answer ) and  Cosine Similarity  metrics to be useful in this scenario. 
 In practice, Pearson Correlation Coefficient seems to be better than Cosine Similarity though. The reason is that Pearson Correlation Coefficient is invariant to scaling of a series (~adding a constant). 
 Here is a little proof: 
 If you take a look at the output [17], cosine similarity decreases when we add a constant to the original series! On the other hand in output [18], the Pearson correlation coefficient values do not change to scaling. For this reason, in my case, I have decided to go with the Pearson Correlation Coefficient.","You could just check the  RMSE  between  A ,  B , C  to  D  and take the minimal one.",,,,61.54407976,52.79311102,54.86615883,64.80434172,54.6797919,57.4711386,,,
15804,Which train test split performs better: 50:50 or 60:40?,machine-learning,"It sounds like you have a lot of data, so probably a simple train-test split is enough. No need for cross validation. 
 I would just use something like 75-25. That is in fact the default value in  sklearn. 
 I would use less data in the training only if your algorithm is too slow and cannot cope with the extra data. In that case, instead of throwing away the data, you might as well use it for validation or for testing hyperparameters. 
 All this being said, more important than all is how you split the data. You should make sure that customers in the testing data are not in the training data to make sure your algorithm is generalizing, and not merely memorizing customers. This is standard procedure in medical data mining, and it is very important. Make sure you do not have customer overlap. 
 You might also want to make sure that the distribution of the variable you want to predict is similar in the training and the testing data.","In a draft copy currently being written by Andrew Ng, he discusses about the amount of data in train-test dataset. My understanding from the book, The traditional and most common value is 70-30 or 75-25. If you have 10k or 30k samples, it is fine to go with 70-30 split. But when dealing with Big-data, for example if you have 1 million samples, it is not recommended to have 30k samples as test data, so in that case, 90-10 is actually okay. Because 10k test samples can pretty much provide an intuition about the model. 
 in brief: for less samples, go with recommended 70-30 split, for much higher samples, go with number of samples 
 Draft copy link :  ML Yearning","I will assume that the dataset here is being split into training and validation sets.  
 When you split up your dataset into training and validation sets, you need to take care of that you don't throw away too much data for validation because it is generally seen that with more training data, we get better results in most of the cases. So 50:50 is too bad, 60:40 is fine but not that good. You can make it 80:20 or 75:25 for getting better results.","Any train-test split which has more data in the training set will most likely give you better accuracy as calculated on that test set. So the direct answer to your question is 60:40. And 99:1 would even give better accuracy... 
 However, at the end of the day, you are not interested in the accuracy on your test set. You are interested in the ""real"" accuracy, which gets estimated by your test set. And you better want to make sure that the test set can predict that accuracy well. 
 So, which split should you pick? 
 
 Make sure ""enough"" data is in the test set. What ""enough"" means depends on your dataset (the number of classes, number of features) 
 If you rather want a good estimate of your real error, make the test set bigger 
 If you doubt that you can get more training data and you think the more data in your training set will improve your model (in real, not on the test set) a lot, then ""sacrifice"" a bit for the training set. 
 
 Whatever you do, make sure you define your training set before you start your experiments.","I read all answers, I think the simplest answer to this question is based on the understanding of the train - test strategy. There is NO exact or correct answer to this question. Any split that can guarantee I am not under fitting or over fitting the data is a good a split. under fitting and over fitting are two different problems and are directly connected to the bias error and the variance error.
You are highly recommended to read these two tutorials:
 Understanding the Bias-Variance Tradeoff  &  Accurately Measuring Model Prediction Error .","If you have enough data, then you can actually go for a 50-50 split but there is no such thing as what would be better, depends completely on the amount of data you have and the complexity of the task you are trying to perform.If you train it on enough data, the size of the test set is of no concern. The whole reason for splits comes from the fact that we often have limited and finite data and we want to make the best use of it and train on as much data as we can. So go by the heuristic, and do a 75-25 split. But don't forget to cross-validate on the training set, I would recommend a stratified-K-fold. If your performance metric is suffering, the split would be the last reason for it.",,,,61.38838928,58.20913511,58.02391578,62.60924808,56.75366235,61.88605864,,,
15590,Deep Learning Project to Predict Stock Prices,deep-learning,"Essentially what I would love to do is create an A.I. app that will be fed the same data that the ""experts"" had and see if I can create something more accurate and beat them at it. Is this a viable approach? 
 
 Sure, you can use one or more supervised learning techniques to train a model here. You have features, a target variable and ground truth for that variable.  
 In addition to applying ML you have learned, all you need to do to test your application fairly is reserve some of the data you have with expert predictions for comparison as test data (i.e. do not train using it).  
 I would caveat that with some additional thoughts: 
 
 You haven't really outlined an ""approach"" here, other than mentioning use of ML. 
 Be careful not to leak future data back into the predictive model when building a test version. 
 Predicting stock and markets is hard, because they react to their own predictability and many professional organisations trade on the slightest advantage they can calculate, with experienced and highly competent staff both gathering and analysing data.  
 
 
 Not directly part of the answer, but to anyone just starting out and discovering machine learning, and finding this Q&A: 
 Please don't imagine rich rewards from predicting markets using stats at home, it doesn't happen. If you think that this is a route to ""beating the market"" be aware that you are far from the first to think of doing this, and such a plan can be summarised like this: 
 
 Market Data + ML 
 ??? 
 Profit 
 
 You can fill in the ??? by learning  loads  about financial markets - i.e. essentially by becoming one of the experts. ML is not a short-cut, but it might be a useful tool if you are, or plan to be, a market analyst.","typically I would add this as a comment but, since my score threshold < 50, I am unable to do so - hence the ""Answer"" response 
 If you're interested in running ML algorithms against historic and future index prices, you might be interested in  Quantopian - Kaggle for Finance Quants . 
 At Quantopian, you can upload, test, and compare your results with other ML Finance Quants. Additionally, you'll learn about certain financial metrics/ratios that are native to the financial sector.","You can take a look at this. 
 https://towardsdatascience.com/simple-stock-price-prediction-with-ml-in-python-learners-guide-to-ml-76896910e2ba 
 This is worth a look too. 
 https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/ 
 Finally, consider this technique, directly below.  This is portfolio optimization, not stock price prediction.  After working on Wall Street for the past 15 years, I've found stock price prediction to be quite useless.  Portfolio optimization, on the other hand, can be lucrative.  I've, personally, made millions and millions in profits, using simple techniques like the one described below. 
 https://www.pythonforfinance.net/2017/01/21/investment-portfolio-optimisation-with-python/","I think you are missing some points about the 'experts' you talk about : 
 
 They have been around a long time, they already make a lot of money out of it, and they want to stay it that way 
 They already use advanced techniques for that (quantitative finance), and always try the new things like ML 
 They are at the top of their academic field / employ people that are the top of their academic field 
 They have all the hardware / software they want for doing so 
 
 Yes, you can use some ML techniques to predict the stock market, but those experts you seems to dismiss are way ahead, see this Quant SE questions from 2011 :  https://quant.stackexchange.com/questions/111/how-can-i-go-about-applying-machine-learning-algorithms-to-stock-markets 
 Actually, applications of ML in finance know a recent boom because those experts start going public with their methods, see  Advances in Financial Machine Learning , and its companion implementation  MLfinLab )","Essentially what I would love to do is create an A.I. app that will be fed the same data that the ""experts"" had and see if I can create something more accurate and beat them at it. Is this a viable approach?  
 
 In theory yes, but not in practice. Plesae consider that a zillion of uber-skilled people already tried to do it, and NOBODY have ever been able to beat the stock market. I applied RNNs to cryptocurrency trading and the results were mediocre. 
 The inventor of Keras François Chollet, at the end of his RNN forecast chapter in  Deep Learning with Python , Manning, p. 224, wrote: 
 
 Markets have very different statistical characteristics than natural
  phenomena such as weather patterns. Trying to use machine learning to
  beat markets, when you only have access to publicly available data, is
  a difficult endeavor, and you’re likely to waste your time and
  resources with nothing to show for it. 
 Always remember that when it comes to markets, past performance is not
  a good predictor of future returns—looking in the rear-view mirror is
  a bad way to drive. Machine learning, on the other hand, is applicable
  to datasets where the past is a good predictor of the future. 
 
 I'm sorry to be so negative about this, but I think he's 100% right.",,,,,56.65175514,52.26395251,66.43099997,55.31555476,53.59631784,,,,
15589,Remove part of string in R,r,"You may use  gsub  function 
 > c <-  ""ce7382""
> gsub(""[a-zA-Z ]"", """", c)
[1] ""7382""
 
 Feel free to add other characters you need to remove to the regexp and / or to cast the result to number with  as.numeric .","If the undesired characters are constant as in the example, like  ce7380  where the  ce  is unwanted, one may try the following: 
 library(stringr)
df <- df %>%
      mutate_at(""INTERACTOR_A"", str_replace, ""ce"", """")
 
 This instructs R to perform the mutation function in the column  INTERACTOR_A  and replace the constant  ce  with nothing. 
 If the undesired characters change from row to row, then other regex methods offered here may be more appropriate.","Try this if you are not sure about characters in string: 
 {as.numeric(gsub("","","""", x)}","Similar to one of the earlier, you can also apply the logic of extracting everything starting from (i.e. including) the first numeric digit: 
 interactor <- c(""ce7380"", ""ce7382"", ""ce7388"")
x <- gregexpr(""[0-9]+"", interactor)
x <- unlist(regmatches(interactor, x))
x
## [1] ""7380"" ""7382"" ""7388""","I'd just do it like so: 
 library(roperators)

# either 
this_text <- c('ce7380', 'ce5932', 'ce1234')

# make a new text vector:
new_text <- this_text %-% '[a-z]'

# or make an integer vector:
new_number <- int(this_text %-% '[a-z]')

# OR change this_text in-place
this_text <- c('ce7380', 'ce5932', 'ce1234')

this_text %-=% '[a-z]'","This is an old question but I hope this helps.  
 md = data.frame(INTERACTOR_A = c(""ce7380"", ""ce7380"", ""ce7382"", ""ce7382"", ""ce7382"", 
""ce7388"", ""ce7388""), 
                INTERACTOR_B = c(""ce6058"", ""ce13812"", ""ce7382"", ""ce5255"", ""ce1103"", 
""ce523"", ""ce8534""))

md_1 = md %>% mutate(INTERACTOR_A = as.character(INTERACTOR_A),
                INTERACTOR_B = as.character(INTERACTOR_B)) %>% 
            mutate(INTERACTOR_A = substr(INTERACTOR_A, 3, nchar(INTERACTOR_A)) %>% 
as.numeric(),
                 INTERACTOR_B = substr(INTERACTOR_B, 3, nchar(INTERACTOR_B)) %>% 
as.numeric())","Although this is an old question, I am still hoping that my answer helps. 
 There are two ways you can deal with this problem: 
 df = data.frame(INTERACTOR_A = c(""ce7380"",""ce7380"",'ce7382','ce7382','ce7382','ce7388','ce7388'),
            INTERACTOR_B = c(""ce6058"",""ce13812"",""ce7382"",""ce5255"",""ce1103"",""ce523"",""ce8534""))
 
 First you could use: 
 num_data <- as.numeric(str_extract(df$INTERACTOR_A, ""[0-9]+""))
         print(num_data)


     # Output – 7380 7380 7382 7382 7382 7388 7388
 
 Second method using the  gsub()  function: 
 num_data <- gsub('\\D','', df$INTERACTOR_B) 
print(num_data)


     # Output – '6058' '13812' '7382' '5255' '1103' '523' '8534'",,,56.25837849,50,61.52155673,50,50,50,50,,
15135,Train/Test/Validation Set Splitting in Sklearn,machine-learning,"You could just use  sklearn.model_selection.train_test_split  twice. First to split to train, test and then split train again into validation and train. Something like this: 
  X_train, X_test, y_train, y_test 
    = train_test_split(X, y, test_size=0.2, random_state=1)

 X_train, X_val, y_train, y_val 
    = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2","There is a great answer to this question over on  SO  that uses numpy and pandas.  
 The command (see the answer for the discussion): 
 train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])
 
 produces a 60%, 20%, 20% split for training, validation and test sets.","Adding to  @hh32's  answer, while respecting any predefined proportions such as (75, 15, 10): 
 train_ratio = 0.75
validation_ratio = 0.15
test_ratio = 0.10

# train is now 75% of the entire data set
x_train, x_test, y_train, y_test = train_test_split(dataX, dataY, test_size=1 - train_ratio)

# test is now 10% of the initial data set
# validation is now 15% of the initial data set
x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio)) 

print(x_train, x_val, x_test)","You can use  train_test_split  twice. I think this is most straightforward. 
 X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.25, random_state=1)
 
 In this way,  train ,  val ,  test  set will be 60%, 20%, 20% of the dataset respectively.",Most often you will find yourself not splitting it once but in a first step you will split your data in a training and test set. Subsequently you will perform a parameter search incorporating more complex splittings like cross-validation with a 'split k-fold' or 'leave-one-out(LOO)' algorithm.,"Extension of  @hh32 's answer with preserved ratios. 
 # Defines ratios, w.r.t. whole dataset.
ratio_train = 0.8
ratio_val = 0.1
ratio_test = 0.1

# Produces test split.
x_remaining, x_test, y_remaining, y_test = train_test_split(
    x, y, test_size=ratio_test)

# Adjusts val ratio, w.r.t. remaining dataset.
ratio_remaining = 1 - ratio_test
ratio_val_adjusted = ratio_val / ratio_remaining

# Produces train and val splits.
x_train, x_val, y_train, y_val = train_test_split(
    x_remaining, y_remaining, test_size=ratio_val_adjusted)
 
 Since the remaining dataset is reduced after the first split, new ratios for the reduced dataset must be calculated: 
 $ R_{new} = \frac{R_{old}}{R_{remaining}}$","Best answer above does not mention that by separating two times using  train_test_split  not changing partition sizes won`t give initially intended partition: 
 x_train, x_remain = train_test_split(x, test_size=(val_size + test_size))
 
 Then  the portion of validation and test sets in the x_remain change  and could be counted as 
 new_test_size = np.around(test_size / (val_size + test_size), 2)
# To preserve (new_test_size + new_val_size) = 1.0 
new_val_size = 1.0 - new_test_size

x_val, x_test = train_test_split(x_remain, test_size=new_test_size)
 
 In this occasion all initial partitions are saved.","Here's another approach (assumes equal three-way split): 
 # randomly shuffle the dataframe
df = df.reindex(np.random.permutation(df.index))

# how many records is one-third of the entire dataframe
third = int(len(df) / 3)

# Training set (the top third from the entire dataframe)
train = df[:third]

# Testing set (top half of the remainder two third of the dataframe)
test = df[third:][:third]

# Validation set (bottom one third)
valid = df[-third:]
 
 This can be made more concise but I kept it verbose for explanation purposes.","Given  train_frac=0.8 , this function creates a 80% / 10% / 10% split: 
 import sklearn

def data_split(examples, labels, train_frac, random_state=None):
    ''' https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
    param data:       Data to be split
    param train_frac: Ratio of train set to whole dataset

    Randomly split dataset, based on these ratios:
        'train': train_frac
        'valid': (1-train_frac) / 2
        'test':  (1-train_frac) / 2

    Eg: passing train_frac=0.8 gives a 80% / 10% / 10% split
    '''

    assert train_frac >= 0 and train_frac <= 1, ""Invalid training set fraction""

    X_train, X_tmp, Y_train, Y_tmp = sklearn.model_selection.train_test_split(
                                        examples, labels, train_size=train_frac, random_state=random_state)

    X_val, X_test, Y_val, Y_test   = sklearn.model_selection.train_test_split(
                                        X_tmp, Y_tmp, train_size=0.5, random_state=random_state)

    return X_train, X_val, X_test,  Y_train, Y_val, Y_test",69.81292434,60.40381177,65.41287404,67.32869835,65.7044219,57.48904562,59.17836283,58.32105855,61.70281758
14999,"Good ""frequent sequence mining"" packages in Python?",python,"I am actively maintaining an efficient  implementation  of both PrefixSpan and BIDE in Python 3, supporting mining both frequent and top-k (closed) sequential patterns.","The only Python package I've found is on  Github . 
 They have an implementation of  BIDE  there, but it's not maintained code.",SPMF  sounds like a useful library for pattern mining.,"Since none of the existing solutions were satisfactory for me, I created  my own Python Wrapper  for  SPMF (the Java library mentioned in other answers here) .","To complement some of the great answers/libraries: 
 Seq2Pat: Sequence-to-Pattern Generation Library  might be relevant to your case. 
 The library is written in Cython to take advantage of a fast C++ backend with a high-level Python interface. It supports  constraint-based  frequent sequential pattern mining. 
 Here is an example that shows how to mine a sequence database while respecting an average constraint for the prices of the patterns found. 
 # Example to show how to find frequent sequential patterns
# from a given sequence database subject to constraints
from sequential.seq2pat import Seq2Pat, Attribute

# Seq2Pat over 3 sequences
seq2pat = Seq2Pat(sequences=[[""A"", ""A"", ""B"", ""A"", ""D""],
                             [""C"", ""B"", ""A""],
                             [""C"", ""A"", ""C"", ""D""]])

# Price attribute corresponding to each item
price = Attribute(values=[[5, 5, 3, 8, 2],
                          [1, 3, 3],
                          [4, 5, 2, 1]])

# Average price constraint
seq2pat.add_constraint(3 <= price.average() <= 4)

# Patterns that occur at least twice (A-D)
patterns = seq2pat.get_patterns(min_frequency=2)
 
 Notice that sequences can be of different lengths, and you can add/drop other Attributes and Constraints. The sequences can be any string, as in the example, or integers. 
 The underlying algorithm uses  Multi-valued Decision Diagrams , and in particular, the  state-of-the-art algorithm from AAAI 20019 . 
 Hope this helps! 
 Disclaimer: I am a member of the research collaboration between Fidelity & CMU on the  Seq2Pat Library .","Have you considered to write it by yourself? Because there is probably no up-to-date maintained library right now. 
 Check  this  out, its the basic - PrefixSpan and Closed/Maximal patterns are actually not that hard to implement.",I've used  fim 's fpgrowth function in the past and it worked well.  It's kind of a pain to install on Windows machines however.  It seems to be an academic website so I'm not sure if they're doing many updates to the code over time...,,,65.60062497,59.62344831,57.07549272,52.6934118,57.47751507,50,50,,
14998,Sharing Jupyter notebooks within a team,software-recommendation,"JupyterHub does not provide version control system nor facilitates sharing of Notebooks. You mentioned yourself limitation of Binder. 
 Try  Zeppelin . Version 0.7 should be released within a few next days.  
 
 As you can see from the  roadmap , this version delivers ""enterprise"" features which are exactly about collaboration. 
 Version control system (git) is integrated. 
 It's self-hosted. 
 
 In essence, I think it meets all requirements you posted. On top of that it delivers richer visualisation capabilities and plethora of other features (works with Shiro, Knox, Kerberos - secure Spark anyone?).","Airbnb recently open sourced their internal data science knowledge repository:  https://github.com/airbnb/knowledge-repo 
 From its readme, it seems it could loosely fit your use case: 
 
 The Knowledge Repository project is focused on facilitating the
  sharing of knowledge between data scientists and other technical roles
  using data formats and tools that make sense in these professions. It
  provides various data stores (and utilities to manage them) for
  ""knowledge posts"", with a particular focus on notebooks (R Markdown
  and Jupyter / iPython Notebook) to better promote reproducible
  research. 
 
 There's also a  blog post  commenting on its motivation.","What I found - sharing notebooks for data scientists is a not a desirable format for communication. Many of them prefer IDE like Spider/RStudio or just a text editors (I know a few data scientists who use  vi ). 
 You might just share code by your source control and data by cloud storages. It will increase flexibility. 
 I've recently open sourced a tool which combines code, data, and the dependencies between data and code to a single environment and makes your data science project reproducible: DVC or dataversioncontrol.com (there is a tutorial). 
 With DVC tool you can just share your project by Git, sync data to S3 by a single DVC command. If some of your data scientists decide to change the code at any stage of your project then the final result could be easily reproduced by a single command  dvc repro data/target_metrics.txt .","The only  self-hosted  solution I know is the paid Anaconda Enterprise cloud setup,  https://anaconda.org/about . The other solutions I am aware of are not self-hostable!","Isn't  this  solution good enough ?  
 You can protect the access with ssh, and the hosted files could be the git repository you want, with different linux (or whatever) user access. You'll need your own server.","Domino Data Lab  offers premises, SaaS, and VPC-based notebook hosting (Jupyter, Zeppelin, RStudio), git integration, scalable compute, environment templates, and a bunch of other useful things. The premises/ VPC offerings may be overkill and too pricey if you're a small team, but the SaaS plans are pretty reasonably priced. 
 [ Full disclosure: I'm a former Domino employee ]","Options: 
 
 Jupyter notebooks are files, so if your IT infrastructure supports it, you can make a file share available to whatever hosts your users are running Jupyter on, and ask them to configure their Jupyter to use that share for their files. 
 Set up a JupyterHub server (we use the  DockerSpawner ) and make a shared  volume  available to your users. This assumes you have the resources to allow your users to all work on that server. If they can put real load on the server, you may want to make it scale by using Kubernetes. 
 
 Or do both. 
 This doesn't provide version control, which would be a good idea, but you didn't ask for that.",,,55.36285093,58.59428334,54.74332276,50,50,59.70288222,67.92602964,,
14957,Machine learning algorithm for ranking,machine-learning,"I think you should get started with "" learning to rank "" , there are three solutions to deal with ranking problem . 
 
 point-wise  , learning the score for  relevance  between each item
within list and specific user is your target . This can be accomplished as recommendation do .  
 pair-wise  , learning the "" relations ""  between items within list ,
which respectively are beat loss or even , is your goal . 
 list-wise  , learning the "" effectiveness "" of specific ranking list
for items is your object .","Your problem sounds like the classical top-N personal recommendation to me. There're lots of possibilities in the literature, for example: 
 
 User-based collaborative filtering 
 Content-based collaborative filtering 
 Matrix factorisation 
 
 You'll need to read the literature and figure out which one is better for you.","Are you trying to recommend a list of ranked items to a user?  
 Take a look at collaborative filtering which is one of the most commonly used technique for recommender systems. 
 For python,  Pyspark  is something you can look at.  This link  contains some example how to implement it.",I think you are looking for  Item Rank . It's a very simple algorithm which is used for ranking and cover your needs.,Apache Mahout is an open-source library that provides a good recommendation engine. I've used it on two projects successfully.,,,,,62.48411653,50,55.49638471,70.25266214,50,,,,
14899,How to draw Deep learning network architecture diagrams?,machine-learning,"I recently found  this online tool  that produces publication-ready NN-architecture schematics. It is called  NN-SVG  and made by  Alex Lenail . 
 You can easily export these to use in, say,  LaTeX  for example. 
 Here are a few examples: 
 AlexNet style 
 
 
 LeNet style 
 
 
 and the good old  Fully Connected  style","I wrote some latex code to draw Deep networks for one of my reports. You can find it here:  https://github.com/HarisIqbal88/PlotNeuralNet 
 With this, you can draw networks like these:","For automated drawing, see  How do you visualize neural network architectures? ,  https://softwarerecs.stackexchange.com/q/28169/903  and   https://softwarerecs.stackexchange.com/q/47841/903 
 For manual drawing, see  https://redd.it/574usi","Netron viewer  is the best tool to draw your model architecture 
 
 I suppose you have a pretrained model stored in .h5 file.","I drew this with  draw.io , you can also choose other structures of the drawing e.g circles.","We can use Powerpoint to get the job done. 
 Draw the diagram (3D rectangles and perspectives come handy) -> select the interested area on the slide -> right-click -> Save as picture -> change filetype to PDF -> :)",I've been working on a python project for drawing various network architectures here:  PyDrawNet,,,52.3865778,62.37975907,71.61549653,56.74741797,59.04878743,57.57109077,71.95080024,,
14581,When to use GRU over LSTM?,neural-network,"GRUs and LSTMs utilize different approaches toward gating information to prevent the vanishing gradient problem. Here are the main points comparing the two: 
 
 The GRU unit controls the flow of information like the LSTM unit, but without having to use a  memory unit . It just exposes the full hidden content without any control. 
 GRUs are relatively new, and in my experience, their performance is on par with LSTMs,  but computationally  more efficient  ( as pointed out, they have a less complex structure ). For that reason, we are seeing it being used more and more. 
 
 For a detailed description, you can explore this research paper on  Arxiv . The paper explains all this brilliantly. 
 You can also explore these blogs for a better idea: 
 
 WildML 
 Colah - Github 
 
 Hope that helps!","*To complement already great answers above. 
 
 From my experience,  GRUs train faster  and perform better than LSTMs on  less training data  if you are doing language modeling (not sure about other tasks).  
 GRUs are simpler  and thus easier to modify, for example adding new gates in case of additional input to the network. It's just less code in general. 
 LSTMs  should in theory  remember longer sequences  than GRUs and outperform them in tasks requiring modeling long-distance relations.   
 
 *Some additional papers that analyze GRUs and LSTMs. 
 
 ""Neural GPUs Learn Algorithms"" (Łukasz Kaiser, Ilya Sutskever, 2015)
 https://arxiv.org/abs/1511.08228 
 ""Comparative Study of CNN and RNN for Natural Language Processing""
(Wenpeng Yin et al. 2017)  https://arxiv.org/abs/1702.01923","FULL GRU Unit 
 $ \tilde{c}_t = \tanh(W_c [G_r * c_{t-1}, x_t ] + b_c) $ 
 $ G_u = \sigma(W_u [ c_{t-1}, x_t ] + b_u) $ 
 $ G_r = \sigma(W_r [ c_{t-1}, x_t ] + b_r) $ 
 $ c_t = G_u * \tilde{c}_t + (1 - G_u) * c_{t-1} $ 
 $ a_t = c_t $ 
 LSTM Unit 
 $ \tilde{c}_t = \tanh(W_c [ a_{t-1}, x_t ] + b_c) $ 
 $ G_u = \sigma(W_u [ a_{t-1}, x_t ] + b_u) $ 
 $ G_f = \sigma(W_f [ a_{t-1}, x_t ] + b_f) $ 
 $ G_o = \sigma(W_o [ a_{t-1}, x_t ] + b_o) $ 
 $ c_t = G_u * \tilde{c}_t + G_f * c_{t-1} $ 
 $ a_t = G_o * tanh(c_t) $ 
 As can be seen from the equations LSTMs have a separate update gate and forget gate. This clearly makes LSTMs more sophisticated but at the same time more complex as well. There is no simple way to decide which to use for your particular use case. You always have to do trial and error to test the performance. However, because GRU is simpler than LSTM, GRUs will take much less time to train and are more efficient. 
 Credits:Andrew Ng","This answer actually lies on the dataset and the use case. It's hard to tell definitively which is better.  
 
 GRU exposes the complete memory unlike LSTM, so applications which
that acts as advantage might be helpful. Also, adding onto why to use
GRU - it is computationally easier than LSTM since it has only 2
gates and if it's performance is on par with LSTM, then why not? 
 This  paper  demonstrates excellently with graphs the superiority
of gated networks over a simple RNN but clearly mentions that it
cannot conclude which of the either are better. So, if you are
confused as to which to use as your model, I'd suggest you to train
both and then get the better of them.","GRU is better than LSTM as it is easy to modify and doesn't need memory units, therefore, faster to train than LSTM and give as per performance.","Actually, the key difference comes out to be more than that: Long-short term (LSTM) perceptrons are made up using the momentum and gradient descent algorithms. When you reconcile LSTM perceptrons with their recursive counterpart RNNs, you come up with GRU which is really just a generalized recurrent unit or Gradient Recurrent Unit (depending on the context) that more closely integrates the momentum and gradient descent algorithms. Were I you, I'd do more research on AdamOptimizers. 
 GRU is an outdated concept by the way. However, I can understand you researching it if you want moderate-advanced in-depth knowledge of TF.",,,,70.79512245,74.71838197,64.22621863,68.85568689,71.41818606,62.45370969,,,
14406,Visualizing items frequently purchased together,python,"I think what you probably want is a discrete version of a heat map.  For example, see below.  The red colors indicate the most commonly purchased together, while green cells are never purchased together.
  
 This is actually fairly easy to put together with Pandas DataFrames and matplotlib. 
 import numpy as np
from pandas import DataFrame
import matplotlib
matplotlib.use('agg') # Write figure to disk instead of displaying (for Windows Subsystem for Linux)
import matplotlib.pyplot as plt

####
# Get data into a data frame
####
data = [
  ['Banana', 'Water', 'Rice'],
  ['Rice', 'Water'],
  ['Bread', 'Banana', 'Juice'],
]

# Convert the input into a 2D dictionary
freqMap = {}
for line in data:
  for item in line:
    if not item in freqMap:
      freqMap[item] = {}

    for other_item in line:
      if not other_item in freqMap:
        freqMap[other_item] = {}

      freqMap[item][other_item] = freqMap[item].get(other_item, 0) + 1
      freqMap[other_item][item] = freqMap[other_item].get(item, 0) + 1

df = DataFrame(freqMap).T.fillna(0)
print (df)

#####
# Create the plot
#####
plt.pcolormesh(df, edgecolors='black')
plt.yticks(np.arange(0.5, len(df.index), 1), df.index)
plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)
plt.savefig('plot.png')","For  R , you can use library  ArulesViz . There is nice  documentation  and on the page 12, there is example how to create this kind of visualization. 
 The code for that is as simple as this: 
 plot(rules, method=""grouped"")","With  Wolfram Language  in  Mathematica . 
 data = {{""Banana"", ""Water"", ""Rice""},
        {""Rice"", ""Water""},
        {""Bread"", ""Banana"", ""Juice""}};
 
 Get pairwise counts. 
 counts = Sort /@ Flatten[Subsets[#, {2}] & /@ data, 1] // Tally
 
 
 {{{""Banana"", ""Water""}, 1}, {{""Banana"", ""Rice""}, 1}, 
 {{""Rice"", ""Water""}, 2}, {{""Banana"", ""Bread""}, 1}, 
 {{""Bread"", ""Juice""}, 1}, {{""Banana"", ""Juice""}, 1}}
 
 
 Get indices for named ticks. 
 indices = Thread[# -> Range[Length@#]] &@Sort@DeleteDuplicates@Flatten[data]
 
 
 {""Banana"" -> 1, ""Bread"" -> 2, ""Juice"" -> 3, ""Rice"" -> 4, ""Water"" -> 5}
 
 
 Plot with  MatrixPlot  using  SparseArray . Could also use  ArrayPlot . 
 MatrixPlot[
 SparseArray[Rule @@@ counts /. indices, ConstantArray[Length@indices, 2]],
 FrameTicks -> With[{t = {#2, #1} & @@@ indices}, {{t, None}, {t, None}}],
 PlotLegends -> Automatic
 ]
 
 
 Note that it is upper-triangular. 
 Hope this helps.","You can do this in python with the seaborn visualization library (built on top of matplotlib). 
 data = [
  ['Banana', 'Water', 'Rice'],
  ['Rice', 'Water'],
  ['Bread', 'Banana', 'Juice'],
]

# Pull out combinations
from itertools import combinations
data_pairs = []
for d in data:
    data_pairs += [list(sorted(x)) + [1] for x in combinations(d, 2)]
    # Add reverse as well (this will mirror the heatmap)
    data_pairs += [list(sorted(x))[::-1] + [1] for x in combinations(d, 2)]

# Shape into dataframe
import pandas as pd
df = pd.DataFrame(data_pairs)
df_zeros = pd.DataFrame([list(x) + [0] for x in combinations(df[[0, 1]].values.flatten(), 2)])
df = pd.concat((df, df_zeros))
df = df.groupby([0, 1])[2].sum().reset_index().pivot(0, 1, 2).fillna(0)

import seaborn as sns
from matplotlib.pyplot import plt
sns.heatmap(df, cmap='YlGnBu')
plt.show()
 
 The final dataframe  df  looks like this: 
 
 and the resulting visualization is:","You can use networkx to create a graph like structure. 
 Example : using python networkx package 
 import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
G1 = nx.DiGraph()
color_map = []
N = 50
colors = np.random.rand(N)
strs = ['r0', 'r1']
for i in range(2):
    G1.add_nodes_from('r'+str(i))
    for a in top_rules.iloc[i]['antecedents']:
        G1.add_nodes_from([a])
        G1.add_edge(a, 'r'+str(i), color = colors[i], weight = 2)
    for c in top_rules.iloc[i]['consequents']:
        G1.add_nodes_from([c])
        G1.add_edge('r'+str(i), c, color = colors[i], weight = 2)
for node in G1:
    found_a_string = False
    for item in strs:
        if node == item:
            found_a_string = True
    if found_a_string:
        color_map.append('red')
    else:
        color_map.append('black')
edges = G1.edges()
print(edges)
colors = [G1[u][v]['color'] for u,v in edges]
weights = [G1[u][v]['weight'] for u,v in edges]
pos = nx.spring_layout(G1, k = 16, scale = 1)
fig = plt.figure(figsize = (4,4))
nx.draw(G1, pos, edges, node_color = color_map, edge_color = colors, width = weights, font_size = 16, with_labels = False)
for p in pos:
    pos[p][1] += 0.07
nx.draw_networkx_labels(G1, pos)",,,,,63.09960042,53.72853403,50,51.82946377,51.14312914,,,,
14346,What is Reinforcement Learning?,reinforcement-learning,"Reinforcement Learning uses a simple logic of learning in which the network tries to learn from the feedback it obtains. This tries to optimise the overall reward in the long run instead of the current reward. 
 This  is one of the best platform to read about it. It also contains some useful links. 
 As stated by the  wiki ,
The basic reinforcement learning model consists of: 
 
 a set of environment states S; 
 a set of actions A; 
 rules of transitioning between states; 
 rules that determine the scalar immediate reward of a transition; and 
 rules that describe what the agent observes. 
 
 The rules are often stochastic. The observation typically involves the scalar immediate reward associated with the last transition. In many works, the agent is also assumed to observe the current environmental state, in which case we talk about full observability, whereas in the opposing case we talk about partial observability. Sometimes the set of actions available to the agent is restricted (e.g., you cannot spend more money than what you possess).","Hima's answer does a good job summarizing the outline and purpose of reinforcement learning. If you are interested in taking a deeper look I'd recommend  this currently free book .  
 It does a great job walking you from a basic reinforcement learning definition through various solutions to dealing with modern challenges.","Reinforcement learning is the intersection of machine learning, decisions & control, and behavioral psychology. The intersection can be approached from all the three sides. Let me give you a short description from every angle- 
 Machine Learning 
 From the ML perspective, RL is the paradigm of learning to control.  
 Think about how you learned to cycle or play a sport. These learning tasks are not supervised - no one tells you the correct move to make in a board position, or exactly the amount of angle to lean sideways to balance the cycle. They are also not completely unsupervised since some feedback is observed - whether you won or lost the game after a sequence of moves, how frequently do you fall from cycle.  
 Thus, RL is learning to make good decisions from partial evaluative feedback. 
 Control & Decision theory 
 In control theory (and AI planning), perfect knowledge about the world is assumed, and the objective is to find the best way to behave.  
 However, for many problems knowledge about the world is not perfect. Hence, exploring the world could increase our knowledge and eventually help us make better decisions.  
 RL is balancing the exploration-exploitation trade-off in sequential decision-making problems. 
 Behavioral Psychology 
 The simplified goal of behavioral psychology is to explain  why, when , and  how  humans make decisions. We consider humans as rational agents, and hence psychology is also to some extent trying to explain rational behavior.  
 One can study the biological principles of how opinions are formed, which have close connections to temporal difference learning and eligibility traces.  
 RL is the paradigm to explain how humans form opinions and learn to make good decisions with experience. 
 This was a short description from every important perspective. For a detail description, kindly go through these -- 
 
 Reinforcement Learning 
 Karpathy - Github RL 
 CS- UBC : RL 
 
 Hope it helps!","Although the previous answers cover a lot to get you started in Reinforcement Learning (RL) field, I give you here an illustrative simple example to understand the concept and also what is the relationship between Supervised Learning (SL) and Unsupervised Learning (UL). 
 Imagine that you have a robot and you want to teach it to drive a car. Every let's say image of the road that the robot receives is going to be an input. One option that you have, in order to teach the robot, is that you can instruct it EVERY time that it receives the image of the road how much to steer the wheel. This is SL as you will have for every input state of the road a mapping to the proper angle of rotating the wheel. The main point here is that you know what is the optimal thing for your robot to do and you teach it by examples. 
 In a RL setting, you just let the robot try whatever it wants and you give it a reward/punishment regarding the action(s) it takes. The magnitude of the reward/punishment might be dependent on e.g. damage to the car, staying long time on the same lane etc. the reward/punishment might be given delayed and not at every single action that the robot takes. 
 In the first example (SL) the robot tries to minimize the error between your recommendation and its choices. In the second example the robot tries to maximize its reward by finding on its own what is the best to do. The SL approach at its best will lead you to a robot that ""mimimcs"" what you taught it. In the RL approach at its best, the robot will have a behavior that will be optimal in terms of driving the car and also might be better than yours. In other words it will create its own strategy. 
 To sum up,in SL you have a teacher that tells you at every single timestep exactly whats the correct response. In RL you try and find it on your own and the teacher gives you a reward/punishment. In UL you dont have any external feedback. So RL falls between SL and RL. 
 I simplified lots of the terms just to conceptualize the learning techniques with the example. 
 Hope it helps!","Reinforcement learning(RL)  is the intersection of machine learning, decisions & control, and behavioral psychology. The intersection can be approached from all the three sides, and a detailed explanation is beyond the scope of a answer. 
SO, I'll try to give a short account of all the three perspectives. Reinforcement Learning (RL) field, I give you here an illustrative simple example to understand the concept and also what is the relationship between Supervised Learning (SL) and Unsupervised Learning (UL). 
 Imagine that you are teaching your child to play super mario and you have two option  given the child the control stick and let him try to gain as many points as he can. or  you play one level at a time and ask him to do as you did so as to reach the end of the level like you did, that is you teach him how to play one level and ask him to play that or a similar level of the game.  This is SL as you will have for every input state of the road a mapping to the proper speed and points of jump and other attributes . The main point here is that you know what is the optimal thing and you try to teach your child to follow the similar steps. by doing so you teach him by examples. 
 In a RL setting, you just let the child try whatever it wants and you let the game give him a reward/punishment regarding the action(s) it takes.  
 In the SL example the child tries to minimize the error between your recommendation and its choices. In the RL example the child tries to maximize its reward by finding on its own what is the best to do. The SL approach at its best will lead you to a child to ""mimics"" what you taught it. In the RL approach at its best, the child will have a behavior that will be optimal in terms of playing Super Mario and also might be better than yours. In other words it will create its own strategy. 
 To sum up, in SL you have a teacher that tells you at every single time step exactly whats the correct response(think of math class where the teacher explains a identical problem to you on broad before giving you a problem to solve on you own). In RL you try and find it on your own and the teacher gives you a reward/punishment (think of a practical class where you are given a chemical and you have find its element/composition of the chemical and you follow you own intuition and do some trail and error to find the composition  of the chemical. In UL you don't have any external feedback. So uL falls between SL and RL(think of bio class where teacher gives you example about some classes of animals and ask you to do some classification. say, dog, cat, lion are vertebrate but snail, crab, earthworm are invertebrate so without knowing the meaning of invertebrate and vertebrate tell me which of these are invertebrate and vertebrate: frog, snake and grasshopper. 
 I simplified a lots just to give you a hint on the learning techniques with the example.",,,,,62.81811748,74.95388899,61.32365906,54.73307402,57.85879334,,,,
14273,How to replace NA values with another value in factors in R?,r,"You can use this function :  
 forcats::fct_explicit_na 
 library(forcats) 
fct_explicit_na(DF$col, na_level = ""None"")
 
 Usage 
 It can be used within the mutate function and piped to edit DF directly:  
 library(tidyverse) # for tidy data packages, automatically loads dplyr
library(magrittr) # for piping
DF %<>% mutate(cols = fct_explicit_na(col, na_level = ""None""))
 
 Note that ""col"" needs to be a factor for this to work.","You need to add ""None"" to the factor level and refactor the column DF$col. I added an example script using the iris dataset.  
 df <- iris

# set 20 Species to NA
set.seed(1234)
s <- sample(nrow(df), 20)
df$Species[s] <- NA

# Get levels and add ""None""
levels <- levels(df$Species)
levels[length(levels) + 1] <- ""None""

# refactor Species to include ""None"" as a factor level
# and replace NA with ""None""
df$Species <- factor(df$Species, levels = levels)
df$Species[is.na(df$Species)] <- ""None""","Your original approach was right, and your intuition about the missing level too. To do what you want you just needed to add add the level ""None"".  
 #Create a factor for the example
x<-factor(c(""S"",NA,""M"",""S"",""S"",""S"",NA,NA,""S"",""M"",""S"",NA,""M"",""S"",NA,""S"",""S"",NA,""M"",""S"",NA,""M""))

levels(x)<-c(levels(x),""None"")  #Add the extra level to your factor
x[is.na(x)] <- ""None""           #Change NA to ""None""","I'd just do an NA assign 
 library(roperators)

vec <- c('1', '2', NA, '4')
vec <- chr(vec) # make it a character vector first

vec %na<-% 0

print(vec)
 
 Then turn it into a factor again if you really need to (eg for plotting). or you could first add the factor level (as done above) and overwrite NAs","change data type 
 d=as.matrix(d)
d[is.na(d)] <-""None""
d=as.data.frame(d)",,,,,51.04307107,53.38908676,54.73527129,53.19680161,50,,,,
14187,What is the difference between model hyperparameters and model parameters?,machine-learning,"Hyperparameters and parameters are often used interchangeably but there is a difference between them. You can call something a 'hyperparameter' if it cannot be learned within the estimator directly. However, 'parameters' is a more general term. When you say 'passing the parameters to the model', it generally means a combination of hyperparameters along with some other parameters that are not directly related to your estimator but are required for your model. 
 For example, suppose you are building a SVM classifier in sklearn: 
 from sklearn import svm
X = [[0, 0], [1, 1]]
y = [0, 1]
clf = svm.SVC(C =0.01, kernel ='rbf', random_state=33)
clf.fit(X, y) 
 
 In the above code an instance of SVM is your estimator for your model for which the hyperparameters, in this case, are  C  and  kernel . But your model has another parameter which is not a hyperparameter and that is  random_state .","In addition to the answer above. 
 Model parameters  are the properties of the training data that are learnt during training by the classifier or other ml model. For example in case of some NLP task: word frequency, sentence length, noun or verb distribution per sentence, the number of specific character n-grams per word, lexical diversity, etc.  Model parameters  differ for each experiment and depend on the type of data and task at hand.  
 Model hyperparameters , on the other hand, are common for similar models and cannot be learnt during training but are set beforehand. A typical set of hyperparameters for NN include the number and size of the hidden layers, weight initialization scheme, learning rate and its decay, dropout and gradient clipping threshold, etc.","Hyper-parameters  are those which we supply to the model, for example: number of hidden Nodes and Layers,input features, Learning Rate, Activation Function etc in Neural Network, while  Parameters  are those which would be learned by the machine like Weights and Biases.","In machine learning, a model $M$ with parameters and hyper-parameters looks like, 
 $Y \approx M_{\mathcal{H}}(\Phi | D)$ 
 where $\Phi$ are parameters and $\mathcal{H}$ are hyper-parameters. $D$ is training data and $Y$ is output data (class labels in case of classification task). 
 The objective during training is to find estimate of parameters $\hat{\Phi}$ that optimizes some loss function $\mathcal{L}$ we have specified. Since, model $M$ and loss-function $\mathcal{L}$ are based on $\mathcal{H}$, then the consequent parameters $\Phi$ are also dependent on hyper-parameters $\mathcal{H}$.  
 The hyper-parameters $\mathcal{H}$ are not 'learnt' during training, but does not mean their values are immutable. Typically, the hyper-parameters are fixed and we think simply of the model $M$, instead of $M_{\mathcal{H}}$. Herein, the hyper-parameters can also be considers as a-priori parameters. 
 The source of confusion stems from the use of $M_{\mathcal{H}}$ and modification of hyper-parameters $\mathcal{H}$ during training routine in addition to, obviously, the parameters $\hat{\Phi}$. There are potentially several motivations to modify $\mathcal{H}$ during training. An example would be to change the learning-rate during training to improve speed and/or stability of the optimization routine. 
 The important point of distinction is that, the result, say label prediction, $Y_{pred}$ is based on model parameters $\Phi$ and not the hyper-parameters $\mathcal{H}$. 
 The distinction however has caveats and consequently the lines are blurred. Consider for example the task of clustering, specifically Gaussian Mixture Modeling (GMM). The parameters set here is $\Phi = \{\bar{\mu}, \bar{\sigma} \}$, where $\bar{\mu}$ is set of $N$ cluster means and $\bar{\sigma}$ is set of $N$ standard-deviations, for $N$ Gaussian kernels. 
 You may have intuitively recognized the hyper-parameter here. It is the number of clusters $N$. So $\mathcal{H} = \{N \}$. Typically, cluster validation is used to determine $N$ apriori, using a small sub-sample of the data $D$. However, I could also modify my learning algorithm of Gaussian Mixture Models to modify the number of kernels $N$ during training, based on some criterion. In this scenario, the hyper-parameter, $N$ becomes part of the set of parameters $\Phi = \{\bar{\mu}, \bar{\sigma}, N \}$. 
 Nevertheless, it should be pointed out that result, or predicted value, for a data point $d$ in data $D$ is based on $GMM(\bar{\mu}, \bar{\sigma})$ and not $N$. That is, each of the $N$ Gaussian kernels will contribute some likelihood value to $d$ based on the distance of $d$ from their respective $\mu$ and their own $\sigma$. The 'parameter' $N$ is not explicitly involved here, so its arguably not 'really' a parameter of the model. 
 Summary: the distinction between parameters and hyper-parameters is nuanced due to the way they are utilized by practitioners when designing the model $M$ and loss-function $\mathcal{L}$. I hope this helps disambiguate between the two terms.","In simplified words, 
 Model Parameters are something that a model learns on its own. 
For example, 
1) Weights or Coefficients of independent variables in Linear regression model. 
2) Weights or Coefficients of independent variables SVM. 
3) Split points in Decision Tree. 
 Model hyper-parameters are used to optimize the model performance.
For example,
1)Kernel and slack in SVM.
2)Value of K in KNN.
3)Depth of tree in Decision trees.","Model parameters are estimated based on the data during model training and model hyperparameters are set manually and are used in processes to help estimate model parameters. 
 Model hyperparameters are often referred to as parameters because they are the parts of the machine learning that must be set manually and tuned. 
 Basically, parameters are the ones that the “model” uses to make predictions etc. For example, the weight coefficients in a linear regression model. Hyperparameters are the ones that help with the learning process. For example, number of clusters in K-Means, shrinkage factor in Ridge Regression. They won’t appear in the final prediction piece, but they have a large influence on how the parameters would look like after the learning step. 
 Reference","Parameters that are set before running a model are model hyperparameters and they are common in alike models. i.e, no of hidden layers or nodes, activation functions etc. 
 Parameters that are estimated and learned by data during running/training a model are model parameters. i.e, weights of nodes or biases etc.",,,69.31078781,67.81953935,54.74258315,59.04332367,60.26053114,75.57098985,70.81784359,,
14092,Predictive modeling on big data set that can't fit into memory,machine-learning,"Whether you're using this for a hobby or for your job, I would recommend EMR on Amazon Web Services. The name is anachronistic (it's no longer just for map reduce), but EMR enables you to quickly stand up an Apache Spark cluster of several machines. Spark comes with machine learning libraries for building trees, and can scale to however many machines you need (to the point that you no longer face memory constraints). EMR is cheap because you're charged hourly and don't have to buy your own hardware. Apache Spark is great for your resume, and it's also a practical skill because it's generally the first tool most data scientists turn to when working with data sets that are too large for a single machine.","Since you're using Python and scikit-learn you could have a look at one of the following online learning algorithms: 
 
 sklearn.naive_bayes.MultinomialNB 
 sklearn.naive_bayes.BernoulliNB 
 sklearn.linear_model.Perceptron 
 sklearn.linear_model.PassiveAggressiveClassifier 
 sklearn.linear_model.SGDClassifier 
 sklearn.linear_model.PassiveAggressiveRegressor 
 sklearn.linear_model.SGDRegressor 
 
 All of these  online learning  algorithms (in particular with SGD) allow for streaming the data through memory one entry at a time. Your memory would be more than sufficient for this approach. In scikit-learn this is implemented via the  partial_fit()  method. More on this  out-of-core approach  can be found in the scikit-learn user guide. 
 If you want stick to tree based algorithms, you can have a look at the xgboost package which also allows for streaming data through memory. However, this approach is a little more involved because it only accepts data in the LIBSVM format in order to parse it in the memory cache preserved for  xgboost . Also, it doesn't allow for parameter tuning, since xgboost works on numpy objects and converting from LIBSVM to numpy dumps the data from the cache to the main memory and therefore doesn't scale. 
 You could also use the  Databricks Community Edition 
which let's you spin up Spark clusters (of limitied size for the  free  edition) where you can run pyspark or plain python scripts.","In general, more training examples means improvement in learning but you can also get a very good (and nearby to the optimal score) if you just fit a good algorithm on a subset of your data set that has enough training examples. Here are a few things you can do in your current case : 
 
 Take a subset of the data say about 4-5GB. The only thing you need to consider is that your target labels should be nearly stratified in the subset other wise the model will perform poorly. 
 Apply PCA on your data and try to minimize the number of features. There may be features that are just redundant in your subset of data. 
 Apply the algorithm, best suited for your dataset, on your current subset. You may need to do a bit of research for that. 
 
 And above all, I suggest one thing. Training such a large dataset on a local machine can be too much of pain. So, it's better if you deploy your model on AWS or Google's Machine Learning platform provided by them on cloud. 
 I hope it helps!!",You need to use an algorithm that does not load all examples in memory at the same time. Use Stochastic Gradient Descent (a good implementation is Vowpal Wabbit).,"You can also use some out of core libraries  like GraphLab  (watch out though, GraphLab is free only for educational purposes). GraphLab works the same way as Scikit-learn but can run 'out of core' (meaning it isn't limited by the amount of memory you have).",,,,,53.23648856,55.13066128,54.11175605,52.16243499,51.5140203,,,,
14039,Tool to label images for classification,machine-learning,"I just hacked together a very basic helper in python
it requires that all images are stored in a pyton list  allImages . 
 import matplotlib.pyplot as plt
category=[]
plt.ion()

for i,image in enumerate(allImages):
    plt.imshow(image)
    plt.pause(0.05)
    category.append(raw_input('category: '))","pidgey - widget for jupyter notebook with active learning 
 I have just found this  open-source tool , and it looks amazing:
 
 It's an interactive widget for Jupyter Notebook and the best thing about it - according to  this commit   you can add your own sklearn-like classifier and use it for predicting classes as you annotate!  The classifier keeps learning as you proceed with labeling. 
 
 tkteach - Super Fast Image Categorization Python Tool 
 Also, there is a tool called  tkteach  and it's great because you can annotate images really fast using only your keyboard. I have improved the original version a little bit. The fork is here:  https://github.com/Serhiy-Shekhovtsov/tkteach","Try this tool. It is very simple and does exactly what you want → assign label(s) to images in a given folder. 
 https://github.com/robertbrada/PyQt-image-annotation-tool","I have created a code doing what you need, it is available on GitHub as  image-sorter2 . Instead of ""labelling"" images, it puts the images into a new folder, but creating the csv you are talking about is a straight forward extension. Compared to the other suggested scripts here image-sorter2 is 100% free of charges and you don't need to spend time on drawing bounding boxes - the script simply opens a GUI for you, you click on one of multiple buttons and correspondingly each image is sorted into the desired class-folder, e.g. ""cats"", ""dogs"", ""trucks"" a.s.o.","Try  Supervisely . 
 For your task you could create classes: 'healthy', 'dead', 'sick' and associate them with Rectangle tool. Then you just put a box around each cell with corresponding class.
Below is an example:   
 
 Definitions of classes
 
 Labeling
 
 
 If your categories are not mutually exclusive, you may create “cell” class (and associate it with rectangle) and then create several tags - one for each of your categories.
Below is an example:   
 
 Definitions of classes and tags
 
 Labeling","Try using EVA annotation tool.  Ericsson/eva  ,this has an excellent tracking function. you mark the object in only 1 frame and rest/many of the frames are automatically annotated. This also has lock unlock feature to help annotate faster and more number of objects in each frame. This supports video upload or image data sets. 
 Best thing, completely Free/open source !",,,,54.50768822,58.08206277,70.92919381,62.51620083,54.48375354,52.93013194,,,
13525,Is there any book for modern optimization in Python?,python,"There is a tutorial  Modern Optimization in Python  and a corresponding video  Modern Optimization in Python | SciPy 2017 Tutoria . Although they are not books, I think they may be helpful to you.","You should look for this book 
 Pyomo — Optimization 
Modeling in Python","You should be able to translate code written in one language -- even pseudo-code -- to another, so I see no reason to avoid books for R. If you want one specifically for python, there's  Machine Learning in Action  by Peter Harrington. 
 One of scikit-learn's core committers is a releasing a book in October:  Introduction to Machine Learning with Python: A Guide for Data Scientists .",Try the ND Pyomo Cookbook:  https://github.com/jckantor/ND-Pyomo-Cookbook . That is a collection of notebooks showing how to use  Pyomo  to solve modeling and optimization problems.,"I found this question useful for me.
Nevertheless, I agree with  Erme  namely you should be able to translate code written in one language to another. 
 Recently I've found this book and strongly recommend  Algorithms for Optimization . 
 Algorithms are shown in Julia Code, but the syntax of this language is very simple and in many cases similar to Python. 
 Here can be found a short review of the mentioned book: 
 
 https://machinelearningmastery.com/books-on-optimization-for-machine-learning/",,,,,91.12495716,61.52023002,55.51697825,52.95015721,55.98356708,,,,
13513,Why do internet companies prefer Java/Python for data scientist job?,beginner,"So you can integrate with the rest of the code base. It seems your company uses a mix of Java and python. What are you going to do if a little corner of the site needs machine learning; pass the data around with a database, or a cache, drop to R, and so on? Why not just do it all in the same language? It's faster, cleaner, and easier to maintain. 
 Know any online companies that run solely on R? Neither do I... 
 All that said Java is the last language I'd do data science in.","There may be a lot of reasons like:  
 
 Workforce flexibility: One Java / Python programmers can be moved to other tasks or projects easily. 
 Candidates availability: there are plenty of Java / Python programmers. You do not want to introduce a new programming language to later find out that there are no qualified workers or they are just too expensive.  
 Integration and ETL: Sometimes getting the data with the right quality is the hardest part of the project. So it is natural to use the same language as the rest of the systems. 
 Business model definition: Most business rules and business models are already written in this languages.  
 Just keeping things simple. It is already hard enough to be up-to-date with the technologies. A diverse base of language can be chaotic. R for this, Ruby for that, Scala, Clojure, F#, Swift, Dart... They may need different servers, different pathes, a hell to administer. All have their own IDEs with tools and plugins (not always free).  See some Uncle Bob's points about  languages choice  and  new technologies 
 
 So even if you have a 5% - 15% productivity advantage using R for the specific task, they may prefer a tool that just does the job even if not in the most efficient way.","It is in general true that for purely data science and statistics exercises R offers the best and fastest (especially if using the  data.table  package) tools and methods, that otherwise would be heavier to implement in Python (I assume by Python we all mean Pandas, though). Most data scientists do in fact use R to perform their models and calculations, or just to see how data behave. 
 Once the exercise is complete it is time to make it available to the rest of the people who have to use it (i. e. to deploy); to this aim it is oftentimes preferred to submit the code in Python for two main reasons: 
 
 Most architectures are written in Python or are Python-friendly, therefore it would be easier to implement models natively written in that language. 
 R syntax and grammar is extremely complicated. I myself strongly favour R other than anything else but have to however admit that the syntax is not really straightforward and has a very picked learning curve. 
 
 The above said, it is still true that one can easily translate R code into any other language, provided methods, libraries and packages are available (in Python most of them are, so that is no problem at all). Plenty of infrastructures and databases support underlying R code, hence portability is not really a problem, especially if one just has to submit the results of the calculations (to that extend, nobody really sees the underlying code anyway).  
 Java is of almost no use for the pure data science itself (although the Stanford University has a collection of machine learning NLP libraries written in Java, as far as I remember - but please check). The only reason why it can be required is just that the rest of the company uses it to big extents and they do not want to replace it with something new.","I've seen quite a few companies using the title Data Scientist for ""Data Engineer"" type roles. Particularly in the big data space.  
 If the company is using Hadoop or a distributed framework like Spark to do it's analytics in then Java or Python (or probably Scala) would be the languages that would make the most sense .","Java 
 I'd have to disagree with the other posters on the java question. There are certain noSQL databases (like hadoop) that one needs to  write mapreduce jobs in java . Now you can use  HIVE  to achieve much the same result. 
 Python 
 The python / R debate continues. Both are extensible languages, so potentially both could have the same ability to process. I only know R and my python knowledge is quite superficial. Speaking as a small business owner, you want to not have too many tools in your business otherwise there will be a general lack of depth in them, and difficulty supporting them. I think it will come down to depth of tool knowledge in the team. If the team is focused on python, then hiring another python data scientist is going to make sense as they can engage with the existing code base and historic experiment code.","At least for my current team (~80 data scientists and engineers), we don't have such preference. Half of the data scientists here use R and another half use Python. Many can code in both. We do deploy Python and R code in production. 
 I don't think any of our data scientists uses Java at all. If they need to deal with big data, they can use SparkSQL or PySpark. The data engineering team uses a mix of Java/Scala/Python/Go.  
 If you are one of few data people in a small company, I can understand why they require certain language skills so you can do both data science and engineering. But tbh, I think most small companies won't have data big enough that Python or R can't handle in production.","My point of view as a general purpose programmer with a tiny bit of R experience:
R is excellent for data science, but it's geared towards people manually interpreting data. If you want to use the results for something automated, you have to interface with something else, and that something else will be hard to do in a problem specific language like R. Can you do a web site in R? :)
On the other hand, python does have ready made libraries for data sciency stuff and is a general purpose programming language that doesn't get in the way of your doing anything else with it.
As for Java, it's good for large programming projects with hundreds of thousands to millions of lines of code. If the data science part needs to interface with that, it may make sense to do everything in Java then. 
 Random whine: Why do I have to sign in to each StackExchange site separately?","The tools in Python are just better than R. Ther R community is pretty stagnant while the Python community is evolving really quick. Especially in tools for Data Science. 
Also Python works way easier with everything around it. You can easily scrape the web, connect to databases and so on. That makes prototyping really fast. 
And if you have a working prototype and care to make it faster or integrate it into the company workflow, it gets usually reimplemented in Java. 
 R has a few neat tools and visualization but it is not that great to build new stuff in it.",,62.25835526,58.70423228,58.10006996,63.32439468,62.65417279,65.88756995,54.43815574,55.68531677,
13504,Skills that school doesn't teach you,machine-learning,"Based on my own experience and in reading what others have written, SQL is one of those skills employers look for, perhaps even assume that you have along with some of the basic skills of communication and teamwork.  The main reason is that lots of data are stored in a relational database with SQL being the primary way to extract that data to get it into your models.","I believe technology is cheap and science is expensive. You can learn R, Python, SQL and Hadoop pretty fast (considering that you know programming) but learning statistics, machine learning and the methodology of working with data is difficult and takes time. (which you know based on your background)
In my eyes, go and apply for jobs with self-confidence. In the meanwhile, consider learning SQL and Python. They are necessary for jobs in industry.","A large component of data science work in industry is data wrangling. It is quite important to have some basic understanding of data storage systems as you will often have to extract the data you need yourself (unless you work for a large company). Hadoop may not be a necessary skill, but knowing something of relational data stores (SQL) and object storage (No-SQL) will be very useful. A person often needs to be able to process data quickly too, so you will need to know something of optimisations such as indexing. 
 I work in python and R (as do most practitioners I know personally), but find that python is easier to deploy in a production environment. Much of the work relies on libraries these days, so it useful to know the language's library landscape (when it comes to project timelines, familiarity with your tools reduces the pressure on yourself and your team immensely). It is quite common that people experiment in jupyter/ipython notebooks and make their code available online (e.g.  KDNuggets post ). We often use notebooks at work and commit them to the code-base to provide the empirical backing for the solution. I would recommend you find some cool notebooks that interface with a database, and see if you can run their code (it is python typically). At least this way you will start to get a feel for more of the grunt work associated with the tools (considering that you already have the theoretical background).","If you find learning SQL and Hadoop unbearably boring, you should not be looking for a data scientist job. Anyhow, feel free to skip Hadoop. There are lots of deployments of Hadoop, but they are being phased out with more modern tech, for example Spark, which is also what most companies use for new deployments. I hope you find learning Spark a bit more bearable. The Edx online courses on ML with Spark are actually very nice.","Hadoop and SQL are things that you will pick up reasonably quickly and in my experience are far from necessary for a lot of jobs.  
 I would focus on Python or on R, not on both. A lot of employers are Python and R and allow you to choose yourself as long as it's on one or both. I feel like Python is growing faster in the Data Science community than R and is in my opinion a much more sophisticated language and has a wider support for things that help with data science but are not directly related. 
 As jab already mentioned, I think soft skills are very important, although difficult to learn without real working experience. 
 The best thing to prepare in my opinion is just by doing some project(s) and figuring things out along the way. Join a Kaggle competition or find some personal project. I spend hours and hours working on those and I keep coming across problems that I haven't faced before, requiring me to read papers, implementing some new ideas, trying things out. This also allows you to build up a github portfolio to show off some cool projects you have been working on.","SQL / Big Data Tools 
 Getting and cleaning data is a central part of a data scientist's job. My team (data science at a mid-to-large internet company) regularly deals with data wrangling at scale -- combining terabytes of browsing history with a multitude of other data sources, structured and unstructured, to investigate problems. SQL and Spark/Hive/Hadoop are nearly daily parts of our workflow. We filter new candidates heavily on their ability to use SQL intelligently and favor those who have worked with big data technologies in production environments.  
 On a side note, distributed processing/storage is a fascinating research area and the mechanics behind databases are pretty cool. Perhaps you can make it a bit more interesting for yourself by not just focusing on the semantics of SQL, but also some under-the-hood bits, like multi-pass algorithms for joins and so forth. You could read the  Amazon DynamoDB paper  or  Google's BigTable paper  for an intro to big, distributed databases.  
 Programming environment 
 Our team is python heavy, though we are using Spark more and more. It's got a great ecosystem for working with big data and has great machine-learning support via scikit-learn and the like. We often prototype in jupyter notebooks and scale solutions by building python apis that run our jobs or handle incoming requests. Other data science teams here use R. Pick one, do a few projects in it, and do a few projects in the other. You'll figure out which syntax and style works best for you.  
 Try to train yourself not to code like a grad student anymore, either. :)","Python is more suited for a 'real' data scientist work whose work needs to be productized and used in a software or website. We need to remember that ultimately whatever we do needs to be productized. Although R has Shiny library that can be used for web hosting, we need to remember that a website will not have only machine learning component, it will have many other aspects, which cannot be built in R, but needs a full fledged software development language. I sent my resume to senior level data scientist for job application and the response i received is in the linked question.
 Why do internet companies prefer Java/Python for data scientist job?","AFTER graduation, you will [continue to] learn how to learn, how to accelerate your learning process with smart people who get smarter because they accelerate how they learn ... away from academia, without institutional support, in difficult environments ... each of us must transcend our own algorithmic or predictable approach to learning or how our machine has been programmed in the past to recognize patterns and learn.   
 How you collaborate, communicate, work with teams of solid, qualified, antifragile LEARNERS will shape your career, success, satisfaction, friendships, life ... and our species.   
 Smart people are often suckers for the propaganda and comforts of respectable human systems, such as academia or bureaucratic machines built on piles of smart people who settle for calcifying into smart, predictable cogs ... but the world is chaotic and throws sand into the working of inflexible systems and smart cogs -- so how you learn will determine whether chaos serves you or defeats you.  Learn to think in a generally  antifragile manner  rather than simply being resilient or defensively robust ... as you wrangle data, remember that the most interesting and useful things will tend to happen in  Extremeistan .    
 Assuring quality of the data you are wrangling [or that some intern has wrangled] for the analysis is always going to be essential; the GIGO rule will always take precedence over the sophisticated wonkery. That is why it is so necessary for us to step back, see our roles in a larger picture. Being proficient in skill like Python or experimental design is awesome and necessary -- but do not settle for being pigeon-holed as just an expert in _____ or a phenomenal code jockey.  
 Data, like humans, have more interesting stories to tell than just the narrow questions we want to ask AND just because we have a bias, it does not mean that it's necessary to wrangle up compliant data to confirm it. Outliers need to be listened to; there's a reason why they show up, why they persist. Listen to extreme, challenging, heretical points of view ... especially when the local inquisition is angry about the uncomfortable  someone can prove that the planets do not orbit around them . Not all uncomfortable, unpopular opinions should taken seriously, but some are absolutely essential and necessary truth. Beware of the zealots,  even brilliant cartoonists , who claim to think independently when 97% of them conform to a generally-accepted truth and adamantly demand that others fall into line ... sometimes, cartoons are just metaphors for how seriously we should take cartoons.",,57.47882206,50,51.60279667,50,51.9577028,50,50,51.24184185,
13490,How to set class weights for imbalanced classes in Keras?,deep-learning,"You could simply implement the  class_weight  from  sklearn : 
 
 Let's import the module first 
 from sklearn.utils import class_weight
 
 In order to calculate the class weight do the following 
 class_weights = class_weight.compute_class_weight('balanced',
                                                 np.unique(y_train),
                                                 y_train)
 
 Thirdly and lastly add it to the model fitting 
 model.fit(X_train, y_train, class_weight=class_weights)
 
 
 Attention : I edited this post and changed the variable name from  class_weight  to  class_weight s  in order to not to overwrite the imported module. Adjust accordingly when copying code from the comments.","If you are talking about the regular case, where your network produces only one output, then your assumption is correct. In order to force your algorithm to treat every instance of  class 1  as 50 instances of  class 0  you have to: 
 
 Define a dictionary with your labels and their associated weights 
 class_weight = {0: 1.,
                1: 50.,
                2: 2.}
 
 Feed the dictionary as a parameter: 
 model.fit(X_train, Y_train, nb_epoch=5, batch_size=32, class_weight=class_weight)
 
 
 EDIT:
""treat every instance of  class 1  as 50 instances of  class 0 "" means that in your loss function you assign higher value to these instances.
Hence, the loss becomes a weighted average, where the weight of each sample is specified by  class_weight  and its corresponding class.  
 From  Keras docs :  
 
 class_weight : Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only).","I use this kind of rule for  class_weight  : 
 import numpy as np
import math

# labels_dict : {ind_label: count_label}
# mu : parameter to tune 

def create_class_weight(labels_dict,mu=0.15):
    total = np.sum(list(labels_dict.values()))
    keys = labels_dict.keys()
    class_weight = dict()
    
    for key in keys:
        score = math.log(mu*total/float(labels_dict[key]))
        class_weight[key] = score if score > 1.0 else 1.0
    
    return class_weight

# random labels_dict
labels_dict = {0: 2813, 1: 78, 2: 2814, 3: 78, 4: 7914, 5: 248, 6: 7914, 7: 248}

create_class_weight(labels_dict)
 
 math.log  smooths the weights for very imbalanced classes !
This returns : 
 {0: 1.0,
 1: 3.749820767859636,
 2: 1.0,
 3: 3.749820767859636,
 4: 1.0,
 5: 2.5931008483842453,
 6: 1.0,
 7: 2.5931008483842453}","class_weight  is fine but as @Aalok said this won't work if you are one-hot encoding multilabeled classes. In this case, use  sample_weight : 
 
 sample_weight : optional array of the same length as x, containing
weights to apply to the model's loss for each sample. In the case of
temporal data, you can pass a 2D array with shape (samples,
sequence_length), to apply a different weight to every timestep of
every sample. In this case you should make sure to specify
 sample_weight_mode=""temporal""  in  compile() . 
 
 sample_weights  is used to  provide a weight for each training sample . That means that you should pass a 1D array with the same number of elements as your training samples (indicating the weight for each of those samples). 
 class_weights  is used to provide a  weight or bias for each output class . This means you should pass a weight for each class that you are trying to classify. 
 sample_weight  must be given a numpy array, since its shape will be evaluated. 
 See also this  answer .","Adding to the solution at  https://github.com/keras-team/keras/issues/2115 . If you need more than class weighting where you want different costs for false positives and false negatives. With the new keras version now you can just override the respective loss function as given below.
Note that  weights  is a square matrix. 
 from tensorflow.python import keras
from itertools import product
import numpy as np
from tensorflow.python.keras.utils import losses_utils

class WeightedCategoricalCrossentropy(keras.losses.CategoricalCrossentropy):

    def __init__(
        self,
        weights,
        from_logits=False,
        label_smoothing=0,
        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,
        name='categorical_crossentropy',
    ):
        super().__init__(
            from_logits, label_smoothing, reduction, name=f""weighted_{name}""
        )
        self.weights = weights

    def call(self, y_true, y_pred):
        weights = self.weights
        nb_cl = len(weights)
        final_mask = keras.backend.zeros_like(y_pred[:, 0])
        y_pred_max = keras.backend.max(y_pred, axis=1)
        y_pred_max = keras.backend.reshape(
            y_pred_max, (keras.backend.shape(y_pred)[0], 1))
        y_pred_max_mat = keras.backend.cast(
            keras.backend.equal(y_pred, y_pred_max), keras.backend.floatx())
        for c_p, c_t in product(range(nb_cl), range(nb_cl)):
            final_mask += (
                weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])
        return super().call(y_true, y_pred) * final_mask","Here's a one-liner using scikit-learn: 
 from sklearn.utils import class_weight
class_weights = dict(zip(np.unique(y_train), class_weight.compute_class_weight('balanced', np.unique(y_train), 
                y_train)))","If  tf dataset  is used you cannot use the  class_weights  parameter. Insted return the weight from a parse_function in your pipeline 
 weight_arr = [1.5, 0.5] #define your custom weights
    
#create a lookup table
key_tensor = tf.constant(list(range(0, len(weight_arr))), dtype=tf.int64)
val_tensor = tf.constant(weight_arr)
init = tf.lookup.KeyValueTensorInitializer(key_tensor, val_tensor)
weight_table = tf.lookup.StaticHashTable(init,default_value=-1)

def parse_function(element):
    features = element{'image'}
    label_integer = element{'label'}

    weight = weight_table.lookup(label_integer) #find the weight based on label

    return features, label_integer, weight

ds = ds.map(parse_function)
model.fit(ds)...
 
 First you create a lookup table, which maps the given label integer to class weight. Then you fetch the weight based on your label in the pipeline.","I found the following example of coding up class weights in the loss function using the minist dataset. See link  here . 
 def w_categorical_crossentropy(y_true, y_pred, weights):
    nb_cl = len(weights)
    final_mask = K.zeros_like(y_pred[:, 0])
    y_pred_max = K.max(y_pred, axis=1)
    y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))
    y_pred_max_mat = K.equal(y_pred, y_pred_max)
    for c_p, c_t in product(range(nb_cl), range(nb_cl)):
        final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])
    return K.categorical_crossentropy(y_pred, y_true) * final_mask","from collections import Counter
itemCt = Counter(trainGen.classes)
maxCt = float(max(itemCt.values()))
cw = {clsID : maxCt/numImg for clsID, numImg in itemCt.items()}
 
 This works with a generator or standard. Your largest class will have a weight of 1 while the others will have values greater than 1 depending on how infrequent they are relative to the largest class.  
 Class weights accepts a dictionary type input.",76.62153532,73.10061709,66.03688017,62.79058786,60.14501035,71.25899866,59.45972213,53.59751532,65.73052871
13461,How can I get prediction for only one instance in Keras?,neural-network,"You can do: 
 q = model.predict( np.array( [single_x_test,] )  )","predict_classes  is expecting a 2D array of shape  (num_instances, features) , like  X_test  is. But indexing a single instance as in  X_test[10]  returns a 1D array of shape  (features,) . 
 To add back the extra axis, you can use  np.expand_dims (X_test[10], axis=0) , or  X_test[10][np.newaxis,:] , or don't get rid of it in the first place (e.g., by using  X_test[10:11] ).","Currently (Keras v2.0.8) it takes a bit more effort to get predictions on single rows after training in batch. 
 Basically, the  batch_size  is fixed at training time, and has to be the same at prediction time. 
 The workaround right now is to take the weights from the trained model, and use those as the weights in a new model you've just created, which has a  batch_size  of 1. 
 The quick code for that is 
 model = create_model(batch_size=64)
mode.fit(X, y)
weights = model.get_weights()
single_item_model = create_model(batch_size=1)
single_item_model.set_weights(weights)
single_item_model.compile(compile_params)
 
 Here 's a blog post that goes into more depth. 
 I've used this approach in the past to have multiple models at prediction time- one that makes predictions on big batches, one that makes predictions on small batches, and one that makes predictions on single items. Since batch predictions are much more efficient, this gives us the flexibility to take in any number of prediction rows (not just a number that is evenly divisible by  batch_size ), while still getting predictions pretty rapidly.","This would be how to predict for one element, this time number 17. 
 model.predict_classes(X_test[17:18])","You should pass a list with just 1 example, I can't test right now but this should work: 
 model1.predict_classes([X_test[10]])","if you try to print out the instance you will see this: 
 x_test:\n
array([[0., 1., 1., ..., 0., 0., 0.],
        [0., 1., 1., ..., 0., 0., 0.],
        [0., 1., 1., ..., 0., 0., 0.],
        ...,
        [0., 1., 0., ..., 0., 0., 0.],
        [0., 1., 1., ..., 0., 0., 0.],
        [0., 1., 1., ..., 0., 0., 0.]])

x_test[0]:
array([0., 1., 1., ..., 0., 0., 0.])
 
 so I think we can just add back a dimension using np.array: 
 mode.predict(np.array(x_test[0],ndmin=2))","self.result = self.model.predict(X)
 
 where X is numpy array. That is all I did and it worked.","I have fixed this by using the following approach: 
 single_test = X_test[10]
single_test = single_test.reshape(1,784)
 
 Please note that amount of features (784) in the reshape function is based on your example above, if you have fewer features then you need to adjust it. 
 Hope it will work for you too.","It means that your training data had the shape of (784, 1). You can just reshape it as the following. It worked for me.  
 model1.predict_classes(X_test[10].reshape(784,1))
 
 You can also do  transpose()  if shape is (1,784),  
 model1.predict_classes(X_test[10].transpose())",53.55274456,54.57948726,58.55918282,62.64972495,52.12596035,53.07969368,52.49475275,50,52.40373008
13754,Feature importance with scikit-learn Random Forest shows very high Standard Deviation,python,"You are using RandomForest with the default number of trees, which is 10.
For around 30 features this is too few. Therefore standard deviation is large. Try at least 100 or even 1000 trees, like 
 clf = RandomForestClassifier(n_estimators=1000)
 
 For a more refined analysis you can also check how large the correlation between your features is.","Your result is not that weird. As  lanenok  states, you should in a first step increase the number of trees in order to make sure that you get a 'statistical' result concerning the feature importances. 
 However, as this  paper  by Genuer et al. (2010) shows, you can actually use the standard deviations in order to eliminate features. To quote: "" We can see that true variables standard deviation is large compared to the noisy variables one, which is close to zero.  ""","There could be multiple reasons. The number of trees and the depth can change your results.  If your model doesn't perform well after selecting the parameters (cross-validation etc.), it's probably because your features are not very predictive, so they get picked almost ""randomly"" which leads to high standard deviations from tree to tree. 
But there are other possibilities, e.g. it could also be that your features are highly correlated. A little more information would be helpful.","Try  clf = RandomForestClassifier(max_features=None) . The  max_features  param defaults to  'auto'  which is equivalent to  sqrt(n_features) .  max_features  is described as ""The number of features to consider when looking for the best split."" Only looking at a small number of features at any point in the decision tree means the importance of a single feature may vary widely across many tree. So, don't look at a random subset, just look at all features at every level of the tree.","A common reason for this is that the parameters you supplied (or defaulted) to  RandomForestClassifier  are not suited for your dataset. 
 A common way to address this problem is to search the hyperparameter space using e.g.  GridSearchCV : 
 from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, make_scorer

param_grid = {'n_estimators': [10, 100, 1000], 'max_features': [5, 10, 20, 30]}
clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring=make_scorer(accuracy_score))
 
 param_grid  here is the permutations of parameters that you want to search in, and the  make_scorer(accuracy_score)  is the measure you want to optimize. 
 Note that  accuracy_score  is suitable for balanced sets, but not for unbalanced sets. Choose a suitable metric to on your particular objective.",,,,,57.7702112,65.08936706,60.55902479,55.48357583,51.07199598,,,,
13216,Intuitive explanation of Noise Contrastive Estimation (NCE) loss?,deep-learning,"Taken from this post: https://stats.stackexchange.com/a/245452/154812 
 The issue 
 There are some issues with learning the word vectors using a ""standard"" neural network. In this way, the word vectors are learned while the network learns to predict the next word given a window of words (the input of the network). 
 Predicting the next word is similar to predicting a class in a classification problem. That is, such a network is just a ""standard"" multinomial (multi-class) classifier. And this network must have as many output neurons as classes there are. When classes are actual words, the number of neurons is, well, huge. 
 A ""standard"" neural network is usually trained with a cross-entropy cost function which requires the values of the output neurons to represent probabilities - which means that the output ""scores"" computed by the network for each class have to be normalized, converted into actual probabilities for each class. This normalization step is achieved by means of the softmax function. Softmax is very costly when applied to a huge output layer. 
 The (a) solution 
 In order to deal with this issue, that is, the expensive computation of the softmax, Word2Vec uses a technique called noise-contrastive estimation. This technique was introduced by [A] (reformulated by [B]) then used in [C], [D], [E] to learn word embeddings from unlabelled natural language text. 
 The basic idea is to convert a multinomial classification problem (as it is the problem of predicting the next word) to a binary classification problem. That is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression (binary classification) is used instead. 
 For each training sample, the enhanced (optimized) classifier is fed a true pair (a center word and another word that appears in its context) and a number of k randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary). By learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors. 
 This is important: instead of predicting the next word (the ""standard"" training technique), the optimized classifier simply predicts whether a pair of words is good or bad. 
 Word2Vec slightly customizes the process and calls it negative sampling. In Word2Vec, the words for the negative samples (used for the corrupted pairs) are drawn from a specially designed distribution, which favours less frequent words to be drawn more often. 
 References 
 [A]  (2005) - Contrastive estimation: Training log-linear models on unlabeled data 
 [B]  (2010) - Noise-contrastive estimation: A new estimation principle for unnormalized statistical models 
 [C]  (2008) - A unified architecture for natural language processing: Deep neural networks with multitask learning 
 [D]  (2012) - A fast and simple algorithm for training neural probabilistic language models . 
 [E]  (2013) - Learning word embeddings efficiently with noise-contrastive estimation .","[I've added this answer as I think others miss the main theoretical gist.] 
 Firstly, NCE and Negative Sampling (NS) serve different purposes: 
 
 NS is a generic trick used to train a classifier if you only have training samples from one `positive' class (e.g. labelled  $y\!=\!1$ ); 
 NCE is a method to learn parameters  $\theta$  of a model  $p_m(x;\theta)$  of a true data distribution  $p_d(x)$ . 
 
 So their purposes are different: NS learns to approximate a conditional label distribution  $p(y|x)$ , NCE approximates  $p_d(x)$ . Since NCE uses negative samples (or a  noise distribution ) to learn  $p_m(x|\theta)$  it can be seen as a case of NS. 
 NS is generic as it can be used just to train the classifier (e.g. in Knowledge Graph link prediction), to learn embeddings/representations (e.g.  word2vec ), or as a step in NCE. NCE is a special case of NS where  $p_n(x)$  is not just sampled from, but the actual density  $p_n(x)$  must be computed. 
 Simple explanation of NCE: 
 NCE is used to estimate the parameters  $\theta$  of a modelled data distribution  $p_m(x;\theta)$  by learning a classifier (optimised w.r.t.  $\theta$ ) that distinguishes true data samples from artificially generated noise samples  $x\!\sim\! p_n(x)$ . When the classifier is optimised, the corresponding  $\theta^*$  gives the desired distribution  $p_m(x;\theta^*)$ . 
 A naturally intuitive description of how this is different from Negative Sampling. 
 NS is not as clearly defined as NCE, but typically refers to when artificially generated samples,  $x\!\sim\!p_n(x)$ , labelled  $y\!=\!0$  (i.e.  $p_n(x)\!\equiv\!p(x|y\!=\!0)$ ) are used to train a classifier  $f(x;\theta)$  that distinguishes them from positive samples,  $x\sim p_d(x)\!\equiv\!p(x|y\!=\!1)$ , i.e. once trained  $f(x;\theta)\!\approx\!p(y\!=\!1|x)$ . 
 If the classifier uses the sigmoid function  $\sigma(t) \!=\! (1\!+\!e^{-t})^{-1}$ , i.e.  $f(x;\theta) \!=\! \sigma(g(x;\theta)$ ), then implicitly  $g(x;\theta) \!\approx\! \log\tfrac{p(y=1|x)}{p(y=0|x)} 
\!=\! \log\tfrac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)} 
\!=\! \log\tfrac{p_d(x)}{p_n(x)k}
$ , where  $k\!=\!\tfrac{p(y=0)}{p(y=1)}$ . Whilst this reformulation may not be of interest generally, it explains e.g. why  word2vec  embeddings learn pointwise mutual information (PMI). 
 In NCE,  $\theta$  is used to specifically parameterise  $p_d(x)$  (not the whole log ratio), i.e.  $p_m(x;\theta)\!\approx\!p_d(x)$ . Making that substitution and reversing the above equation gives a formula that approximates  $p(y\!=\!1|x)$  in terms of  $p_m(x;\theta), p_n(x)$  and  $k$  that fits into a binary cross entropy loss function. When that loss is minimised  $p_m(x;\theta)$  is the best approximation of  $p_d(x)$ . 
 Intuition for negative sampling in word2vec: we randomly sample from the vocabulary V and update only those as |V| is large and this offers a speedup. Correct if wrong. 
 In my view this isn't quite right. Yes, negative sampling seems to have been implemented as a trick to reduce computation time, but it fundamentally changes the maths and means the model parameters - which become word embeddings - learn different values (PMI) due to the choice of noise distribution (see Levy & Goldberg (2014)). That seems to have been an important aspect of why word2vec embeddings work as they do. 
 When to use which one and how to decide? Is NCE better than NS? Better in what manner? 
 Hopefully it's clear that you do the same thing in either case (generate negative samples, train a classifier). Whether you call it NCE or NS depends on what you want from it. A key choice affecting performance in all cases is the negative sampling distribution. The NCE paper looks into this but (I believe) the optimal choice is an open research question.","Honestly there is no intuitive way to understand why NCE loss will work without deeply understanding its math. To understand the math, you should read the original paper. 
 The reason why NCE loss will work is because NCE approximates maximum likelihood estimation (MLE) when the ratio of noise to real data  $k$  increases.  
 The TensorFlow implementation works in practice. Basically, for each data  $(x, y)$ ,  $y$  is the labeled class from the data, TensorFlow NCE loss samples  $k$  classes from noise distributions. We calculate a special version of the digits for each of the classes (1 from data +  $k$  from noise distributions) using equation 
 $$\Delta s_{\theta^0}(w,h) = s_{\theta^0}(w,h) - \log kP_n(w)$$ 
 Where  $P_n(w)$  is the noise distribution. With the digits for each classes calculated, TensorFlow use the digits to compute softmax loss for binary classification (log loss in logistic regression) for each of the classes, and add these losses together as the final NCE loss. 
 However, its implementation is conceptually wrong because the ratio of noise to real data  $k$  is different to the number of classes  $n$  sampled from noise distributions. TensorFlow NCE does not provide a variable for the noise to data ratio, and implicitly assumes  $n=k$  which I think is conceptually incorrect. 
 The original NCE papers skipped the derivations of the proof a lot so that it is really hard to understand NCE. To understand the math about NCE easier, I have a blog post on this annotating the math from the NCE papers: 
 https://leimao.github.io/article/Noise-Contrastive-Estimation/ .  
 College sophomore or above should be able to understand it.","Basically, this is selecting a sample from the true distribution which consists of the true class and some other noisy class labels. Then taking the softmax over it.  
 This is based on sampling words from true distribution and noise distribution.  
 Here the basic Idea is to train logistic regression classifier which can separate the samples obtained from true distribution and sample obtained from noise distribution. Remember When we are talking about the samples obtained from the true distribution we are talking about only one sample which is the true class obtained from the model distribution. 
 Here I have explained about NCE loss and how it differ from the NCE loss .  
 Noise Contrastive Estimation : Solution for expensive Softmax .","In simple words, the NCE is just a multi-label classification loss function with only 1 positive label and k sampled negative ones. 
 Illustration of multilabel classification:
 
 Source:  Approaches to Multi-label Classification",,,,,61.34940463,60.50279024,71.58049134,65.98550351,54.85735127,,,,
13120,How to overcome training example's different lengths when working with Word Embeddings (word2vec),word-embeddings,"Let me suggest three simple options: 
 
 average the vectors (component-wise), i.e., compute the word embedding vector for each word in the text, and average them.  (as suggested by others). 
 take the (component-wise) maximum of the vectors.  (max, instead of average) 
 take the (component-wise) minimum of the vectors.  (min, instead of average) 
 
 Each of these yields a feature vector that is independent of the length of the text. 
 There is some research suggesting that concatenating the max and the min yields a pretty effective feature space: it's not the absolute optimal, but it's close to optimal, and is simple and easy to implement.  See  this question on Statistics.SE  for details. 
 
 Here is an alternative idea, inspired by  cubone's answer , that as far as I know hasn't been tested before.  The idea is to tag the text using a part-of-speech tagger and then use those tags to inform the featurization process. 
 In particular, write down a list of all possible POS tags that could be emitted by the POS tagger.  Suppose there are 20 possible tags (CC, DT, JJS, MD, NNP, ...).  Then the feature vector will be 20*300 = 6000 elements long: it will have one 300-vector per POS tag, concatenated in some canonical order.  The 300-vector for each tag could be computed by averaging the word embedding vectors of all words that are tagged by the POS-tagger with that tag.  Or, you could get one 600-vector per POS tag, obtained by computing the min and max over all vectors of words with that tag. 
 This might yield a richer feature space, I don't know if it would yield any improvement, but it's something you could try if you wanted to experiment with different ideas.","Two very different suggestions here to avoid averging the vectors: 
 
 Use Word Mover's Distance ( https://github.com/mkusner/wmd ) to compute distance between the tweets (not sure how well it would work on short texts like tweets, I still need to try that myself...) 
 Cluster the word vectors themselves (using e.g. kmeans), then for each tweet create a vector with k entries (one for each cluster) that encodes whether it contains words belonging to that cluster. I think I saw this in a Kaggle tutorial on word2vec, will be happy for the link if you find it!","Instead of averaging and getting a single vector for the tweet, you can instead get vectors for each word and for different length vector sizes, padding can be done with zeros.","In my work, I have done the same way by averaging the word vectors. But there is another idea I wanted to try. It is with the help of POS tags. 
 First construct a most complicated sentence with all the POS tags as possible and set these POS tags as a template. For each sentence in the twitter corpus, POS tag all the words in it and apply those word vectors respective to the POS tags in the template. So, the unseen POS tags will have zeros. 
 For example:  NNP PDT DT NNS VB MD JJS CC PRP RBS  is the template. Thus each position will contain 300-dimensional vector totally a 3000-dimensional vector. And if  the first tweet's POS tags are  NNP VB JJS PRP , then the word vectors are applied on these positions and have vectors on  NNP VB JJS PRP  positions and 300-dimensional zero vectors on other positions. 
 This method not only solves the problem of representing the sentence by a single vector but also preserves the syntactic structure of the sentence by positioning in the right POS. 
 Ofcourse, there will be problems when there are more than one POS tags or jumbled positions of the tags. This is just one of the possibility.","I can think of a few possibilities that might fit your usecase: 
 
 Use the  hasshing trick  to turn arbitrary length vectors into constant length. 
 Trade out word vectors for  paragraph vectors .  Gensim  and  Deeplearning4j  both have implementations you can look at. 
 Check out the  awesome-2vec  list which links to a  tweet2vec  implementation!",,,,,55.00083349,58.78072064,58.56586657,52.22943702,58.9796281,,,,
12909,Definition of a model in machine learning,machine-learning,"I was interested in the same question recently and came to the realization that there is no single definition of a ""model"" in machine learning. It's highly dependent on the sources you're consulting, which may be the documentation for a particular software program, the slang adopted by its user community, or the definitions used in published academic papers, which can vary widely from journal to journal. Moreover, I had to learn to keep in mind that such papers are written not just by specialists in machine learning, but by experts in other disciplines who have a need for applying machine learning techniques (such as imaging, various medical fields, etc.). Many of them do not explicitly define the term ""model,"" which is often used loosely. Here are just a couple of different definition of ""model"" I've seen in all this widely scattered literature: 
 • Statistical models, particularly the stats related to probability distributions. 
 • Regression data and related statistics. 
 • Mathematical models as mentioned by Neil Slater above. 
 • The data models used in machine learning, such as the columns involved, their data types, the data sources and other metadata. This is particularly tricky because there's nothing mathematical about this definition at all, unlike the first three I listed. For an example, see all the documentation for SQL Server ""mining models,"" which serve double-duty for machine learning purposes. 
 • Sometimes all of the definitions above are expanded to include machine learning structures built on top of the equations and the metadata, such as the specifications of neural nets. In other cases, these are considered separate entities.  
 All of the above are sometimes mixed and matched together, depending on the source. I'm sure there are other definitions of ""model"" I've left off this list, which will complicate the matter even further. To deal with this ambiguity, I'm trying to train myself to divine the intentions of the author whenever they use the term ""model."" Sometimes it's easy to determine based on the context or the field the author works in, but other times I have to read deeply into an article or documentation before figuring it out. I wish I could be more definitive about it, but it's really a naturally fuzzy term; there's never going to be a simple one-size-fits-all answer to this. I hope that helps.","From article on  Amazon Machine Learning 
 The process of training an ML model involves providing an ML algorithm (that is, the learning algorithm) with training data to learn from.  The term ML model refers to the model artifact that is created by the training process.","I like the Machine Learning definition given by  Tom Mitchell . 
 
 A computer program is said to learn from  experience   E  with
respect to some class of  tasks   T  and  performance  measure  P 
if its performance at tasks in T, as measured by P, improves with
experience E. 
 
 So, given this definition, I should say that a model is the acquired experience after doing some task T.","A model, loosely speaking, is a simplification of some thing or process. For example, the shape of the Earth not actually a sphere, but we might treat it as one if we are designing a globe. Similarly, assuming the universe is deterministic, there is some natural process which determines if a customer will buy a product on a website. We might construct a something that approximates that process, which we could give some information about a customer and which tells us if it thinks that customer will buy a product. 
 A ""machine learning model"", then, is a model constructed by a machine learning system. 
 (Apologies for this not being a rigorous answer, but I hope this is still useful.)","In machine learning paradigm, model refers to a mathematical expression of model parameters along with input place holders for each prediction, class and action for regression, classification and reinforcement categories respectively.  
 This expression is embedded in the single neuron as a model.  
 For single layer perceptron and deep learning model, one needs to extract this model by carefully walking the neurons and layers to collect and stitch activation function in an ordered fashion.","In machine learning, a model is an abstraction that can perform a prediction, (re-)action or transformation to or in respect of an instance of input values. A model could be a single number such as the mean value of a set of observations which is often used as a baseline model, a polynomial expression or a set of rules (e.g. decision tree) that define how to get to generate the output.  
 In general, a model is defined by a set of rules and hyper-parameters that define the model's structure and capacity to be optimized to perform the task at hand. A hyper-parameter could be the degree of the polynomial or the depth of the decision tree. A model can be subjected to an optimization process where parameters are optimized against a certain objective. 
 The optimization process is often referred to as training for fitting and results in a fitted model, which also can be simply referred to as model. If a model was trained or not often needs to be deduced from the context.","In machine learning, the model is the center of gravity,  everything revolves around it. Yet, people have different definitions of 'model'; but in my opinion, the best definition of model in ML is ""the  hypothesis  that has learnt to predict, i.e. to  fit , unseen data"".","This is a fun discussion! My two cents are that a model is stored information that a computer can interpret to estimate mappings from some set of possible inputs to a set of appropriate outputs. A model is nothing more or less than the definition of a simple function that approximates a more complex function. It’s not necessary for the complex function to be a real-world phenomenon, only for the model to approximate the complex function without storing sufficient information to reproduce it perfectly.",,73.14346701,58.57090698,57.55006105,57.33796242,57.0402089,55.62725345,74.17677811,53.31952254,
12883,Best approach for this unsupervised clustering problem with categorical data?,machine-learning,"K-means is a reasonable approach and a sensible way to understand the data. 
 I've never used mahout, but I would use R or Python for this sort of analysis because of the nice libraries available to quickly implement K-means. 
 The clustering approach with the tags is fairly straightforward. You can essentially encode this using an indicator variable (also known as a binary encoding). You can set this variable/feature to 1 if the tag appeared in the list of tags and 0 otherwise. Then you only need to allocate space for the total number of tags that exist. If you have a large set of tags, you can limit them by taking tags with at least some frequency or some other ""sensible"" way. 
 You can choose $k$ in a number of ways. Typically people choose K arbitrarily because they want, e.g., 10 groups to segment their customers or data into. In the simulation I've provided, it'll give you a lame way to optimize for K using an incremental improvement.  
 
 I've made a notebook with a simulation walking you through how to ""tokenize"" your tags and represent them with a binary/one-hot encoding.  It's worth noting that this tokenization ignores the order of the tags, which may be okay for your use case. 
 It's also worth noting that K-means certainly isn't the only way to measure the similarity of your data but I think it's a nice intuitive start. 
 Again, for choosing $k$ the approach outlined in the notebook is exhaustive, since it starts from 1 and goes until each observation is a cluster. This means you'll have to run K-means $n$ times, which is silly in practice to do but useful from a learning perspective here. In general, this isn't ideal because it's expensive and typically you don't want to set $k = n$ but this simulation gives nice intuition about what's happening.  
 In practice, you can just do it in gaps for a large number of clusters (e.g., 5, 10, 15, 20, .., 100) or something like that and choose the one that has the biggest drop-off by eye-balling it. This is a very arbitrary and unsatisfying way to choose $k$, but it seems to work okay for many people.","as already answered  here , k-means in its original way won't be very effective, as Euclidean distances won't do the job with categorical data. 
 Some extensions exist (e.g. k-modes) or modifications with other distances (e.g. Gower). The discussion is expanded  here . 
 I don't have a straight answer, but I suggest to look to the different possibilities already implemented and consider pros and cons of each one.
 Here you can find a lot of different methodologies  that maybe can be adapted to your problem. 
 Hope it helps","If you are looking to cluster a very large amount of data located in a relational SQL or Hadoop type database you may want to use some of the algorithms built for parallel processing on Spark. The ML Lib main package for Spark can do this but I would recommend the H2O package as the documentation seems top notch, gives you a choice of many languages and can consume Terabytes of data quickly. If you already have a good development background this may play more to your engineering skill set and with H2O's great documentation you can focus more on your data pipeline, and feature selection than the theoretical underpinnings of your clustering model. You can implement H2O on Spark in Java or Scala as well staying with more of your engineering prospective, rather than working in a scripting language like R or Python. 
 Link to their K-means algorithm documentation 
 http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/k-means.html",K-means is neat strategy for these sort of problems. However you could also explore recent methods including  topological data analyses  to see clusters of features/individuals.,"You first need to convert this data into some numerical representation, and then you can use clustering.  
 One of such ways is applying TF-IDF weighting to tags, and then calculate the cosine similarity between them and, finally, apply some hierarchical clustering to the results. Or you can encode the entire record (including category, sector, etc) in a similar way, and do the same thing afterwards. 
 Also, you can apply K-Means to these TF-IDF weights - but if dimensionality of this matrix is large, prior to that you may need to reduce it with SVD or something similar. 
 I wouldn't use Mahout for this and start with scikit-learn. You may want to have a look at these classes and methods: 
 
 TfidfVectorizer 
 cosine_similarity 
 AgglomerativeClustering 
 TruncatedSVD 
 KMeans",,,,,53.22239264,55.65595036,53.02523047,54.02645909,52.33247153,,,,
12872,How can I get a measure of the semantic similarity of words?,nlp,"Word2vec does not capture similarity based on antonyms and synonyms. Word2vec would give a higher similarity if the two words have the similar context. Eg
The weather in California was _____ .
The blank could be filled by both hot and cold hence the similarity would be higher. This concept is called Paradigmatic relations. 
 If you are interested to capture relations such as hypernyms, hyponyms, synonyms, antonym you would have to use any wordnet based similarity measure. There are many similarity measures based on wordnet. You may check  this link","In  Text Analytic Tools for Semantic Similarity , they developed a algorithm in order to find the similarity between 2 sentences. But if you read closely, they find the similarity of the word in a matrix and sum together to find out the similarity between sentences. So, it might be a shot to check word similarity. 
 Also in  SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation , they explain the difference between  association  and  similarity  which is probably the reason for your observation as well. For example, coffee and a cup. They are not similar but they are associative. So just considering similarity would give a different result. The authors suggest various models to estimate them.","Word2vec is a good starting point for most scenarios. It  does  capture semantics by way of prediction using CBOW method. It allows translations (as most repeated example I can put here again), V(King) - V(Queen) ~~ V(men) - V(women) and so on.  
 So what is the problem? The issue lies in word sense ambiguity. Whenever the word itself has two different meaning in two different context, the word vector will tend to really be away from either context. Python ~ Boa (both snakes) and Python - Java (both programming languages).. 
 Any alternative? 
 For the very specific purpose of ""synonyms"" if you want  Wordnet  would be ideal place. It captures explicit relationship of two words rather than implicit relation based on usage and occurrences.  
 Wordnet is mostly crafted as a dictionary - where as word2vec is mined by usage.","GloVe Will ""Most Likely"" Work For Your Purposes 
 I found myself with a question similar to yours about 1 month ago. I met with some fellow data scientists that had more experience with NLP word vectorization than me. After reviewing many options, I felt that Global Vectors (GloVe) would work best for me. It is doing well for my purposes, and, for my purposes, I have found that training on my own available specialized corpora (plural for corpus = a bunch of documents), I was able to get good utility for my synonym searching needs.  
 The process was introduced  HERE , and clarified for me  HERE , but I found the most help as a python user  HERE , which will give you guidance on how to use trained models. Using python, I found I that I could only make use of  
 pip install glove==1.0.0  per  THIS StackOverflow Answer 
 Follow  THIS  for an idea of how to train your own corpus.  IF you need to train your GloVe model from your own corpus , 80%+ of your work will be deciding how to collect and condition your corpus to create your vocabulary and your co-occurrence matrix -  you want to do this part very well . Justification for training on a specialized corpus in another domain is reported  HERE , which was nice to find for encouragement to do the training work on a specialized corpus. I encourage anyone to evaluate whether or not this is necessary given your application.  
 I'm in the process of having domain experts blindly evaluate pretrained models against the models trained on our corpora. I'll try to remember to update this post once I have those results.","In a context free grammar, I think it is really kind of impossible to determine the closeness of words. What you can do is use lexicon vectors and then if a word is close in values between two lexicons then the value should be close.",,,,,59.57736379,68.48142919,56.62922111,51.44457587,53.1920505,,,,
12851,How do you visualize neural network architectures?,machine-learning,"I recently created a tool for drawing NN architectures and exporting SVG, called  NN-SVG","Tensorflow, Keras, MXNet, PyTorch 
 If the neural network is given as a Tensorflow graph, then you can  visualize this graph with TensorBoard . 
 Here is how the MNIST CNN looks like: 
 
 You can add names / scopes (like ""dropout"", ""softmax"", ""fc1"", ""conv1"", ""conv2"") yourself. 
 Interpretation 
 The following is only about the left graph. I ignore the 4 small graphs on the right half. 
 Each box is a layer with parameters that can be learned. For inference, information flows from bottom to the top. Ellipses are layers which do not contain learned parameters. 
 The color of the boxes does not have a meaning. 
 I'm not sure of the value of the dashed small boxes (""gradients"", ""Adam"", ""save"").","There is an open source project called  Netron 
 
 Netron is a viewer for neural network, deep learning and machine learning models. 
 Netron supports ONNX (.onnx, .pb), Keras (.h5, .keras), CoreML (.mlmodel) and TensorFlow Lite (.tflite). Netron has experimental support for Caffe (.caffemodel), Caffe2 (predict_net.pb), MXNet (-symbol.json), TensorFlow.js (model.json, .pb) and TensorFlow (.pb, .meta).","I would add ASCII visualizations using  keras-sequential-ascii  (disclaimer: I am the author). 
 A small network for CIFAR-10 (from  this tutorial ) would be: 
        OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)

           Input   #####     32   32    3
          Conv2D    \|/  -------------------       896     2.1%
            relu   #####     30   30   32
    MaxPooling2D   Y max -------------------         0     0.0%
                   #####     15   15   32
          Conv2D    \|/  -------------------     18496    43.6%
            relu   #####     13   13   64
    MaxPooling2D   Y max -------------------         0     0.0%
                   #####      6    6   64
         Flatten   ||||| -------------------         0     0.0%
                   #####        2304
           Dense   XXXXX -------------------     23050    54.3%
         softmax   #####          10
 
 For VGG16 it would be: 
        OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)

          Input   #####      3  224  224
     InputLayer     |   -------------------         0     0.0%
                  #####      3  224  224
  Convolution2D    \|/  -------------------      1792     0.0%
           relu   #####     64  224  224
  Convolution2D    \|/  -------------------     36928     0.0%
           relu   #####     64  224  224
   MaxPooling2D   Y max -------------------         0     0.0%
                  #####     64  112  112
  Convolution2D    \|/  -------------------     73856     0.1%
           relu   #####    128  112  112
  Convolution2D    \|/  -------------------    147584     0.1%
           relu   #####    128  112  112
   MaxPooling2D   Y max -------------------         0     0.0%
                  #####    128   56   56
  Convolution2D    \|/  -------------------    295168     0.2%
           relu   #####    256   56   56
  Convolution2D    \|/  -------------------    590080     0.4%
           relu   #####    256   56   56
  Convolution2D    \|/  -------------------    590080     0.4%
           relu   #####    256   56   56
   MaxPooling2D   Y max -------------------         0     0.0%
                  #####    256   28   28
  Convolution2D    \|/  -------------------   1180160     0.9%
           relu   #####    512   28   28
  Convolution2D    \|/  -------------------   2359808     1.7%
           relu   #####    512   28   28
  Convolution2D    \|/  -------------------   2359808     1.7%
           relu   #####    512   28   28
   MaxPooling2D   Y max -------------------         0     0.0%
                  #####    512   14   14
  Convolution2D    \|/  -------------------   2359808     1.7%
           relu   #####    512   14   14
  Convolution2D    \|/  -------------------   2359808     1.7%
           relu   #####    512   14   14
  Convolution2D    \|/  -------------------   2359808     1.7%
           relu   #####    512   14   14
   MaxPooling2D   Y max -------------------         0     0.0%
                  #####    512    7    7
        Flatten   ||||| -------------------         0     0.0%
                  #####       25088
          Dense   XXXXX ------------------- 102764544    74.3%
           relu   #####        4096
          Dense   XXXXX -------------------  16781312    12.1%
           relu   #####        4096
          Dense   XXXXX -------------------   4097000     3.0%
        softmax   #####        1000","In Caffe you can use   caffe/draw.py  to draw the NetParameter protobuffer: 
 
 In Matlab, you can use   view(net) 
 
 Keras.js : 
 
 Also, see  Can anyone recommend a Network Architecture visualization tool? (Reddit/self.MachineLearning) .","Keras 
 The  keras.utils.vis_utils module  provides utility functions to plot a Keras model (using graphviz) 
 The following shows a network model that the first hidden layer has 50 neurons and expects 104 input variables. 
 plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)","PlotNeuralNet LaTex tool 
 This solution is not automatically generated (you need to construct the graph by yourself) but  the PlotNeuralNet github repo  allows you to build images directly from LaTex, and the result is great ! See for example the image below from the README :
 
 or my example :","Here is yet another way -  dotnets , using  Graphviz , heavily inspired by  this post  by Thiago G. Martins.","I've been working on a drag-and-drop neural network visualizer (and more). Here's an example of a visualization for a LeNet-like architecture.  
Models with fan-out and fan-in are also quite easily modeled. You can visit the website at  https://math.mit.edu/ennui/ 
 The open-source implementation is available at  https://github.com/martinjm97/ENNUI .",55.37304665,55.43611114,53.43779626,50.6929334,65.24136416,51.08251116,50,50,64.48623526
12761,Should a model be re-trained if new observations are available?,machine-learning,"When new observations are available, there are three ways to retrain your model: 
 
 Online:  each time a new observation is available, you use this single data point to further train your model (e.g. load your current model and further train it by doing backpropagation with that single observation). With this method, your model learns in a sequential manner and sort of adapts locally to your data in that it will be more influenced by the recent observations than by older observations. This might be useful in situations where your model needs to dynamically adapt to new patterns in data. It is also useful when you are dealing with extremely large data sets for which training on all of it at once is impossible. 
 Offline:  you add the new observations to your already existing data set and entirely retrain your model on this new, bigger data set. This generally leads to a better global approximation of the target function and is very popular if you have a fixed data set, or if you don't have new observations to often. However it is unpractical for large data sets. 
 Batch/mini batch:  this is sort of a middle ground approach. With batch, you wait until you have a batch of $n$ new observations and then train your already existing model on this whole batch. It is not offline as you are not adding this batch to your preexisting data set and then retraining your model on it and it is not online as your are training your model on $n$ observations at once and not just a single one. So it's a bit of both :)  Mini batch  is exactly the same except that the batch size is smaller so it tends towards online learning. Actually online learning is just batch with batch size 1 and offline is batch with batch size the size of the whole data set.  
 
 Most models today will use  batch/mini batch  and the choice for the size of the batch depends on your application and model. Choosing the right size batch is equivalent to choosing the right frequency with which to re-train your model. If your new observation have a low variance with your existing data, I'd suggest larger batches (256-512 maybe) and if on the contrary new observations tend to vary greatly with your existing data, use small batches (8-256). At the end of the day, batch size is kind of like another hyper-parameter which you need to tune and which is specific to your data","Once a model is trained and you get new data which can be used for training, you can load the previous model and train onto it. For example, you can save your model as a  .pickle  file and load it and train further onto it when new data is available. Do note that for the model to predict correctly,  the new training data should have a similar distribution as the past data . 
 Predictions tend to degrade based on the dataset you are using. For example, if you are trying to train using twitter data and you have collected data regarding a product which is widely tweeted that day. But if you use use tweets after some days when that product is not even discussed, it might be biased.  The frequency will be dependent on dataset  and there is no specific time to state as such.  If you observe that your new incoming data is deviating vastly, then it is a good practise to retrain the model . 
 Optimizing parameters on the aggregated data is not overfitting. Large data doesn't imply overfitting. Use cross validation to check for over-fitting.","When should you re-train? 
 Theoretically, a model will only degrade (become outdated and no longer useful) if the system you are modelling or the nature of the data has changed. Ideally you can spot this by setting up automated monitoring of the model in production. This could mean that predictions on new incoming data will be compared with the ground-truth data and you will be alerted if your error metric exceeds your desired range. Or it could mean you keep tabs on an indirectly related KPI, and if it exceeds your desired range, you must reevaluate whether the model is still serving your cause. If your model is no longer so useful, it is time to re-train, and the same best practices should be followed as when you created the original model, particularly with regards to model validation. 
 There is no reason to re-train if the error metric / KPI stays within your desired range (if your model is serving its purpose). There is no benefit to incrementally updating a model when the nature of the data and the system being modeled have not changed, but there are downsides: 
 
 increased cloud computation costs, 
 loss of ""unseen"" data that could be used for model validation, 
 unnecessary extra work for the data scientist who must validate these new models. 
 
 What are best-practices for re-training? 
 It is risky to set up this re-training in an  automated  fashion because automated model training cannot yet produce models which match the quality and reliability of  human-validated  models. 
 Proper model validation cannot be done in absentia. Ideally, it looks like a semblance of the following (depends on the type of model you are building): 
 
 ensure the data still meets the assumptions of the algorithm (e.g. for linear regression, is the Y-variable normally distributed? Are the errors independently scattered about the mean? Etc.) 
 train/validation/test set (keep an eye on over-fitting) 
 use of cross-validation and/or bootstrapped samples 
 validation of key model metrics (i.e. error, accuracy, F-value, p-value, etc.) 
 comparison of model scores (e.g. accuracy) with an ANOVA F-statistic to determine whether there is a statistically significant difference between models (bonus points if those scores are averaged CV scores for each model) 
 approximation and evaluation of a confidence interval for model score (e.g. ""the 95% CI for the accuracy of this model is within range [78.04%, 79.60%]"") 
 use of an ROC curve to compare models 
 a cost/benefit analysis of the best models:
 
 time to train model 
 time to query model 
 scalability 
 scrutability (can I easily explain to stakeholders how this model works?) 
 interpretability (can I easily see which factors are deemed important and actionable by this model?) 
 updatability 
 
 
 etc. 
 
 If you believe your model will benefit from incremental updates, that implies the underlying system or nature of the data is changing over time, and thus the model you validated yesterday is not necessarily valid today. You should not assume that simply optimizing the parameters on the old model (given new data) will produce a statistically significant improvement in your model, or a valid model for that matter. Such incremental updates should be done with proper model validation overseen by a data scientist. This is because validating a model is a complex task, and semi-qualitative challenges will likely arise. For example: 
 
 it could be the case that some predictor variables are no longer relevant and should be removed 
 maybe the nature of data fundamentally changed after 2015 so the training set should be filtered 
 perhaps some new data needs to be collected to better reflect the system being modeled 
 a change reflected in the new data could introduce multicollinearity into the model, violating a model assumption and thereby invalidating it 
 maybe the algorithm or general approach needs to be changed altogether 
 
 When can re-training be automated? 
 Of course, you  can  (caution) apply machine learning with loose regard for the fundamental assumptions and concepts. This can be a matter of cost/benefit in your application area. 
 For example, it could be beneficial to set up automated re-training when you have tens of thousands of models that need to be updated on a regular basis and the stakes on the predictions are low. For example you could have a model for each individual user of an application, and you just want to predict some semi-trivial behavior. Perhaps the cost of an inaccurate prediction is low, but the benefit of an accurate prediction is high. This could make good business sense. 
 But if the stakes are high on your predicted outcome, I would apply the fundamental concepts and follow the best-practices.","The question: SHOULD you retrain? 
 The answer depends on what your model attempts to do and in what environment it is applied.  
 Let me explain by a couple of examples:  
 Suppose that your model attempts to predict customers' behaviour, e.g. how likely is a customer to purchase your product given an offer tailored for him. Clearly, the market changes over time, customers' preferences change, and your competitors adjust. You should adjust as well, so you need to retrain periodically. In such a case I would recommend to add new data, but also omit old data that is not relevant anymore. If the market is fast changing, you should even consider retraining periodically based on new data only. 
 On the other hand, if your model classify some imaging (e.g x-ray or MRI) to medical conditions, and the model performs well, you do not need to retrain if there is no change in the technology or in the medical know-how. Adding more data will not improve much.","Your problem comes under the umbrella of Online Learning methods. Assuming a stream of data coming, you can use Stochastic Gradient Descent method to update your model parameters using that single example.  
 If your cost function is :  
 $ \min_\theta J(x,y,\theta) $ ,  
 where $\theta$ is parameter vector, then assuming a streaming data of form ($x^{i}, y^{i}$), you can update your parameter vector using SGD with the following update equation : 
 $ \theta^{t} = \theta^{t-1} - \nabla_\theta J(x^{i}, y^{i}) $.  
 This is essentially SGD with batch size 1.  
 There is one other trick, you can adopt a window/buffer based method, where you buffer some examples from stream and treat it as batch and use batch SGD. In that case the update equation will become: 
 $ \theta^{t} = \theta^{t-1} - \sum_{i} \nabla_\theta J(x^{i}, y^{i}) $. 
 This is essentially mini-batch SGD.","I don't understand the third question  Is it over-fitting if the parameters are re-optimised for the aggregated data? , but I get the answers to these three: 
 
 I have not been able to find any literature on this subject 
 
 You can refer to chapter 9: Model Deployment and Prediction Service of this book:  Continual Learning and Test in Production . 
 
 What are the best practices in model training and optimization if new observations are available? 
 
 Depending on the practice of AI in the company you are working for, you can follow the following four stages: 
 
 Manual, stateless retraining
When you don't have a MLOps platform, you monitor the performance of your model and update it if it has degraded to the point that it's doing more harm than good, and your team has time to update it. 
 Automated retraining
When you have the infrastructure, you write a script to automatically update the model, each time retrain the model from scratch. This is a norm for most companies. 
 Automated, stateful trianing
Your infrastructure matures enough and you continue training the model automatically using fresh data according to a predefined script. 
 Continual learning
You write a script to automatically detect data distributions shift and automatically update the model with a dynamic schedule. 
 
 
 Is there any way to determine the period/frequency of re-training a model before the predictions begin to degrade? 
 
 One way to figure out the gain is by training your model on the data from different time windows in the past and evaluating it on the data from today to see how the performance changes.
 
 You decide the frequency based on the performance results.",,,,68.14105311,60.53556957,57.49734574,52.74695953,50.25901157,60.41027412,,,
12721,Time series prediction using ARIMA vs LSTM,time-series,"Statement 1 is correct, statement 2 is correct, but requires elaboration, and statement 3 is incorrect for seasonal ARIMA: 
 The following might point you in the right direction but hopefully you'll get a few more answers with more depth in the arena of LSTM. 
 You mention that you have tried both algorithms and that you are simply trying to figure out which one is better, which leads me to think you may be having more trouble with the data science process and cross validation than with the specifics of the models. 
 Time series in general: 
 Time series, in general, are difficult to forecast. If they were easy to forecast then all data scientists would be wealthy, having accurately forecast the value of all of the stocks. The reality is that hedge funds, on average, do not outperform the market and that time series forecasting is typically very poor and applies only to very short durations.  The main problems are that there is a lot of noise, there are many hidden influences, models are overly simplistic, influencers do not behave as we think they should, the interplay between linearity and nonlinearity is subtle and confusing, ... ad infinitum.  
 ARIMA 
 You are incorrect in your assessment that ARIMA requires stationary time series to forecast on.   Non-seasonal ARIMA has three input values to help control for smoothing, stationarity, and forecasting  ARIMA(p,d,q), where: 
 
 p is the number of autoregressive terms, 
 d is the number of nonseasonal differences needed for stationarity, and 
 q is the number of lagged forecast errors in the prediction equation. 
 
 By contrast  seasonal ARIMA has six input values  ARIMA(p,d,q,P,D,Q), where: 
 
 P is the number of seasonal autoregressive terms,  
 D is the number of seasonal differences, and  
 Q is the number of seasonal moving-average
terms. 
 
 Subject to the qualifying statements above, I suggest playing with seasonal ARIMA to get a feel for the intricacies involved in smoothing, de-seasoning, de-trending, de-noiseing, and forecasting. 
 LSTM 
 I don't know enough about LSTM to add much here. I will add that red flags tend to be raised when someone begins at data science exercise with deep learning. I suggest learning as much as you can using ARIMA and then applying some of your ARIMA expertise to help you learn LSTM. Neural networks can be a very powerful tool, but they: 
 
 can take a long time to run, 
 often require more data to train than other models, and 
 have lots of input parameters to tune. 
 
 Cross validation and comparing models: 
 Time series are fun in that all training data can usually be turned into supervised learning training sets.  Once can simply take a time series and roll back time. That is... pick a point in time and pretend that you don't have any additional data, then produce a forecast and see how well you did.  You can  march through the time series doing this $n$ times in order to get an assessment of the performance of your model  and to compare models while taking the  necessary precautions  to  prevent overfitting . 
 Hope this helps and good luck!","Adding to @AN6U5's respond. 
 From a purely theoretical perspective, this  paper  has show RNN are universal approximators. I haven't read the paper in details, so I don't know if the proof can be applied to LSTM as well, but I suspect so. The biggest problem with RNN in general (including LSTM) is that they are hard to train due to gradient exploration and gradient vanishing problem. The practical limit for LSTM seems to be around 200~ steps with standard gradient descent and random initialization. And as mentioned, in general for any deep learning model to work well you need a lot of data and heaps of tuning. 
 ARIMA model is more restricted. If your underlying system is too complex then it is simply impossible to get a good fit. But on the other hand, if you underlying model is simple enough, it is much more efficient than deep learning approach.","ARIMA models are linear and LSTM models are nonlinear. Some other parametric nonlinear time series models that statisticians have studied are Threshold Autoregressive Models (TAR) and Smooth Transition Autoregressive Models (STAR). The R package tsDyn implements these models. 
 I wonder how STAR models do vs. LSTM.","I’ve come to the same conclusion as yourself and others, traditional forecasting is still probably the most applicable and maybe reliable for time series of numeric values. There is some slight bleed in deep learning in discussion where time series for numeric values gets mixed into deep learning, where deep learning (currently) applies to modern challenges in pattern recognition for image, sound, clean text, or anomaly detection. I often have good results with VAR / VECM for daily transactional data, which could probably be applied to your signal processing use case.","As an extreme case, I had a chance to study on Forex (Foreign Exchange Rate) forecast and intensively compared performances of LSTM, windowed-MLP and ARIMA. As many articles say, Forex time series is close to the random walk series (it is completely non-stationary). None of these algorithms can predict next day's spot rate. For example, if there is no (or little) change, then it will maintain current value and it looks fit. However, if there is a sudden (substantial) change in tomorrow's spot rate, then it always fails to predict. The problem is that there is nothing to learn from the training data (say, past 10 years' spot rates history) whether it is large enough or not, because it is completely random (perhaps, due to exchange dealers' sentiments ...). 
 In fact, LSTM with just '1' lag, or ARIMA with (1,0,0) will perform the best since storing data with more than 1 time lag doesn't help at all. For this kind of time series, I would say, in terms of tracking performance, 1) ARIMA, 2) LSTM, 3) MLP. Last comment... MLP with macro/micro economic parameters (say, GDP difference, bond rates, crude oil price, dollar index, etc) as additional features didn't work at all also.","ARIMA gives Trend (or Regression) - can see its slope (that reflects speed of change dx/dy)... 
 LSTM gives MovingAverage - posessing curvature & slope in each moment - that can characterize not only speed but also acceleration changes to up or down 
 p.s. 
 in all times Lines in e.g. classical supply-demand charts in economics were considered to be  Static Models  (as all regressions), and Curves on such charts (instead of lines) reflects  Dynamic Models  (showing not only speed but also acceleration)",The library forecast in R can calculate all the parameters for Arima models automatically for you.  No need to figure it out.  You can ever provide it additional timeseries that could explain the residual error.,"I think, you are misusing MachineLearning & Deep Learning when trying to predict tomorrow... - any statistical Approximations of the chaos can show just averaged Tendency & its borders (variance) according probability distributions, but not the obligations of the evaluated system to be determined exactly one way (because social systems and their derivatives are rather volatile) and of course not tomorrow -- all predictions according existing Tendency can be realized in time -  TILL something will change this Tendency ... 
 and recall once again: calculations in ProbabilityTheory cannot give prediction of exact event, but the range of possible events - each of them having their own probabilities, but still having these probabilities (even if being low) in a very volatile environment of social needs, habits, laws, the daily routine & regulations in each country & etc. influencing financial markets",,72.77784343,58.34224987,63.94680144,57.99929483,67.75027631,57.28379404,57.04383285,53.44620219,
12679,Has anyone been able to run Tensorflow with GTX 1070 GPU on Ubuntu 16.04/15.10/15.04?,tensorflow,"I was in the same boat, but now that I have figured it out, I have listed the steps for installing tensorflow 0.9 with cuda toolkit 8.0, cudnn 5.1, bazel 0.3 on Ubuntu 16.04 LTS here:  http://abhay.harpale.net/blog/machine-learning/deep-learning/getting-tensorflow-to-work-with-gpu-nvidia-gtx-1080-on-ubuntu-16-04-lts/ 
 Here's the gist  
 
 Install NVidia Cuda Toolkit 
 Install NVidia CuDNN 
 Install Tensorflow dependencies such as swig, python-dev, numpy, python-wheel, zlib1g, g++ 
 Configure and build tensorflow using Bazel","This helped me run on GTX 1080 on a 16.04 ubuntu machine (driver 367.27):
 http://yangcha.github.io/Tensorflow/ 
 It basically says install CUDA 8RC, CuDNN 5 and build TensorFlow from source by following the instructions. 
 I assume the GTX 1070 should behave the same to GTX 1080 on that regard.","This took me quite a while to get just right but here is what I did
    Download Ubuntu 16.04 .iso
    Download unetbootin
    Boot from drive to install ubuntu use 3rd parter drivers to avoid wireless issues
    Download the driver for the NVIDIA 1070 card 367.27
    Hit ctrl-alt-f1 to open a virtual terminal
    sudo service lightdm stop
    cd ~/Downloads
    sudo chmod 755 ""name of driver"".run
    sudo ./""name of driver"".run
    cd
    reboot 
 Download CUDA 8.0 and patch 1
Hit ctrl-alt-f1 to open a virtual terminal
sudo service lightdm stop
cd ~/Downloads
sudo chmod 755 ""name of CUDA installer"".run
sudo ./""name of CUDA installer"".run --override
*Do not install the driver since we already did
sudo chmod 755 ""name of CUDA installer"".run
sudo ./""name of CUDA installer"".run --override
sudo chmod 755 ""name of CUDA patch"".run
sudo ./""name of CUDA patch"".run
cd
reboot

Download cuDNN 5.1
cd ~/Downoads

tar xvzf cudnn-8.0-linux-x64-v5.1.tgz
sudo cp cuda/include/cudnn.h /usr/local/cuda-8.0/include
sudo cp cuda/lib64/libcudnn* /usr/local/cuda-8.0/lib64
sudo chmod a+r /usr/local/cuda-8.0/include/cudnn.h /usr/local/cuda-8.0/lib64/libcudnn*

Run and add the following to bash file

export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64""
export CUDA_HOME=/usr/local/cuda-8.0


sudo apt-get install python-pip python-dev

sudo apt-get install git


$ git clone https://github.com/tensorflow/tensorflow

sudo apt-get install openjdk-8-jdk


sudo apt-get install pkg-config zip g++ zlib1g-dev unzip

Download bazel-0.3.1 for linux


 chmod +x bazel-version-installer-os.sh
 ./bazel-version-installer-os.sh --user

sudo apt-get install python-numpy swig python-dev python-wheel

cd ~/tensorflow

./configure

use CUDA 8.0 and cudnn 5.1.5

bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer


bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu

bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

sudo pip install /tmp/tensorflow_pkg/tensorflow-0.10.0rc0-py2-none-any.whl


cd tensorflow/models/image/mnist
python convolutional.py","I've created some GIST file with steps how to ""Install TensorFlow v 0.11 and CUDA 8.1 | CUDNN 5.1 on Ubuntu 16.04"" 
 https://gist.github.com/denti/41860cb6b55e0847b4f2685016c7f14e 
 It works perfect for me for fresh 16.04.
You can skip last part with install bazel and tensorflow from scratch.","The easiest thing would be to download a Docker Image. I recently created a Docker Image with CUDA 9.0, cudnn, tensorlflow and keras. I have a gtx 1070 and it worked without any issue. 
 https://hub.docker.com/r/deejay217/cuda9_python3_tensorflow17/",,,,,57.47318089,73.93617399,54.67574357,60.06398171,55.49220234,,,,
12645,How to count the number of missing values in each row in Pandas dataframe?,python,"When using pandas, try to avoid performing operations in a loop, including  apply ,  map ,  applymap  etc. That's slow! 
 A DataFrame object has two axes: “axis 0” and “axis 1”. “axis 0” represents rows and “axis 1” represents columns. 
 If you want to count the missing values in each column, try: 
 df.isnull().sum()  as default or  df.isnull().sum(axis=0) 
 On the other hand, you can count in each row (which is your question) by: 
 df.isnull().sum(axis=1) 
 It's roughly 10 times faster than Jan van der Vegt's solution(BTW he counts valid values, rather than missing values): 
 In [18]: %timeit -n 1000 df.apply(lambda x: x.count(), axis=1)
1000 loops, best of 3: 3.31 ms per loop

In [19]: %timeit -n 1000 df.isnull().sum(axis=1)
1000 loops, best of 3: 329 µs per loop","You can apply a count over the rows like this: 
 test_df.apply(lambda x: x.count(), axis=1)
 
 test_df: 
     A   B   C
0:  1   1   3
1:  2   nan nan
2:  nan nan nan
 
 output: 
 0:  3
1:  1
2:  0
 
 You can add the result as a column like this: 
 test_df['full_count'] = test_df.apply(lambda x: x.count(), axis=1)
 
 Result: 
     A   B   C   full_count
0:  1   1   3   3
1:  2   nan nan 1
2:  nan nan nan 0","The simplist way: 
 df.isnull().sum(axis=1)","Or, you could simply make use of the info method for dataframe objects: 
 df.info()
 
 which provides counts of non-null values for each column.","null values along the column, 
 df.isnull().sum(axis=0)
 
 blank values along the column, 
 c = (df == '').sum(axis=0)
 
 null values along the row, 
 df.isnull().sum(axis=1)
 
 blank values along the row, 
 c = (df == '').sum(axis=1)",">>> df = pd.DataFrame([[1, 2, np.nan],
...                    [np.nan, 3, 4],
...                    [1, 2,      3]])

>>> df
    0  1   2
0   1  2 NaN
1 NaN  3   4
2   1  2   3

>>> df.count(axis=1)
0    2
1    2
2    3
dtype: int64","This snippet will return integer value of total number of columns with missing value: 
 (df.isnull().sum() > 0).astype(np.int64).sum()","If you want count of missing values: 
 np.logical_not(df.isnull()).sum()",,60.29658777,54.01864668,50,55.93632707,53.61055546,53.68121594,58.80949606,64.93876031,
12575,Similarity between two words,nlp,"The closest would be like Jan has mentioned inhis answer, the  Levenstein's distance  (also popularly called the edit distance). 
 
 In information theory and computer science, the Levenshtein distance
  is a string metric for measuring the difference between two sequences.
  Informally, the Levenshtein distance between two words is the minimum
  number of single-character edits (i.e. insertions, deletions or
  substitutions) required to change one word into the other. 
 
 It is a very commonly used metric for identifying similar words. Nltk already has an implementation for the edit distance metric, which can be invoked in the following way: 
 import nltk
nltk.edit_distance(""humpty"", ""dumpty"")
 
 The above code would return  1 , as only one letter is different between the two words.","Apart from very good responses here, you may try SequenceMatcher in difflib python library. 
 https://docs.python.org/2/library/difflib.html 
 import difflib

a = 'Thanks for calling America Expansion'
b = 'Thanks for calling American Express'

seq = difflib.SequenceMatcher(None,a,b)
d = seq.ratio()*100
print(d) 
### OUTPUT: 87.323943
 
 Now Consider the below code: 
 a = 'Thanks for calling American Expansion'
b = 'Thanks for calling American Express'

seq = difflib.SequenceMatcher(None,a,b)
d = seq.ratio()*100
print(d)
### OUTPUT: 88.88888
 
 Now you may compare the d value to evaluate the similarity.","If your dictionary is not too big a common approach is to take the Levenshtein distance, which basically counts how many changes you have to make to get from one word to another. Changes include changing a character, removing a character or adding a character. An example from  Wikipedia : 
 lev(kitten, sitting) = 3 
 
 k  itten ->  s  itten 
 sitt  e  n -> sitt  i  n 
 sittin -> sittin  g 
 
 Here  are some Python implements on Wikibooks. 
 The algorithm to compute these distances is not cheap however. If you need to do this on a big scale there are ways to use cosine similarity on bi-gram vectors that are a lot faster and easy to distribute if you need to find matches for a lot of words at once. They are however only an approximation to this distance.","An old and well-known technique for comparison is the  Soundex  algorithm.
The idea is to compare not the words themselves but approximations of how they are pronounced.  To what extent this actually improves the quality of the results I don't know. 
 However it feels a bit strange to apply something like Soundex to results from a speech-to-text recognition engine.  First you throw away information about how the words are pronounced, then you try to add it back again.  It would be better to combine these two phases. 
 Hence, I expect the state of the art technology in this area to do that, and be some form of adaptive classification, e.g. based on neural networks.  Google does return recent research on  Speech Recognition with Neural Networks .","I think the function  get_close_matches  in module  difflib  could be more suitable for such a requirement. 
 get_close_matches(word, possibilities, n=3, cutoff=0.7)

possibilities -> is the list of words
n = maximum number of close matches
cutoff = accuracy of matches.


data=[""drain"",""rain"",""brain"",""stackexchange""]

word=""rainnn""

if len(get_close_matches(w, data,  n=3, cutoff=0.7)) > 0:
   return data[get_close_matches(w, data,  n=3, cutoff=0.7)[0]]
 
 This piece of code will return the best first match and that will be word rain.","To complement other answers: 
 You can featurize both sentences and then look at  cosine similarity  between their feature representations. 
 To featurize text, there are many methods you can use; from simple counting-based operators like TFIDF to word embeddings like word2vec or more complex language models like BERT. 
 The TextWiser Library  might come in handy if you want to experiment with several text featurization methods including their transformations for dimensionality reduction like SVD, LDA, UMAP etc. 
 Here is a usage example: 
 
# Conceptually, TextWiser is composed of an Embedding, potentially with a pretrained model,
# that can be chained into zero or more Transformations
from textwiser import TextWiser, Embedding, Transformation, WordOptions, PoolOptions

# Data
documents = [""Some document"", ""More documents. Including multi-sentence documents.""]

# Model: TFIDF `min_df` parameter gets passed to sklearn automatically
emb = TextWiser(Embedding.TfIdf(min_df=1))

# Model: TFIDF followed with an NMF + SVD
emb = TextWiser(Embedding.TfIdf(min_df=1), [Transformation.NMF(n_components=30), Transformation.SVD(n_components=10)])

# Model: Word2Vec with no pretraining that learns from the input data
emb = TextWiser(Embedding.Word(word_option=WordOptions.word2vec, pretrained=None), Transformation.Pool(pool_option=PoolOptions.min))

# Model: BERT with the pretrained bert-base-uncased embedding
emb = TextWiser(Embedding.Word(word_option=WordOptions.bert), Transformation.Pool(pool_option=PoolOptions.first))

# Features
vecs = emb.fit_transform(documents)
 
 You can easily switch between different Embedding and Transformation options and see how they impact your downstream tasks, in your case, the similarity between the sentences. 
 Notice you can even chain the Transformations; e.g., NMF followed by SVD operation. 
 Disclaimer: I am a member of the TextWiser team.",,,,62.07041543,50.68498421,53.70387648,53.24548455,52.7983991,53.53954343,,,
12554,Does XGBoost handle multicollinearity by itself?,feature-selection,"Decision trees are by nature immune to multi-collinearity. For example, if you have 2 features which are 99% correlated, when deciding upon a split the tree will choose only one of them. Other models such as Logistic regression would use both the features. 
 Since boosted trees use individual decision trees, they also are unaffected by multi-collinearity. However, its a good practice to remove any redundant features from any dataset used for training, irrespective of the model's  algorithm. In your case since you're deriving new features, you could use this approach, evaluate each feature's importance and retain only the best features for your final model. 
 The importance matrix of an xgboost model is actually a data.table object with the first column listing the names of all the features actually used in the boosted trees. The second column is the Gain metric which implies the relative contribution of the corresponding feature to the model calculated by taking each feature's contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction.","I was curious about this and made a few tests. 
 I’ve trained a model on the diamonds dataset, and observed that the variable “x” is the most important to predict whether the price of a diamond is higher than a certain threshold.
Then, I’ve added multiple columns highly correlated to x, ran the same model, and observed the same values.  
 It seems that when the correlation between two columns is 1, xgboost removes the extra column before calculating the model, so the importance is not affected.
However, when you add a column that is partially correlated to another, thus with a lower coefficient, the importance of the original variable x is lowered.  
 For example if I add a variable xy = x + y, the importance of both x and y decrease. Similarly, the importance of x decreases if I add new variables with r=0.4, 0.5 or 0.6, although just by a bit. 
 I think that collinearity is not a problem for boosting when you calculate the accuracy of the model, because the decision tree doesn’t care which one of the variables is used. However it might affect the importance of the variables, because removing one of the two correlated variables doesn't have a big impact on the accuracy of the model, given that the other contains similar information. 
 library(tidyverse)
library(xgboost)

evaluate_model = function(dataset) {
    print(""Correlation matrix"")
    dataset %>% select(-cut, -color, -clarity, -price) %>% cor %>% print

    print(""running model"")
    diamond.model = xgboost(
        data=dataset %>% select(-cut, -color, -clarity, -price) %>% as.matrix, 
        label=dataset$price > 400, 
        max.depth=15, nrounds=30, nthread=2, objective = ""binary:logistic"",
        verbose=F
        )

    print(""Importance matrix"")
    importance_matrix <- xgb.importance(model = diamond.model)
    importance_matrix %>% print
    xgb.plot.importance(importance_matrix)
    }

> diamonds %>% head
carat   cut color   clarity depth   table   price   x   y   z
0.23    Ideal   E   SI2 61.5    55  326 3.95    3.98    2.43
0.21    Premium E   SI1 59.8    61  326 3.89    3.84    2.31
0.23    Good    E   VS1 56.9    65  327 4.05    4.07    2.31
0.29    Premium I   VS2 62.4    58  334 4.20    4.23    2.63
0.31    Good    J   SI2 63.3    58  335 4.34    4.35    2.75
0.24    Very Good   J   VVS2    62.8    57  336 3.94    3.96    2.48
 
 Evaluate a model on the diamonds data 
 We predict whether the price is higher than 400, given all numeric variables available (carat, depth, table, x, y, x) 
 Note that x is the most important variable, with an importance gain score of 0.375954. 
 evaluate_model(diamonds)
    [1] ""Correlation matrix""
               carat       depth      table           x           y          z
    carat 1.00000000  0.02822431  0.1816175  0.97509423  0.95172220 0.95338738
    depth 0.02822431  1.00000000 -0.2957785 -0.02528925 -0.02934067 0.09492388
    table 0.18161755 -0.29577852  1.0000000  0.19534428  0.18376015 0.15092869
    x     0.97509423 -0.02528925  0.1953443  1.00000000  0.97470148 0.97077180
    y     0.95172220 -0.02934067  0.1837601  0.97470148  1.00000000 0.95200572
    z     0.95338738  0.09492388  0.1509287  0.97077180  0.95200572 1.00000000
    [1] ""running model""
    [1] ""Importance matrix""
       Feature       Gain      Cover  Frequency
    1:       x 0.37595419 0.54788335 0.19607102
    2:   carat 0.19699839 0.18015576 0.04873442
    3:   depth 0.15358261 0.08780079 0.27767284
    4:       y 0.11645929 0.06527969 0.18813751
    5:   table 0.09447853 0.05037063 0.17151492
    6:       z 0.06252699 0.06850978 0.11786929
 
 Model trained on Diamonds, adding a variable with r=1 to x 
 Here we add a new column, which however doesn't add any new information, as it is perfectly correlated to x. 
 Note that this new variable is not present in the output. It seems that xgboost automatically removes perfectly correlated variables before starting the calculation. The importance gain of x is the same, 0.3759. 
 diamonds_xx = diamonds %>%
    mutate(xx = x + runif(1, -1, 1))
evaluate_model(diamonds_xx)
[1] ""Correlation matrix""
           carat       depth      table           x           y          z
carat 1.00000000  0.02822431  0.1816175  0.97509423  0.95172220 0.95338738
depth 0.02822431  1.00000000 -0.2957785 -0.02528925 -0.02934067 0.09492388
table 0.18161755 -0.29577852  1.0000000  0.19534428  0.18376015 0.15092869
x     0.97509423 -0.02528925  0.1953443  1.00000000  0.97470148 0.97077180
y     0.95172220 -0.02934067  0.1837601  0.97470148  1.00000000 0.95200572
z     0.95338738  0.09492388  0.1509287  0.97077180  0.95200572 1.00000000
xx    0.97509423 -0.02528925  0.1953443  1.00000000  0.97470148 0.97077180
               xx
carat  0.97509423
depth -0.02528925
table  0.19534428
x      1.00000000
y      0.97470148
z      0.97077180
xx     1.00000000
[1] ""running model""
[1] ""Importance matrix""
   Feature       Gain      Cover  Frequency
1:       x 0.37595419 0.54788335 0.19607102
2:   carat 0.19699839 0.18015576 0.04873442
3:   depth 0.15358261 0.08780079 0.27767284
4:       y 0.11645929 0.06527969 0.18813751
5:   table 0.09447853 0.05037063 0.17151492
6:       z 0.06252699 0.06850978 0.11786929
 
 Model trained on Diamonds, adding a column for x + y 
 We add a new column xy = x + y. This is partially correlated to both x and y. 
 Note that the importance of x and y is slightly reduced, going from 0.3759 to 0.3592 for x, and from 0.116 to 0.079 for y. 
 diamonds_xy = diamonds %>%
    mutate(xy=x+y)
evaluate_model(diamonds_xy)

[1] ""Correlation matrix""
           carat       depth      table           x           y          z
carat 1.00000000  0.02822431  0.1816175  0.97509423  0.95172220 0.95338738
depth 0.02822431  1.00000000 -0.2957785 -0.02528925 -0.02934067 0.09492388
table 0.18161755 -0.29577852  1.0000000  0.19534428  0.18376015 0.15092869
x     0.97509423 -0.02528925  0.1953443  1.00000000  0.97470148 0.97077180
y     0.95172220 -0.02934067  0.1837601  0.97470148  1.00000000 0.95200572
z     0.95338738  0.09492388  0.1509287  0.97077180  0.95200572 1.00000000
xy    0.96945349 -0.02750770  0.1907100  0.99354016  0.99376929 0.96744200
              xy
carat  0.9694535
depth -0.0275077
table  0.1907100
x      0.9935402
y      0.9937693
z      0.9674420
xy     1.0000000
[1] ""running model""
[1] ""Importance matrix""
   Feature       Gain      Cover  Frequency
1:       x 0.35927767 0.52924339 0.15952849
2:   carat 0.17881931 0.18472506 0.04793713
3:   depth 0.14353540 0.07482622 0.24990177
4:   table 0.09202059 0.04714548 0.16267191
5:      xy 0.08203819 0.04706267 0.13555992
6:       y 0.07956856 0.05284980 0.13595285
7:       z 0.06474029 0.06414738 0.10844794
 
 Model trained on Diamonds data, modified adding redundant columns 
 We add three new columns that are correlated to x (r = 0.4, 0.5 and 0.6) and see what happens. 
 Note that the importance of x gets reduced, dropping from 0.3759 to 0.279. 
 #' given a vector of values (e.g. diamonds$x), calculate three new vectors correlated to it
#' 
#' Source: https://stat.ethz.ch/pipermail/r-help/2007-April/128938.html
calculate_correlated_vars = function(x1) {

    # create the initial x variable
    #x1 <- diamonds$x

    # x2, x3, and x4 in a matrix, these will be modified to meet the criteria
    x234 <- scale(matrix( rnorm(nrow(diamonds) * 3), ncol=3 ))

    # put all into 1 matrix for simplicity
    x1234 <- cbind(scale(x1),x234)

    # find the current correlation matrix
    c1 <- var(x1234)

    # cholesky decomposition to get independence
    chol1 <- solve(chol(c1))

    newx <-  x1234 %*% chol1 

    # check that we have independence and x1 unchanged
    zapsmall(cor(newx))
    all.equal( x1234[,1], newx[,1] )

    # create new correlation structure (zeros can be replaced with other r vals)
    newc <- matrix( 
    c(1  , 0.4, 0.5, 0.6, 
      0.4, 1  , 0  , 0  ,
      0.5, 0  , 1  , 0  ,
      0.6, 0  , 0  , 1  ), ncol=4 )

    # check that it is positive definite
    eigen(newc)

    chol2 <- chol(newc)

    finalx <- newx %*% chol2 * sd(x1) + mean(x1)

    # verify success
    mean(x1)
    colMeans(finalx)

    sd(x1)
    apply(finalx, 2, sd)

    zapsmall(cor(finalx))
    #pairs(finalx)

    all.equal(x1, finalx[,1])
    finalx
}
finalx = calculate_correlated_vars(diamonds$x)
diamonds_cor = diamonds
diamonds_cor$x5 = finalx[,2]
diamonds_cor$x6 = finalx[,3]
diamonds_cor$x7 = finalx[,4]
evaluate_model(diamonds_cor)
[1] ""Correlation matrix""
           carat        depth       table           x           y          z
carat 1.00000000  0.028224314  0.18161755  0.97509423  0.95172220 0.95338738
depth 0.02822431  1.000000000 -0.29577852 -0.02528925 -0.02934067 0.09492388
table 0.18161755 -0.295778522  1.00000000  0.19534428  0.18376015 0.15092869
x     0.97509423 -0.025289247  0.19534428  1.00000000  0.97470148 0.97077180
y     0.95172220 -0.029340671  0.18376015  0.97470148  1.00000000 0.95200572
z     0.95338738  0.094923882  0.15092869  0.97077180  0.95200572 1.00000000
x5    0.39031255 -0.007507604  0.07338484  0.40000000  0.38959178 0.38734145
x6    0.48879000 -0.016481580  0.09931705  0.50000000  0.48835896 0.48487442
x7    0.58412252 -0.013772440  0.11822089  0.60000000  0.58408881 0.58297414
                 x5            x6            x7
carat  3.903125e-01  4.887900e-01  5.841225e-01
depth -7.507604e-03 -1.648158e-02 -1.377244e-02
table  7.338484e-02  9.931705e-02  1.182209e-01
x      4.000000e-01  5.000000e-01  6.000000e-01
y      3.895918e-01  4.883590e-01  5.840888e-01
z      3.873415e-01  4.848744e-01  5.829741e-01
x5     1.000000e+00  5.925447e-17  8.529781e-17
x6     5.925447e-17  1.000000e+00  6.683397e-17
x7     8.529781e-17  6.683397e-17  1.000000e+00
[1] ""running model""
[1] ""Importance matrix""
   Feature       Gain      Cover  Frequency
1:       x 0.27947762 0.51343709 0.09748172
2:   carat 0.13556427 0.17401365 0.02680747
3:      x5 0.13369515 0.05267688 0.18155971
4:      x6 0.12968400 0.04804315 0.19821284
5:      x7 0.10600238 0.05148826 0.16450041
6:   depth 0.07087679 0.04485760 0.11251015
7:       y 0.06050565 0.03896716 0.08245329
8:   table 0.04577057 0.03135677 0.07554833
9:       z 0.03842355 0.04515944 0.06092608","There is an answer from Tianqi Chen (2018). 
 
 This difference has an impact on a corner case in feature importance analysis: the correlated features.
  Imagine two features perfectly correlated, feature A and feature B. For one specific tree, if the algorithm needs one of them, it will choose randomly (true in both boosting and Random Forests™). 
 However, in Random Forests™ this random choice will be done for each tree, because each tree is independent from the others. Therefore, approximatively, depending of your parameters, 50% of the trees will choose feature A and the other 50% will choose feature B. So the importance of the information contained in A and B (which is the same, because they are perfectly correlated) is diluted in A and B. So you won’t easily know this information is important to predict what you want to predict! It is even worse when you have 10 correlated features… 
 In boosting, when a specific link between feature and outcome have been learned by the algorithm, it will try to not refocus on it (in theory it is what happens, the reality is not always that simple). Therefore, all the importance will be on feature A or on feature B (but not both). You will know that one feature has an important role in the link between the observations and the label. It is still up to you to search for the correlated features to the one detected as important if you need to know all of them. 
 
 To summarise, Xgboost does not randomly use the correlated features in each tree, which random forest model suffers from such a situation.  
 Reference : 
 Tianqi Chen, Michaël Benesty, Tong He. 2018. “Understand Your Dataset with Xgboost.”  https://cran.r-project.org/web/packages/xgboost/vignettes/discoverYourData.html#numeric-v.s.-categorical-variables .","A remark on Sandeep's answer:
Assuming 2 of your features are highly colinear (say equal 99% of time)
Indeed only 1 feature is selected at each split, but for the next split, the xgb can select the other feature. Therefore, the xgb feature ranking will probably rank the 2 colinear features equally. Without some prior knowledge or other feature processing, you have almost no means from this provided ranking to detect that the 2 features are colinear. 
 Now, as for the relative importance that outputs the xgboost, it should be very similar (or maybe exactly similar) to the sklearn gradient boostined tree ranking. See  here  for explainations.","Remark on PSAfrance's answer, there is no such thing as equal ranking for 2 collinear features for xgb as tested by @dalloliogm. In fact, the equal ranking might be a case for random forests as the informational value of two correlated features is split due to random bagging. 
 From an understanding-feature-importance POV, XGB does it clearly and somewhat reliable interpretation (re-emphasizing the answer of Tianqi Chen) is possible. But it seems the feature importance from Random Forest can't be taken for granted for ranking as the value is split between the correlated features.","Adding to the answer of @dalloliogm, I tried to modify his  diamond_xx  dataframe by simply swapping  x  and  xx  via  diamonds_xx <- diamonds_xx[,c(1:7, 11, 9:10, 8)] , and here is the result: 
 > evaluate_model(diamonds_xx)
[1] ""Correlation matrix""
           carat       depth      table          xx           y          z           x
carat 1.00000000  0.02822431  0.1816175  0.97509423  0.95172220 0.95338738  0.97509423
depth 0.02822431  1.00000000 -0.2957785 -0.02528925 -0.02934067 0.09492388 -0.02528925
table 0.18161755 -0.29577852  1.0000000  0.19534428  0.18376015 0.15092869  0.19534428
xx    0.97509423 -0.02528925  0.1953443  1.00000000  0.97470148 0.97077180  1.00000000
y     0.95172220 -0.02934067  0.1837601  0.97470148  1.00000000 0.95200572  0.97470148
z     0.95338738  0.09492388  0.1509287  0.97077180  0.95200572 1.00000000  0.97077180
x     0.97509423 -0.02528925  0.1953443  1.00000000  0.97470148 0.97077180  1.00000000
[1] ""running model""
[13:41:45] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[1] ""Importance matrix""
   Feature       Gain      Cover  Frequency
1:      xx 0.37595393 0.54788335 0.19607102
2:   carat 0.19699834 0.18015576 0.04873442
3:   depth 0.15358272 0.08780079 0.27767284
4:       y 0.11645935 0.06527969 0.18813751
5:   table 0.09447860 0.05037063 0.17151492
6:       z 0.06252706 0.06850978 0.11786929
 
 So as you can see, the  x  was discarded in the  importance matrix  and been replaced by  xx . So as long as your dataset keeps the original order, adding new highly correlated features will not alter your result. Worth noticing if you need to analyze those features of importance.",,,,51.8121759,51.33519309,52.83989314,52.62855243,50,51.85456149,,,
12413,What does it mean when people say a cost function is something you want to minimize?,machine-learning,"No, it means you are trying to find the inputs that make the output of the cost function the smallest. It doesn't mean that you should ""minimize"" use of it.","A cost function is something you use to penalize high deviations from the expected results when compared to your actual predictions. 
 You can think of a cost function as a sign of how bad your prediction was. A high cost function value means the prediction was really off, hence, the focus on minimizing the cost function, thereby producing an accurate prediction model.","Cost functions in the context of Machine Learning often calculate some kind of metric that signifies how well your model is performing. A common one is for example the mean squared error, where you look at all your test examples where you know the true value and the predicted, take the difference between that and square it. By minimizing this error (cost function) you assume your predictions will be better.",Consider you have some data and you want to model a function that fits the data. This function should fit well and should not have error (ideally). How do I define this error? and voila here comes the cost function.,"Minimize a (cost) function means that you want to find good values for its parameters. Good parameters means that the function can produce the best possible outcomes, namely the smallest ones, because small values mean  less errors . This is an optimization problem: the problem of finding the best solution from all possible solutions. ( source )",,,,,70.00315175,72.54649957,64.21571551,62.2081834,61.78839918,,,,
12321,What's the difference between fit and fit_transform in scikit-learn models?,python,"To  center the data  (make it have zero mean and unit standard error), you subtract the mean and then divide the result by the standard deviation: 
 $$x' = \frac{x-\mu}{\sigma}$$ 
 You do that on the training set of the data. But then you have to apply the same transformation to your test set (e.g. in cross-validation), or to newly obtained examples before forecasting. But you have to use the exact same two parameters  $\mu$  and  $\sigma$  (values) that you used for centering the training set. 
 Hence, every scikit-learn's transform's  fit()  just calculates the parameters (e.g.  $\mu$  and  $\sigma$  in case of  StandardScaler ) and saves them as an internal object's state. Afterwards, you can call its  transform()  method to apply the transformation to any particular set of examples. 
 fit_transform()  joins these two steps and is used for the initial fitting of parameters on the training set  $x$ , while also returning the transformed  $x'$ . Internally, the transformer object just  calls first  fit()  and then  transform()  on the same data.","The following explanation is based on  fit_transform  of  Imputer  class, but the idea is the same for  fit_transform  of other scikit_learn classes like  MinMaxScaler . 
 
 transform  replaces the missing values with a number. By default this number is the means of columns of some data that you choose.
Consider the following example: 
 imp = Imputer()
# calculating the means
imp.fit([
         [1,      3], 
         [np.nan, 2], 
         [8,      5.5]
        ])
 
 Now the imputer have learned to use a mean  ${{(1+8)}\over {2}} = 4.5$  for the first column and mean  ${{(2+3+5.5)}\over {3}} = 3.5$  for the second column when it gets applied to a two-column data: 
 X = [[np.nan, 11], 
     [4,      np.nan], 
     [8,      2],
     [np.nan, 1]]
print(imp.transform(X))
 
 we get 
 [[4.5, 11], 
 [4, 3.5],
 [8, 2],
 [4.5, 1]]
 
 So by  fit  the imputer calculates the means of columns from some data, and by  transform  it applies those means to some data (which is just replacing missing values with the means). If both these data are the same (i.e. the data for calculating the means and the data that means are applied to) you can use  fit_transform  which is basically a  fit  followed by a  transform . 
 Now your questions: 
 
 Why we might need to transform data? 
 
 ""For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical"" ( source ) 
 
 What does it mean fitting model on training data and transforming to test data? 
 
 The  fit  of an imputer has nothing to do with  fit  used in model fitting.
So using imputer's  fit  on training data just calculates means of each column of training data. Using  transform  on test data then replaces missing values of test data with means that were calculated from training data.","These methods are used for dataset transformations in scikit-learn: 
 Let us take an example for scaling values in a dataset: 
 Here the  fit  method, when applied to the training dataset, learns the model parameters (for example, mean and standard deviation). We then need to apply the  transform   method on the training dataset to get the transformed (scaled) training dataset. We could also perform both of these steps in one step by applying  fit_transform  on the training dataset. 
 Then why do we need 2 separate methods -  fit  and  transform ? 
 In practice, we need to have separate training and testing dataset and that is where having a separate  fit  and  transform  method helps. We apply  fit  on the training dataset and use the  transform  method on both - the training dataset and the test dataset. Thus the training, as well as the test dataset, are then transformed(scaled) using the model parameters that were learned on applying the  fit  method to the training dataset. 
 Example Code: 
 scaler = preprocessing.StandardScaler().fit(X_train)
scaler.transform(X_train) 
scaler.transform(X_test)","fit  computes the mean and std to be used for  later  scaling. (jsut a computation), nothing is  given  to you. 
 transform  uses a  previously computed  mean and std to autoscale the data (subtract mean from all values and then divide it by std). 
 fit_transform  does both at the same time. So you can do it with 1 line of code instead of 2. 
 Now let's look at it in practice: 
 For  X training set , we do  fit_transform  because we need to compute mean and std, and then use it to autoscale the data. For  X test set , well, we already have the mean and std, so we only do the  transform  part. 
 It's super simple. You are doing great. Keep up your good work my friend :-)","This isn't a technical answer but, hopefully, it is helpful to build up our intuition: 
 Firstly, all estimators are trained (or ""fit"") on some training data. That part is fairly straightforward. 
 Secondly, all of the scikit-learn estimators can be used in a pipeline and the idea with a pipeline is that data flows  through  the pipeline. Once fit at a particular level in the pipeline, data is passed on to the next stage in the pipeline but obviously the data needs to be changed (transformed) in some way; otherwise, you wouldn't need that stage in the pipeline at all. So, transform is a way of transforming the data to meet the needs of the next stage in the pipeline. 
 If you're not using a pipeline, I still think it's helpful to think about these machine learning tools in this way because, even the simplest classifier is  still  performing a classification function. It takes as input some data and produces an output. This is a pipeline too; just a very simple one. 
 In summary,  fit  performs the training,  transform  changes the data in the pipeline in order to pass it on to the next stage in the pipeline, and  fit_transform  does both the fitting and the transforming in one possibly optimized step.","In layman's terms,  fit_transform  means to do some calculation and then do transformation (say calculating the means of columns from some data and then replacing the missing values).
So for training set, you need to both calculate and do transformation. 
 But for testing set, machine learning applies prediction based on what was learned during the training set and so it doesn't need to calculate, it just performs the transformation.","By applying the transformations you are trying to make your data behave normally. For example, if you have two variables  $V_1$  and  $V_2$  both measure the distances but  $V_1$  has centimeters as the units and  $V_2$  has kilometers as the units so in order to compare these two you have to convert them to same units...just like that transforming is making similar behavior or making it behave like a normal distribution. 
 Coming to the other question you first build the model in training set that is (the model learns the patterns or behavior of your data from the training set) and when you run the same model in the test set it tries to identify the similar patterns or behaviors once it identifies it makes its conclusions and gives results according to the training data.","Consider a task that requires us to normalize the data. For example, we may use a min-max normalization or z-score normalization. There are some inherent parameters in the model. The minimum and maximum values in min-max normalization and the mean and standard deviation in z-score normalization.
The  fit()  function calculates the values of these parameters. 
 
 The transform function applies the values of the parameters on the actual data and gives the normalized value. 
 
 The  fit_transform()  function performs both in the same step. 
 
 Note that the same value is got whether we perform in 2 steps or in a single step.","fit, transform, and fit_transform. keeping the explanation so simple. 
 When we have two Arrays with different elements we use 'fit' and transform separately, we fit 'array 1' base on its internal function such as in MinMaxScaler (internal function is to find mean and standard deviation). For example, if we fit 'array 1' based on its mean and transform array 2, then the mean of array 1 will be applied to array 2 which we transformed. In simple words, we transform one array on the basic internal functions of another array. 
 Showing you with code; 
 import numpy as np
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')

temperature = [32., np.nan, 28., np.nan, 32., np.nan, np.nan, 34., 40.]
 windspeed  = [ 6.,  9., np.nan,  7., np.nan, np.nan, np.nan,  8., 12.]

n_arr_1 = np.array(temperature).reshape(3,3)
print('temperature:\n',n_arr_1)
n_arr_2 = np.array(windspeed).reshape(3,3)
print('windspeed:\n',n_arr_2)
 
 Output: 
 temperature:
 [[32. nan 28.]
 [nan 32. nan]
 [nan 34. 40.]]
windspeed:
 [[ 6.  9. nan]
 [ 7. nan nan]
 [nan  8. 12.]]
 
 fit and transform separately, transforming array 2 for fitted (based on mean) array 1; 
 imp.fit(n_arr_1)
imp.transform(n_arr_2)
 
 Output 
 Check the output below, observe the output based on the previous two outputs you will see the difference. Basically, on Array 1 it is taking the mean of every column and fitting in array 2 according to its column where ever missing value is missed. 
 array([[ 6.,  9., 34.],
       [ 7., 33., 34.],
       [32.,  8., 12.]])
 
 This is what we doing when we want to transform one array based on another array. but when we have a single array and we want to transform it based on its own mean. In this condition, we use  fit_transform together. 
 See below; 
 imp.fit_transform(n_arr_2)
 
 Output 
 array([[ 6. ,  9. , 12. ],
       [ 7. ,  8.5, 12. ],
       [ 6.5,  8. , 12. ]])
 
 (Above) Alternativily we doing: 
 imp.fit(n_arr_2)
imp.transform(n_arr_2)
 
 Output 
 array([[ 6. ,  9. , 12. ],
       [ 7. ,  8.5, 12. ],
       [ 6.5,  8. , 12. ]])
 
 Why we fitting and transforming the same array separately, it takes two line code, why don't we use simple  fit_transform  which can fit and transform the same array in one line code. That's what the difference is between fit and transform and  fit_transform . 
 Check  this Google Colab link , you can run it by yourself, and can understand it well.",64.96329558,66.03924882,65.10206511,60.17967607,60.50088525,58.8842204,53.42384436,56.38101014,64.06351666
11930,Dimension-Hopping in Machine Learning,machine-learning,"Welcome to DataScience.SE! I'd never heard of this problem so I looked it up. It is explained on the third slide of  this  presentation by Geoff Hinton: 
 
 More things that make it hard to recognize objects 
 • Changes in viewpoint cause changes in images that standard learning
  methods cannot cope with. 
 –   Information hops between input dimensions  (i.e. pixels) 
 • Imagine a medical database in which the age of a patient sometimes
  hops to the input dimension that normally codes for weight! 
 –  To apply machine learning we would first want to eliminate this
  dimension-hopping. 
 
 In other words, it is about conceptual features migrating or hopping from one input feature dimension to another while still representing the same thing. One would like to be able to capture or extract the essence of the feature while being invariant to which input dimension it is encoded on.","As far as I understand the issue is the following:
In image recognition the inputs to your network could be the pixels (grayscale or only 1 and 0 for black and white). If you want to, e.g. recognize handwritten numbers it is very difficult to only work with such values as you never know where exactly the number (i.e. the black values) will be. 
 Is pixel 140 black or 142 black? In both cases it could well be a three. In the age/weight example these inputs are well defined. Feature 2 is weight. Feature 3 is age. These ""dimensions"" shouldnt ""hop"" in your dataset. 
 So: In your picture training the ""threes"" or ""cars"" or ""houses"" have to be recognized independent of their location in the picture, i.e. the pixel values, i.e. the feature/input vector, i.e. the dimensions as opposed to clearly defined inputs such as patient data. 
 How do you solve this in image recognition? You use additional tricks, e.g. convolution.","I read the previous answers, and Neil Slater's comment to Emre's post, copied again below, hits the nail. ""Dimension hopping"" is a term created by Dr. Hinton of machine learning pioneer fame in the context of view point.  To quote Dr. Hinton ""So, typically envision the input dimensions correspond to pixels, and, if an object moves in the world and you don't move your eyes to follow it, the information about the object will occur on different pixels.""  Age and weight are input dimension that are not easily confused.  Dr. Hinton used this obviously NOT likely dimension hopping situation of age and weight of patients to mean we would certainly be able to spot and fix any erroneous between these types of data (It's hard not to notice most adults are under 100 years old and more than 100 pounds).  The likely problem of dimension hopping, which Dr. Hinton was addressing, is pixels could be displaced because we have a different view point (e.g. the object could have moved or we are looking at it from a different angle).  Linear neural networks would not be able to detect this, whereas convolutional neural networks by design would. 
 ""The age example is supposed to highlight a dataset that does not have dimension-hopping. Age and weight do not ""hop"" or swap values randomly between examples - they are not interchangeable and the example is showing how odd that would be (and how difficult it would make simple tasks such as linear regression). Pixel values in images (and similar data in many signal processing tasks) do interchange or move easily due to the nature of the problem. – Neil Slater May 29 '16 at 18:01""","Explanation straight from Hinton's course on Neural Networks for Machine Learning ....
 
 
"" Dimension hopping occurs when  one can take the information contained in the dimensions of some input, and move this between dimensions while not changing the target . The canonical example is taking an image of a handwritten digit and translating it within the image. The dimensions that contain ""ink"" are now different (they have been moved to other dimensions), however the label we assign to the digit has not changed. Note that this is not something that happens consistently across the dataset, that is we may have a dataset containing two handwritten digits where one is a translated version of the other, however this still does not change the corresponding label of the digits.""","Hoping is only about issues with portion of image or pixels moving within dimension (mostly) and sometime into other dim (different receptive field) but output remains same.  
 This issue is dealt with invariance or equivariance and looks like weight and age example is easy way to state. Suppose if we are aware of this weight and age hopping we would easily make changes to the algo and get right result. But like data/information hopping, image hopping also happens, if we considered a '4' and a '4' shifted several pixels to the left to be different classes which has different target. 
 With Translation Invariance or better equivariance throguh filter this movement or hopping is not much issue though it increases complexity and at the cost of throwing away information, such as location. 
 Pls let me know if you need more clarity I will try to.",,,,,73.81808831,55.99499326,73.94051755,66.82205998,63.91570874,,,,
11928,"ValueError: Input contains NaN, infinity or a value too large for dtype('float32')",python,"With  np.isnan(X)  you get a boolean mask back with True for positions containing  NaN s. 
 With  np.where(np.isnan(X))  you get back a tuple with i, j coordinates of  NaN s. 
 Finally, with  np.nan_to_num(X)  you ""replace nan with zero and inf with finite numbers"". 
 Alternatively, you can use:  
 
 sklearn.impute.SimpleImputer  for mean / median imputation of missing values, or 
 pandas'  pd.DataFrame(X).fillna() , if you need something other than filling it with zeros.","For anybody happening across this, to actually modify the original: 
 X_test.fillna(X_train.mean(), inplace=True)
 
 To overwrite the original: 
 X_test = X_test.fillna(X_train.mean())
 
 To check if you're in a copy vs a view: 
 X_test._is_view","Assuming  X_test  is a pandas dataframe, you can use  DataFrame.fillna  to replace the NaN values with the mean: 
 X_test.fillna(X_test.mean())","I faced similar problem and saw that numpy handles NaN and Inf differently. 
Incase if you data has Inf, try this: 
 np.where(x.values >= np.finfo(np.float64).max)
Where x is my pandas Dataframe 
 
 This will be giving a tuple of location of places where NA values are present.  
 Incase if your data has Nan, try this: 
 np.isnan(x.values.any())","Don't forget 
 col_mask=df.isnull().any(axis=0) 
 
 Which returns a boolean mask indicating np.nan values. 
 row_mask=df.isnull().any(axis=1)
 
 Which return the rows where np.nan appeared. Then by simple indexing you can flag all of your points that are np.nan. 
 df.loc[row_mask,col_mask]","Do not forget to check for inf values as well. The only thing that worked for me: 
 df[df==np.inf]=np.nan
df.fillna(df.mean(), inplace=True)
 
 And even better if you are using sklearn 
 def replace_missing_value(df, number_features):

    imputer = Imputer(strategy=""median"")
    df_num = df[number_features]
    imputer.fit(df_num)
    X = imputer.transform(df_num)
    res_def = pd.DataFrame(X, columns=df_num.columns)
    return res_def
 
 When number_features would be an array of the number_features labels, for example: 
 number_features = ['median_income', 'gdp']","In most cases getting rid of infinite and null values solve this problem.  
 get rid of infinite values. 
 df.replace([np.inf, -np.inf], np.nan, inplace=True)
 
 get rid of null values the way you like, specific value such as 999, mean, or create your own function to impute missing values  
 df.fillna(999, inplace=True)
 
 or   
 df.fillna(df.mean(), inplace=True)","Here is the code for how to ""Replace NaN with zero and infinity with large finite numbers."" using  numpy.nan_to_num . 
 df[:] = np.nan_to_num(df)
 
 Also see  fernando's answer .","If your values are larger than  float32 , try to run some  scaler  first. It'd be rather unusual to have deviation spanning more than  float32 .",59.64877494,50,54.40625963,60.90691863,56.61745207,52.33662293,54.2945492,62.65603487,59.76983898
11921,Convolutional Neural Networks in R,r,"I guess there is no package for cnn but you can write your own convolutional layer. mxnet or h2o will be useful for it.  
 check this out: 
 http://dmlc.ml/rstats/2015/11/03/training-deep-net-with-R.html","The following 2 packages are available in R for deep neural network training: 
 
 darch : Package for Deep Architectures and Restricted Boltzmann Machines.
The darch package is built on the basis of the code from G. E. Hinton and R. R. Salakhutdinov (available under Matlab Code for deep belief nets). This package is for generating neural networks with many layers (deep architectures), train them and fine tuning with common known training algorithms like backpropagation or conjugate gradients. Additionally, supervised fine-tuning can be enhanced with maxout and dropout, two recently developed techniques to improve fine-tuning for deep learning. CRAN link:  http://cran.um.ac.ir/web/packages/darch/index.html 
 deepnet : deep learning toolkit in R. Implement some deep learning architectures and neural network algorithms, including BP,RBM,DBN,Deep autoencoder and so on. CRAN link:  https://cran.r-project.org/web/packages/deepnet/index.html","I think mxnet is one of the best options if you code in R. They have an  R wrapper  but the core is in C++. 
 They have several examples in the web. One of them is the  character recognition  with MNIST database. They have support for multi-gpus and also for Spark.","The MXNetR package is an interface of the MXNet library written in C++. It contains feed-forward neural networks and convolutional neural networks (CNN) (MXNetR 2016a).    
 https://www.is.uni-freiburg.de/resources/r-oeffentlicher-zugriff/deep-learning-in-r/deep-learning-in-r-en?set_language=en","Tensorflow for R  is available. 
 It provides full access to  Tensorflow API , the  Keras API , and  Tensorflow Estimators . 
 Installation of Tensorflow (excerpt below) ->  https://tensorflow.rstudio.com/tensorflow/ 
 
 
 Installation 
 To get started, install the tensorflow R package from GitHub as
  follows: 
 devtools::install_github(""rstudio/tensorflow"")
 
 Then, use the install_tensorflow() function to install TensorFlow: 
 library(tensorflow)
install_tensorflow() 
 
 You can confirm that the installation succeeded with: 
 sess = tf$Session() hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello) 
 
 This will provide you with a default installation of
  TensorFlow suitable for getting started with the tensorflow R package.
  See the article on installation to learn about more advanced options,
  including installing a version of TensorFlow that takes advantage of
  Nvidia GPUs if you have the correct CUDA libraries installed.",,,,,57.15519387,58.12233993,50,75.82561351,50,,,,
11898,Automatic Feature Engineering,machine-learning,"In my experience, when people claim to have an automated approach to feature engineering, they really mean ""feature generation"", and what they're actually talking about is that they've built a deep neural network of some sort. To be fair, in a limited sense, this could be a true claim. Properly trained deep neural networks can handle any number of pairwise correlations between individual features or groups of features. That said, without a great deal of up-front data pre-processing tools that know how to intelligently handle different types of input data (e.g., free text, images, etc.), none of this would be possible. Bottom line, it takes a great deal of manual effort to do something automatically.","There are several methodologies to do that... For the tool you have been talking about (which I will not mention it's name with respect to you) all sorts of pre-coded functions are run at once, it's basically a vast rule engine. 
 1) Easiest method is to run mini-trees with a random combination of variables (sounds like random forest)... Each tree that has some significant predictive power on classification is a variable and the nodes are it's categories...  
 2) You can build auto-encoders ( https://en.wikipedia.org/wiki/Autoencoder ), it's easy to do but hard to interpret, basically what deep learning does is automatic feature engineering, that's why it takes so much time to compute ;) 
 3) You can do symbolic regression ( https://en.wikipedia.org/wiki/Symbolic_regression ), possibly using a genetic optimization algorithm to select variables and some mathematical operators to come up with a nice formula that has some power of classification. So when you have some data on a company balance sheet it provides many formulas like EBITDA...","Well, there is some serious research going on in this direction under the label of  ""feature learning"" . But as far as I know, it's not mature enough yet to be packaged into a software tool that renders manual software engineering superfluous.  
 But there are important successes being achieved in that direction. Modern image recognition with deep neural networks often relies on features that the deeper layers of the neural network compute themselves instead of hand-crafted features developed by humans. So it's not entirely science-fiction. If you want to learn more about the image recognition part of the topic, you may want to watch this beautiful  video lecture  by Andrew Ng, one of the leading researchers in this field.","Have a look at h2o's driverless AI or featuretools.py, I don't believe either use deep learning, but rather apply an array of mathematical functions on your data set and then performs Feature Synthesis/merging features.
The auto-generated feature set is then iteratively tested against your model and discarded or improved. 
 So yes, while you need domain knowledge/experts to realise that adding or creating new data could make a huge impact, for exploratory purposes you could definitely use auto-features.","Almost anything can be automated but that doesn't mean it makes sense theoretically. 
 Developing features takes insight and content knowledge from the field that you are studying in order to make sense. This is why even the best data scientists need to work with content experts to make theoretically sound models","Maybe relevant for this question: 
 http://www.orges-leka.de/automatic_feature_engineering.html 
 The method is based on Bourgain Embedding and works whenever one has a distance between two data points. But to have a ""good"" distance solving the job at hand, one needs domain knowledge as there are many distances around for the same data type.",,,,62.30387084,61.39934888,55.69088233,55.7678395,51.86147769,50,,,
11730,How to scrape a website with a searchbar,data-mining,"I would suggest reading about http query methods, specifically about
GET and POST. You can pass parameters with query and open directly
company page. 
 For example: 
 http://google.com/search?q=GET+and+POST 
 where (q=GET+and+POST) is a parameter. 
 Once you have page you can parse it with your favorite library.
(for example  beautifulsoup ) 
 
 EXAMPLE: 
 Getting number of results from couple of google queries with python 3 and beautifulsoup: 
 from bs4 import BeautifulSoup
import urllib.request

# List with google queries I want to make
desired_google_queries = ['Word' , 'lifdsst', 'yvou', 'should', 'load', 'from']

for query in desired_google_queries:
    # Constracting http query
    url = 'http://google.com/search?q=' + query
    # For avoid 403-error using User-Agent
    req = urllib.request.Request(url, headers={'User-Agent' : ""Magic Browser""})
    response = urllib.request.urlopen( req )
    html = response.read()
    # Parsing response
    soup = BeautifulSoup(html, 'html.parser')
    # Extracting number of results
    resultStats = soup.find(id=""resultStats"").string
    print(resultStats)","I would suggest using a combination of rvest and rselenium, depending on the way the web page is set up. 
 
 Rselenium  to navigate the page (if needed) 
 Rvest  to scrape the data from the page","Here are some scrapers that have Free Credits or Free Trial 
 https://www.scraping-bot.io 
 https://www.scrapingbee.com/ 
 https://www.scraperapi.com/ 
 https://www.octoparse.com/ 
 There are also good companies that create scrapers with individual parameters for each client. 
 https://www.zyte.com/ 
 https://apify.com/ 
 https://data-ox.com/ 
 https://www.diffbot.com/","Thanks guys but I found a program called Mozenda that even idiots like me understand :) you basically click on the searchbar, import an excellist of stuff you want to search and then just click on the datafield you want to extract.","I think the easiest way to do that is by using some machine, some bots. 
 Recently I found one called  Octoparse  and the solution goes like this: 
 
 Drop an “Loop Item” into the Workflow Designer in the bot. 
 Then select a “loop mode” > Choose “text list” 
 Enter the terms you want to search in the search bar. 
 Next, click on the search box. Choose “Enter text value”. 
 Drag “Enter text value” into the “Loop Item” box so that the program will loop to enter the keywords, and automatically search them in the search box. 
 Then select “Use current loop text to fill the text box”. Then click ""save"". 
 Next, capture the term entered. Click the search box and select “Extract value of this item”. 
 
 The search item you just captured will be added to the extracted result.
8. Click search button > choose “Click an item”. 
 
 The information I want is on the detail page. So I need to create a list of item to get into that page. Click on the title > Select Create a list a item > Add current item to the list > Continue to edit the list. 
 
 Click on the send title > Add current item to the list again. Then click “loop”. 
 You may check  the link  to see if that's what you want.",,,,,50,58.37920436,50,50,50,,,,
11699,Backprop Through Max-Pooling Layers?,neural-network,"There is no gradient with respect to non maximum values, since changing them slightly does not affect the output. Further the max is locally linear with slope 1, with respect to the input that actually achieves the max. Thus, the gradient from the next layer is passed back to only that neuron which achieved the max. All other neurons get zero gradient. 
 So in your example,  $\delta_i^l$  would be a vector of all zeros, except that the  $i^{*^{th}}$  location will get a values  $\left\{\delta_j^{l+1}\right\}$  where  $i^* = argmax_{i} (z_i^l)$","Max Pooling 
 So suppose you have a layer P which comes on top of a layer PR. Then the forward pass will be something like this: 
 $ P_i = f(\sum_j W_{ij} PR_j)$, 
 where $P_i$ is the activation of the ith neuron of the layer P, f is the activation function and W are the weights. So if you derive that, by the chain rule you get that the gradients flow as follows:  
 $grad(PR_j) = \sum_i grad(P_i) f^\prime W_{ij}$. 
 But now, if you have max pooling,  $f = id$ for the max neuron and $f = 0$ for all other neurons, so $f^\prime = 1$ for the max neuron in the previous layer and $f^\prime = 0$ for all other neurons. So: 
 $grad(PR_{max\ neuron}) = \sum_i grad(P_i) W_{i\ {max\ neuron}}$, 
 $grad(PR_{others}) = 0.$","@Shinvu's answer is well written, I would like to point to a  video that explains the gradient of Max() operation  and  this  within a computational graph which is quick to grasp.! 
 while implementing the maxpool operation(a computational node in a computational graph-Your NN architecture), we need a function creates a ""mask"" matrix which keeps track of where the maximum of the matrix is. True (1) indicates the position of the maximum in X, the other entries are False (0). We keep track of the position of the max because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. So, backprop will ""propagate"" the gradient back to this particular input value that had influenced the cost.","Maybe it is easier to understand the derivative of pooling layer after we write down the matrix format of function  max{x}=xW , where  x  is a tensor. Let us see an example with four different values,  x = [x1, x2, x3, x4]  and  x1  is the largest value, such that  max{x} = x1 . Now,  max{x}  can be written down as matrix multiplication, such that  xW or Wx' , where  W = [I(x1>x2)*I(x1>x3)*I(x1>x4), I(x2>x1)*I(x2>x3)*I(x2>x4), I(x3>x1)*I(x3>x2)*I(x3>x4), I(x4>x1)*I(x4>x2)*I(x4>x3)]' = [1, 0, 0, 0]' , where  I(.)  is the identification function to compare two values. Normally, there are two derivatives ---  dWx/dx =W' (W transpose)  and  dWx/dW = x'  --- required in the backpropagation algorithm to update the previous and the current layer's weights or biases, respectively. However, in the case of max pooling, there is no need to update  W . Because we can and have already written down the closed-form of max pooling layer function, that is  W=[I(x1>x2)*I(x1>x3)*I(x1>x4), I(x2>x1)*I(x2>x3)*I(x2>x4), ...]' . Now to find out  dWx/dx , we have  dWx/dx =W' = [1, 0, 0, 0] , and  W'  can then be inserted as one member in the derivative chain suitably. 
 In general, the  max{x}  can be written down as a linear function, such that  Wx (or xW) , where  W  is a matrix whose entry is a production of a set of identification functions, like  I(x1>x2)*I(x1>x3)*I(x1>x4) . One property of  W  is that, in one column, there is only one entry is 1 and others are all 0. Since this linear function's weights have been determined, and there is no need to use gradient descent to update it anymore. As for the case of using  max{x}  to update the previous layers' weights and biases using the chain rule, we know that  dWx/dx =W' .","In my case, although intuitive, I was unable to see why the derivative of the max pooling function equals one for the pooled value. Searching for ""derivative of the max pooling function"" did not help. But  understanding the two variables case  did. 
 For completion, I will discuss the max pooling derivative.
First, we consider max pooling as a multivariable function  $f$  of the filter map values  $f(x_1, \cdots, x_n) = max(x_1, \cdots, x_n)$ . Also, we will assume that all values are different (no two max values, see  here  why I make this simplification).
Now, it is better to write the function in bracket notation: 
 \begin{equation}\label{eq:max_pooling_backprop}
    f(x_1, \cdots, x_n) = max(x_1, \cdots, x_n) =
        \begin{cases}
            x_1 & \text{if } x_1 > x_2, x_1 > x_3, \cdots x_1 > x_n\\
x_2 & \text{if } x_2 > x_1, x_2 > x_3, \cdots x_2 > x_n\\
\vdots & \vdots \quad \vdots \qquad \vdots \qquad \vdots \qquad \vdots \quad\vdots\\
x_n & \text{if } x_n > x_1, x_n > x_3, \cdots x_n > x_{n - 1}
        \end{cases}
\end{equation} 
 To see why (citing abora above) 
 
 the max is locally linear with slope 1, with respect to the input that actually achieves the max, 
 
 one can find the partial derivatives of  $f$  with respect to  $x_1$  (without loss of generality), say: 
 \begin{equation}
    \frac{\partial{f}}{x_1} =
        \begin{cases}
            1 & \text{if } x_1 > x_2, x_1 > x_3, \cdots x_1 > x_n\\
            0 & \text{otherwise},
        \end{cases}
\end{equation} 
 where is now clear that the partial derivative (gradient)  $\frac{\partial{f}}{x_1}$  equals  $1$ , when the max pooled value was  $x_1$ . Similarly,  $f(x_1, \cdots, x_n) = x_1$  if  $x_1$  was the max pooled value. Therefore,  $\frac{\partial{f}}{x_2} = \frac{\partial{f}}{x_3} = \cdots = \frac{\partial{f}}{x_n} = 0$ .","import numpy as np

def max_pool_backward(d_output, input, pool_size):
   """"""
   Perform back-propagation through a max-pooling layer.

   Parameters:
   - d_output: Gradient of the loss with respect to the output of the max-pooling layer (same shape as the pooled output).
   - input: Input tensor to the max-pooling layer.
   - pool_size: Size of the pooling window (e.g., 2 for 2x2 pooling).

    Returns:
    - d_input: Gradient of the loss with respect to the input of the max-pooling layer (same shape as the input).
    """"""
    d_input = np.zeros_like(input)
    input_height, input_width = input.shape
    pool_height, pool_width = pool_size, pool_size
    output_height, output_width = d_output.shape

    for i in range(output_height):
        for j in range(output_width):
            # Identify the region in the input corresponding to the current output value
            region = input[i*pool_height:(i+1)*pool_height, j*pool_width:(j+1)*pool_width]
            # Find the index of the max value in the region
            max_index = np.unravel_index(np.argmax(region), region.shape)
            # Assign the gradient from d_output to the position of the max value in d_input
            d_input[i*pool_height + max_index[0], j*pool_width + max_index[1]] =
      d_output[i, j]

    return d_input

tensor = np.array([[1, 2, 3], 
               [4, 5, 6], 
               [7, 8, 9]])
pool_size = 2
d_output = np.array([[1, 1], [1, 1]])
d_input = max_pool_backward(d_output, tensor, pool_size)
print(d_input.shape)
print(d_input)",,,,56.42994781,65.68720242,57.35688379,67.89620372,67.47405374,73.19157622,,,
11619,RNN vs CNN at a high level,machine-learning,"A CNN will learn to recognize patterns across space.  So, as you say, a CNN will learn to recognize components of an image (e.g., lines, curves, etc.) and then learn to combine these components to recognize larger structures (e.g., faces, objects, etc.).  
 You could say, in a very general way, that a RNN will similarly learn to recognize patterns across time.  So a RNN that is trained to translate text might learn that ""dog"" should be translated differently if preceded by the word ""hot"". 
 The mechanism by which the two kinds of NNs represent these patterns is different, however.  In the case of a CNN, you are looking for the  same  patterns on all the different subfields of the image.  In the case of a RNN you are (in the simplest case) feeding the hidden layers from the previous step as an additional input into the next step.  While the RNN builds up memory in this process, it is not looking for the same patterns over different slices of time in the same way that a CNN is looking for the same patterns over different regions of space. 
 I should also note that when I say ""time"" and ""space"" here, it shouldn't be taken too literally.  You could run a RNN on a single image for image captioning, for instance, and the meaning of ""time"" would simply be the order in which different parts of the image are processed.  So objects initially processed will inform the captioning of later objects processed.","Difference between CNN and RNN are as follows : 
 CNN: 
 
 CNN take a fixed size input and generate fixed-size outputs. 
 CNN is a type of feed-forward artificial neural network - are variations of multilayer perceptrons which are designed to use minimal amounts of preprocessing. 
 CNNs use connectivity pattern between its neurons is inspired by the organization of the animal visual cortex, whose individual neurons are arranged in such a way that they respond to overlapping regions tiling the visual field. 
 CNNs are ideal for images and videos processing. 
 
 RNN: 
 
 RNN can handle arbitrary input/output lengths. 
 RNN, unlike feedforward neural networks, can use their internal memory to process arbitrary sequences of inputs. 
 Recurrent neural networks use time-series information (i.e. what I spoke last will impact what I will speak next.) 
 RNNs are ideal for text and speech analysis.","From a general point of view, CNN does not break the component into subcomponents but rather use shared weights on all the overlapping subcomponents (recpetive fields) to find the same pattern. It is not a divide and conquer algorithm. 
 In general case, CNNs tend to extract local and position-invariant (independently of their position) features, and RNNs tend to find diffenret patterns across the time even if they are far. 
 For example in the case of applying both to natural language, CNNs are good at extracting local and position-invariant features but it does not capture long range semantic dependencies. It just consider local key-phrases. 
 So when the result is determined by the entire sentence or a long-range semantic dependency CNN is not effective as shown in  this  paper where the authors compared both architechrures on NLP taks.","If I want to tell you, both are based on a same concept, and that is  weight sharing . It is better to think about them in this way. 
 In CNNs, we try to find similar patterns throughout the input which can be image, text, or other things. This can be done by sliding a same filter that its parameters do not change while scanning the image to find similar patterns. Due to the fact that an image can have multiple patterns, we employ this idea with multiple filters. 
 On the other hand, In RNNs, the idea is that we want to find a similar patterns throughout the sequence. For instance, wherever you see a cat in a sentence, it is still a cat. Consequently, it should not matter where you see it. RNNs also employ the idea of weight sharing. They use a network that faces each input in the sequence separately to see whether a similar pattern is observed or not. However, there is a difference between CNNs and RNNs. The input of RNNs is a sequence, and the order  matters . Consequently, at each time step, the RNN encounters the input of that time step, and it also receives some information from the past. 
 You can see that these intuitions are different than yours.","Other than the fact that both are types of neural networks, there is not too much of a similarity between them. The two statements you have given are correct. However there is more to it. 
 At a very generic level - CNNs are used to convert images into features. These features can then be used by the model to do whatever it wants to do. RNNs are used to featurize time-series data - for e.g. data like stock markets, speech, language etc - all situations where the order matters. 
 The first letter of both terms represent the key difference. 'C' is for convolutions - use an input filter of choice to scan thru' out the image (its numeric representation) for identifying patterns similar to the filter. By using 1000's of such filters and scanning the entire image again and again, and repeating this entire process layer after layer, we get higher and higher level of abstractions from the raw pixel data...ultimately we reach a point where a simple layer can now use all these higher level features to do what we want to do (say identify the object in the image) 
 'R' is for recurring, this means each layer has multiple neural nets within (all share the same weight). The idea of having multiple neural nets is to account for the dependency of the data on the previous time-step and somehow build a better representation of the given data. This 'better representation' can now be used by a regular layer to do what we want (like classifying a twitter sentiment into +ve or -ve). 
 Based on situations sometimes we can leverage both (CNN and RNN) of them in one model (say describing an image in a sentence) or even surprisingly we can interchange them - for e.g. use CNNs where RNN's should have been used..",,,,,59.62350345,65.37040436,57.32955127,59.37391164,58.68080468,,,,
11404,Python: Handling imbalance Classes in python Machine Learning,machine-learning,"This paper  suggests using  ranking  (I wrote it). Instead of using, for instance, SVM directly, you would use RankSVM. Since rankers compare observation against observation, training is necessarily balanced. There are two ""buts"" however: training is much slower, and, in the end, what these models do is rank your observations from how likely they are to belong to one class to how likely they are to belong to another so you need to apply a threshold afterwards. 
 If you are going to use  pre-processing  to fix your imbalance I would suggest you look into  MetaCost . This algorithm involves building a bagging of models and then changing the class priors to make them balanced based on the hard to predict cases. It is very elegant. The cool thing about methods like SMOTE is that by fabricating new observations, you might making small datasets more robust. 
 Anyhow, even though I wrote some things on class imbalance, I am still skeptic that it is an important problem in the real world. I would think it is very uncommon that you have imbalance priors in your training set, but balanced priors in your real world data. Do you? What usually happens is that type I errors are different than type II errors and I would bet most people would be better off using a cost matrix, which most training methods accept or you can apply it by pre-processing using MetaCost or SMOTE. I think many times ""fixing imbalance"" is short to ""I do not want to bother thinking about the relative trade-off between type I and II errors."" 
 Addendum: 
 
 I tried for in-built python algorithms like Adaboost, GradientBoost
  techniques using sklearn. I read these algorithms are for handling
  imbalance class. 
 
 AdaBoost gives better results for class imbalance when you initialize the weight distribution with imbalance in mind. I can dig the thesis where I read this if you want. 
 Anyhow, of course, those methods won't give good accuracies. Do you have class imbalance in both your training and your validation dataset? You should use metrics such as F1 score, or pass a cost matrix to the accuracy function. ""Fixing"" class imbalance is when your priors are different in your training and your validation cases.","Some of sklearn's algorithms have a parameter called  class_weight  that you can set to  ""balanced"" . That way sklearn will adjust its class weights depending on the number of samples that you have of each class. 
 For the random forest classifier, try the following and see if it improves your score: 
 rf = RandomForestClassifier(class_weight=""balanced"") # also add your other parameters!","Yes, this is a fine technique to tackle the problem of class-imbalance. However, under-sampling methods do lead to the loss of information in the data set (say, you just removed an interesting pattern among the remaining variables, which could have contributed to a better training of the model). This is why over-sampling methods are preferred, specifically in case of smaller data set. 
 In response to your query regarding Python packages, the  imbalanced-learn  toolbox is specially dedicated for the same task. It provides several under-sampling and over-sampling methods. I would recommend trying the  SMOTE  technique.","It depends on the ensemble technique you want to use. The basic problem that you are working with multi-class data imbalance problem. Under sampling can be used efficiently in bagging as well as in boosting techniques. 
SMOTE algorithm is very efficient in generating new samples.
Data imbalance problem has been widely studied in literature. 
I recommend you to read about one of these algorithms: 
 SMOTE-Boost
SMOTE-Bagging
Rus-Boost
EusBoost
These are boosting /bagging techniques designed specifically for imbalance data problem.
Instead of SMOTE you can try ADA-SMOTE or Border-Line SMOTE.
I have used and modified the Border-Line SMOTE for multi-class and it is very efficient. 
If your data base is very large and the problem is easy try : viola - jones classifier. I have used also with data imbalance problem and it is really efficient",There are already some good answers here. I just thought I would add one more technique since you look to be using ensembles of trees. In many cases you are looking to optimize the Lift curve or the AUC for the ROC. For this I would recommend  Hellinger distance criterion  for splitting the branches in your trees. At the time of writing this it is not in the  imbalanced-learn  package but it looks like there is a  plan .,"When dealing with class imbalance problem you should mainly concentrate on error metric and you should choose F1 score as an error metric. 
 After choosing the correct metric we can use different Techniques for dealing with this issue. 
 If interested you can look into this  blog , it is explained very nicely about the techniques used to solve this class imbalance problem.",,,,63.41183686,53.67436671,54.77425876,55.45903461,50.55796003,56.27296397,,,
11356,Merging multiple data frames row-wise in PySpark,python,"Stolen from:  https://stackoverflow.com/questions/33743978/spark-union-of-multiple-rdds 
 Outside of chaining unions this is the only way to do it for DataFrames. 
 from functools import reduce  # For Python 3.x
from pyspark.sql import DataFrame

def unionAll(*dfs):
    return reduce(DataFrame.unionAll, dfs)

unionAll(td2, td3, td4, td5, td6, td7, td8, td9, td10)
 
 What happens is that it takes all the objects that you passed as parameters and reduces them using unionAll (this reduce is from Python, not the Spark reduce although they work similarly) which eventually reduces it to one DataFrame. 
 If instead of DataFrames they are normal RDDs you can pass a list of them to the union function of your SparkContext 
 EDIT: For your purpose I propose a different method, since you would have to repeat this whole union 10 times for your different folds for crossvalidation, I would add labels for which fold a row belongs to and just filter your DataFrame for every fold based on the label","Sometime, when the dataframes to combine do not have the same order of columns, it is better to  df2.select(df1.columns)  in order to ensure both df have the same column order before the union. 
 import functools 

def unionAll(dfs):
    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs) 
 
 Example: 
 df1 = spark.createDataFrame([[1,1],[2,2]],['a','b'])
# different column order. 
df2 = spark.createDataFrame([[3,333],[4,444]],['b','a']) 
df3 = spark.createDataFrame([555,5],[666,6]],['b','a']) 

unioned_df = unionAll([df1, df2, df3])
unioned_df.show() 
 
 
 else it would generate the below result instead. 
 from functools import reduce  # For Python 3.x
from pyspark.sql import DataFrame

def unionAll(*dfs):
    return reduce(DataFrame.unionAll, dfs) 

unionAll(*[df1, df2, df3]).show()","How about using recursion? 
 def union_all(dfs):
    if len(dfs) > 1:
        return dfs[0].unionAll(union_all(dfs[1:]))
    else:
        return dfs[0]

td = union_all([td1, td2, td3, td4, td5, td6, td7, td8, td9, td10])","def unionAll(a,b):
    return a.unionByName(b)

sdf1_sdf2 = reduce(unionAll,[sdf1,sdf2])","I just did this: 
 union_data = td1.unionAll(td2).unionAll(td3).unionAll(td4)
 
 worked like a charm.","You could do it like this. 
 val myDFCsv = spark.read.format(""csv"")
   .option(""sep"",""|"")
   .option(""inferSchema"",""true"")
   .option(""header"",""false"")
   .load(""mnt/rawdata/2019/01/01/client/ABC*.gz"")

myDFCsv.show()
myDFCsv.head()
myDFCsv.count()


//////////////////////////////////////////
// If you also need to load the filename
val myDFCsv = spark.read.format(""csv"")
   .option(""sep"",""|"")
   .option(""inferSchema"",""true"")
   .option(""header"",""false"")
   .load(""mnt/rawdata/2019/01/01/client/ABC*.gz"")
   .withColumn(""file_name"",input_file_name())


myDFCsv.show(false)
myDFCsv.head()
myDFCsv.count()",,,,52.87649187,51.5664889,50,50,50.75121192,50,,,
11247,Sentiment Analysis of Movie Reviews using Python,python,"Given that exact case, I would assume that you are getting negative decision due to names, mentioned in the review (in your training dataset actors were more often met in negative reviews). You should, probably, remove all non-relevant words from reviews, and that includes not only stop words, but all person names (since they are less of a sentiment marker except, maybe, justin bieber, who is a really negative marker to anything :) ).","I'm assuming that you are using bag of words, you can try adding bigrams and/or trigrams (or really any other arbitrary n-grams) to your vocabulary.  
 I have also had a lot of success using latent dirichlet allocation to predict the distribution of topics for a particular sentence. Feeding that distribution into the naive bayes algorithm as features. It is something of a hack but it seems to work really well for the limited sentiment analysis that I have done. My guess is that the LDA does a decent job of picking up on sarcasm. It isn't full proof, but it might help you get better predictions. These are some of the things that I would try at the very least.","Mainly the accuracy depends upon pre-processing steps, features extracted and the learning model used. 
 Pre-processing steps normally includes removal of stop words and that is fine. Features extraction is of various methods. Word embeddings is gaining its popularity in NLP, due to its interesting characteristics of vectors generated.  Gensim  provides a nice python library for word embeddings both word2vec as well as doc2vec models. For the detailed algorithm of how it works, read  word2vec ,  doc2vec 
 There are lot of learning models from naive bayes, svm to neural network models. The accuracy of it depends upon the dataset used and the features generated and so each models need to be tested under trial and error method.  sklearn  provides a nice support for ML models.","Two starting places: 
 
 Do a Google Scholar search for ""sentiment analysis"" and read the papers from the past few years. 
 Work through Scikit-Learn's  text classification tutorial . Copy the 20 Newsgroups classification code there and modify it for your task. This will give you a baseline to work from.","I ran this movie review using my code, and it correctly classifies as a positive review. 
 Please feel free to test it on this link:   https://swetakesurnlp-playground.herokuapp.com 
 I first recommend you to have a look at my code on this link:  https://github.com/sweta-kesur-nlp-playground/nlp-play1-movie-reviews/blob/master/Sweta-1-NLP-play-Movies.ipynb  and then please feel free to ask me any questions if you have any doubts. I will be happy to help.","If you want, you can use our  tutorial  on how to do it. Worked pretty well for our test reviews.",,,,58.07979125,55.49715871,50.8872015,57.97683792,64.44357291,55.35522632,,,
11220,Training Dataset for Sentiment Analysis of Movie Reviews,machine-learning,"You can use the  SAR14 dataset  of 234K IMDb movie reviews. The construction of the SAR14 dataset is detailed in the paper "" Sentiment Classification on Polarity Reviews: An Empirical Study Using Rating-based Features "".","There are many datasets available. 
 
 Multi-Domain Sentiment Dataset 
 Twitter sentiment 
 UCI 
 Sentiment Analysis Dataset 
 Large Movie Review Dataset","~7000 sample data entries is definitely not enough considering that a more-or-less reliable dictionary of sentiment-loaded words consists of several thousand words each for both positive and negative sentiments. Basically, when you train the model you build such a dictionary in some sense. 
 There is, however, an  existing training/test dataset  consisting of 50000 reviews which is a bit better than what you have. 
 At the same time, while the amount of training sample data  contributes to the quality of the classifier, it's also important that the style and dictionary used by authors of texts in the training set was similar to your test texts. In addition, text processing tricks like stemming may increase the training efficiency. 
 For more information, you might want to look at  this blog posts  several colleagues and I wrote about creating training and test data sets.","The  Stanford Sentiment Analysis  dataset is based on Rotten Tomatoes reviews, has parses and sentiment annotation down to the syntactic component level.","According to your requirements, this dataset will be a good choice for you to work on. It is available on Kaggle:  https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?select=IMDB+Dataset.csv 
 IMDB dataset has 50K movie reviews for natural language processing or Text analytics.
This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms. 
 Extra : 
 
 You can also check out my Sentiment Analysis on Movie Reviews using this link:  https://github.com/sweta-kesur-nlp-playground/nlp-play1-movie-reviews/blob/master/Sweta-1-NLP-play-Movies.ipynb 
 I have also deployed it on Heroku:  https://swetakesurnlp-playground.herokuapp.com/",,,,,72.83071611,77.07596708,57.94920214,70.11012384,77.99383719,,,,
11091,How does deep learning helps in detecting multiple objects in single image?,deep-learning,"Although many solutions in production systems still use a sliding window as described below in this answer, the field of computer vision is moving quickly. Recent advances in this field include  R-CNN  and  YOLO . 
 
 Detecting object matches in an image, when you already have an object classifier trained, is usually a matter of brute-force scanning through image patches. 
 Start with the largest expected patch size. E.g. if your image is 1024 x 768, but always a distance shot of a road maybe you do not expect any car to take up more than 80 x 80 pixels in the image. So you take an 80x80 block of pixels from one corner of the image, and ask your classifier what chance there is a car in that corner. Then take the next patch - perhaps move by 20 pixels.  
 Repeat for all possible positions, and decide which patches are most likely to contain cars. 
 Next, take a block size down (maybe 60 x 60, moving 15 pixels at a times) and repeat the same exercise again. Repeat this until you have hit the expected smallest block size for your goal. 
 Eventually you will have a list of areas within the image, with the probability that each contains a car. 
 Overlapped blocks both with high probability are most likely the  same  car, so the logic needs to have thresholds for merging blocks - usually taking the overlapped area with the highest probability score - and declaring there is only one car in that area. 
 As usual with ML approaches, you will need to experiment with correct meta-params - in this case block sizes, step sizes and rules for merging/splitting areas - in order to get the most accurate results.","I'd want to add @Neil_Slater's answer by sharing my application. 
 In my application, I want to train a model that can automatically load a chess position from a chess book like this: 
 
 
 Before I did anything, I made sure I had a model that can accurately detect a chess piece. 
 
 
 It was not a hard-problem because it was like training the MINST digits. I collected enough samples, randomly add some noise to those samples. My model was a 2-layer convolutional deep-learning. 
 
 Since chess board is always a square. I use square-detection available in OpenCV to give me a list of candidates. I would throw away any square that is too small, too large or not divisible by 64 (since there are 64 squares). 
 Next, I'd crop the image to something like this: 
 
 
 Now, I have  another  multi-layer convolutional network to check each square in the board. The stride length is the dimension of the image divided by 8 (since there're eight squares in each dimension). The patch size is the same as the stride length. 
 My pipework worked as I was able to  combine  two different classifiers. I personally prefer to train two classifiers, as it'd be easier to train and verify than trying to put everything into a single model.","The question itself is not quite clear, since you don't state that you have a model that can detect one car per run for an image or you are just asking what tools, algorithms or frameworks to use to detect cars (or other objects) in an image. 
 Answering second variant, you should be using developed algorithms for object detection, which are either Haar Cascades (which are embedded into OpenCV and there are clear tutorials of how to train your custom object detector, for example,  banana tutorial ) or CNNs, which are the choice for object detection using neural networks, personally, I enjoy working with  that implementation  - simple and comprehensive code and amazing results. 
 Both approaches (Haar Cascades and CNNs) basically find patterns of mutually connected and co-located shapes that  describe  your particular object (be it face, banana, car or UFO) and use these patterns to find objects on a new image. Mutual inclusion of detected objects (when borders of objects intersect or one is included by another) is used to find best match for every region.","Your question explicitly states that your are only looking for multiple cars rather than multiple objects, so the answer is in the question. You are not looking for multiple objects, rather multiple occurrences of the same object.   
 Providing you trained the system well enough to recognise both types of car then they should both be detected using standard cascading filter approaches ... This is like asking how can I detect 2 faces in one photograph?   
 If you were looking for a car and a monkey then the situation is very different and using common approaches with tools like open CV you would generally train 2 classifiers (one for cars and one for monkeys) the iterate over the image twice.  
 The more different classes of object you want to detect the more classifiers and iterations you would need.",Have a simple CNN such as Efficientnet B0 or B1 do the trick for you. Curate your training data such that you have 2 classes: single vs multi. And let the classifier take care of the business for you if localization is not required.,,,,,54.06128598,54.703756,61.03421094,65.1011174,51.83426197,,,,
11060,Merging sparse and dense data in machine learning to improve the performance,machine-learning,"This seems like a job for Principal Component Analysis. In Scikit is  PCA  implemented well and it helped me many times. 
 PCA, in a certain way, combines your features. By limiting the number of components, you fetch your model with noise-less data (in the best case). Because your model is as good as your data are. 
 Consider below a simple example. 
 from sklearn.pipeline import Pipeline
pipe_rf = Pipeline([('pca', PCA(n_components=80)),
                    ('clf',RandomForestClassifier(n_estimators=100))])
pipe_rf.fit(X_train_s,y_train_s)

pred = pipe_rf.predict(X_test)
 
 Why I picked 80? When I plot cumulative variance, I got this below, which tells me that with ~80 components, I reach almost all the variance. 
 
 So I would say give it a try, use it in your models. It should help.","The best way to combine features is through ensemble methods.
Basically there are three different methods: bagging, boosting and stacking.
You can either use Adabbost augmented with feature selection (in this consider both sparse and dense features) or stacking based (random feature - random subspace) 
I prefer the second option you can train a set of base learners ( decisions. Trees) by using random subsets and random feature ( keep training base learners until you cover the whole set of features) 
The next step is to test the Training set to generate the meta data. Use this meta data to train a meta classifier. 
The meta classifier will figure out which feature is more important and what kind of relationship should be utilized","The variable groups may be multicollinear or the conversion between sparse and dense might go wrong. Have you thought about using a voting classifier/ ensemble classification?  http://scikit-learn.org/stable/modules/ensemble.html 
That way you could deal with both above problems.","In addition to some of the suggestions above, I would recommend using a  two-step modeling  approach.  
 
 Use the sparse features first and develop the best model.  
 Calculate the predicted probability from that model. 
 Feed that probability estimate into the second model (as an input feature), which would incorporate the dense features. In other words, use all dense features  and  the probability estimate for building the second model.  
 The final classification will then be based on the second model.","Try PCA only on sparse features, and combine PCA output with dense features.  
 So you'll get dense set of (original) features + dense set of features (which were originally sparse). 
 +1 for the question. Please update us with the results.",,,,,50.51511341,54.78096302,62.39205217,53.94643537,60.70653044,,,,
11032,How to predict the probability of an event?,predictive-modeling,"Please double-check if there's the only data you have got, because all you have is a single predictor  date . 
 If this is indeed your only data source, then you only have a single predictor, and your independent variable is continuous. Now, you should plot  date  vs  amount  and fit a single linear regression. Does the fitting look good? Only you can tell because we don't have the full data-set. 
 If it's not a good fit, look at the plot and ask yourself does this look like a curve? If so, you might want to fit a spline curve or something like that. 
 You should also check the autocorrelation. This makes sense because your data look like a time series (you'll need to check it yourself). If this is the case, you might want to consider MA and ARCH model. 
 It's not possible for us to give you accurate advice because we don't know your data.","You can use Binary Logistic Regression for this analysis. 
 Prior to using Binary Logit, you'd have to spend some time preparing the data for this analysis. 
 You can create several RFM types of features from this data set. Examples: Number of donations, time between donations, time since the most recent donation, time since the first donation, average donation amount, the amount of first donation, the most recent donation amount, etc. (I can provide more examples, if needed.) 
 Since your task is to predict the probability of donation during a four-month timeframe (Mar-Jun 2016), you can create those features (leading indicators) for each donor  as of  the end of October 2015. All leading indicators would be created based off of timeframes prior to that cut-off point. Your  observation window  is from Nov-2015 to Feb-2016. This is where your event flag (dependent variable) should come from: 1 if a donor donated (again) during the observation window, and 0 otherwise.  
 In order to make this model generalizable, I'd recommend pulling several such cross-sections of your data (in addition to the October 2015 slice explained above,)","You could try using the markov model!
(An illustration of which can be found  here ) 
 Also, you could detect patterns in the dataset by plotting it and then figure out which algorithm to use based on what is the degree of correlation and the nature of the plot. 
 Also, what is the number of users you have? 
You could group together data of each user and run the algorithm for the user that is being asked for. 
 Regression is the way to go if you want to know if a user will donate or not, to find the probability, try markov!","I think you could use time series modeling algorithm as @Student_T said. Also you can make window time to find relation between new donate and previous donate and you can use amount, may be people with high payment and low payment have different behavior. first of all you should change your data in a way that fill gaps. I mean you should add data about month that a person have not any payment. 
after that you should make a table like this:
person_id / month(or day or week or 3month) / count of payment / count of payment last month/ sum of amount that paid last month/ Is paid last month?/ 
 then you should find whether your filed is useful and independent or not. and try to add other filed. and then build your model. 
 good luck.","First you need to find in what kind of distribution is your data: linear, exponential, normal ... Etc. 
After that you need to find the area size under the equation in which your event will occur. 
It's all come down to what kind of distribution you are on.",,,,,50,53.85765405,51.17502866,50,52.75978906,,,,
10932,Difference between AlphaGo's policy network and value network,machine-learning,"In brief each net has a different purpose as you mentioned: 
 
 The value network was used at the leaf nodes to reduce the depth of the tree search. 
 The policy network was used to reduce the breadth of the search from a node (guiding towards promising immediate actions). 
 
 In general, you can use value function methods to find an optimal policy or directly search in the policy space to optimize a parametrized policy function (of course there are pros and cons). You can use function approximators (e.g Deep Nets) in each case. I see that mainly you are confused about the policy net so I focus my answer into this. 
 The policy net was first: 
 trained to do the moves that most likely a human would do given a board state (so input is a board state and output is a histogram that shows the probability of each action given that state). The net can approximate the probability function underlying the mapping from states to actions. It is reasonable to think to start building your policy from available data after all. After supervised training using experts moves the policy net could play the game sufficient (although far from a Master's level). Simply, you attempted to capture the general pattern of action selection of professional players. 
 Then, 
 it was trained in games with opponent itself, in order to optimize the previous-learned policy. This time its weights were updated using the REINFORCE algorithm. By doing this, you update the net parameters towards maximization of expected reward. Eventually you have a net that not only selects the actions like a professional player but also towards winning the game (However it cannot plan!). 
 After this step, they approximated the value function of a bit more noisy version of the learned policy, by regression (input is the state board and target the result of the game). You can use this network to affect the leaf node evaluation. 
 Conceptually speaking, the policy net gives you a probability over actions, but this doesn't indicate that you will end up in a good, for winning the game, state. AlphaGo had some ""blind spots"" and during the tournament did some really bad moves but also one exceptional move that a human could never had thought. 
 Finally you can use your planning algorithm (MCTS) in combination with these nets. Why we took all these steps? Briefly, the simple MCTS without any ""intuition"" would have failed.","Here is my concise thought process in understanding the two different networks. 
 First of all, the goal is to find an optimal solution (or very near-optimal) without using an exhaustive search, which is definitely challenging. 
 Per position or state, there will be N moves possible, and on each move there will be its own depth D in a full search tree. It is theoretically or mathematically possible to walk through all paths and find an optimal solution(s). However, we don't want to do a full search. 
 Now we got two separate questions for developing an approximation approach. 
 Q1. How can we skip or disregard some moves out of N per position? (i.e., breath reduction) 
 Q2. How can we stop at an intermediate depth in a search tree rather than walking through until the end of game, without failing to find an optimal solution? (i.e., depth reduction) 
 The policy network is mainly designed for filtering out useless moves out of N, yet without failing to find an optimal solution. Here this network initially relies on human expert moves, i.e., SL, and improved by RL later. 
 The value network is mainly designed for finding the winning probability without a full search.  
 These two networks have a common goal of finding an optimal solution, However, in each strategic choice of move, each network plays a different role. 
 I just hope this helps. I know it'd be still at a high level.","I think the OP was confusing about AlphaGo with alpha-beta. In alpha-beta, you'd indeed use the policy network for helping with pruning, but not here. Again, there is no pruning as the algorithm relies on Monte-Carlo tree search (MCTS). 
 Anyone who thinks my answer is too long might skip to the summary section, where I state why the two networks are not redundant. 
 In the following example, I'll do some simplification to make my ideas easier to understand. 
 Example: 
 Imagine you have a position where there are two legal moves. The first move is a dead-lost for you, however, the second move gives you a winning advantage. 
 
 First move: forced loss for you 
 Second move: forced win for you 
 
 Evaluation network 
 Let's assume the evaluation network Google gives you is perfect. It can evaluate any leaf position in our example perfectly. We won't change our value network in the example. 
 To simplify our example, let's assume our value network gives: 
 
 -1000 for any leaf position which is a loss for you 
 +1000 for any leaf position which is a win for you 
 
 Policy network 
 Let's assume Google gives you two policy networks. The probabilities generated for our position is: 
 
 Policy 1: 0.9 for move 1 and 0.1 for move 2 
 Policy 2: 0.2 for move 1 and 0.8 for move 2. 
 
 Note that our first policy network gives  incorrect  prior probability for our example. It gives 0.9 for move 1, which is a losing move. This is fine because not even Google could train a perfect policy network. 
 Playing with the first policy network 
 AlphaGo needs to generate a simulation with Monte-Carlo, and it needs to choose move 1 or 2. Now, AlphaGo draws a uniform-distributed random variable, and it'll pick: 
 
 Move 1 if the random number is <= 0.9 
 Move 2 if the random number is > 0.9 
 
 So AlphaGo is much more likely to pick the losing move to simulate (in our very first simulation). In our first simulation, we will also use the value network to get a score for the simulation. In the paper, it's: 
 
 This value would be -1000, because this simulation would lead to a loss. 
 Now, AlphaGo needs to generate the second simulation. Again, the first move would be much more likely to pick. But eventually, the second move would be pick because: 
 
 Our prior probability for the second move is 0.1, not zero 
 AlphaGo is encouraged to try moves which haven't been explored much. In the paper, this is done by this equation: 
 
 
 Note that  N  is the number of moves searched for the move and it's in the denominator. The more likely our first move is searched, the smaller the  u  function is. Thus, the probability for selecting our second move improves because AlphaGo actually picks a move by this equation: 
 
 
 This is the  key  equation. Please look at it carefully: 
 
 It has a term  P  for the prior probability (given by the policy network) 
 It has a term  Q  for the evaluation scores (given by the value network) 
 
 Now, we know our second move will eventually be chosen. When it does happen, the value network gives a +1000. This will increase  Q , which makes the second move much more  likely  be chosen in the later simulations. 
 Given enough simulations, the number of times the second move is chosen for simulation should be more than the number of times the first move is chosen. 
 Finally, the move that AlphaGo decides to make is (quoted from the paper): 
 
 Once the search is complete, the algorithm chooses the most visited move from the root position. 
 
 Playing with the second policy network 
 Our second policy network will need less iterations to pick move 2 because it's prior probability given by the policy network is correct in the first place. 
 Remarks 
 Everything here is very similar to  Bayesian  analysis. We start off with some prior probability (given by the policy network), then we generate data to move the probability distirubtion (given by the value network). 
 Summaries 
 
 Policy network is used to generate prior probabilities to guide what move the Monte-Carlo search should pick 
 Value network is used to generate data to validate the policy network. If the policy network is bad, AlphaGo would need more computing resources to converge (if ever). 
 You can think of it like Bayesian analysis","Policy Network : The Network which learns to give a definite output by giving a particular Input to the game is known as Policy Network. 
 Value Networks : The value network assigns value/score to the state of the game by calculating an expected cumulative score for the current state  s . Every state goes through the value network. The states which get more reward obviously get more value in the network. 
 Better understanding  with Animations  Go here:  Policy Networks vs Value Networks in Reinforcement Learning","From what I understand the difference is in the outputs. Where policy network outputs a probability distribution over the possible moves, the value network returns a real value that can be interpreted as the probability of winning given this board configuration. From there Monte-Carlo tree search is performed via taking top K moves from and then narrowing the search tree again by taking top K value network outputs. 
 Feel obligated to correct me if I am wrong.","I just have a very shallow understanding. But I feel in an idea network where you have trained every possible case, one network is enough. But it's not possible to do so in limited resources. So policy to reduce breadth and value to reduce depth. It's a probability result and but not a 100% perfect result.","Yes, Policy-Net & Value-Net are redundant. If you have a Policy-Network you don't need a Value-Network and vice versa. At first glance you might think that these 2 networks would behave exactly the same once fully trained but that ain't the case. 
 A good example for that is the following problem: 
 Imagine there are 2 states (S0 and S1) and in S0 the agent can use 2 different actions (a0, a1) which will both lead to S1. In S1 however the Agent is rewarded or punished depending on the action the Agent has taken. If it reached S1 by using a0 it's rewarded and if it reached S1 by using a1 it is punished. 
 A Value-Network would be helpless in this situation cause it only looks at the states and not the actions that lead to the states. In one pass it would increase the value of S1 and in another pass were it was punished it would decrease the value of S1 again. 
 A Policy-Network on the other hand just looks at the actions that where taken and ignores the states alltogether. It would pick up the fact that a0 leads to reward and a1 leads to punishment very quickly. 
 Imho Value-Networks work best with Board-Games and similar problems where each state clearly defines a reward/punishment value. If that's not the case or the number of possible states is too big then a Policy-Network is the way-to-go.",,,60.68988907,58.0109016,74.25936544,86.51091712,66.07114221,58.04725628,68.59481067,,
10615,Number of parameters in an LSTM model,deep-learning,"The LSTM has a set of 2 matrices: U and W for each of the (3) gates. The (.) in the diagram indicates multiplication of these matrices with the input  $x$  and output  $h$ . 
 
 U has dimensions  $n \times m$   
 W has dimensions  $n \times n$ 
 there is a different set of these matrices for each of the three gates(like  $U_{forget}$  for the  forget  gate etc.) 
 there is another set of these matrices for updating the cell state S 
 on top of the mentioned matrices, you need to count the biases (not in the picture) 
 
 Hence total # parameters =  $4(nm+n^{2} + n)$","Following previous answers,
The number of parameters of LSTM, taking input vectors of size $m$ and giving output vectors of size $n$ is: 
 $$4(nm+n^2)$$ 
 However in case your LSTM includes bias vectors, ( this is the default in keras for example ), the number becomes: 
 $$4(nm+n^2 + n)$$","According to  this : 
 LSTM cell structure 
 
 LSTM equations 
 
 Ingoring non-linearities 
 
 If the input  x_t  is of size n×1, and there are  d  memory cells, then the size of each of  W∗  and  U∗  is  d×n , and  d×d  resp. The size of  W  will then be  4d×(n+d) . Note that each one of the dd memory cells has its own weights  W∗  and  U∗ , and that the only time memory cell values are shared with other LSTM units is during the product with  U∗ . 
 Thanks to Arun Mallya for great presentation.","to completely receive you'r answer and to have a good insight visit :
 https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889 
 g, no. of FFNNs in a unit (RNN has 1, GRU has 3, LSTM has 4) 
 h, size of hidden units 
 i, dimension/size of input 
 Since every FFNN(feed forward neural network) has h(h+i) + h parameters, we have 
 num_params = g × [h(h+i) + h] 
 Example 2.1: LSTM with 2 hidden units and input dimension 3. 
 
 g = 4 (LSTM has 4 FFNNs) 
 h = 2 
 i = 3 
 num_params 
 = g × [h(h+i) + h] 
 = 4 × [2(2+3) + 2] 
 = 48 
     input = Input((None, 3))
    lstm = LSTM(2)(input)
    model = Model(input, lstm)
 
 thanks to RAIMI KARIM","To make it clearer , I annotate the diagram from  http://colah.github.io/posts/2015-08-Understanding-LSTMs/ .  
 ot-1 : previous output , dimension , n (to be exact, last dimension's units is n ) 
 i: input , dimension , m 
 fg: forget gate 
 ig: input gate 
 update: update gate 
 og: output gate 
 Since at each gate, the dimension is n, so for ot-1 and i to get to each gate by matrix multiplication(dot product), need n n+m n parameters, plus n bias .so total is 4(n n+m n+n).",,,,,53.3719459,64.76625435,57.39450864,62.79317563,51.24372701,,,,
10394,which technology should I invest into SAS In-memory analytics or R?,r,Go for R. SAS is a monster and is not fun. Having said that your market value will be determined more by what you can and want to do and the general education level than just by this or that technology.,"This all depends. SAS is great and is backed by the SAS Institute, meaning if you're working for an organization that has invested in SAS, you can contact the support team for anything funky happening with the software. 
 R is free and open source, and there are also organizations being created that are building on to and supporting R much like SAS Institute has done. The difference being that SAS has been around much longer and is much more structured than R. 
 They both have their ups and downs, and it depends on what type of analytics you plan on doing in your career. So, to answer your questions, I would decide what you envision yourself doing. To my understanding R is good for Big Data applications and Machine Learning, SAS is great for statistical analysis (ARIMA, etc). 
 SAS and R comparisons: 
 http://support.sas.com/resources/papers/proceedings13/348-2013.pdf 
 http://www.learnanalytics.in/blog/?p=9","In my experience, there are a few things to consider here: 
 
 What field you're going into 
 What kinds of technology you believe you'll be working with 
 What kinds of teams you believe you'll be working with 
 
 Field 
 This is a huge determiner. To my understanding, SAS is standard in finance, banking, some biostats, and other industries. R, on the other hand, is open source, free, and is receiving a lot of attention currently from Microsoft after their acquisition of Revolution Analytics. 
 Technology 
 Are you thinking you'll be at a large corporation or a small shop? Do you think you'll need to work with creating production ready algorithms or simply producing insights and analysis? 
 The reason this matters is that open source technology can often times be a bit easier to convert to production ready use. The cost of software can also be a barrier at smaller companies and may dictate what you are capable of using. 
 Collaboration with Other Teams 
 If you sit firmly on the business side, it's possible that everyone you work with may use SAS or may be comfortable with the outputs. In this case, you may not need to collaborate across any additional technology. 
 If you work with technology, however, it may be difficult to integrate SAS or proprietary solutions into other workflows. An example would be creating a real-time user scoring system. If you were able to program this into R or Python, you may be able to pass the code directly to a developer for implementation. This would be more difficult if a proprietary solution was involved. 
 Other Considerations 
 The analytics space is evolving rapidly. On top of the above, Python is coming out as a very popular technology for use in machine learning and data mining and pairs well with Spark as well as some other large data technology. A specific example of a library here would be scikit-learn. 
 Closing Thoughts/My Experience 
 I'm an R/Python user by experience, though I have some experience with SAS in school as well as in work. Generally, I've found the level of support with R and Python to be great - especially since they've started to rise in popularity. SAS has its uses and has definitely carved out piece of the industry for itself. 
 All in all, however - getting a solid base in statistical theory, programming (scripting), and understanding the application and value that analysis can provide will go a long way. The syntax between these tools is, generally, not worlds apart. SAS can be a bit strange compared to R and Python, but all of these tools generally have syntax which is fairly readable and is not difficult to adapt to another tool.","You'll need both or even more for your ""career in analytics"".  Start with SAS, SAS studio is intuitive and easy, once you are comfortable with the ""stats"" and looking for next level than is coming R in place. 
 SAS also have free courses for intro in stat and their platform.
 SAS free courses 
 For long term believe me that is not about the tools/platform. You can check the free book listed bellow and most important modeling and prediction techniques listed in the book. Theoretical and practical understanding of many important methods is essential and after that you will be able to compute in any language. 
 An Introduction to Statistical Learning","@ImperativelyAblative has many good points.  
 Adding to that, many companies, especially banks, that rooted in SAS have started to experiment with R. This is because of the cost saving and maturing enterprise support, think of RStudio Server and Cloudera. SAS is great, but it comes with a premium price tag (some may argue it's even overpriced).  
 I am a management consultant working at a top tier firm with specialty in data science. We serve most of the global companies and work with their executives and operating teams. Moving to open source tools, such as R, has gained the blessing of many top executives and building tractions among core analytics teams (Marketing, Modelling, etc.). The point being - the market is moving towards open source tools 
 The other consideration, in my opinion, is the role you aspire to play in an organization. Here are some archetypes based on my experience in working with the analytics teams:  
 
 General Business Analyst : R, Interactive visualization tools (Tableau / QlikView), some SQL, and Excel / Powerpoint (of course)  
 Modeller : SAS is must, R (my clients are all trying to pick this up), deep in SQL / HQL (querying on Hadoop stack), and strong domain knowledge (risk model, pricing, operation optimization, etc.) 
 Application Developer  (people who put things into production): SAS is must, Python, and automation language  
 
 Of course, this is not an exhaustive list, but hope it provides some industry trend for your reference. There is always the superstar in any organization who knows all these stuff, could be you :)",,,,,64.70802691,73.48217358,67.38615324,66.58553301,62.89246338,,,,
10228,When should I use Gini Impurity as opposed to Information Gain (Entropy)?,machine-learning,"Gini impurity and Information Gain Entropy are pretty much the same. And people do use the values interchangeably. Below are the formulae of both: 
 
 $\textit{Gini}: \mathit{Gini}(E) = 1 - \sum_{j=1}^{c}p_j^2$ 
 $\textit{Entropy}: H(E) = -\sum_{j=1}^{c}p_j\log p_j$ 
 
 Given a choice, I would use the Gini impurity, as it doesn't require me to compute logarithmic functions, which are computationally intensive. The closed-form of its solution can also be found. 
 
 Which metric is better to use in different scenarios while using
decision trees? 
 
 The Gini impurity, for reasons, stated above. 
 So,  they are pretty much the same when it comes to CART analytics. 
 Helpful reference for computational comparison of the two methods","Generally, your performance will not change whether you use Gini impurity or Entropy.  
 Laura Elena Raileanu and Kilian Stoffel compared both in "" Theoretical comparison between the gini index and information gain criteria "". The most important remarks were: 
 
 It only matters in 2% of the cases whether you use gini impurity or entropy.  
 Entropy might be a little slower to compute (because it makes use of the logarithm).  
 
 I was once told that both metrics exist because they emerged in different disciplines of science.","Gini is intended for continuous attributes and Entropy is for attributes that occur in classes   
 
 Gini  is to minimize misclassification 
 Entropy  is for exploratory analysis   
 
 Entropy is a little slower to compute","For the case of a variable with two values, appearing with fractions  $f$  and  $(1-f)$ ,  
the gini and entropy are given by: 
 $gini = 2*f(1-f)$ 
 $entropy = f*ln\big({1\over f}\big) + (1-f)*ln\big({1\over(1-f)}\big)$ 
 These measures are very similar if scaled to  $1.0$ 
(plotting  $2*gini$  and  ${entropy\over ln(2)}$ ):","To add upon the fact that there are more or less the same, consider also the fact that:
$$
\begin{split}
\forall \; 0 < u < 1,\; \log (1-u) &= -u - u^2/2 - u^3/3 \, + \, \cdots\\
\forall \; 0 < p < 1,\; \log (p) &= p-1 - (1-p)^2/2 - (1-p)^3/3 \, + \, \cdots\\
\end{split}
$$
so that:
$$
\forall \; 0 < p < 1,\; -p \log (p) = p(1-p) + p(1-p)^2/2 + p(1-p)^3/3 \, + \, \cdots
$$
See the following plot of the two functions normalised to get 1 as maximum value: red curve is for Gini while black one is for entropy.
 
 In the end as explained by @NIMISHAN Gini is more suitable to minimise misclassfication as it is symetric to 0.5, while entropy will more penalised small probabilities.","Entropy takes slightly more computation time than Gini Index because of the log calculation, maybe that's why Gini Index has become the default option for many ML algorithms. 
But, from Tan et. al book Introduction to Data Mining 
 ""Impurity measure are quite consistent with each other...
Indeed, the strategy used to prune the tree has a greater impact on the final tree than the choice of impurity measure."" 
 So, it looks like the selection of impurity measure has little effect on the performance of single decision tree algorithms.  
 Also. 
""Gini method works only when the target variable is a binary variable."" - Learning Predictive Analytics with Python.","As per parsimony, principal Gini outperform entropy as of computation ease (log is obvious has more computations involved rather that plain multiplication at processor/machine level). 
 But, entropy definitely has an edge in some data cases involving high imbalance. 
 Since entropy uses log of probabilities and multiplying with probabilities of event, what is happening at background is value of lower probabilities are getting scaled up. 
 If your data probability distribution is exponential or Laplace (like in case of deep learning where we need probability distribution at sharp point) entropy outperform Gini. 
 To give an example if you have  $2$  events one  $.01$  probability and other  $.99$  probability. 
 In Gini probability squared will be  $.01^2+.99^2$ ,  $.0001 + .9801$  means that lower probability does not play any role as everything is governed by the majority probability. 
 Now in case of entropy  $.01*log(.01)+.99*log(.99)= .01*(-2)+ .99*(-.00436) 
= -.02-.00432$ 
now in this case clearly seen lower probabilities are given better weight-age.","I've been doing optimizations on binary classification for the past week+, and in every case, entropy significantly outperforms gini. This may be data set specific, but it would seem like trying both while tuning hyperparameters is a rational choice, rather than making assumptions about the model ahead of time.  
 You never know how data will react until you've run the statistics.","Gini Impurity  $G$  is a first order approximation to Information Gain  $IG$ . 
 To see that first note that the sum of all probabilities  $p_i$  over  $N$  classes is  $\sum_{i=1}^N p_i=1$ . So we can rewrite Gini Impurity like
 $$
G = 1 - \sum_{i=1}^N p_i^2 = \sum_{i=1}^N p_i - \sum_{i=1}^N p_i^2 = \sum_{i=1}^N p_i(1-p_i)
$$ 
To rewrite Information Gain note that the natural logarithm can be expressed as an infinite series (see  https://en.wikipedia.org/wiki/Mercator_series ) like
 $$
\ln(1-x) = -x - \frac{1}{2}x^2 - \frac{1}{3}x^3 - \ldots
$$ 
The Information Gain can then be expressed like
 $$
\begin{align}
IG &= - \sum_{i=1}^N p_i \ln(p_i) = - \sum_{i=1}^N p_i \ln(1 - (1-p_i)) \\
&= - \sum_{i=1}^N p_i \left[-(1-p_i) - \frac{1}{2}(1-p)^2 - \frac{1}{3}(1-p)^3 + \ldots      \right] \\
&= \sum_{i=1}^N \color{red}{p_i } \left[\color{red}{(1-p_i)} + \frac{1}{2}(1-p)^2 + \frac{1}{3}(1-p)^3 + \ldots      \right]
\end{align}
$$ 
The first term (in red) in this infinite series to compute the Information Gain is exactly the Gini Impurity  $G$ . 
 So which metric to use? If you need maximum speed use the Gini Impurity. If you need the theoretical exact solution use Information Gain. And as a compromise you can for example use the second order approximation to Information Gain:
 $$
IG \approx \sum_{i=1}^N p_i (1-p_i) + p_i\frac{1}{2}(1-p)^2 = \frac{1}{2}\sum_{i=1}^N p_i(1-p_i)(3-p_i) 
$$ 
The plot below (screenshot from  Wolfram Alpha  with  $x=p_i$ ) shows Gini Impurity in red, Information Gain in blue and the second order approximation in green. The second order approximation to Information Gain sits basically in the middle between the Gini Impurity and Information Gain.",78.99060209,82.74084031,64.95638238,61.66206971,56.15181118,63.01453434,58.78394284,54.91584808,77.99873967
10188,Why do cost functions use the square error?,machine-learning,"Your loss function would not work because it incentivizes setting  $\theta_1$  to any finite value and  $\theta_0$  to  $-\infty$ . 
 Let's call  $r(x,y)=\frac{1}{m}\sum_{i=1}^m {h_\theta\left(x^{(i)}\right)} -y$  the  residual  for  $h$ . 
 Your goal is to make  $r$   as close to zero  as possible,  not just minimize it . A high negative value is just as bad as a high positive value. 
 EDIT:  You can counter this by artificially limiting the parameter space  $\mathbf{\Theta} $ (e.g. you want  $|\theta_0| < 10$ ). In this case, the optimal parameters would lie on certain points on the boundary of the parameter space. See  https://math.stackexchange.com/q/896388/12467 . This is not what you want. 
 Why do we use the square loss 
 The squared error forces  $h(x)$  and  $y$  to match. It's minimized at  $u=v$ , if possible, and is always  $\ge 0$ , because it's a square of the real number  $u-v$ . 
 $|u-v|$  would also work for the above purpose, as would  $(u-v)^{2n}$ , with  $n$  some positive integer. The first of these is actually used (it's called the  $\ell_1$  loss; you might also come across the  $\ell_2$  loss, which is another name for squared error).  
 So, why is the squared loss better than these? This is a  deep  question related to the link between  Frequentist  and  Bayesian  inference. In short, the squared error relates to  Gaussian Noise . 
 If your data does not fit all points exactly, i.e.  $h(x)-y$  is not zero for some point no matter what  $\theta$  you choose (as will always happen in practice), that might be because of  noise . In any complex system there will be many small  independent  causes for the difference between your  model   $h$  and  reality   $y$ : measurement error, environmental factors etc. By the  Central Limit Theorem (CLT), the total noise would be distributed  Normally , i.e. according to the  Gaussian distribution . We want to pick the best fit  $\theta$  taking this noise distribution into account. Assume  $R = h(X)-Y$ , the part of  $\mathbf{y}$  that your model cannot explain, follows the Gaussian distribution  $\mathcal{N}(\mu,\sigma)$ . We're using capitals because we're talking about random variables now. 
 The Gaussian distribution has two parameters, mean  $\mu = \mathbb{E}[R] = \frac{1}{m} \sum_i h_\theta(X^{(i)})-Y^{(i))}$  and variance  $\sigma^2 = E[R^2] = \frac{1}{m} \sum_i \left(h_\theta(X^{(i)})-Y^{(i))}\right)^2$ . See  here  to understand these terms better. 
 
 Consider  $\mu$ , it is the  systematic error  of our measurements. Use  $h'(x) = h(x) - \mu$  to correct for systematic error, so that  $\mu' = \mathbb{E}[R']=0$  (exercise for the reader). Nothing else to do here. 
 $\sigma$  represents the  random error , also called  noise . Once we've taken care of the systematic noise component as in the previous point, the best predictor is obtained when  $\sigma^2 = \frac{1}{m} \sum_i \left(h_\theta(X^{(i)})-Y^{(i))}\right)^2$  is minimized. Put another way, the best predictor is the one with the tightest distribution (smallest variance) around the predicted value, i.e. smallest variance.  Minimizing the the least squared loss is the same thing as minimizing the variance!  That explains why the least squared loss works for a wide range of problems. The underlying noise is very often Gaussian, because of the CLT, and minimizing the squared error turns out to be the  right  thing to do! 
 
 To simultaneously take both the mean and variance into account, we include a  bias  term in our classifier (to handle systematic error  $\mu$ ), then minimize the square loss. 
 Followup questions: 
 
 Least squares loss = Gaussian error. Does every other loss function also correspond to some noise distribution?  Yes. For example, the  $\ell_1$  loss (minimizing absolute value instead of squared error) corresponds to the  Laplace distribution  (Look at the formula for the PDF in the infobox -- it's just the Gaussian with  $|x-\mu|$  instead of  $(x-\mu)^2$ ). A popular loss for probability distributions is the  KL-divergence .
-The Gaussian distribution is very well motivated because of the  Central Limit Theorem , which we discussed earlier. When is the Laplace distribution the right noise model? There are some circumstances where it comes about naturally, but it's more commonly as a regularizer  to enforce  sparsity : the  $\ell_1$  loss is the  least convex  among all convex losses.  
 
 As  Jan  mentions in the comments, the minimizer of  squared  deviations is the mean and the minimizer of the sum of  absolute  deviations is the  median . Why would we want to find the median of the residuals instead of the mean? Unlike the mean, the median isn't thrown off by one very large outlier. So, the  $\ell_1$  loss is used for increased robustness. Sometimes a combination of the two is used. 
 
 Are there situations where we minimize both the Mean and Variance?  Yes. Look up  Bias-Variance Trade-off . Here, we are looking at a set of classifiers  $h_\theta \in H$  and asking which among them is best. If we ask which  set  of classifiers is the best for a problem, minimizing both the bias and variance becomes important. It turns out that there is always a trade-off between them and we use  regularization  to achieve a compromise. 
 
 Regarding the  $\frac{1}{2}$  term 
 The 1/2 does not matter and actually, neither does the  $m$  - they're both constants. The optimal value of  $\theta$  would remain the same in both cases.  
 
 The expression for the gradient becomes prettier with the  $\frac{1}{2}$ , because the 2 from the square term cancels out. 
 
 When writing code or algorithms, we're usually concerned more with the gradient, so it helps to keep it concise. You can check progress just by checking the norm of the gradient. The loss function itself is sometimes omitted from code because it is used only for validation of the final answer. 
 
 The  $m$  is useful if you solve this problem with gradient descent. Then your gradient becomes the average of  $m$  terms instead of a sum, so its' scale does not change when you add more data points. 
 
 I've run into this problem before: I test code with a small number of points and it works fine, but when you test it with the entire dataset there is loss of precision and sometimes over/under-flows, i.e. your gradient becomes  nan  or  inf . To avoid that, just normalize w.r.t. number of data points. 
 
 These aesthetic decisions are used here to maintain consistency with future equations where you'll add  regularization  terms. If you include the  $m$ , the regularization parameter  $\lambda$  will not depend on the dataset size  $m$  and it will be more interpretable across problems.","The 1/2 coefficient is merely for convenience; it makes the derivative, which is the function actually being optimized, look nicer. The 1/m is more fundamental; it suggests that we are interested in the  mean  squared error. This allows you to make fair comparisons when changing the sample size, and prevents overflow. So called ""stochastic"" optimizers use a subset of the data set (m' < m). When you introduce a regularizer (an additive term to the objective function), using the 1/m factor allows you to use the same coefficient for the regularizer regardless of the sample size. 
 As for the question of why the square and not simply the difference: don't you want underestimates to be penalized similarly to overestimates? Squaring eliminates the effect of the sign of the error. Taking the absolute value (L1 norm) does too, but its derivative is undefined at the origin, so it requires more sophistication to use. The L1 norm has its uses, so keep it in mind, and perhaps ask the teacher if (s)he's going to cover it.","The error measure in the loss function is a 'statistical distance'; in contrast to the popular and preliminary understanding of distance between two vectors in Euclidean space. With 'statistical distance' we are attempting to map the 'dis-similarity' between estimated model and optimal model to Euclidean space. 
 There is no constricting rule regarding the formulation of this 'statistical distance', but if the choice is appropriate then a progressive reduction in this 'distance' during optimization translates to a progressively improving model estimation. Consequently, the choice of 'statistical distance' or error measure is related to the underlying data distribution. 
 In fact, there are several well defined distance/error measures for different classes of statistical distributions. It is advisable to select the error measure based on the distribution of the data in hand. It just so happens that the Gaussian distribution is ubiquitous, and consequently its associated distance measure, the L2-norm is the most popular error measure. However, this is not a rule and there exist real world data for which an 'efficient'* optimization implementation would adopt a different error measure than the L2-norm. 
 Consider the set of  Bregman divergences . The canonical representation of this divergence measure is the L2-norm (squared error). It also includes relative entropy (Kullback-Liebler divergence), generalized Euclidean distance (Mahalanobis metric), and Itakura-Saito function. You can read more about it in this paper on  Functional Bregman Divergence and Bayesian Estimation of Distributions . 
 Take-away: The L2-norm has an interesting set of properties which makes it a popular choice for error measure (other answers here have mentioned some of these, sufficient to the scope of this question), and the squared error will be the appropriate choice most of the time. Nevertheless, when the data distribution requires it, there are alternate error measures to choose from, and the choice depends in large part on the formulation of the optimization routine.  
 *The 'appropriate' error measure would make the loss function convex for the optimization, which is very helpful, as opposed to some other error measure where the loss function is non-convex and thereby notoriously difficult.","In addition to the key points made by others, using squared error puts a greater emphasis on larger error (what happens to 1/2 when you square it vs 3/2?). 
 Having an algorithm that moves the fractional errors, that would likely result in correct classification or very small difference between estimate and ground truth, if left alone close to zero, while leaving the large errors as large errors or misclassifications, is not a desirable characteristic of an algorithm. 
 Using squared error uses the error as an implied importance weight for adjusting prediction.","In your formulation, you try to obtain the mean deviation of your approximation from the observed data. 
 If the mean value of your approximation is close or equal to the mean value of the observed data (something which is desirable and often happens with many approximation schemes) then the result of your formulation would be zero or negligible, because positive errors compensate with negative errors. This might lead to the conclusion that your approximation is wonderful at each observed sample, while it might not be the case. That's why you use the square of the error at each sample and you add them up (your turn each error positive). 
 Of course this is only a possible solution, as you could have used L1-norm (absolute value of the error at each sample) or many others, instead of L2-norm.","You need each data point distances in your mother data sample (training subset, if wanting generalization not just description of the data), not just the difference of their signed values to your candidate model. 
 The set of all functions that the candidate model being searched can span by changing its parameters, can be called a function space over which one can define on its whole domain a distance function or norm in the case of L2 and L1 norms (having a blank about norm versus distance, but norm is distance, and L2 and L1 norms ring a bell). 
 You should think of the objective function as the surface between the data x, y points and the x, f(x)  (f being the model given parametrization room, i.e. which thetas can be adjusted in the search). 
 That surface has to be positive if you accumulate.  Minimizing something that can go negative, will end up going in the negative. 
 The guarantee that the minimization problem has a solution and does not drift down, is that you need a non-negative definite, functional of your function AND data sample matrix).  Functional just to mean function that has a function space as domain and gives a non-negative real number as output. 
 I bet I just rephrased already given answers. But if this worded attempt helps. I leave it here. 
 L2 has an inner product, a nice property, which allows a bunch of implementation level calculus to be had.  But, as others stated, you can try other distance (metric) or norm functions, such as L1.  All distances or norm have the property that their output being zero, as a functional of the difference between 2 functions as input variable (whole function, i.e. whole data matrix of the difference vector), being a distance (or norm) implies that the functions are identical. 
 I now doubt I helped at all.",,,,61.19347756,63.40005375,58.48756304,73.90563994,61.04838498,54.92575258,,,
10103,Improve the speed of t-sne implementation in python for huge data,python,"You must look at  this Multicore implementation  of t-SNE. 
 I actually tried it and can vouch for its superior performance.","Check out FFT-accelerated Interpolation-based t-SNE ( paper ,  code , and  Python package ). 
 From the abstract:  
 
 We present Fast
  Fourier Transform-accelerated Interpolation-based t-SNE (FIt-SNE),
  which dramatically accelerates the computation of t-SNE. The most
  time-consuming step of t-SNE is a convolution that we accelerate by
  interpolating onto an equispaced grid and subsequently using the fast
  Fourier transform to perform the convolution. We also optimize the
  computation of input similarities in high dimensions using
  multi-threaded approximate nearest neighbors.  
 
 The paper also includes an example of a dataset with a million points and 100 dimensions (similar to OP's setting), and it seems to take ~1 hour.","Try  UMAP . 
 It's significantly faster than t-SNE.","Since, there are no answers in SO, I have asked myself in github page and the issue has been closed by stating the following reply by GaelVaroquaux.. 
 
 If you only want to parallelise vector operation, then you should use a
  build of numpy compiled with MKL (don't attempt to do it yourself, it's
  challenging). 
 There could be approaches to high-level parallelism in the algorithm
  itself, which would probably lead to larger gains. However, after a quick
  look at the code, I didn't see any clear way of doing that. 
 I am going to ahead and close this issue, as it is more of a blue-sky
  whish list. I completely agree, I would like TSNE to go faster, and it
  would be great is parallelism was easy. But in the current state of
  affairs, more work is required to be in a state where we can tackle such
  wish list.","Since version 0.22, there is a new parameter called n_jobs in the  scikit-learn t-SNE implementation . This parameter specifies the number of parallel jobs to run for neighbors search. 
 The  Multicore-TSNE  project mentioned in another answer seems to be dead.",,,,,60.58248118,59.57446037,60.04684334,50,55.73082271,,,,
9945,R: machine learning on GPU,machine-learning,"As for a complete machine learning package on GPU's, no such package exists.  However, there are actually a handful of R packages that can use GPU's.  You can see these packages on the  CRAN High Performance Computing page .  You should note that most of these packages do require you to have a NVIDIA card.  Of the packages available, there are three packages you most likely would utilize unless you have a special case. 
 
 gputools  - if interested in distance computations (only NVIDIA). 
 gmatrix  - general numeric computations (only NVIDIA). 
 gpuR  - general numeric computations (any GPU via OpenCL).* 
 
 *  NOTE  - At the risk of self promotion I am the author of the gpuR package. 
 You can likely use the latter two packages to reproduce existing machine learning algorithms.  I am actually using my gpuR package to create a GPU accelerated neuralnet package but this is in progress. 
 So in summary, if you are determined, the basic resources are available in R.  But if you need something in the immediate future, you will need to explore other resources/approaches as pointed out by @YCR.","This  is really a wrapper over tensorflow, caffe, mxnet, but may be useful to you.","If you use SVM, you can try the  Rgtsvm  package for GPU which is backwards compatible with the e1071 implementation.","A good library for machine learning with GPUs is  mxnet . The package is mostly deep learning though, so if you are looking for specific machine learning algorithms you might not find them there. However they have a good set of deep learning algorithms.","The question is quite old, but  LightGBM  implementing various tree based learning algorithms : 
 
 GBDT , Gradient boosting decision tree 
 DART , or Dropouts meet Multiple Additive Regression Trees 
 GOSS, or Gradient-based One-Side Sampling 
 Random Forest 
 
 Has a GPU support and an  R package  which can call the GPU version. 
 However, as often with GPUs it is more complex than a simple  install.package 
You may find detailed build instructions for  lightgbm on the GPU here  and once built a  couple of steps  are needed to have the R interface.",,,,,63.72939994,50,62.04362533,71.04193079,66.31638436,,,,
9850,Neural networks: which cost function to use?,machine-learning,"This answer is on the  general  side of cost functions, not related to TensorFlow, and will mostly address the ""some explanation about this topic"" part of your question. 
 In most examples/tutorial I followed, the cost function used was somewhat arbitrary. The point was more to introduce the reader to a specific method, not to the cost function specifically. It should not stop you to follow the tutorial to be familiar with the tools, but my answer should help you on how to choose the cost function for your own problems. 
 If you want answers regarding Cross-Entropy, Logit, L2 norms, or anything specific, I advise you to post multiple, more specific questions. This will increase the probability that someone with specific knowledge will see your question. 
 
 Choosing the right cost function for achieving the desired result is a critical point of machine learning problems. The basic approach, if you do not know exactly what you want out of your method, is to use  Mean Square Error (Wikipedia)  for regression problems and Percentage of error for classification problems. However, if you want  good  results out of your method, you need to  define good , and thus define the adequate cost function. This comes from both domain knowledge (what is your data, what are you trying to achieve), and knowledge of the tools at your disposal.  
 I do not believe I can guide you through the cost functions already implemented in TensorFlow, as I have very little knowledge of the tool, but I can give you an example on how to write and assess different cost functions. 
 
 To illustrate the various differences between cost functions, let us use the example of the binary classification problem, where we want, for each sample  $x_n$ , the class  $f(x_n) \in \{0,1\}$ . 
 Starting with  computational properties ; how two functions measuring the ""same thing"" could lead to different results. Take the following, simple cost function; the percentage of error. If you have  $N$  samples,  $f(y_n)$  is the predicted class and  $y_n$  the true class, you want to minimize 
 
 $\frac{1}{N} \sum_n \left\{
\begin{array}{ll}
1 & \text{ if } f(x_n) \not= y_n\\
0 & \text{ otherwise}\\
\end{array} \right. = \sum_n y_n[1-f(x_n)] + [1-y_n]f(x_n)$ . 
 
 This cost function has the benefit of being easily interpretable. However, it is not smooth; if you have only two samples, the function ""jumps"" from 0, to 0.5, to 1. This will lead to inconsistencies if you try to use gradient descent on this function. One way to avoid it is to change the cost function to use probabilities of assignment;  $p(y_n = 1 | x_n)$ . The function becomes 
 
 $\frac{1}{N} \sum_n y_n p(y_n = 0 | x_n) + (1 - y_n) p(y_n = 1 | x_n)$ . 
 
 This function is smoother, and will work better with a gradient descent approach. You will get a 'finer' model. However, it has other problem; if you have a sample that is ambiguous, let say that you do not have enough information to say anything better than  $p(y_n = 1 | x_n) = 0.5$ . Then, using gradient descent on this cost function will lead to a model which increases this probability as much as possible, and thus, maybe, overfit. 
 Another problem of this function is that if  $p(y_n = 1 | x_n) = 1$  while  $y_n = 0$ , you are certain to be right, but you are wrong. In order to avoid this issue, you can take the log of the probability,  $\log p(y_n | x_n)$ . As  $\log(0) = \infty$  and  $\log(1) = 0$ , the following function does not have the problem described in the previous paragraph: 
 
 $\frac{1}{N} \sum_n y_n \log p(y_n = 0 | x_n) + (1 - y_n) \log p(y_n = 1 | x_n)$ . 
 
 This should illustrate that in order to optimize the  same thing , the percentage of error, different definitions might yield different results if they are easier to make sense of, computationally. 
 It is possible for cost functions  $A$  and  $B$  to measure the  same concept , but  $A$  might lead your method to better results than  $B$ . 
 
 Now let see how different costs function can measure different concepts. In the context of information retrieval, as in google search (if we ignore ranking), we want the returned results to 
 
 have high  precision , not return irrelevant information 
 have high  recall , return as much relevant results as possible 
 Precision and Recall (Wikipedia) 
 
 Note that if your algorithm returns  everything , it will return every relevant result possible, and thus have high recall, but have very poor precision. On the other hand, if it returns only  one  element, the one that it is the most certain is relevant, it will have high precision but low recall. 
 In order to judge such algorithms, the common cost function is the  $F$ -score (Wikipedia) . The common case is the  $F_1$ -score, which gives equal weight to precision and recall, but the general case it the  $F_\beta$ -score, and you can tweak  $\beta$  to get 
 
 Higher recall, if you use  $\beta > 1$ 
 Higher precision, if you use  $\beta < 1$ . 
 
 In such scenario,  choosing the cost function is choosing what trade-off your algorithm should do . 
 Another example that is often brought up is the case of medical diagnosis, you can choose a cost function that punishes more false negatives or false positives depending on what is preferable: 
 
 More healthy people being classified as sick (But then, we might treat healthy people, which is costly and might hurt them if they are actually not sick) 
 More sick people being classified as healthy (But then, they might die without treatment) 
 
 
 In conclusion, defining the cost function is defining the goal of your algorithm. The algorithm defines how to get there. 
 
 Side note: Some cost functions have nice algorithm ways to get to their goals. For example, a nice way to the minimum of the  Hinge loss (Wikipedia)  exists, by solving the dual problem in  SVM (Wikipedia)","To answer your question on Cross entropy, you'll notice that both of what you have mentioned are the same thing. 
 $-\frac{1}{n} \sum(y\_train * \log(y\_output) + (1 - y\_train) \cdot \log(1 - y\_output))$ 
 that you mentioned is simply the binary cross entropy loss where you assume that $y\_train$ is a 0/1 scalar and that $y\_output$ is again a scalar indicating the probability of the output being 1. 
 The other equation you mentioned is a more generic variant of that extending to multiple classes 
 -tf.reduce_sum(y_train * tf.log(y_output)) 
is the same thing as writing 
 $-\sum_n train\_prob \cdot \log (out\_prob)$ 
 where the summation is over the multiple classes and the probabilities are for each class. Clearly in the binary case it is the exact same thing as what was mentioned earlier. The $n$ term is omitted as it doesn't contribute in any way to the loss minimization as it is a constant.","BLUF: iterative trial-and-error with subset of data and matplotlib. 
 Long Answer: 
 My team was struggling with this same question not that long ago. All the answers here are great, but I wanted to share with you my ""beginner's answer"" for context and as a starting point for folks who are new to machine learning. 
 You want to aim for a cost function that is smooth and convex for your specific choice of algorithm and data set. That's because you want your algorithm to be able to confidently and efficiently adjust the weights to eventually reach the global minimum of that cost function. If your cost function is ""bumpy"" with local max's and min's, and/or has no global minimum, then your algorithm might have a hard time converging; its weights might just jump all over the place, ultimately failing to give you accurate and/or consistent predictions. 
 For example, if you are using linear regression to predict someone's weight (real number, in pounds) based on their height (real number, in inches) and age (real number, in years), then the mean squared error cost function should be a nice, smooth, convex curve. Your algorithm will have no problems converging. 
 But say instead you are using a logistic regression algorithm for a binary classification problem, like predicting a person's gender based on whether the person has purchased diapers in the last 30 days and whether the person has purchased beer in the last 30 days. In this case, mean squared error might not give you a smooth convex surface, which could be bad for training. And you would tell that by experimentation. 
 You could start by running a trial with using MSE and a small and simple sample of your data or with mock data that you generated for this experiment. Visualize what is going on with matplotlib (or whatever plotting solution you prefer). Is the resulting error curve smooth and convex? Try again with an additional input variable... is the resulting surface still smooth and convex? Through this experiment you may find that while MSE does not fit your problem/solution, cross entropy gives you a smooth convex shape that better fits your needs. So you could try that out with a larger sample data set and see if the hypothesis still holds. And if it does, then you can try it with your full training set a few times and see how it performs and if it consistently delivers similar models. If it does not, then pick another cost function and repeat the process. 
 This type of highly iterative trial-and-error process has been working pretty well for me and my team of beginner data scientists, and lets us focus on finding solutions to our questions without having to dive deeply into the math theory behind cost function selection and model optimization. 
 Of course, a lot of this trial and error has already been done by other people, so we also leverage public knowledge to help us filter our choices of what might be good cost functions early in the process. For example, cross entropy is generally a good choice for classification problems, whether it's binary classification with logistic regression like the example above or a more complicated multi-label classification with a softmax layer as the output. Whereas MSE is a good first choice for linear regression problems where you are seeking a scalar prediction instead of the likelihood of membership in a known category out of a known set of possible categories, in which case instead of a softmax layer as your output you'd could just have a weighted sum of the inputs plus bias without an activation function. 
 Hope this answer helps other beginners out there without being overly simplistic and obvious.","A loss function is a guide for the model to decide its path using the optimizer.
So, it will try to bring some number which must correctly reflect the gap with the actual value and also (though not limited to) -  
 Understand Outliers, Understand the model's purpose, Model's approach, Understand the prediction type i.e. Number, Binary label etc.   
 I agree that this question is too vast to answer in a short text, but still, I would try to list a summary of usage which I found most of the Authors suggesting. 
 This might help you to start your model but must be accompanied by individual research based on scenario and data. 
 It might also trigger multiple WHYs and HOWs. Ask a new question Or use the already answered questions on these(there are many) 
 mean_squared_error 
Default for regression 
 mean_absolute_error 
Regression when you have outliers 
 mean_squared_logarithmic_error 
Regression. Further scaled-down the error. Use when you expect big values in your prediction 
 huber_loss 
A mid-way of MSE and MAE. This function is quadratic for small values, and linear for large values 
 logcosh 
It's again a mid way to get the benefits of both MSE and MAE
log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x. This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. 
 mean_absolute_percentage_error 
When we are interested in % measurement, not values. e.g. while dealing with the data of scale of a country's population, % would be more important than a big number ~10000 
 hinge 
SVM. It takes care of the margin around support vector.   
 categorical_crossentropy 
Multiclass Classification - we have one target probability per class for each instance (such as one-hot vectors, e.g. [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3 
 sparse_categorical_crossentropy 
Multiclass Classification -  we have sparse labels (i.e., for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive 
 binary_crossentropy 
Use it for simple Binary Classification 
 Notes ::   These are the ""loss"" from Keras library. The Concept would be same but other libraries may use some other text variance to name these.","Regrading your question 
 
 Where is the (1 - y_train) * log(1 - y_output) part in most
TensorFlow examples? Isn't it missing? 
 
 The answer is that most output functions are softmax. That means you don't necessarily need to reduce all the probabilities in wrong cases as they will automatically be reduced when you increase probability of the right one 
 For Example: 
 
 before optimisation 
 
 y_output = [0.2, 0.2, 0.6]  and  y_train = [0, 0, 1] 
 
 after optimisation 
 
 y_output = [0.15, 0.15, 0.7]  and  y_train = [0, 0, 1] 
 
 here observe that even though we just increased third term, all the other terms automatically reduced",,,,,74.71246369,50,61.98636266,52.0724551,51.09891036,,,,
9832,What is the Q function and what is the V function in reinforcement learning?,machine-learning,"$V^\pi(s)$  is the ""state"" value function of an MDP (Markov Decision Process). It's the expected return starting from state  $s$  following policy  $\pi$ : 
 $$V^\pi(s) = E_{\pi} \{G_t \vert s_t = s\} $$ 
 $G_t$  is the total DISCOUNTED reward from time step  $t$ , as opposed to  $R_t$  which is an immediate return. Here you are taking the expectation over ALL actions according to the policy  $\pi$ . 
 $Q^\pi(s, a)$  is the ""state action"" value function, also known as the quality function. It is the expected return starting from state  $s$ , taking action  $a$ , then following policy  $\pi$ . It's focusing on the particular action at the particular state. 
 $$Q^\pi(s, a) = E_\pi \{G_t | s_t = s, a_t = a\}$$ 
 The relationship between  $Q^\pi$  and  $V^\pi$  (the value of being in that state) is: 
 $$V^\pi(s) = \sum_{a ∈ A} \pi (a|s) * Q^\pi(s,a)$$ 
 You sum every state action-value multiplied by the probability of taking that action (given by the policy  $\pi(a|s)$ ). 
 If you think of the grid world example, you multiply the probability of (up/down/right/left) with the one step ahead of the state value of (up/down/right/left).","Q-values are a great way to the make actions explicit so  you can deal with problems where the transition function is not available (model-free). However, when your action-space is large, things are not so nice and Q-values are not so convenient. Think of a huge number of actions or even continuous action-spaces. 
 From a sampling perspective, the dimensionality of  $Q(s, a)$  is higher than  $V(s)$  so it might get harder to get enough  $(s, a)$  samples in comparison with  $(s)$ . If you have access to the transition function sometimes  $V$  is good.  
 There are also other uses where both are combined. For instance, the advantage function where  $A(s, a) = Q(s, a) - V(s)$ . If you are interested, you can find a recent example using advantage functions here: 
 
 Dueling Network Architectures for Deep Reinforcement Learning 
 
 by Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot and Nando de Freitas.","You have it right, the  $V$  function gives you the value of a state (technically, its  $V^\pi$ , the value obtained by following a given policy  $\pi$ , but if  $\pi$  is omitted it refers to the current policy). And  $Q$  gives you the value of an action in a state. I found the clearest explanation of Q-learning and how it works in Tom Mitchell's book ""Machine Learning"" (1997), ch. 13, which is downloadable.  $V$  is defined as the sum of an infinite series but its not important here. What matters is the  $Q$  function is defined as 
 $$
Q(s,a ) = r(s,a ) + \gamma V^{*}(\delta(s,a))
$$ 
where  $\gamma$  is the discount applied to the reward obtained in the next state, and  $V^*$  is the best value of a state if you could follow an optimum policy which you don't know. However it has a nice characterization in terms of  $Q$ 
 $$
     V^{*}(s)= \max_{a'} Q(s,a')
$$ 
Computing  $Q$  is done by replacing the  $V^*$  in the first equation to give
 $$
Q(s, a) = r(s, a) + \gamma \max_{a'} Q(\delta(s, a), a')
$$ 
 This may seem an odd recursion at first because its expressing the Q value of an action in the current state in terms of the best Q value of a  successor  state, but it makes sense when you look at how the backup process uses it: The exploration process stops when it reaches a goal state and collects the reward, which becomes that final transition's Q value. Now in a subsequent training episode, when the exploration process reaches that predecessor state, the backup process uses the above equality to update the current Q value of the predecessor state. Next time  its  predecessor is visited that state's Q value gets updated, and so on back down the line (Mitchell's book describes a more efficient way of doing this by storing all the computations and replaying them later). Provided every state is visited infinitely often this process eventually computes the optimal Q 
 Sometimes you will see a learning rate  $\alpha$  applied to control how much Q actually gets updated:
 $$
Q(s, a) = (1-\alpha)Q(s, a) + \alpha(r(s, a) + \gamma \max_{a'} Q(s',a'))
$$ 
 $$
= Q(s, a) + \alpha(r(s, a) + \gamma \max_{a'} Q(s',a') - Q(s,a))
$$ 
Notice now that the update to the Q value  does  depend on the current Q value. Mitchell's book also explains why that is and why you need  $\alpha$ : its for stochastic MDPs. Without  $\alpha$ , every time a state,action pair was attempted there would be a different reward so the Q^ function would bounce all over the place and not converge.  $\alpha$  is there so that as the new knowledge is only accepted in part. Initially  $\alpha$  is set high so that the current (mostly random values) of Q are less influential.  $\alpha$  is decreased as training progresses, so that new updates have less and less influence, and now Q learning converges","Here is a more detailed explanation of the relationship between state value and action value in Aaron's answer. Let's first take a look at the definitions of value function and action value function under policy  $\pi$ :
 \begin{align}
    &v_{\pi}(s)=E{\left[G_t|S_t=s\right]} \\
    &q_{\pi}(s,a)=E{\left[G_t|S_t=s, A_t=a\right]}
\end{align} 
where  $G_t=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}$  is the return at time  $t$ . The relationship between these two value functions can be derived as
 \begin{align}
     v_{\pi}(s)&=E{\left[G_t|S_t=s\right]} \nonumber \\
     &=\sum_{g_t} p(g_t|S_t=s)g_t \nonumber \\
     &= \sum_{g_t}\sum_{a}p(g_t, a|S_t=s)g_t \nonumber \\
     &= \sum_{a}p(a|S_t=s)\sum_{g_t}p(g_t|S_t=s, A_t=a)g_t \nonumber \\
     &= \sum_{a}p(a|S_t=s)E{\left[G_t|S_t=s, A_t=a\right]} \nonumber \\
     &= \sum_{a}p(a|S_t=s)q_{\pi}(s,a) 
\end{align} 
The above equation is important. It describes the relationship between two fundamental value functions in reinforcement learning. It is valid for any policy. Moreover, if we have a  deterministic  policy, then  $v_{\pi}(s)=q_{\pi}(s,\pi(s))$ . Hope this is helpful for you.
(to see more about  Bellman optimality equation )","Value function 
 
 The value function estimates the expected cumulative reward of being in a particular state. 
 It is a state function, meaning that it only takes the state as input. 
 The value function can be used to evaluate different policies, and to find the optimal policy. 
 
 Q-function 
 
 The Q function estimates the expected cumulative reward of taking a particular action in a given state. 
 It is a state-action function, meaning that it takes both the state and the action as input. 
 The Q function is used to learn an optimal policy, which is a policy that maximizes the expected cumulative reward. 
 
 Key Difference between Q-Function and Value Function 
 The Q function and the value function are both used to estimate the expected cumulative reward, but they do so in different ways. The Q function takes both the state and the action as input, while the value function only takes the state as input. This means that the Q function can be used to learn an optimal policy, while the value function can only be used to evaluate different policies. The Q function is more complex than the value function, but it can also be more accurate. The value function is simpler, but it is less accurate. 
 Moreover, the concept of Q function and Value Function is illustrated with a Grid-World ->  https://www.youtube.com/watch?v=GzHvZ_sSvQE","Adding onto the other answers, note that sometimes it can be useful to express   $V$  as an expectation of  $Q$  over the distribution of actions according to the policy  $\pi$ : 
 $$
V^\pi(s) = \mathbb{E}_{a \sim \pi}[Q^\pi(s, a)]
$$ 
 Note that the subscript of the expectation operator there is using a shorthand notation. Sometimes you may see it written as  $a \sim \pi(\, \cdot \,| s)$  to indicate that the expectation is across a random variable  $a$  which is distributed according to the probability measure  $\pi$  given the current state  $s$ . 
 This expectation expands out to the summation seen in other answers: 
 $$
V^\pi(s) = \sum_{a \in \mathcal{A}}\pi(a|s) Q^\pi(s, a)
$$",The value function is an abstract formulation of utility. And the Q-function is used for the Q-learning algorithm.,,,53.41751778,61.69551009,52.87571651,56.01155713,65.22817524,50,61.39987378,,
9819,Number of epochs in Gensim Word2Vec implementation,gensim,"Increasing the number of epochs usually benefits the quality of the word representations. In experiments I have performed where the goal was to use the word embeddings as features for text classification setting the epochs to 15 instead of 5, increased the performance.","I trained my w2v model on google news 300 for [2, 10, 100] epochs and the best one was on 10 epochs. After all that waiting, I was shocked that 100 epochs was bad. 
 epoch   wall                    
------ ------                    
2       56 s                    
10      4m 44s (284s)           
100     47m 27s (2847 s)","I looked  here , and found that the default value changed from 1 to 5. Apparently the authors believe that  more epochs will improve the results .  
 I cannot tell from experience, yet.","You can use a call back to output the loss at every epoch to help you decide how many to use: 
 import gensim
from gensim.models.callbacks import CallbackAny2Vec

# Your model params:
CONTEXT_WINDOW = 5
NEGATIVES = 5
MIN_COUNT = 5
EPOCHS = 20

class LossLogger(CallbackAny2Vec):
    '''Output loss at each epoch'''
    def __init__(self):
        self.epoch = 1
        self.losses = []

    def on_epoch_begin(self, model):
        print(f'Epoch: {self.epoch}', end='\t')

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        self.losses.append(loss)
        print(f'  Loss: {loss}')
        self.epoch += 1

loss_logger = LossLogger()
mod = gensim.models.word2vec.Word2Vec(sentences=sentences,
                                      sg=1,
                                      window=CONTEXT_WINDOW,
                                      negative=NEGATIVES,
                                      min_count=MIN_COUNT,
                                      callbacks=[loss_logger],
                                      compute_loss=True,
                                      epochs=EPOCHS)
 
 ...and you can use  loss_logger.losses  to retrieve them later (if you want to plot them, for example...)","Increasing the iter count (number of epochs) dramatically increases the training time. Word2Vec gives quality results only if you feed a massive amount documents, therefore looping even twice on them is not reasonable although it actually makes the resulting word embeddings more accurate.",,,,,66.821507,63.46237399,54.19047292,66.86767902,67.9223081,,,,
9818,Is there any domain where Bayesian Networks outperform neural networks?,machine-learning,"One of the areas where Bayesian approaches are often used, is where one needs interpretability of the prediction system. You don't want to give doctors a Neural net and say that it's 95% accurate. You rather want to explain the assumptions your method makes, as well as the decision process the method uses.  
 Similar area is when you have a strong prior domain knowledge and want to use it in the system.","Bayesian networks and neural networks are not exclusive of each other. In fact, Bayesian networks are just another term for ""directed graphical model"". They can be very useful in designing objective functions neural networks. Yann Lecun has pointed this out here:  https://plus.google.com/+YannLeCunPhD/posts/gWE7Jca3Zoq . 
 One example. 
 The variational auto encoder and derivatives are directed graphical models of the form $$p(x) = \int_z p(x|z)p(z) dz.$$ A neural networks is used to implemented $p(x|z)$ and an approximation to its inverse: $q(z|x) \approx p(z|x)$.","Sometimes you care as much about changing the outcome as predicting the outcome.   
 A neural network given enough training data will tend to predict the outcome better, but once you can predict the outcome, you then may wish to predict the effect of making changes in the input features on the outcome.   
 An example from real life, knowing that someone is likely to have a heart attack is useful, but being able to tell the person that if they stopped doing XX, the risk would reduce by 30% is of much greater benefit. 
 Likewise for customer retention, knowing why customers stop shopping with you, is worth as much as predicting the customers that are likely to stop shopping with you. 
 Also a simpler Bayesian Network that predicts less well but  leads to more action being taken  may often be better than a more “correct” Bayesian Network. 
 The biggest advantage of Bayesian networks over neural networks is that they can be used for causal inference . This branch is of fundamental importance to statistics and machine learning and Judea Pearl has won the  Turing award  for this research.","Excellent answers already. 
 One domain which I can think of, and is working extensively in, is the  customer analytics  domain. 
 I have to understand and predict the moves and motives of the customers in order to inform and warn both the customer support, the marketing and also the growth teams. 
 So here, neural networks do a really good job in churn prediction, etc. But, I found and prefer the Bayesian networks style, and here are the reasons for preferring it: 
 
 Customers always have a pattern. They always have a  reason  to act. And that reason would be something which my team has done for them, or they have learnt themselves. So, everything has a prior here, and in fact that reason is very important as it fuels most of the decision taken by the customer. 
 Every move by the customer and the growth teams in the marketing/sales funnel is cause-effect. So, prior knowledge is vital when it comes to converting a prospective lead into a customer. 
 
 So, the concept of  prior  is very important when it comes to customer analytics, which makes the concept of Bayesian networks very important to this domain. 
 
 Suggested Learning:  
 Bayesian Methods for Neural Networks 
 Bayesian networks in business analytics","Bayesian networks might outperform Neural Networks in small data setting. If the prior information is properly managed via the network structure, priors and other hyperparameters, it might have an edge over Neural Networks. Neural Networks, especially the ones with more layers, are very well known to be data hungry. Almost by definition lots of data is necessary to properly train them.","I've posted  this link on Reddit  and got a lot of feedback. Some have posted their answers here, others didn't. This answer should sum the reddit post up. (I made it community wiki, so that I don't get points for it) 
 
 Auto-Encoding Variational Bayes  is a combination of a Bayes Network and a neural network. The paper  Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference  seems to go in the same direction. 
 Dropout: A Simple Way to Prevent Neural Networks from
Overfitting  is an example where Bayesian neural networks outperform their dropout approach (see section 6.4 ""Comparison with Bayesian Neural Networks"") 
 Human-level concept learning through probabilistic program induction  is a paper ""on a Bayesian net network that does one-shot classification that way outperformed neural networks"" (according to trashacount12345 - I didn't check that by now). 
 Yann LeCun wrote a  Google+ post  in which he argues that neural networks and probabilisitc graphical models are not orthogonal concepts.","Bayesian networks are preferred for genome interpretation. See, for example,  this dissertation  discussing computational methods for genome interpretation.","I did a small example for this once. From that, I think Bayesian Networks are preferred if you want to capture a distribution but your input training set doesn't cover the distribution well. In such cases, even a neural network that generalised well would not be able to reconstruct the distribution.","All of them, given a sufficiently small dataset. Proof: Consider a dataset with 0 data points. A neural network can't be fit to this dataset or provide predictions, whereas a Bayesian network can, assuming you set proper priors on all parameters; so the Bayesian network will do better. For small but nonzero datasets, priors are still very helpful. 
 Given a sufficiently large dataset, a neural network will eventually perform at least as well as a Bayesian network, since it will converge to the correct answer (thanks to the universal approximation theorem). But Bayesian nonparametrics are also a thing, so picking the right Bayesian network  also  guarantees convergence to the right answer. Moreover, ""sufficiently large"" can be  really  big, requiring way more data than you could ever realistically get your hands on. Neural networks exist because they can be trained quickly on large datasets, not because they're better at learning from the same dataset--the reality is just the opposite. If it weren't for the internet giving us absolutely insane amounts of data for free, we really wouldn't be using neural nets at all.",54.62516807,69.42479765,64.17213643,67.35978751,73.06291242,74.53916196,60.98643349,61.81982448,69.64045073
9554,Open source data science projects to contribute,beginner,"The  Julia project  is one which I actively contribute to, including the advanced computing and XGBoost libraries. So, I can definitely vouch for it's maintenence and the quality of the community. 
 Some really good open source data science projects where even the beginners can contribute are: 
 
 Sklearn : Always developing at a rapid pace, the sklearn community is always open to new developers and contributors. 
 H2O : H2O is another fast growing data science projects, working on scalable machine learning and Deep Learning solutions.  
 Go : Open source data science road map and resources. Not really a technical project, but is very helpful for absolute beginners and aspiring analysts. 
 Pylearn2 : Another fast growing Machine Learning and Deep Learning project.  
 Vowpal Wabbit : The Vowpal Wabbit (VW) project is a fast out-of-core learning system sponsored by Microsoft Research and (previously) Yahoo! Research. 
 
 Here is a  Quora discussion  on such projects and some more which are not mentioned in this answer.  
 Here is a another  nice discussion  about open source Data Science and ML projects in Python.","There are plenty of them available. I do not know if I am allowed to do this (please let me know if it is wrong), but I develop one and it has already over 2 years on git hub (it actually started one years before github). 
The project is called rapaio, is on git hub  here  and recently I started to write a manual for it (some of my friends asked me about that). The manual can be found  here . 
 It fits your needs if you are willing to develop in Java 8, if you like to do yourself any tool and if you like to experiment. There are only two principles which I enforce. The first one is  write something only when you need it . That is because I strongly believe that only when you need a tool you also know what you really want from it in terms of output, performance, information. The second principle is  you depend only on jdk, if you need something you will write it . I can agree that I am old fashioned, but you can tailor any feature for your purpose in this way.  
 If I am not allowed to do that as an aswer, again, please let me know. Although, since it's an open source initiative, a  give something back to the people with no profit  type of project I see not reason why I could not do it.","If one likes cross-platform visual programming tools,  Orange  is an option. Having recently moved to Python 3, they haven't yet got all the widgets ported.
It's bringing the PyData stack (NumPy, SciPy, SciKit Learn, ...) to Python 3, PyQt, PyQtGraph, and it's GPL'd  on GitHub .","Check  this  project on github. It contains a comprehensive list of open source projects grouped by language, with some short descriptions. I think you can find there some of them which meet you needs.","ELKI  (also on  GitHub ) is data mining and data science open-source project. It is unique with respect to its modular architecture: you can combine algorithms, distance functions, and indexes for acceleration with very few limitations (of course, algorithms that do not use distances cannot be combined with distances). It is not the easiest code because of efficiency. For data mining, you need to be careful about memory - using  ArrayList<Integer>  is a no-go if you want scalability. 
 Because of the modular architecture, it is easy to contribute just small modules, like a single distance function or algorithm. 
 We keep a list of  data mining project ideas , roughly grouped by difficulty. Most projects are the implementation of some variant of an algorithm. ELKI aims at allowing comparative studies of algorithms, so we try to allow any combination, and cover also variants of algorithms. For example with k-means, we not only have Lloyds algorithm, but 10 variants of the general k-means theme. Over 220 articles have been (at least partially) reimplemented in ELKI. 
 By implementing everything in the same tool, we get much more comparable results. If you use R for benchmarking, you are usually comparing apples and oranges. k-means in R itself is actually an old Fortran program, and very fast. k-means in R but in the ""flexclust"" package is 100x slower, because it is written in real R code. So don't trust a benchmark in R... also, R modules tend to be incompatible, so you often can't use distance A from modules A with algorithm B from module B. in ELKI we try to share as much code as possible across implementations to reduce such artifacts (it will, of course, never be possible to have a 100% fair benchmark - there is always room for optimization), but also to allow combining modules easily. 
 You could start with something small such as the Hartigan&Wong k-means variant, and then continue into spherical k-means (which is meant for sparse data, where different performance optimizations may become necessary) and continue into adding better support for categorical data; or adding indexing functionality. 
 I'd also  love to see a better UI for ELKI , but that is a major effort.",,,,,81.31870084,54.74297105,50,62.21173536,56.24768182,,,,
9325,Python library that can compute the confusion matrix for multi-label classification,python,"Also take a look at  scikit-multilearn . It is a very good library that extends sklearn for multi-label learning. However, I'm not sure how the confusion matrix works for multi-label problems... 
 This guy  claims  he has solved it.","Although this question is old, I am writing this answer for new audience.
 scikit-learn now supports confusion matrix for multi-label classification. 
 https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html",Try  mlxtend .  Here 's an example of multi-class case.,There are many different parameters which can evaluate the performance of your method by comparing the real and predicted labels. I suggest  PyCM  module which can give a vast variety these parameters which are suitable for multi-class classification.,"Sklearn has a method for it using which you can compute confusion matrix for multi class. 
 from sklearn import cross_validation
confusion_matrix(original, Predicted)","Scikit-learn does support multi-label confusion matrix. See the links below for documentation and user guide: 
 http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html   
 http://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix","Look at  sed_eval  library. It is developed for evaluating event detection in audio which is a multi-label problem (as in each audio, multiple events exist). They have many evaluation options, which might fit to your needs. 
You can get the true-positive rate, ... and from there computing the confusion matrix is not that hard.",,,68.35047746,78.99067555,53.89882862,55.10104759,73.75324504,66.30665267,66.26746066,,
9302,The cross-entropy error function in neural networks,machine-learning,"One way to interpret cross-entropy is to see it as a (minus) log-likelihood for the data  $y_i'$ , under a model  $y_i$ . 
 Namely, suppose that you have some fixed model (a.k.a. ""hypothesis""), which predicts for  $n$  classes  $\{1,2,\dots, n\}$  their hypothetical occurrence probabilities  $y_1, y_2,\dots, y_n$ . Suppose that you now observe (in reality)  $k_1$  instances of class  $1$ ,  $k_2$  instances of class  $2$ ,  $k_n$  instances of class  $n$ , etc. According to your model the likelihood of this happening is:
 $$
P[data|model] := y_1^{k_1}y_2^{k_2}\dots y_n^{k_n}.
$$ 
Taking the logarithm and changing the sign:
 $$
-\log P[data|model] = -k_1\log y_1 -k_2\log y_2 - \dots -k_n\log y_n = -\sum_i k_i \log y_i
$$ 
If you now divide the right-hand sum by the number of observations  $N = k_1+k_2+\dots+k_n$ , and denote the empirical probabilities as  $y_i'=k_i/N$ , you'll get the cross-entropy:
 $$
-\frac{1}{N} \log P[data|model] = -\frac{1}{N}\sum_i k_i \log y_i = -\sum_i y_i'\log y_i =: H(y', y)
$$ 
 Furthermore, the log-likelihood of a dataset given a model can be interpreted as a measure of ""encoding length"" - the number of bits you expect to spend to encode this information if your encoding scheme would be based on your hypothesis.  
 This follows from the observation that an independent event with probability  $y_i$  requires at least  $-\log_2 y_i$  bits to encode it (assuming efficient coding), and consequently the expression 
 $$-\sum_i y_i'\log_2 y_i,$$ 
is literally the expected length of the encoding, where the encoding lengths for the events are computed using the ""hypothesized"" distribution, while the expectation is taken over the actual one. 
 Finally, instead of saying ""measure of expected encoding length"" I really like to use the informal term ""measure of surprise"". If you need a lot of bits to encode an expected event from a distribution, the distribution is ""really surprising"" for you. 
 With those intuitions in mind, the answers to your questions can be seen as follows: 
 
 Question 1 . Yes. It is a problem  whenever the corresponding  $y_i'$  is nonzero at the same time . It corresponds to the situation where your model believes that some class has zero probability of occurrence, and yet the class pops up in reality. As a result, the ""surprise"" of your model is infinitely great: your model did not account for that event and now needs infinitely many bits to encode it. That is why you get infinity as your cross-entropy.  
 To avoid this problem you need to make sure that your model does not make rash assumptions about something being impossible while it can happen. In reality, people tend to use sigmoid or ""softmax"" functions as their hypothesis models, which are conservative enough to leave at least some chance for every option. 
 If you use some other hypothesis model, it is up to you to  regularize  (aka ""smooth"") it so that it would not hypothesize zeros where it should not. 
 Question 2 . In this formula, one usually assumes  $y_i'$  to be either  $0$  or  $1$ , while  $y_i$  is the model's probability hypothesis for the corresponding input. If you look closely, you will see that it is simply a  $-\log P[data|model]$  for binary data, an equivalent of the second equation in this answer. 
 Hence, strictly speaking, although it is still a log-likelihood, this is not syntactically equivalent to cross-entropy. What some people mean when referring to such an expression as  cross-entropy  is that it is, in fact, a  sum  over binary cross-entropies for individual points in the dataset:
 $$
\sum_i H(y_i', y_i),
$$ 
where  $y_i'$  and  $y_i$  have to be interpreted as the corresponding binary distributions  $(y_i', 1-y_i')$  and  $(y_i, 1-y_i)$ .","The first logloss formula you are using is for multiclass log loss, where the $i$ subscript enumerates the different classes in an example. The formula  assumes  that a single $y_i'$ in each example is 1, and the rest are all 0. 
 That means the formula only captures error on the target class. It discards any notion of errors that you might consider ""false positive"" and does not care how predicted probabilities are distributed other than predicted probability of the true class. 
 Another assumption is that $\sum_i y_i = 1$ for the predictions of each example. A softmax layer does this automatically - if you use something different you will need to scale the outputs to meet that constraint. 
 Question 1 
 
 Isn't it a problem that the $y_i$ (in $log(y_i)$) could be 0? 
 
 Yes that can be a problem, but it is usually not a practical one. A randomly-initialised softmax layer is extremely unlikely to output an exact  0  in any class. But it is possible, so worth allowing for it. First, don't evaluate $log(y_i)$ for any $y_i'=0$, because the negative classes always contribute 0 to the error. Second, in practical code you can limit the value to something like  log( max( y_predict, 1e-15 ) )  for numerical stability - in many cases it is not required, but this is sensible defensive programming. 
 Question 2 
 
 I've learned that cross-entropy is defined as $H_{y'}(y) := - \sum_{i} ({y_i' \log(y_i) + (1-y_i') \log (1-y_i)})$ 
 
 This formulation is often used for a network with one output predicting two classes (usually positive class membership for 1 and negative for 0 output). In that case $i$ may only have one value - you can lose the sum over $i$. 
 If you modify such a network to have two opposing outputs and use softmax plus the first logloss definition, then you can see that in fact it  is the same error measurement  but folding the error metric for two classes into a single output. 
 If there is more than one class to predict membership of, and the classes are  not exclusive  i.e. an example could be any or all of the classes at the same time, then you will need to use this second formulation. For digit recognition that is not the case (a written digit should only have one ""true"" class)","Given  $y_{true}$ , you want to optimize your machine learning method to get the  $y_{predict}$  as close as possible to  $y_{true}$ . 
 First question: 
 Above answer has explained the background of your first formula, the cross entropy defined in information theory. 
 From a opinion other than information theory: 
 you can examine yourself that first formula does not have penalty on false-positiveness(truth is false but your model predict that it is right), while the second one has penalty on false-positiveness. Therefore, the choice of first formula or second, will affect your metrics(aka what statistic quantity you would like to use to evaluate your model). 
 In layman word: 
 If you want to accept almost all good people to be your friend but willing to accept some bad people become your friend, then use first formula for criterion. 
 If you want to punish yourself accepting some bad people to be your friend,but at the same time your good-people accepting rate might be lower than the first condition, then use second formula. 
 While, I guess most of us are critical and would like to choose the second one(so as many ML package assume what is cross entropy). 
 Second question: 
 Cross entropy per sample per class:  $$-y_{true}\log{(y_{predict})}$$ 
 Cross entropy for whole datasets whole classes:  $$\sum_i^n \sum_k^K -y_{true}^{(k)}\log{(y_{predict}^{(k)})}$$ 
 Thus, when there are only two classes (K = 2), you will have the second formula.","Those issues are handled by the tutorial's use of softmax.  
 For 1) you're correct that softmax guarantees a non-zero output because it exponentiates it's input. For activations that do not give this guarantee (like relu), it's simple to add a very small positive term to every output to avoid that problem. 
 As for 2), they aren't the same obviously, but I the softmax formulation they gave takes care of the the issue. If you didn't use softmax, this would cause you to learn huge bias terms that guess 1 for every class for any input. But since they normalize the softmax across all classes, the only way to maximize the output of the correct class is for it to be large relative to the incorrect classes.","Isn't it a problem that  $y_i$  (in  $\log(y_i)$ ) could be 0? 
 
 Yes it is, since  $\log(0)$  is undefined, but this problem is avoided using  $\log(y_i + \epsilon)$  in practice. 
 
 What is correct? 
  (a)  $H_{y'} (y) := - \sum_{i} y_{i}' \log (y_i)$  or 
  (b)  $H_{y'}(y) := - \sum_{i} ({y_i' \log(y_i) + (1-y_i') \log(1-y_i)})$ ? 
 
 (a) is correct for multi-class prediction (it is actually a double summation), (b) is the same as (a) for two-class prediction. Both are cross-entropy. 
 Example: 
 Suppose each training data  $x_i$  has label  $c_i' \in \{0, 1\}$ , and model predicts  $c_i \in [0, 1]$ . 
 For 5 data points, true label  $c_i'$  and model prediction  $c_i$  are:   
 $(c_i', c_i)=\{(0, 0.1), (0, 0.4), (0, 0.8), (1, 0.8), (1, 0.2)\}$      (1),    
 Define vectors  $y_i'$  and  $y_i$  as    
 
 $y_{ik}':=1$  if  $c_i'=k$ , and  $:=0$  otherwise,   
 $y_{ik}:=p(k|x_i)$  is the probability of  $x_i$  belonging to class  $k$ , which is estimated by model.  
 
 Example (1) in  $(y_i', y_i)$  notation turns into:    
 $(y_i', y_i)=\{([1, 0], [0.9, 0.1]),$   $([1, 0], [0.6, 0.4]),$   $([1, 0], [0.2, 0.8]),$   $([0, 1], [0.2, 0.8]),$   $([0, 1], [0.8, 0.2])\}$ ,    
 Both (a) and (b) are calculated as:  
 $H_{y'}(y)=-1/5([log(0.9)+log(0.6) + log(0.2)]_{c_i=0} + [log(0.8) + log(0.2)]_{c_i=1}) = 0.352$ 
 Derivation: 
 Suppose there is multiple classes  $1$  to  $K$ . 
For training point  $(x_i, c_i')$ ,  $c_i' = k$  is equivalent to  $y_i'=[0,..,1,0,..]$  which is 1 in  $k^{th}$  position and 0 elsewhere. When  $y_{ik}'=1$ , we want model's output  $y_{ik}=p(k|x_i)$  to be close to 1. Therefore, loss of  $(x_i, k)$  can be defined as  $-log(y_{ik})$ , which gives  $y_{ik} \rightarrow 1 \Rightarrow -log(y_{ik}) \rightarrow 0$ . Loss over all classes can be combined as: 
 $L(y_i', y_i) = -\sum_{k=1}^{K}y_{ik}'log(y_{ik})$ . 
 When  $y_{ik}' = 1$ , loss of all other classes  $k' \neq k$  is disabled as  $0log(y_{ik'})=0$ , so for example when true label is  $y_{im}'=1$ , loss would be: 
 $L(y_i', y_i)=-log(y_{im})$ . 
 Final formula over all training points is: 
 $H_{y'}(y)=-\sum_{(x_i, y_i')}\sum_{k=1}^{K}y_{ik}'log(y_{ik})$ . 
 For binary classification, we have  $y_{i0}' = 1 - y_{i1}'$  (true labels) and  $y_{i0} = 1 - y_{i1}$  (model predictions), therefore (a) can be rewritten as: 
 $\begin{align*}
H_{y'}(y)&=-\sum_{(x_i, y_i')}y_{i1}'log(y_{i1})+y_{i0}'log(y_{i0})\\
&=-\sum_{(x_i, y_i')}y_{i1}'log(y_{i1})+(1-y_{i1}')log(1-y_{i1})
\end{align*}$ 
 which is the same as (b). 
 Cross-entropy (a) over classes (one summation) 
 Cross-entropy (a) over classes is: 
 $H_{y'}(y)=-\sum_{k=1}^{K}y_{k}'log(y_{k})$ , 
 This version cannot be used for the classification task. Lets reuse the data from the previous example: 
 $(c_i', c_i)=\{(0, 0.1), (0, 0.4), (0, 0.8), (1, 0.8), (1, 0.2)\}$   
 Empirical class probabilities are:  $y'_0 = 3/5 = 0.6$ , and  $y'_1 = 0.4$ , 
 Class probabilities estimated by model are:  $y_0 = 3/5 = 0.6$ , and  $y_1 = 0.4$ 
 (a) is calculated as:  $-y'_0logy_0 - y'_1logy_1 = - 0.6log(0.6) -0.4log(0.4) = 0.292$ . 
 Two data points  $(0, 0.8)$  and  $(1, 0.2)$  are miss-classified but  $y'_0$  and  $y'_1$  are estimated correctly! 
 If all 5 points where classified correctly as: 
 $(c_i', c_i)=\{(0, 0.1), (0, 0.4), (0, \color{blue}{0.2}), (1, 0.8), (1, \color{blue}{0.8})\}$  , 
 (a) still remains the same, since  $y'_0$  is again estimated as  $y_0=3/5$ .","Question 2
I've learned that cross-entropy is defined as... 
 
 These two means either two things. 
 
 The person who said you that has no idea about he/she is talking about 
 You incorrectly learned it. 
 
 In Mathematics Kullback-Leiber divergence(KL), Cross-Entropy(CE), Entropy(H) always mean only one thing, but the term Entropy unfortunately can vary from the scientific community. In any case, the good book on the subject ""Information Theory is the book ""Elements of Information Theory"" by Thomas M. Cover, Joy A. Thomas."" from 1991. 
 But if you don't want to understand mathematical properties (they maybe are not used by you) then you can check the definition from Wikipedia: 
 
 The function is defined in two discrete (or continuous distributions) 
 In the case of discrete distributions the summation happens not for all possible  $x=x(w_i)$  values of random variables but only for which the two p.m.f. has a positive value of probabilities. (In math languages the summation happens over the intersection of the support of two functions) 
 
 How to make this computation numerical stable for such a situation when  $\exists i:p_i\sim 0, q_i \sim0$  it's another question. And one way consider how  $p,q$  are generated a-priori. For example in case of using some form of symmetric logistic transformation (name from STATs) or softmax (one of the name from Machine Learning) then it's nice to take into account log-sum-exp trick.
References:   https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss 
 https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/",,,,59.65911164,55.63594522,60.4945688,50,54.51465734,60.21873375,,,
9262,Calculating KL Divergence in Python,python,"First of all,  sklearn.metrics.mutual_info_score  implements  mutual information  for evaluating clustering results, not  pure  Kullback-Leibler divergence! 
 
 This is equal to the Kullback-Leibler divergence of the joint distribution with the product distribution of the marginals. 
 
 KL divergence (and any other such measure) expects the input data to  have a sum of 1 . Otherwise, they are  not  proper  probability distributions . If your data does not have a sum of 1, most likely it is usually not proper to use KL divergence! (In some cases, it may be admissible to have a sum of less than 1, e.g. in the case of missing data.) 
 Also note that it is common to use base 2 logarithms. This only yields a constant scaling factor in difference, but base 2 logarithms are easier to interpret and have a more intuitive scale (0 to 1 instead of 0 to log2=0.69314..., measuring the information in bits instead of nats). 
 > sklearn.metrics.mutual_info_score([0,1],[1,0])
0.69314718055994529
 
 as we can clearly see, the MI result of sklearn is scaled using natural logarithms instead of log2. This is an unfortunate choice, as explained above. 
 Kullback-Leibler divergence is fragile, unfortunately. On above example it is not well-defined:  KL([0,1],[1,0])  causes a division by zero, and tends to infinity. It is also  asymmetric .","Scipy's  entropy function  will calculate KL divergence if feed two vectors p and q, each representing a probability distribution. If the two vectors aren't pdfs, it will normalize then first. 
 Mutual information is  related to, but not the same  as KL Divergence. 
 ""This weighted mutual information is a form of weighted KL-Divergence, which is known to take negative values for some inputs, and there are examples where the weighted mutual information also takes negative values""","I'm not sure with the scikit-learn implementation, but here is a quick implementation of the KL divergence in Python: 
 import numpy as np

def KL(a, b):
    a = np.asarray(a, dtype=np.float)
    b = np.asarray(b, dtype=np.float)

    return np.sum(np.where(a != 0, a * np.log(a / b), 0))


values1 = [1.346112,1.337432,1.246655]
values2 = [1.033836,1.082015,1.117323]

print KL(values1, values2)
 
 Output:   0.775279624079 
 There might be  conflict of implementation  in some libraries, so make sure you read their docs before using.","This trick avoids conditional code and may therefore provide better performance. 
 import numpy as np

def KL(P,Q):
"""""" Epsilon is used here to avoid conditional code for
checking that neither P nor Q is equal to 0. """"""
     epsilon = 0.00001

     # You may want to instead make copies to avoid changing the np arrays.
     P = P+epsilon
     Q = Q+epsilon

     divergence = np.sum(P*np.log(P/Q))
     return divergence

# Should be normalized though
values1 = np.asarray([1.346112,1.337432,1.246655])
values2 = np.asarray([1.033836,1.082015,1.117323])

# Note slight difference in the final result compared to Dawny33
print KL(values1, values2) # 0.775278939433","Kullback-Leibler divergence is basically the sum of the  relative entropy  of two probabilities: 
 vec = scipy.special.rel_entr(p, q)    
kl_div = np.sum(vec)
 
 As mentioned before, just make sure p and q are probability distributions (sum up to 1). You can always normalize them before: 
 p /= np.sum(p)
 
 Relative entropy is defined as p*log(p/q), so where q==0, the result is inf.
You can mask those values using: 
 vec = np.ma.masked_invalid(vec).compressed()","Consider the three following samples from a distribution(s). 
 values1 = np.asarray([1.3,1.3,1.2])
values2 = np.asarray([1.0,1.1,1.1])
values3 = np.array([1.8,0.7,1.7])
 
 Clearly, values1 and values2 are closer, so we expect the measure of  surprise  or entropy, to be lower when compared to values3.  
 from scipy.stats import entropy
print(""\nIndividual Entropy\n"")
print(entropy(values1))
print(entropy(values2))
print(entropy(values3))

print(""\nPairwise Kullback Leibler divergence\n"")
print(entropy(values1, qk=values2))
print(entropy(values1, qk=values3))
print(entropy(values2, qk=values3))
 
 We see the following output: 
 Individual Entropy

1.097913446793334
1.0976250611902076
1.0278436769863724 #<--- this one had the lowest, but doesn't mean much.

Pairwise Kullback Leibler divergence

0.002533297351606588
0.09053972625203921 #<-- makes sense
0.09397968199352116 #<-- makes sense
 
 We see this makes sense because the values between values1 and values3 and values 2 and values 3 are simply more drastic in change than values1 to values 2. This is my validation to understanding KL-D and the packages that can be leveraged for it.",,,,62.62804708,63.70988879,56.72220425,58.05786156,54.37781907,52.89765655,,,
9228,Decision tree vs. KNN,machine-learning,"They serve different purposes.  
 KNN is unsupervised, Decision Tree (DT) supervised.
( KNN is supervised learning while K-means is unsupervised, I think this answer causes some confusion. )
KNN is used for clustering, DT for classification. ( Both are used for classification. ) 
 KNN determines neighborhoods, so there must be a distance metric. This implies that all the features must be numeric. Distance metrics may be affected by varying scales between attributes and also high-dimensional space. 
 DT, on the other hand, predicts a class for a given input vector. The attributes may be numeric or nominal. 
 So, if you want to find similar examples you could use KNN. If you want to classify examples you could use DT.","Classifiers like  Decision Tree, Bayesian, Back-propagation, Support Vector Machine  come under the category of  ""Eager Learners"" , because they first build a classification model on the  training dataset  before being able to actually classify an [unseen] observation from  test dataset . The learned model is now ""eager"" (read hungry) to classify previously unseen observations, hence the name. 
 
 The KNN-based classifier, however, does not build any classification model. It directly learns from the training instances (observations). It starts processing data only after it is given a test observation to classify. Thus, KNN comes under the category of  ""Lazy Learner""  approaches. 
 Based on the above foundational differences, we can conclude the following:- 
 
 Since KNN performs on-the-spot learning, it requires frequent database lookups, hence, can be computationally expensive. Decision Tree Classifier does not require such lookups as it has in-memory classification model ready. 
 Since KNN performs instance-based learning, a well-tuned K can model complex decision spaces having arbitrarily complicated decision boundaries, which are not easily modeled by other ""eager"" learners like Decision Trees. 
 ""Eager"" learners work in batches, modeling one group of training observations at a time. So they are not fit for incremental learning. But KNN naturally supports incremental learning (data streams) since it is an instance-based learner. 
 Further, KNN classifier gives test error rates closer to that of Bayesian classier (the gold standard). As quoted in  ISLR : 
 
 
 The Bayes error rate is analogous to the irreducible error","From Sebastian Raschka's  Python Machine Learning : 
 
 The main advantage of such a memory-based approach [the KNN] is that
  the classifier immediately adapts as we collect new training data.
  However, the downside is that the computational complexity for
  classifying new samples grows linearly with the number of samples in
  the training dataset in the worst-case scenario—unless the dataset has
  very few dimensions (features) and the algorithm has been implemented
  using efficient data structures such as KD-trees. J. H. Friedman, J.
  L. Bentley, and R. A. Finkel. An algorithm for finding best matches in
  logarithmic expected time. ACM Transactions on Mathematical Software
  (TOMS), 3(3):209–226, 1977. Furthermore, we can't discard training
  samples since no training step is involved. Thus, storage space can
  become a challenge if we are working with large datasets. 
 
 The decision tree, however, can rapidly classify new examples. You're just running a series of boolean comparisons.","I would add that decision trees can be used for both classification and regression tasks.  DT on the other hand predicts a class  in the accepted answer would be more specific by describing  Classification trees  which is technically a subtype of the generic DT concept.
One reference(ignoring the bottom layers that discuss specific implementations): 
 
From  here","k-NN 
 Decision Trees 
 
 
 
 
 There is no real 'training' time since no computation is performed while training. All that needs to be done is storing the training data. 
 Training involves iteratively building a tree (considering the ID-3 algorithm) so considerably more time than k-NN. 
 
 
 High testing time since the distance needs to be computed between the test point and every training point. 
 Low testing time since it's a traversal down a tree. 
 
 
 The user needs to choose a distance metric in order to use a k-NN for testing. 
 No need to choose a distance metric since the splits will occur based on values inherent to each feature. 
 
 
 Updating an existing model with new data simply means adding that point to the existing dataset 
 An existing tree cannot be updated - an entire new tree needs to be created 
 
 
 Need to store all the training data in order to classify a new incoming point 
 No need to store the training data - only store the tree model 
 
 
 
 
 Some additional notes: 
 Both, k-NN and decision trees are  supervised  algorithms (unlike mentioned in one of the answers). They both require labelled training data in order to label the test data. 
 k-D trees are a neat way of optimizing the k-NN algorithm. They reject large sections of the data so that classification doesn't take too long. 
 For more info on decision trees, refer to these excellent lecture notes:  http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote17.html",,,,,69.94188424,69.03128303,58.60202392,60.7759093,62.7424052,,,,
9175,How do subsequent convolution layers work?,neural-network,"I am not sure about the alternatives described above, but the commonly used methodology is: 
 Before the application of the non-linearity, each filter output depends linearly on all of the feature maps before within the patch, so you end up with $k_2$ filters after the second layers. The overall number of parameters is $3 \dot{} 3\dot{}k_1 + k_1\dot{} 5 \dot{} 5 \dot{} k_2$. 
 Bonus 1: Pooling is done per feature map, separately. 
 Bonus 2: The order of ""sliding"" does not matter. In fact, each output is computed based on the previous layer, so the output filter responses do not depend on each other. They can be computed in parallel.","I have just struggled with this same question for a few hours.  Thought I'd share the insite that helped me understand it. 
 The answer is that the filters for the second convolutional layer do not have the same dimensionality as the filters for the first layer.  In general,  the filter has to have the same number of dimensions as its inputs .  So in the first conv layer, the input has 2 dimensions (because it is an image).  Thus the filters also have two dimensions.  If there are 20 filters in the first conv layer, then the output of the first conv layer is a stack of 20 2D feature maps. So the output of the first conv layer is  3 dimensional, where the size of the third dimension is equal to the number of filters in the first layer. 
 Now this 3D stack forms the input to the second conv layer.  Since the input to the 2nd layer is 3D, the filters also have to be 3D.  Make the size of the second layer's filters in the third dimension equal to the number of feature maps that were the outputs of the first layer. 
 Now you just convolve over the first 2 dimensions; rows and columns. Thus the convolution of each 2nd layer filter with the stack of feature maps (output of the first layer) yields a single feature map.   
 The size of the third dimension of the output of the second layer is therefore equal to the number of filters in the second layer.","Check this  lecture  and this  visualization 
 Usually it is used type 2.1 convolution. In the input you have  $NxMx1$  image, then after first convolution you will obtain  $N_1xM_1xk_1$ , so your image after first convolution will have  $k_1$  channels. The new dimension  $N_1$  and  $M_1$  will depend on your stride  $S$  and padding  $P: N_1 = (N - 3 + 2P)/S + 1$ , you compute  $M_1$  in analogy. For the first conv layer you will have  $3x3xk_1 + k_1$  weights. There is added  $k_1$  for biases in nonlinear function. 
 In the second layer you have as an input image with size  $N_1xM_1xk_1$ , where  $k_1$  is new number of channels. And after second convolution you obtain  $N_2xM_2xk_2$  image (array). You have  $5x5xk_2xk_1+k_2$  parameters in the second layer. 
 For  $1x1$  convolution with  $k_3$  filters and input  $NxMxC$  ( $C$  is number of input channels) you will obtain new image (array)  $NxMxk_3$ , so  $1x1$  make sense. They were introduced in this  paper 
 Bonus 1: pooling is applied per feature map. 
 For details please see  slides  for CNN course on Stanford - you have there nice visualisation how convolution is summed from several input channels.","The first layer consists of $k_1$ kernels with size $3 \cdot 3 \cdot 1$ to give $k_1$ feature maps which are stacked depth-wise. 
 The second layer consists of $k_2$ kernels with size $5 \cdot 5 \cdot k_1$ to give $k_2$ feature maps which are stacked depth-wise. 
 That is, the kernels in a convolutional layer span the depth of the output of the previous layer. 
 A layer with $1 \times 1$ convolutional layer actually has $k_n$ kernels of size $1 \cdot 1 \cdot k_{n-1}$. 
 Speculation: 
 Bonus question 2 is not something I'm familiar with, but I will guess the depth parameter in the convolution becomes an extra dimension. 
 e.g. If the output of a layer is size $m \cdot n \cdot k_{n}$, a 3D convolution with padding would result in an output of size $m \cdot n \cdot k_{n+1} \cdot k_{n}$","About the type of convolutions 
 
 Suppose the input layers has the  $k_{input}$  channels, than the number of parameters to be learned by neural network is:
 $$
3 \cdot 3 \cdot k_{input} \cdot k_1 + 5 \cdot 5 \cdot k_1 \cdot k_2 
$$ 
Because each of the input channels of the image is mapped to one of the output channels. There is a separate filter to each pair  $(c_{input}, c_{output})$ , which are indexing the channels of the image. One can think of the convolution filter as a tensor of shape  $(c_{input}, c_{output}, s_1 \ldots s_d)$ , where  $d$  is the spatial dimensionality of the data. 
 
 Usefulness of the 1x1 convolutions 
 
 On the one hand, one can put nonlinearity after, such that the filter + activation performs a nonlinear operation, changing the output in some complicated way. Another point, which makes them useful and is the cornerstone in the MobileNet  https://arxiv.org/abs/1704.04861 , that number of operations in ordinary convolution scales multiplicatively with the increase of the filters size and number of channels:
 $$
c_{input} \cdot c_{output} \cdot n_1 \cdot n_2 
$$ 
For 2D convolution. Setting the  $n_1 = n_2$  one works with not so much parameters , combining them  with  depthwise  convolutions. Via  $1 \times 1$  convolutions one can reduce the number of feature from  $c_1$  to  $c_2 < c_1$  in some educated way, where the network itself learns, hopefully, the optimal way to perform the dimensionality reduction. 
 
 Bonus questions 
 
 
 Pooling is applied feature wise, for each channel one obtains a downsampled image contructed via some aggregation function ( max ,  average ) of the multiple pixels, belonging to the same channel - no interaction between different channels (R, G, B), for instantce. 
 
 You do not slide over feature map, convolution kernel comprises all feature maps from the  $c_{in}$  to  $c_{out}$ .",,,,,52.34381019,61.03504041,61.2638677,60.74472762,57.50365882,,,,
9085,Reducing the effect of down voters with rating system,statistics,"You should look into other estimators of location. 
 What you want is a  robust  estimator, with a  high break-down point . 
 The extreme approach would be the median. 
 But you may get more numerically interesting results with a  trimmed mean . 
 You define a threshold, say 2%. Then you remove the top 2% of votes, and the bottom 2% of votes, and take the mean only of the remaining entries. An app with 98% 5 stars will still get a 5.0 
 But to prevent manipulation, I would look into other signals. Such as clustered votes from a single region, for example.","I like @Anony-Mousse's answer. Using robust estimators is good. 
 I want to add a different direction to cope with the problem.
It seems that there are some ""malicious"" users casting these down votes so you might want to identify them. 
 Create a dataset of the users and use ""casted  unjustified  down vote on leading item"" as the label. You can use ""casted down vote on leading item"" as the default value and then manually modify them and make the rule more delicate like ""casted more than twice  down vote on leading item after the  item reached the top charts""
I guess that features like number of low votes, number of low votes to leading items, etc will be useful. 
 Now you are in a supervised learning framework. Once you identify malicious users, ignore their votes and avoid the manipulations.","To robustify your estimator, you might model your ratings as a Gaussian mixture model (GMM) that is a mixture of two Gaussian rvs: 1) true ratings, 2) junk rating that are equal to one.  Scikit-learn already has a canned GMM classifier:  http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_classifier.html#example-mixture-plot-gmm-classifier-py 
 Digging in a little more, a simple approach would be to let scikit-learn partition your ratings into two gaussians.  If one of the partitions ends up with a mean near one, then we can throw out those ratings.  Or, more elegantly, we can take the mean of the other, non-near-one Gaussian, as the true rating mean. 
 Here is a bit of code for a ipython notebook that does this: 
 from sklearn.mixture import GMM
import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt
import collections

def make_ratings(mean,std,rating_cnt):
    rating_sample = np.random.randn(rating_cnt)*std + mean
    return np.clip(rating_sample,1,5).astype(int)

def make_collection(true_mean,true_std,true_cnt,junk_count):
    true_ratings = make_ratings(true_mean,true_std,true_cnt)
    junk_ratings = make_ratings(1,0,junk_count)
    return np.hstack([true_ratings,junk_ratings])[:,np.newaxis]

def robust_mean(X, th = 2.5, agg_th=2.5, default_agg=np.mean):
    classifier = GMM(n_components=2)
    classifier.fit(X)
    if np.min(classifier.means_) > th or default_agg(X)<agg_th:
        return default_agg(X)
    else:
        return np.max(classifier.means_)

r_mean = 4.2
X = make_collection(r_mean,2,40,10)
plt.hist(X,5)
classifier = GMM(n_components=2)
classifier.fit(X)
plt.show()
print ""vars ="",classifier.covars_.flatten()
print ""means = "",classifier.means_.flatten()
print ""mean = "",np.mean(X)
print ""median = "",np.median(X)
print ""robust mean = "", robust_mean(X)
print ""true mean = "", r_mean
print ""prob(rating=1|class) = "",classifier.predict_proba(1).flatten()
print ""prob(rating=true_mean|class) = "",classifier.predict_proba(r_mean).flatten()
print ""prediction: "", classifier.predict(X)
 
 The output for one run looks like: 
 vars = [ 0.22386589  0.56931527]
means =  [ 1.32310978  4.00603523]
mean =  2.9
median =  3.0
robust mean =  4.00603523034
true mean =  4.2
prob(rating=1|class) =  [  9.99596493e-01   4.03507425e-04]
prob(rating=true_mean|class) =  [  1.08366762e-08   9.99999989e-01]
prediction:  [1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0
 1 1 1 0 0 0 0 0 0 0 0 0 0]
 
 We can simulate how will this works with a few monte carlo trials: 
 true_means = np.arange(1.5,4.5,.2)
true_ratings = 40
junk_ratings = 10
true_std = 1
m_out = []
m_in = []
m_reg = []
runs = 40
for m in true_means:
    Xs = [make_collection(m,true_std,true_ratings,junk_ratings) for x in range(runs)]
    m_in.append([[m]*runs])
    m_out.append([[robust_mean(X, th = 2.5, agg_th=2,default_agg=np.mean) for X in Xs]])
    m_reg.append([[np.mean(X) for X in Xs]])

m_in = np.array(m_in).T[:,0,:]
m_out = np.array(m_out).T[:,0,:]
m_reg = np.array(m_reg).T[:,0,:]

plt.plot(m_in,m_out,'b.',alpha=.25)
plt.plot(m_in,m_reg,'r.',alpha=.25)
plt.plot(np.arange(0,5,.1),np.arange(0,5,.1),'k.')
plt.xlim([0,5])
plt.ylim([0,5])
plt.xlabel('true mean')
plt.ylabel('predicted mean')
plt.title(""true_ratings="" + str(true_ratings)
          + ""; junk_ratings="" + str(junk_ratings)
         + ""; std=""+str(true_std))
 
 The output is pasted below.  The red is the mean rating and the blue is the proposed rating.  You can tweak the parameters to get slightly different behaviors.","Record all votes   
 Ratio number of 1 votes when on the front page compared to not    
 Only apply a fraction of number 1 votes while on the first page 
Basically remove the page 1 bias based on the page 1 bias as a whole   
 1 vote applied = 1 votes item when on first page * (1 votes second page total / 1 votes first page total)","While it is technically probably easiest to implement one of the above solutions, I think you should also consider disincentivising the voters to downvote. For example, if the downvotes come from a minority of users who are clearly abusing the system, their repeated downvotes should count (negatively) toward their reputation - like this site.",,,,,50,50,59.98238683,50,52.55487077,,,,
9074,"Is there a difference between ""classification"" and ""labeling""?",classification,"Totally disagree with @Derek Janni.  Be careful about notation however you should not get lost in terminology. Those papers you mentioned used the term ""labeling"" literally but in Machine Learning/Data Mining community,  labeling is the process of preparing data for supervised learning (classification)! It has nothing to do with the ML task! 
 Those papers used the term to show that after supervised learning they can recognize different labels of different objects so they used the term labeling but you probably can not find in any literature that they use these two terms as synonyms.","After having read a lot more papers and having talked to many people about machine learning topics, this is how I would define the words: 
 A  class  as an abstract concept which exists. Each class has properties and can have a lot of different labels. For example, the class  cat  has the properties ""feet"" (with the value 4), the property ""Genus"" with the value ""Felis"". There are many way members of the class can look like. Also many labels: cat, Katze, Felis silvestris, 🐱, 🐈. 
 A  label  is just a sticker you put on the concept. A name. We need a word to be able to talk about the concept. 
 I use  labeling  for the manual process of defining which parts of the dataset belong to which class. And I use  classification  for the process of the automatic classifier deciding which part of the data belongs to which class. So typically, labeling is done by a human and proceeds classification which is done by the machine.","The way I view it: 'Classification' (in the context of machine learning) is  a type of problem  in which you assign a 'label' to an object. Formally, 'Classification' is a type of problem whereas labeling is a function from an object to a set of labels (maybe infinite). 
 Much the same way Regression is a type of problem where you, again, assign a label to an object only this time the label is some real number. 
 Both in Classification and in Regression you are attempting to find the 'best' labeling function with respect to some metric/loss function.","Short answer:  
 No, there is no difference between labelling and classification.  
 Class - a set or category of things having some property or attribute in common and differentiated from others by kind, type or quality. See 'category'. 
 Label - word or phrase indicating that what follows belongs in a particular category or class. 
 To classify something is to label it, they are the necessarily the same thing. The term labelling probably evolved because ""label"" allows you to avoid saying ""class"" which has other connotations in Computer Science.  
 Label is much simpler, and in all cases, classification is just the act of putting labels on objects (or learning to correctly do so). 
 The discrepancy you're seeing in the use of labelling/classification comes from the simple fact that a title like: 
 ""Semantic Classification: Classification of 3D Entities Based on Spatial Feature Descriptors""
or
""Knowledge-based classification and tissue classification of MR images of human brain"" 
 Sound really awkward. 
 Like most academic paper titles, these are just overly complex descriptions of what is in the paper that explain exactly what is going on without sounding redundant. 
 TL;DR - Don't get hung up on terminology!","Labels come up in conjunction with classification when the object does not belong to a single class but to a broader set, hence the term ""multilabel learning"" or ""multilabel classification"". Since they refer to discrete classes, they can be used synonymously, but I would recommend using the traditional terminology ( classification  when a single class is to be assigned) to avoid confusion.",A picture my be worth a thousand words:,,,,64.73585535,65.38909734,71.2898856,78.56170163,63.24085796,50,,,
9038,Purpose of visualizing high dimensional data?,machine-learning,"I take Natural Language Processing as an example because that's the field that I have more experience in so I encourage others to share their insights in other fields like in Computer Vision, Biostatistics, time series, etc. I'm sure in those fields there are similar examples. 
 I agree that sometimes model visualizations can be meaningless but I think the main purpose of visualizations of this kind are to help us check if the model actually relates to human intuition or some other (non-computational) model. Additionally, Exploratory Data Analysis can be performed on the data. 
 Let's assume we have a word embedding model built from Wikipedia's corpus using  Gensim 
 model = gensim.models.Word2Vec(sentences, min_count=2)
 
 We would then have a 100 dimension vector for each word represented in that corpus that's present at least twice. So if we wanted to visualize these words we would have to reduce them to 2 or 3 dimensions using the t-sne algorithm. Here is where very interesting characteristics arise. 
 Take the example: 
 vector(""king"") + vector(""man"") - vector(""woman"") = vector(""queen"") 
 
 Here each direction encode certain semantic features. The same can be done in 3d 
 
 (source:  tensorflow.org ) 
 See how in this example past tense is located in a certain position respective to its participle. The same for gender. Same with countries and capitals. 
 In the word embedding world, older and more naive models, didn't have this property. 
 See this Stanford lecture for more details.
 Simple Word Vector representations: word2vec, GloVe 
 They only were limited to clustering similar words together without regard for semantics (gender or verb tense weren't encoded as directions). Unsurprisingly models which have a semantic encoding as directions in lower dimensions are more accurate. And more importantly, they can be used to explore each data point in a more appropriate way. 
 In this particular case, I don't think t-SNE is used to aid classification per se, it's more like a sanity check for your model and sometimes to find insight in the particular corpus you are using. As for the problem of the vectors not being in original feature space anymore. Richard Socher explains in the lecture (link above) that low dimensional vectors share statistical distributions with its own larger representation as well as other statistical properties which make plausible visually analyse in lower dimensions embedding vectors. 
 Additional resources & Image Sources: 
 
 A Word is Worth a Thousand Vectors 
 
 Motivation Why Learn Word Embeddings","First of all your explanation about the methods are right. The point is that Embedding algorithms are not to only visualize but basically reducing the dimentionality to cope with two main problems in Statistical Data Analysis, namely  Curse of Dimentionaliy  and  Low-Sample Size Problem  so that they are not supposed to depict physically understood features and they are not only  meaningful  but also necessary for data analysis! 
 Actually the visualization is almost the last usage of embedding methods. Projecting high-dimensional data into a lower-dimension space helps to preserve the actual pair-wise distances (mainly Euclidean one) which get distorted in the high dimensions or capturing the most information embedded in the variance of different features.","Richard Hamming is attributed with the sentence: ""The purpose of computing is insight, not numbers.""  In this 1973  academic paper  (see discussion in  What is the famous data set that looks totally different but has similar summary stats? ), Francis Anscombe argues that ""graphs are essential to good statistical analysis."" Anscombe's quartet is a long time favorite: same stats and regression, low dimension, yet very different behavior, regarding noise, outliers, dependancy. The projection of data in 11 dimensions onto two dimensions shown below is quite misleading: one has correlation and  dispersion, the second (bottom down) has  exact match, except one outlier. The third has clear relationship, but not linear. The fourth shows the variables are potentially not related, except for a threshold. 
 
 In the book  Multivariate Analysis for the Biobehavioral and Social Sciences  by Bruce L. Brown  et al. , we can find: 
 
 In his 1990 work ""Drawing Things Together,"" Latour claims that the
  mindset of hard scientists is one of intense “obsession” with graphism 
 
 Whether limited to 3D space, up to  six dimension plots  (space, color, shape and time), or  even imagining the tenth dimension , humans have limited sights. Relationships between observable phenomena: not. 
 Additionally, the curse of dimensions is assorted with even low dimension paradoxes, to give a few:  
 
 Why is the curse of dimensionality also called the empty space phenomenon? 
 Curse of dimensionality 
 Why is Euclidean distance not a good metric in high dimensions? 
 Paradox about high-dimensional spheres! 
 The n-ball game 
 
 Even if all norms are equivalent in finite dimensions, relationships between variables might be misleading. This is one reason for preserving distances from one space to another. Such concepts are at of the heart of lower dimension embeddings for signals (such as  compressive sensing  and the  Johnson-Lindenstauss lemma  concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space) or features ( scattering transforms  for classifications). 
 So visualization is another help in getting insights in the data, and it goes hand in hand with calculations, including dimension reduction. 
 Last example: put touching   $n$-spheres in an $n$-cube (the bubble inside the box, taken from  Do good mathematicians visualize everything (even algebra)? ): 
 
 In two dimensions, the  center blue  ball is small. In 3D too. But very quickly, the center ball grows and its radius exceeds that of the cube. This insight is vital n clustering, for instance.","Based on the statements and the discussions, I think there is an important point to distinct. A transformation to a lower dimensional space may  reduce  the information, which is something different from making the information  meaningless . Let me use a following analogy: 
 Observing (2D) pictures of our world (3D) is a usual practice. A visualization method provides only different “glasses” to see a high dimensional space. 
 A good thing to “trust” a visualization method is to understand the internals. My favourite example is the  MDS   . It is easy possible to implement this method at your own using some optimization tool (e.g. R  optim ). So you can  see  how the method words, you may  measure the error  of the result etc. 
 At the end you get a picture preserving the similarity of the original data with some degree of precision. Not more, but not less.","Sometimes, it is meaningful to visualize high dimensional data since it may tell us physics. 
 There is at least one example in astrophysics where you project your data down to principal components generated by PCA and those principal components correspond to much physical insight about the galaxies. For detail, see the  paper . 
 Here is the basic idea. The authors apply PCA to many spectra (e.g., 10,000) from a telescope. Each spectrum has ~1000 attributes. Since this data set has large dimensions, it's difficult to visualize it. However, the first 4 components from PCA reveal much physics about the spectra (see sections 4.1-4.4 in the paper above).","Taking a slightly different approach than the other great answers here, the ""pretty picture"" is worth a thousand words. Ultimately, you will need to convey your findings to someone who is not as statistically literate, or who simply does not have the time, interest, or whatever, to grasp the full situation. That doesn't mean we cannot help the person to understand, at least a general concept or a piece of the reality. This is what books like Freakonomics do - there's little to no math, no data sets, and yet the findings are still presented. 
 From the arts, look at  Marshal Ney at Retreat in Russia . This massive oversimplification of the Napoleonic wars nevertheless conveys great meaning and allows people with even the most ignorant knowledge of the war to understand the brutality, the climate, the landscape, the death, and decorum that permeated the invasion of Russia. 
 Ultimately the charts are simply communication, and for better or worse, human communication is often times focused on conflation, simplification, and brevity.","Excellent question. In chapter 4 of ""Illuminating the Path, The Research and Development Agenda for Visual Analytics"" by James J. Thomas and Kristin A. Cook is a discussion on data representations and data transformations. In my research I have approached this question in the context of PCA and factor analysis. My brief answer is that the visualizations are useful if one has the data transformation to move from the visualization space to the original data space. This would additionally be conducted within a visual analytics framework.","An easy way to describe the problem: If we would be able to always see our reality on our planet in 4 space dimensions instead of 3, we would see everything, it wouldn't matter if there are walls, or locked areas or if something is completely burried in a deep hole, we would alway be able to see it by having access to 4 dimensions at once. Now consider the problem, if we can acces only 3 out of the 4 dimensions and we are searching for a chest of gold which is burried 10m deep in an an area of some square km. If we first try to see what is behind walls or burried, we can only choose 2 of the 3 space dimensions, so we have to rasterize everything in that area, permanently switching the dimensions which we use. We would either need a lot of luck to find the chest, or a lot of work to try out every possibility. There would be a huge benefit by finding techniques, which can visualize all dimensions at once.",,54.62804168,64.96527077,59.98333861,60.2707783,62.82484949,50.48227375,59.27180091,51.27234862,
9000,Data Science Podcasts?,reference-request,"I strongly suggest  Talking Machines . It's a very well put together podcast from a professor at Harvard. They cater to both machine learning experts and enthusiasts. 
 Their interviews are often done from NIPS, and the guests are usually top tier practitioners.","In  Data Skeptic  they talk about different aspects of data science: 
 
 Data Skeptic is a podcast that alternates between short mini episodes with the host explaining concepts from data science to his non-data scientist wife, and longer interviews featuring practitioners and experts on interesting topics related to data, all through the eye of scientific skepticism. 
 
 This is a good intro to Data Science which also explains the basic concepts in a not too technical way. 
 
 A bit more technical is  Partially Derivative : 
 
 Partially Derivative is a podcast about the data of everything. Each week we look at a new way that data has changed how we understand and experience the world. From ancient warfare to modern love and everything in between. 
 
 
 A related Quora question:  What are the best data science podcasts?","Some which I regularly hear to, are: 
 
 What's the point  by FiveThirtyEight 
 
 It is a very nice podcast, where not only the concepts, but also the applications of data science to a wide range of domains, are discussed. 
 
 Linear digressions  by Udacity 
 
 It focuses more on ML and concepts of data science. Hosted by two really nice data scientists Katie and Ben.  All their episodes are fun to watch. 
 
 Partially Derivative  (Already mentioned by agold) 
 Data Skeptic  (Already mentioned by agold) 
 Talking Machine  (Already mentioned by jamesmf) 
 Freakonomics 
 
 It is a fun podcast which focuses on data and it's importance in various domains and how to make sense of it.","Not So Standard Deviations  by Hilary Parker and Roger Peng! Fun little podcast where they talk about a variety of things. They only have 4 episodes out currently, but they seem to be releasing one every two weeks, so there's that.","Here are two other podcasts not mentioned previously: 
 
 OCDQ : for some interesting discussions on data quality and Big Data. 
 Learning Machine 101 : for begginer level discussion on machine learning.",The R-Podcast  is a podcast about data analysis in R. Other cases are  O'Reilly Data Show  and  O'Reilly Radar Podcasts,"Data Skeptic  has become one of the best information science podcasts as well as a resource for such a fresh take on science, data-driven analysis, and scepticism on a wide range of issues, including large datasets, machine learning, statistics, and data science in general. 
 This is a podcast that comes out every week. It implies that now the blog publishes data-science-related programmes and blogs virtually weekly so that listeners can get a clear view of the world of information. There are a total of 277 episodes.",,,56.41882531,73.62089103,64.77166629,58.08440114,60.42113525,71.39773185,73.79243832,,
8941,Data Science conferences?,community,"PyData - talks about Python Data tools 
Link:  http://pydata.org/events/   
 There is one PyData conference on the east coast and one on the west coast each year. 
 
 NIPS - Neural Information Processing Systems (NIPS) 
Link:  https://nips.cc/ 
 This is one of the hardest / most prestigious academic Machine Learning conferences to get an abstract / poster accepted.     
 
 The 5th International Workshop on Parallel and Distributed Computing for Large Scale Machine Learning and Big Data Analytics (IEEE IPDPS 2016) 
Link:  http://parlearning.ecs.fullerton.edu/   
 This one is also an academic conference with paper submission. 
 
 Note:
I am not sure if you want academic or nor academic conferences (have conference proceedings / papers associated with the conference).
Some conferences are not about new data science methodologies  but the tools and libraries (e.g. PyData) that implement existing methodologies.
Also, data science is very broad and includes Stat, Machine Learning and data warehousing / mining etc.","Data science is still a domain in fusion, borrowing from neighboring fields. 
 A recent interesting contender is DSAA, ""IEEE International Conference on Data Science and Advanced Analytics"". The first edition ( DSAA 2014 ) held in Shanghai. The second  DSAA 2015  was in Paris, the  DSAA 2016  edition is announced in Montreal, Canada, on October 17-19, 2016.  
 Nuit Blanche  recently anounced the PCMI Summer school "" The mathematics of data "", June-July 2016, Midway, Utah, USA. 
 Other related conferences and workshops are: 
 
 COLT:  Annual Conference on Learning Theory  ( COLT 2016 , 23-26 June, New-York, USA) 

 
 videos:  2013 Princeton ,  2014 Barcelona ,  2015 Paris 
 
 MMDS:  Workshop on Algorithms for Modern Massive Data Sets  ( MMDS 2016  edition Berkeley CA 21-24/06/2016)

 
 videos:  MMDSworkshop YouTube channel , and follow tabs for MMDS 2012 videos, or  here 
 
 ICML:  International Conference on Machine Learning  ( ICML 2016 , June, New York, USA)

 
 videos:  2015 Lille   
 
 NIPS:  Annual Conference on Neural Information Processing Systems  with NIPS 2016 in Barcelona, 5-10/12/2016

 
 videos: NIPS 2015  tutorials ,  invited speakers ,  spotlights ,  NIPS 2014","Just went to this one last week (Open Data Science Conference):  http://odsc.com/ 
 It was really enjoyable. Heavy focus on open source technologies. Speakers from high profile (within the community) projects. And they have posted a lot of the videos from it too. 
 Playlist  of the talk video.","Annual UseR! conference: 
 
 link:  UseR!2015 
 link:  UseR!2016 
 
 Data lead has an annual conference. Last year in the US, this year in Paris: 
 
 http://www.datalead2015.com/index.php 
 
 H2O has a conference  H2O world 
 Since Data Science is a very broad subject: 
 Effective applications of the R language: 
 
 http://www.earl-conference.com/ 
 
 Strata + Hadoop World: 
 
 http://conferences.oreilly.com/strata 
 
 Joint Statistical Meetings: 
 
 https://www.amstat.org/meetings/jsm.cfm 
 
 And of course your local  meetups","My favorites ones are Wrangle, Spark Summit and ampcamp. 
 
 
 Wrangle is a new, single-day, single-track industry event about the principles, practice, and application of Data Science, across multiple data-rich industries. It includes talks from data scientists from companies like Salesforce, Pinterest, Facebook, and Uber about the hardest problems they've faced, and the solutions they found for them.
  If you're a practicing Data Scientist, Wrangle is for you! 
 spark-summit. DATA SCIENCE AND ENGINEERING AT SCALE  
 AMP Camps are Big Data training events organized by the UC Berkeley AMPLab about big data analytics, machine learning, and popular open-source software projects produced by the AMPLab. All AMP Camp curricula, and whenever possible videos of instructional talks presented at AMP Camps, are published here and accessible for free. 
 
 
 
 http://www.wrangleconf.com/ 
 https://spark-summit.org/ 
 http://ampcamp.berkeley.edu/","Strata + Hadoop World  by O'Reilly 
 DataEDGE  by Berkeley School of Information.  Link  to the videos.","DataSciCon.Tech 
 http://datascicon.tech/ 
 This is a 3-day developer-focused conference in Atlanta USA in November/December 
 Full-day workshops on: 
Data Science for Discovery, Innovation, and Value Creation
Data Science with R Workshop
Introduction to Machine Learning with Python and TensorFlow
Data Analytics with Tableau 
 Followed by 2 days with deep dive content in 4 tracks on topics such as:
Data Science,
Data Analytics,
Artificial Intelligence,
Machine Learning,
Deep Learning,
Big Data,
Data Visualisation, 
and Deep Learning.","Annual Wolfram Technology Conference. 
 
 Wolfram Technology Conference 2015 
 
 Technical training, analysis topics on a wide range of industries, software development, curated data, on so on.","ACM KDD  is the top conference for both industry and academia.  
 Last time it took place in Sydney. Have a look to  the program and participants .",71.28608169,63.76283123,63.57129956,67.94795762,60.03366167,50,69.38644329,61.89242154,58.15409546
8909,Best Julia library for neural networks,machine-learning,"MXNet Julia Package - flexible and efficient deep learning in Julia 
 
 https://github.com/dmlc/MXNet.jl 
 Pros 
 
 Fast 
 Scales up to multi GPUs and distributed setting with auto parallelism. 
 Lightweight, memory efficient and portable to smart devices. 
 Automatic Differentiation 
 
 Cons 
 
 Doesn't have yet low level operations for algorithm implementation. But they are working on this issue ( https://github.com/dmlc/mxnet/issues/586 )","Mocha.jl  - Mocha is a Deep Learning framework for Julia, inspired by the C++ framework Caffe. 
 Project with good  documentation  and examples.
Can be run on CPU and GPU backend.","Just to add a more recent (2019) answer:  Flux . 
 Flux is an elegant approach to machine learning. It's a 100% pure-Julia stack,
and provides lightweight abstractions on top of Julia's native GPU and
AD support. Flux makes the easy things easy while remaining fully hackable.
 
 For example: 
 model = Chain(
  Dense(768, 128, σ),
  LSTM(128, 256),
  LSTM(256, 128),
  Dense(128, 10),
  softmax)

loss(x, y) = crossentropy(model(x), y)

Flux.train!(loss, data, ADAM(...))",As of Oct 2016 there's also a  Tensorflow wrapper for Julia .,One newer library to look at as well is  Knet.jl . It will do things like use GPUs under the hood.,,,,,60.94833948,58.57882247,58.42127966,65.83013484,53.97142167,,,,
8869,Best way to search for a similar document given the ngram,nlp,"You could use a hashing vectorizer on your documents. The result will be a list of vectors. Then vectorize your ngrams in the same way and calculate the projection of this new vector on the old ones. This is equivalent to the database join on an index, but may have less overhead.","The data structure typically uses is  inverted index  (e.g., in databases). 
 Please note that matching all ngram is a good heuristic but you might want to improve it.  
 Taking into account the probability of each term and  stemming  are directions you might benefit from.","table   
 ngram 
docID   
 PK (primary key) ngram, docID   
 depending the database may change a bit but this is for TSQL 
x is the document you are matching  
 select top(1) with ties *    
from 
(  select tm.docID, count(*) as count 
     from table td
     join table tm
       on tm.docID <> td.docID 
      and tm.ngram = td.ngram 
      and td.docID = x
    group by tm.docID 
) tt 
order by count desc
 
 The join is on an index (PK) so this is very fast. I do this on a million documents in just a few seconds (with more advanced conditions).      
 This is going to favor larger documents but that is what you asked for.   
 Question seems to be changing   
 declare table @query (varchar ngram);
insert into @query values ('ng1'), ('ng2'), ('ng3');
select top(10) with ties *    
from 
(  select tm.docID, count(*) as count 
     from table td
     join @query
       on tm.ngram = @query.ngram
    group by tm.docID 
) tt 
order by count desc","From your clarification - 
 
 By database, lets just say that there is a huge list of the ngram
  model that represents the document 
 
 You would do well to do something a bit more structured and put the data into a relational database. This would allow you to do much more detailed analysis more easily and quickly.  
 I guess when you say ""ngram"" you mean ""1gram"". You could extend the analysis to include 2grams, 3grams etc, if you wanted. 
 I would have a table structure that looks something like this - 
 1Grams 
ID 
Value    
 Docs 
ID 
DocTitle 
DocAuthor 
etc.    
 Docs1Grams 
1GramID 
DocID 
1GramCount    
 So, in the record in the  Docs1Grams  table when 1GramID points to the 1gram ""the"" and the DocID points to the document ""War and Peace"" then 1GramCount will hold the number of times the 1gram ""the"" appears in War and Peace. 
 If the DocID for 'War and Peace"" is 1 and the DocId for ""Lord of the Rings"" is 2 then to calculate the 1gram similarity score for these two documents you would this query - 
 Select count(*) from Docs1Grams D1, Docs1Grams D2   
where D1.DocID = 1 and   
D2.DocID = 2 and   
D1.1GramID = D2.1GramID and   
D1.1GramCount > 0 and   
D2.1GramCount > 0   
 
 By generalizing and expanding the query this could be easily changed to automatically pick the highest such score / count comparing your chosen document with all the others. 
 By modifying / expanding the  D1.1GramCount > 0 and D2.1GramCount > 0  part of the query you could easily make the comparison more sophisticated by, for instance, adding 2Grams, 3Grams, etc. or modifying the simple match to score according to the percentage match per ngram.  
 So if your subject document has 0.0009% of the 1grams being ""the"", document 1 has 0.001% and document 2 has 0.0015% then document 1 would score higher on ""the"" because the modulus of the difference (or whatever other measure you chose to use) is smaller.","If you want to check for the presence of your n grams in the document you will need to convert the query document also into n grams. To accomplish this you can use the TFIDF vectorizer. 
 from nltk.tokenize import word_tokenize               
from sklearn.feature_extraction.text import TfidfVectorizer
vect = TfidfVectorizer(tokenizer=word_tokenize,ngram_range=(1,2), binary=True, max_features=10000)
TFIDF=vect.fit_transform(df_lvl0['processed_cv_data'])
 
 To explain the above code:  
 word_tokenize  : this function converts the text into string tokens but you will have to clean the data accordingly.  
 ngram_range  : sets the number of ngrams you need. In this case it will take both 1 word and 2 words.  
 max_features  : limits the total no. of features. I recommend using it if you want to tokenize a couple of documents as the no. of features is very high(in the order of 10^6).   
 Now after you fit the model the features are stored in ""vect"". You can view them using: 
 keywords=set(vect.get_feature_names())
 
 Now that you have the grams of your query document stored in a set, you can perform set operations which are much faster compared to loops.  Even if the length of each set is in the order of 10^5 you will get results in seconds. I highly recommend trying sets in python. 
 matching_keys = keywords.intersection(all_grams)   //all_grams is the set of your collected grams
 
 Finally you will get all the matching keywords into ""matching_keys"".",,,,,59.72726085,55.53712279,62.87311429,57.82998832,57.79797345,,,,
8762,How to define a distance measure between two IP addresses?,feature-extraction,"If I understood them correctly, both Jeremy and Edmund's (first) solutions are the same, namely, plain euclidean distance in a 4-dimensional space of IP addresses.BTW, I think a very fast alternative to euclidean distance would be to calculate a hamming distance bit-wise. 
 Edmund's first update would be better than his second. The reason is simple to state: his 2nd update tries to define a distance measure by considering a  non-linear  function of the coordinates of a 4D vector. That however will most likely destroy the key properties that it needs to satisfy in order to be a metric, namely  
 
 Injectivity: $d(IP_1,IP_2)=0 \iff IP_1=IP_2$,  
 Symmetry: $d(IP_1,IP_2)=d(IP_2,IP_1)$, and  
 Triangular inequality: $d(IP_1,IP_2)\leq d(IP_1,IP_3)+d(IP_3,IP_2)\,\forall IP_3$.  
 
 The latter is key for later interpreting small distances as close points in IP space. One would need a linear (in the coordinates) distance function. However, simple euclidean distance is not enough as you saw.  
 Physics (well, differential geometry actually) could lead to a nice solution to this problem: define a metric tensor $g$. In plain english, give weights to each  pair of coordinates , take each pair difference, square it and multiply it by its weight, and then add those products. Take the square root of that sum and define it as your distance. 
 For the sake of simplicity, one could start trying with a diagonal metric tensor. 
 
 Example: Say you take $g=\begin{pmatrix}1000 &0 &0 &0 \\0 &100&0&0\\0&0&10&0\\0&0&0&1\end{pmatrix}$ $IP_1=(x_1,x_2,x_3,x_4)$ and
  $IP_2=(y_1,y_2,y_3,y_4)$. Then the square of the distance is given by
  $$d(IP_1,IP_2)^2=1000*(x_1-y_1)^2+100*(x_2-y_2)^2+\\ \,+10*(x_3-y_3)^2+1*(x_4-y_4)^2$$
  For $IP_1=192.168.1.1,\,IP_2=192.168.1.2$ the distance is clearly 1.
  However, for $192.168.1.1$ and $191.168.1.1$ the distance is
  $\sqrt{1000}\approx 32$ 
 
 Eventually you could play around with different weights and set a kind of normalization where you could fix the value of the maximal distance $d(0.0.0.0,FF.FF.FF.FF)$.  
 Furthermore, this set up allows for more complex descriptions of your data where the relevant distance would contain ""cross-products"" of coordinates like say $g_{13}*(x_1-y_1)*(x_3-y_3)$. 
 EDIT: While this would be a better ""weighting"" method than using those other I addressed, I realize now it is actualy meaningless: As Anony-Mousse and Phillip mention, IP are indeed 32 dimensional. This means in particular that giving the same weight to all bits in say the 2nd group is in general not sound: One bit could be part of the netmask while the other not. See Anony-Mousse answer for additional objections.","That's a very interesting question. Similarity here should be computed component-wise, but the thing is from a ""business logic"" perspective, the similarity of the last 3 numbers doesn't matter if the other 3 sets of numbers are not the same. Keeping that in mind, I would probably do something like the following (there is probably a more elegant way of doing it, and I don't have much time to think about it so forgive me if it doesn't answer your question and for the poor formatting). 
 Assuming IPv4 of the form aaa.bbb.ccc.ddd, I would so something like: 
 If aaa_1 == aaa_2:
  If bbb_1 == bbb_2:
    If ccc_1 == ccc_2:
        If ddd_1 == ddd_2:
            Dist = 1;
        Else:
            Dist = (3 + distance(ddd_1,ddd_2))/4;
        End if;
    Else:
        Dist = (2 + distance(ccc_1,ccc_2))/4;
    End if;
  Else:
    Dist = (1 + distance(bbb_1,bbb_2))/4;
  End if;
 Else:
  Dist = distance(aaa_1,aaa_2);
  Return 1/Dist;","20 years ago, I would have suggested to use the  length of the shared prefix  as similarity measure. 
 So you take two IPs. In their 32 bit representation, not the ""pretty printed"" x.y.z.w form; the real ""int"" reoresentation your network stack uses. Then XOR them, count the leading zeros, and you get 
 distance = 32 - leadingZeros(ip1 XOR ip2)
 
 However, we have exhausted the IPv4 namespace long ago. The last 10 years, the few remaining netblock have been more or less ""randomly"" (at least from a similarity perspective) been distributed.  IP ranges have been relocated  and so on. 
 A lomg time ago, people would have told you routing happens on trees, based on their prefix. So If you wanted to read an IP 10.2.3.4 it would go to 10.0.0.0 then 10.2.0.0 then 10.2.3.0. But that was just the theory. If you  manually  configured your router, that is what you would do. 10.2 isthe second building, 10.2.3. is the third floor router.
History.
Within networks, IPs are assigned by DHCP, often first-come-first-served. Within intranet, you have mostly switches not routers. And on the global level, the  BGP  is responsible for taking care of the  big mess  of todays routing tables. 
 In other words:
use some database like GeoIP to map the IPs to (approximate) coordinates. Best you can do.
IP based similarity is mostly useful on a /24 prefix, but a binary yes/no similarity won't make you happy I guess.","IP (v4) addresses are a 32-bit integer, which trivially gives you a metric. However, it may not be a particularly  useful  metric - 10.255.255.255 and 11.0.0.0 are almost certainly significantly more different than 192.168.1.1 and 192.168.1.2.","Like someone else mentioned, treating IPs as int automatically gives higher bits higher weights.
I've used variance of IPs which is log scaled. 
 math.log(np.std([IP(ip).int() for ip in ips]))","2nd Update 
 The below can be improved as it does not consider the hierarchical structure of an IP address.  To account for this the elements of the IP vectors can be non-linearly scaled before computing the distance vector and its norm. This gives more weight to the elements higher in the hierarchy. 
 Mathematica code 
 Once we have the 4D vectors from the 1st update each element is scaled based on its position $[x^{2}_{1},x^{\frac{3}{2}}_{2},x^{1}_{3},x^{\frac{1}{2}}_{4}]$. 
 Subtract @@ (MapIndexed[
       Function[{value, index}, 
        value^((5 - First@index)/2)], #] & /@ {ip1, ip2}) // Norm // N
(* 2209.17 *)
 
 There is information lost in collapsing from 4D down to 1D but this can't be helped if you are looking for a 1D distance metric. 
 
 1st Update 
 An IP address is made up of 4 numbers. Takes these as vectors in 4D and calculate the distance between them (  Distance in Euclidean space  ). 
 Mathematica code 
 (* Make some IP addresses *)
{ip1, ip2} = 
 StringRiffle[#, "".""] & /@ 
  Map[ToString, RandomInteger[{1, 255}, {2, 4}], {2}]
(* {""50.229.29.146"", ""27.167.216.58""} *)

(* Extract 4D vector *)
{ip1, ip2} = Map[FromDigits, StringSplit[#, "".""] & /@ {ip1, ip2}, {2}]
(* {{50, 229, 29, 146}, {27, 167, 216, 58}} *)

(* Calculate distance *)
Norm[ip1 - ip2] // N
(* 216.993 *)
 
 
 Consider an IP address as a 4D vector. Subtract and calculate the norm.","First you need to distinguish private and public addresses (check wikipedia IP_address#Private_addresses). 
 Private IPs:  the best you can do is compute if 2 addresses COULD be on the same network or not, then you need clues from other features to KNOW if it is the case or no. 
 Public IPs: 
For geographic distance, you may want to check web services/API that try to map IP and geographical locations (one google search turned this one for instance  enter link description here ).  
 Another point which could be interesting is the ""organisational distance"", from the IP address you can try to identify the owner of the address (the ISP), check ARIN for instance  http://whois.arin.net/rest/net/NET-8-8-8-0-1/pft?s=8.8.8.8 .  
 From there you can try to figure out if two addresses belong to the same organisation or not. You will have to find some way to tell if the organisation is an ISP with private customers or a company with their own network. Please be also aware that some organisations have started to resell blocks from their IPV4 addresses to other companies for money with the effect that addresses that used to be in the same organisation/location can now be thousands of miles away in different companies. 
 I think it would be wise to consider these informations as probabilities only.",,,64.10368604,52.14858277,52.11131823,52.85607142,50,58.15403722,61.43475669,,
8622,Tool to Generate 2D Data via Mouse Clicking,data,"I recently discovered this site:  https://guoguibing.github.io/librec/datagen.html 
 Outputs list of points, and color ID (class) for each point. 
 Screenshot:","In R: 
 First set up a blank plot with whatever x and y scale limits you need: 
 plot(NA, xlim=c(11,20),ylim=c(10,99))
 
 Then click click click with mouse-button 1 and end with mouse-button 2 (probably): 
 pts = data.frame(locator(type=""p""))
 
 Then save as a CSV file: 
 write.csv(pts,""pts.csv"",row.names=FALSE)
 
 producing: 
 ""x"",""y""
20.9461142167608,54.0921852908633
11.6463003491398,24.5409354249845
14.4239385175408,44.1769632963908
14.7755382856928,29.5957544809901
14.7931182741004,62.8409105801038","https://drawdata.xyz/ 
 I think this one also good, the highest vote one, you have to click one by one to generate more data points, this one is much faster...","I found a simple Python solution, adapted from  https://stackoverflow.com/q/25521120/1265192 
 This also works in a Jupyter Notebook, if desired. 
 import numpy as np
import matplotlib.pyplot as plt

%matplotlib qt

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(111)
ax.set_xlim(0,800)
ax.set_ylim(0,600)

plt.grid(True)

coords = []

def onclick(event):
    x, y = event.xdata, event.ydata
    
    print(f'{x:0.1f},{y:0.1f}')

    global coords
    coords.append((x, y))

    ax.scatter([x], [y], c='b', s=150)
    plt.draw()
    
cid = fig.canvas.mpl_connect('button_press_event', onclick)

fig.show()
 
 The coordinates are stored in  coords  and also printed to the screen (but with 1 decimal place). One could save the coordinates to a file, but I just copy/paste the printed coordinates wherever I want them.","Just updating the other answers. 
 You could use this site:
 https://guoguibing.github.io/librec/datagen.html 
 to make it a dataframe: 
 points = pd.read_csv('/content/pontos.csv', sep='\n', delimiter=',', names=['x','y','class'])",,,,,50,63.64377268,59.51244642,50,50,,,,
8606,What are some nice algorithms/techniques for optimizing and predicting Click Through Rates (CTR)?,r,"Logistic regression  is very common and efficient. 
 This will provide you with a probability of click for a given impression. You will likely need additional information for optimization, like keywords or A/B testing.","There are a group of algorithms (or techniques) called the  Bandit algorithms , which deal especially with the problem statement, which is the optimization of Click-through rates of advertisements. 
 The problem is framed in a setting of multiple bandits with vending machines. There are various strategies which can be implemented: 
 
 Epsilon-greedy strategy 
 Epsilon-first strategy 
 Epsilon-decreasing strategy 
 Contextual Epsilon strategy 
 
 Reference on why Bandit algorithms are better than A/B testing frameworks.","I've tried the following algorithms: 
 
 Factorization Machines  by Steffen Rendle  - 
Really good algorithm for sparse feature sets, in construct to polynomial-regression.
Measures the interaction between features. Model that in addition to learning linear weights on features, learn a vector space for each feature to learn pairing interactions between features in this new space.  
 Field-Aware Factorization Machines  - an improvement of the FM model.
Recently, have been used to win two Kaggle's click-through rate prediction competitions. 
 FTRL - ""Follow The (Proximally) Regularized Leader"" algorithm  - regularized online logistic regression. Equivalent to Online (Stochastic) Gradient Descent when no regularization is used. Very easy to implement.  
 AdPredictor  algorithm by Microsoft  
The algorithm is based on a probit regression model that maps discrete or real-valued input features to probabilities. It maintains Gaussian beliefs over weights of the model and performs Gaussian online updates derived from approximate message passing.","You can see nice algorithms in kaggle competitions about CTR: 
 
 https://www.kaggle.com/c/avito-context-ad-clicks 
 https://www.kaggle.com/c/avazu-ctr-prediction 
 
 Just go to forum of each competition and search for winning solutions ;)","Factorization machines -  libfm  open source software has been used widely among top companies in recommender applications.  
 Check it out. It contains various algorithms to play with (e.g. stochastic gradient descent). 
 I guess your data has like ""click/view"" label for each pair of user-product or say cookie-advertisement saying  ""whether user clicked or just viewed product"" . You can treat this as binary classification with two classes click or just view.",,,,,55.15238571,60.02416771,55.52186322,64.2235309,55.31564946,,,,
8460,Python library to implement Hidden Markov Models,python,"For another alternative approach, you can take a look at the PyMC library.
There is a good  gist  created by Fonnesbeck which walks you through the HMM creation. 
 And if you become really eager about the PyMC, there is an awesome open-source  book  about Bayesian Modeling. It does not explicitly describe Hidden Markov Processes, but it gives a very good tutorial on the library itself with plenty of examples.","As an update on this question, I believe the accepted answer is not the best as of 2017. 
 As suggested in comments by Kyle,  hmmlearn  is currently the library to go with for HMMs in Python. 
 Several reasons for this: 
 
 The  up-to-date documentation , that is very detailed and includes tutorial  
 The  _BaseHMM  class from which custom subclass can inherit for implementing HMM variants 
 Compatible with the last versions of Python 3.5+ 
 Intuitive use 
 
 Opposite to this, the  ghmm   library  does not support Python 3.x according to the current documentation. Most of the documentation pages have been generated in 2006. It does not seem at first glance a library of choice... 
 Edit:  Still valid in 2018.","pomegranate  library has support for HMM and the documentation is really helpful. After trying with many hmm libraries in python, I find this to be quite good.","For an alternative approach, perhaps even to help foster understanding, you will probably find some utility in doing some analysis via R. Simple time series based tutorials abound for [wannabe] quants that should provide a bootstrap.  Part 1 ,  Part 2 ,  Part 3 ,  Part 4 . These provide sources for data generation/intake as well as manipulation, allowing you to bypass much of the work to be able to see the actual HMM methods at work. There are direct analogues to the Python implementations. 
 As a side note, for a more theoretical introduction, perhaps  Rabiner  might provide some insights","The  ghmm  library might be the one which you are looking for.  
 As it is said in their website: 
 
 It is used for implementing efficient data structures and algorithms
  for basic and extended HMMs with discrete and continuous emissions. It
  comes with Python wrappers which provide a much nicer interface and
  added functionality. 
 
 It also has a nice documentation and a step-by-step tutorial for getting your feet wet.",,,,,63.16338488,56.49520825,55.12200601,51.63969439,53.42480458,,,,
8457,Python library for segmented regression (a.k.a. piecewise regression),python,"numpy.piecewise  can do this. 
 
 piecewise(x, condlist, funclist, *args, **kw) 
 Evaluate a piecewise-defined function. 
 Given a set of conditions and corresponding functions, evaluate each
      function on the input data wherever its condition is true. 
 
 An example is given on SO  here . For completeness, here is an example: 
 from scipy import optimize
import matplotlib.pyplot as plt
import numpy as np

x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,11, 12, 13, 14, 15], dtype=float)
y = np.array([5, 7, 9, 11, 13, 15, 28.92, 42.81, 56.7, 70.59, 84.47, 98.36, 112.25, 126.14, 140.03])

def piecewise_linear(x, x0, y0, k1, k2):
    return np.piecewise(x, [x < x0, x >= x0], [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])

p , e = optimize.curve_fit(piecewise_linear, x, y)
xd = np.linspace(0, 15, 100)
plt.plot(x, y, ""o"")
plt.plot(xd, piecewise_linear(xd, *p))","The method proposed by Vito M. R. Muggeo[1] is relatively simple and efficient. It works for a specified number of segments, and for a continuous function. 
 The positions of the breakpoints are iteratively estimated  by performing, for each iteration, a segmented linear regression allowing jumps at the breakpoints. From the values of the jumps, the next breakpoint positions are deduced, until there are no more discontinuity (jumps). 
 
 ""the process is iterated until possible convergence, which is not, in
  general, guaranteed"" 
 
 In particular, the convergence or the result may depends on the first estimation of the breakpoints. 
 This is the method used in the R  Segmented package . 
 Here is an implementation in python: 
 import numpy as np
from numpy.linalg import lstsq

ramp = lambda u: np.maximum( u, 0 )
step = lambda u: ( u > 0 ).astype(float)

def SegmentedLinearReg( X, Y, breakpoints ):
    nIterationMax = 10

    breakpoints = np.sort( np.array(breakpoints) )

    dt = np.min( np.diff(X) )
    ones = np.ones_like(X)

    for i in range( nIterationMax ):
        # Linear regression:  solve A*p = Y
        Rk = [ramp( X - xk ) for xk in breakpoints ]
        Sk = [step( X - xk ) for xk in breakpoints ]
        A = np.array([ ones, X ] + Rk + Sk )
        p =  lstsq(A.transpose(), Y, rcond=None)[0] 

        # Parameters identification:
        a, b = p[0:2]
        ck = p[ 2:2+len(breakpoints) ]
        dk = p[ 2+len(breakpoints): ]

        # Estimation of the next break-points:
        newBreakpoints = breakpoints - dk/ck 

        # Stop condition
        if np.max(np.abs(newBreakpoints - breakpoints)) < dt/5:
            break

        breakpoints = newBreakpoints
    else:
        print( 'maximum iteration reached' )

    # Compute the final segmented fit:
    Xsolution = np.insert( np.append( breakpoints, max(X) ), 0, min(X) )
    ones =  np.ones_like(Xsolution) 
    Rk = [ c*ramp( Xsolution - x0 ) for x0, c in zip(breakpoints, ck) ]

    Ysolution = a*ones + b*Xsolution + np.sum( Rk, axis=0 )

    return Xsolution, Ysolution
 
 Example:
 
 import matplotlib.pyplot as plt

X = np.linspace( 0, 10, 27 )
Y = 0.2*X  - 0.3* ramp(X-2) + 0.3*ramp(X-6) + 0.05*np.random.randn(len(X))
plt.plot( X, Y, 'ok' );

initialBreakpoints = [1, 7]
plt.plot( *SegmentedLinearReg( X, Y, initialBreakpoints ), '-r' );
plt.xlabel('X'); plt.ylabel('Y');
 
 
 [1]: Muggeo, V. M. (2003). Estimating regression models with unknown  breakpoints. Statistics in medicine, 22(19), 3055-3071.","I've been looking for the same thing, and unfortunately it seems like there isn't one at this time. Some suggestions for how to proceed can be found in this  previous question .  
 Alternatively you could look into some R libraries eg segmented, SiZer, strucchange, and if something there works for you try embedding the R code in python with  rpy2 . 
 Editing to add a link to  py-earth , ""A Python implementation of Jerome Friedman's Multivariate Adaptive Regression Splines"".","There is a  blog post  with a recursive implementation of piecewise regression. That solution fits discontinuous regression. 
 If you are unsatisfied with discontinuous model and want continuous seting, I would propose to look for your curve in a basis of  k  L-shaped curves, using Lasso for sparsity: 
 import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso
# generate data
np.random.seed(42)
x = np.sort(np.random.normal(size=100))
y_expected = 3 + 0.5 * x + 1.25 * x * (x>0)
y = y_expected + np.random.normal(size=x.size, scale=0.5)
# prepare a basis
k = 10
thresholds = np.percentile(x, np.linspace(0, 1, k+2)[1:-1]*100)
basis = np.hstack([x[:, np.newaxis],  np.maximum(0,  np.column_stack([x]*k)-thresholds)]) 
# fit a model
model = Lasso(0.03).fit(basis, y)
print(model.intercept_)
print(model.coef_.round(3))
plt.scatter(x, y)
plt.plot(x, y_expected, color = 'b')
plt.plot(x, model.predict(basis), color='k')
plt.legend(['true', 'predicted'])
plt.xlabel('x')
plt.ylabel('y')
plt.title('fitting segmented regression')
plt.show()
 
 This code will return a vector of estimated coefficients to you: 
 [ 0.57   0.     0.     0.     0.     0.825  0.     0.     0.     0.     0.   ]
 
 Due to Lasso approach, it is sparse: the model found exactly one breakpoint among 10 possible. Numbers 0.57 and 0.825 correspond to 0.5 and 1.25 in the true DGP. Although they are not very close, the fitted curves are: 
 
 This approach does not allow you to estimate the breakpoint exactly. But if your dataset is large enough, you can play with different  k  (maybe tune it by cross-validation) and estimate the breakpoint precisely enough.","The  piecewise-regression  Python library  does exactly this. 
 import piecewise_regression
import matplotlib.pyplot as plt

x = [45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85]
y = [5.31, 4.5, 4.4, 4.1, 4.55, 3.47, 4.55, 3.67, 3.86, 3.56, 3.94, 2.75, 3.14, 2.89, 3.01, 2.01, 1.95, 1.4, 1.35, 1.17, 0.33]

pw_fit = piecewise_regression.Fit(x, y, n_breakpoints=1)
pw_fit.plot()

plt.show()
 
 
 And it gives you a statistical summary: 
 pw_fit.summary()
 
 
 It uses the same method as the segmented R package.",,,,,63.68508237,54.49199258,59.50484222,56.70728863,83.21011094,,,,
8358,Prohibitive size of random forest when saved to disk,python,"The size of each tree depends very much on its depth. Thus, change the maximal depth ( max_depth ). Try to set it to finite number (as opposed to the default ""None"") and then try to reduce this number. In addition (or as alternative) try to increase  min_samples_split  or  min_samples_leaf . 
 You can also analyze you features and keep only important ones. The simplest way would be to have a look at the  clf.feature_importances_  of your forest. (In general, finding important features is an art and science on itself.) Exclude non-relevant features and rebuild the forest.","Try this： 
 import pickle
with open('rforest.pickle', 'wb') as f:
    pickle.dump(rforest, f, -1)
 
 Notice: with the parameter ""-1"" the model file size will largely be reduced. 
 According to the documentation: 
 
 pickle.dump(obj, file[, protocol]) 
 Protocol version 0 is the original ASCII protocol and is backwards compatible with earlier versions of Python. 
  Protocol version 1 is the old binary format which is also compatible with earlier versions of Python. 
  Protocol version 2 was introduced in Python 2.3. It provides much more efficient pickling of new-style classes. 
 If the protocol parameter is omitted, protocol 0 is used. If protocol
  is specified as a negative value or HIGHEST_PROTOCOL, the highest
  protocol version will be used.","I ran into a similar issue and was surprised to find out that indeed decision trees can easily take a lot of memory (range of MBs) and random forests will easily multiply that in the GB range. Details here:  https://stackoverflow.com/a/72633003/4178189 
 See also some other consideration in this other answer:  https://stackoverflow.com/a/72701704/4178189","I ran into a similar issue. Even with small tree sizes, I got a file of hundreds of megabytes. 
 Check if you've set  oob_score=True .
For large training datasets this can result in a large matrix in  oob_decision_function_ . I kept the  oob_score_ , but deleted this matrix. Alternatively, you can set it to  False .","You can also use  joblib.dump  to  save your model . 
 This package is already a dependency of scikit learn, so is installed anyway, and has a  compress  option. When this is used the saved model is ~15 times smaller in my experience (1 GB to 70 MB). 
 joblib.dump(model, output_path, protocol=-1, compress=6)",,,,,53.62483854,50.41139907,57.17232398,51.01967844,52.91356801,,,,
8322,Filling missing data with other than mean values,data-mining,"There are of course other choices to fill in for missing data. The median was already mentioned, and it may work better in certain cases. 
 There may even be much better alternatives, which may be very specific to your problem. To find out whether this is the case, you must find out more about the  nature  of your missing data. When you understand in detail why data is missing, the probability of coming up with a good solution will be much higher.  
 You might want to start your investigation of missing data by finding out whether you have  informative  or  non-informative  missings. The first category is produced by random data loss; in this case, the observations with missing values are no different from the ones with complete data. As for  informative  missing data, this one tells you something about your observation. A simple example is a customer record with a missing contract cancellation date meaning that this customer's contract has not been cancelled so far. You usually don't want to fill in informative missings with a mean or a median, but you may want to generate a separate feature from them.  
 You may also find out that there are several kinds of missing data, being produced by different mechanisms. In this case, you might want to produce default values in different ways.","When it comes to missing data, there are many different methods of filling these values. However, the imputation method you choose, depends largely on the amount of missing data and the type of variable. For example, you won't impute the mean value for missing categorical data, you would choose the mode instead. No matter which method you choose, there will be some bias associated with it. One method which does a good job at reducing the bias associated with imputing missing values, is multiple imputation. It can be quite a long-winded approach but it is the most sound approach I've seen so far to imputing large amounts of missing values. I believe there may be an R library for multiple imputation.  
 Of course, another alternative may be that if variable x has 50% missing data for example, there may be a good explanation as to why this is. Rather than trying to impute it or lose the information associated with the variable, it can sometimes be useful to create a new variable, called variable_x_flag_missing. This would be a binary indicator variable where an observation is coded as 1 if variable x contains a missing value and coded as a 0 if it does not.","There is a difference between data with missing values and sparse data. Missing values are generally there because of invalid input, loss or error during data collection or are created when cleaning or processing data.  
 If these values are very less in number, the corresponding instances can be ignored or if are around 5-10% of the data, can be filled using various methods (carry forward last observation, fill with mean/median, interpolate the data etc). If you are working in Python, go through Pandas documentation for  Working with Missing Values , to learn in detail about these options (even if you aren't working in Python, this is a good read). 
 But if your data set has a large number of missing values (say ~ >30% ), then the data is  sparse . Such data sets create various bias in your modelling, and there are special ways to deal with them, though I don't about them much.","If the values are missing at random  and  you are sure that your data matrix is of low rank, you can use nuclear norm basis pursuit method (also known as  matrix completion ). The method (among others) is implemented in  TFOCS . 
 In many real-world applications, the data matrix has rarely full rank, so the assumption of low-rank matrix can be acceptable. On the other hand, the values might not be missing truly at random. 
 Another approach would be to use Singular Spectrum Analysis ( SSA ), also known as the Caterpillar algorithm. It can be used for time-series data with missing values. This algorithm is not very well-known but in literature it is sometimes called ""PCA for time-series data"".","If missingness process could be assumed as MAR (Missing at Random) I strongly suggest multiple imputation.  
 The idea of multiple imputation for missing data was proposed by Rubin in 1977. 
 The idea is attractive because enable to separate the imputation and the analysis steps. 
 
 The first step of multiple imputation for missing data is to multiply impute the missing values by using an appropriate model, the imputation model which mostly depends on the type of variable at hands.  
 The second step is to separately analyze the imputed datasets. 
 The third step is to combine the estimates of the analysis model in each imputed dataset. 
 
 This allows to have robust estimates. 
 I am able only to perform it on R with the  mice package .",,,,,69.297205,64.60247696,60.87565537,55.70998515,57.5236041,,,,
8309,Quick way to visually explore data?,visualization,"When you hit the limits of an application like Apple Numbers, or Excel, you need to start using a programming language like Python, R or C. Using a programming language, you write your own application, which is not constrained by arbitrary limits like 2 20  rows by 2 14  columns. (Of course, you are still constrained by the physical memory and the way that a particular language addresses it: R and Python tend to be more resource-intensive than C). 
 Let me give you an example using Python. You can install Python for your system from the  official site . If you install one of the latest versions,  you will already  get  pip  as an installer of further Python packages. Install Pandas by starting a terminal (on OS X, there is a 'Terminal' application, on Windows this used to be a MS-DOS window ...), and typing into that terminal: 
 pip install pandas
 
 You can then start Python in an interactive session by typing  python  in a terminal. In the Python shell, you can read the data as mentioned in  @tasos  's answer: 
 >>> import pandas as pd
>>> df = pd.read_csv('name_of_your_file.csv')
 
 Here you can use the  describe  method of the  df  DataFrame to get some statistics about the read data. These numbers are for a randomly generated data frame: 
 >>> df.describe()
          0         1         2
count  3.000000  3.000000  3.000000
mean  -0.150869  0.444380 -0.117066
std    0.751421  0.697880  0.565328
min   -1.017424 -0.356764 -0.506030
25%   -0.386514  0.206466 -0.441313
50%    0.244396  0.769696 -0.376595
75%    0.282409  0.844952  0.077416
max    0.320422  0.920208  0.531427
 
 Then you can generate plots for different columns, one against the other, using the  Pandas plotting functions . Check the many examples in the linked documentation. Here is a small example from there: 
 import matplotlib.pyplot as plt
import pandas as pd
import bumpy as np

df = pd.DataFrame({'a': np.random.randn(1000) + 1, 'b': np.random.randn(1000),
                   'c': np.random.randn(1000) - 1}, columns=['a', 'b', 'c'])
plt.figure()
df.plot(kind='hist', alpha=0.5)
 
 Usually, you will need to use another call more in order to save it to a file: 
 plt.save('output.png')  ## or *.svg, *.pdf
 
 In the file you should find something similar to this figure.","Unfortunately, the highest number of rows in both Numbers and Microsoft excel is 1,048,576. So, if you have a file bigger than this, you are not able to open it. 
 Some ideas: 
 
 Connect your file in an SQL database and work with it from there. 
 Use another tool like  delimitware  which let you open files up to 2 billions rows. 
 Use Python Pandas with  pandas.read_csv('filename')  and then  df.head(5)  to check the first 5 rows. 
 Split the file in smaller files with less than 1m rows per each chunk and use Numbers or Microsoft Excel to read it. 
 Upload it to Azure ML and work on a new experiment with it.","You may install  R  - you are limited only with your RAM 
  ## read csv file
 df <- read.csv(""l.csv"")

 ## column names
 > colnames(df) 
 [1] ""a"" ""b""

 > head(df)
     a  b
   1 1  1
   2 2  4
 ...

 # quick overview
 > summary(df)
        a                 b            
  Min.   :    1.0   Min.   :        1  
  1st Qu.: 2500.8   1st Qu.:  6253751  
  Median : 5000.5   Median : 25005000  
  Mean   : 5000.5   Mean   : 33338334  
  3rd Qu.: 7500.2   3rd Qu.: 56253750  
  Max.   :10000.0   Max.   :100000000  

 > plot(df2,type='l')
 
 
 Check docu for other graphic and statistics features.","Since you are on OS X you can use the terminal to explore your file without storing everything in memory. The  head -5 filename.csv  command for example will display the first 5 lines of your file. You could even do  head -10 filename.csv > newfile.csv  to store the first 10 lines in a new file and open it with Apple Numbers to examine what's inside.  
 For anything more than that I'm afraid you would need to write a script to run through each line of the file one by one and compute the relevant descriptive statistics.","There is a neat online tool called  RAWGraphs  which will chart your data in your browser.  You drag and drop your file and it will try and make sense of it. It's made for web designers, so in the end you export an HTML representation of your visualization, but it's a fine tool in its own right.",,,,,51.10886042,50,53.18601361,52.05809113,53.9648909,,,,
8286,Are there any tools for feature engineering?,feature-selection,"Very interesting question (+1). While I am not aware of any software tools that currently offer comprehensive functionality for  feature engineering , there is definitely a wide range of options in that regard. Currently, as far as I know, feature engineering is still largely a  laborious  and  manual  process (i.e., see  this blog post ). Speaking about the feature engineering subject domain,  this excellent article  by Jason Brownlee provides a rather comprehensive overview of the topic. 
 Ben Lorica, Chief Data Scientist and Director of Content Strategy for Data at O'Reilly Media Inc., has written a  very nice article , describing the state-of-art (as of June 2014) approaches, methods, tools and startups in the area of  automating  (or, as he put it,  streamlining ) feature engineering. 
 I took a brief look at some  startups  that Ben has referenced and a product by  Skytree  indeed looks quite impressive, especially in regard to the subject of this question. Having said that, some of their claims sound really suspicious to me (i.e.,  ""Skytree speeds up machine learning methods by up to 150x compared to open source options"" ). Continuing talking about commercial data science and machine learning offerings, I have to mention solutions by Microsoft, in particular their  Azure Machine Learning Studio . This Web-based product is quite powerful and elegant and offers some feature engineering functionality (FEF). For an example of some simple FEF, see  this nice video . 
 Returning to the question, I think that the simplest approach one can apply for automating feature engineering is to use corresponding  IDEs . Since you (me, too) are interested in R language as a data science backend, I would suggest to check, in addition to RStudio, another similar open source IDE, called  RKWard . One of the advantages of RKWard vs RStudio is that it supports  writing plugins  for the IDE, thus, enabling data scientists to automate feature engineering and streamline their R-based data analysis. 
 Finally, on the other side of the spectrum of feature engineering solutions we can find some  research projects . The two most notable seem to be Stanford University's  Columbus project , described in detail in the  corresponding research paper , and  Brainwash , described in  this paper .","Featuretools  is a recently released python library for automated feature engineering.  It's based on an algorithm called  Deep Feature Synthesis  originally developed in 2015 MIT and tested on public data science competitions on Kaggle. 
 Here is how it fits into the common data science process.  
 
 The aim of the library is to not only help experts build better machine learning models faster, but to make the data science process less intimidating to people trying to learn. If you have event driven or relational data, I highly recommend you check it it out! 
 Disclaimer: I am one of the developers on the project.","Feature Engineering is at the heart of Machine Learning and is rather laborious and time consuming. There have been various attempts at automating feature engineering in hopes of taking the human out of the loop. One specific implementation that does this for classification problems is  auto-sklearn . It uses an optimization procedure called  SMAC  under the hood to choose the appropriate set of transforms and algorithm (and algorithm parameters).  
 Note that Trifacta offers a really easy to use tool for data transformation. It has a highly intuitive GUI that allows to set up transformation/ feature engineering maps. There is also a free trial version that can be used for reasonably sized problems.","Scikit-learn  has recently released new transformers that tackle many aspects of feature engineering. For example: 
 
 You can do multiple missing data imputation techniques with the  SimpleImputer  ( http://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html ), including mean, median and arbitrary value imputation in both numerical and categorical variables. 
 
 You can do multivariate imputation using several estimators, like Bayes, random forest and others (equivalent to R's MICE, Amelia and MissForest) with the  IterativeImputer  ( https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer ) 
 
 You can do categorical one hot encoding with the  OneHotEncoder()  from Scikit-learn 
 
 You can encode categorical variables by numbers with the  LabelEncoder . 
 
 You can do Yeo-Johnson variable transformation with the  PowerTransformer  ( http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html ) 
 
 You can do discretisation with the  KBinsDiscretiser  ( https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization.html ) 
 
 
 There are potentially other feature engineering transformers in Scikit-learn and the developers update the library quite regularly. 
 As an alternative to the well known Scikit-learn library, there is a new recently released open source library called  feature-engine . With feature engine you can: 
 
 Do mean, median, arbitrary, end of tail and random imputation in numerical and categorical variables 
 Do various types of categorical encoding, including one hot, integer, ordinal, mean encoding and weight of evidence. 
 Do various variable transformations including log, reciprocal, exp and box cox 
 Various types of discretisation including equal frequency, equal distance and tree based 
 Outlier handling. 
 
 More details in the github repo and docs ( https://feature-engine.trainindata.com ) 
 Disclaimer: I created feature engine and made it open source. 
 Another open source python package allows for different types of categorical variable encoding:  https://contrib.scikit-learn.org/categorical-encoding/","You should consider checking the  Azure Machine Learning  platform. It is online and you can use it with a free account. 
 Azure ML provides you with a workflow by using modules in a graphic user interface. Many of them are related with Data Munging and you can easily clean your data. If there is something that you cannot do in the GUI, then you can just add a module which let you run custom R or Python script to manipulate your data. 
 The nice part of it, is that you can easily visualise your data at any time and check simple stats like the  dataframe.describe()  of the R.","Amazon Machine Learning  is a tool, which I use for feature engineering some times. 
 As Amazon AWS services have shown a lot of promise and standard, I would definitely count on Amazon ML, with it's prospects and promises for making the workflow of data scientists simpler. But as of now, it's still small. 
 But, as you asked for a tool for feature engineering, so this is one of them. 
 Some  FAQ's  about/for using Amazon ML.",,,,77.29188623,58.26953127,71.89309416,66.27170771,50,71.22963345,,,
8278,Covariate shift detection,machine-learning,"There are methods like the  Kullback-Leibler divergence model , the  Wald-Wolfowitz test  for detecting non-randomness and covariance shift.  
 A simple test for quick analysis of covariance test would be to build a machine learning model, where the model is repeatedly tested with inputting training data and the production data.  
 In case, the model can make out the difference between the training and production datasets then it can be a sign of covariance shift.","Adaptive learning with covariate shift-detection for motor imagery-based brain–computer interface
 http://link.springer.com/article/10.1007/s00500-015-1937-5 
 EWMA model based shift-detection methods for detecting covariate shifts in non-stationary environments ( http://www.sciencedirect.com/science/article/pii/S0031320314002878 )","Here is a simple procedure you can use: 
 
 learn a classifier to distinguish between train/test data (using regular X features) 
 compute the  phi correlation coefficient  to estimate the quality of the classifier = the separability of the train/test data 
 set a threshold (e.g. .2) above which you can claim there is a covariate shift (and start looking as corrections)","The problem of covariate shift ultimately results in datasets with different underlying mathematical structure. Now,  Manifold Learning  estimates a low dimensional representation of high-dimensional data thereby revealing the underlying structure. Often Manifold Learning techniques are not projections -- therefore, different and more powerful, than standard PCA.  
 I've used Manifold Learning techniques (for eg: IsoMap, MDS, etc) to visualize (and, if possible, quantify) the ""(dis)similarity"" between train and test datasets.","You don't give many clues about what properties of the images you might be considering, but it seems that what you might want to measure is the difference in the distributions of the training and tests sets. A useful place to start would be with the Kullback–Leibler divergence, which is a measure of the difference of two distributions.",,,,,71.65080166,90.94955604,61.01371059,57.71700739,50,,,,
8277,Credit card fraud detection - anomaly detection based on amount of money to be withdrawn?,machine-learning,"What's the underlying model of how much someone requests from an ATM? It doesn't seem like it's a simple distribution like a Gaussian, where comparing new amounts to the mean is sensible. Consider a person who always pulls out either \$40 or \$400. Ideally we want to build a distribution of what normal transactions from a user look like, and notice if new datapoints don't look like they're sampled from that distribution. 
 idclark's suggestion, to look at the nearest  n  datapoints from that user and compute the distance from just them, is a good and fast implementation of that sort of test. 
 One other possibility is to try to find similar users, and then aggregate data across users. If I only have 10 withdrawals from each user, I'm not going to be able to reject any new withdrawals with confidence, but if I have seven clusters of users, with a thousand withdrawals per cluster, I can notice when a user who was in a particular cluster deviates from the overall cluster distribution. (This also helps you make use of knowledge about which previous transactions were fraudulent.)","I was thinking to calculate the average amount of money, say for the
  last ten transactions, and check how far is the next transaction
  amount from the average. Too much deviation would signal an anomaly.
  But this does not sound much, does it? 
 
 A typical outlier detection approach. This would work in most cases. But, as the problem statement deals with credit card fraud detection, the detection technique/algorithm/implementation should be more robust. 
 You might want to have a look at the  Mahalanobis Distance  metric for this type of outlier detection. 
 Coming to the algorithms for fraud detection, I would point out to the standards used in the industry (as I have no experience in this, but felt these resources would be useful to you). 
 Check  my answer  for this question. It contains the popular approaches and algorithms used in the domain of fraud detection. The  Genetic Algorithm  is the most popular amongst them.","I was thinking to calculate the average amount of money, say for the
  last ten transactions, and check how far is the next transaction
  amount from the average. 
 
 This sounds like a good start. I'd look into Local Outlier Probabilities. For a given data point you could calculate the distance from  n  nearest neighbors and figure out if the data point under consideration is an outlier.  
 basic overview can be found here 
I'd also consider the source, destination, volume and frequency of transactions as features.","I isn't actually answering your question, but it is an idea of how you can improve it. In my opinion, I don't believe that you will be able to build a classification model with only those data. And if you do it, it will not have high enough accuracy. In your position, I would start looking for more data to use as features. 
 Here are a few examples: 
 
 ATM's code of the withdrawal. People use most of the time similar ATM in their daily routine. If you know the lat and long of their previous ATM, you can check if one of them is far away and combining it with the other features, you will increase your accuracy. 
 Seconds spent on the ATM for each withdrawal. People tend to follow specific patterns when they withdraw money. If all of their previous data are similar on the spending time and then you see lower or higher time on a data point, you will be able to increase the accuracy of the model. 
 Labeled data. In models like this, it is far better if you use supervised algorithms instead of unsupervised. Thus, I would seek for labeled data for fraud usages. This will also let you to calculate the actual accuracy of your model. 
 Time between the two withdrawals. As I said before, people tends to follow patterns. An ""anomaly"" on this with a sooner withdrawal than the expected will also raise your accuracy. 
 
 As far as the algorithms are concerned, I am not keen on choosing one, because it is popular. If you have done all the Data Munging and the feature selection, you have the 90% of the job and the algorithm that you will choose is 2-3 lines on the code (in case you are using a language like Python). What I usually do, is to check all the possible algorithms and evaluate their accuracy. Then I either use a combination of them or the one with the highest accuracy.","Firstly you should probably be creating models of classes/segments of users (unsupervised clustering).  Otherwise it is difficult to predict what a given user will do.  (More on that further below.) 
 Nextly, I think ""deviation from recent transactions"" is also fundamentally flawed.  Most likely there are time  patterns  (time of day, days of the week, working hours, holidays and so on). To understand how to conceptualise time as useful features, see  this excellent answer  on  Machine learning - features engineering from date/time data   And similarly there are amount patterns, partly having to do with practical reasons (eg by withdrawing 38, one can receive 20, 10, 5 and 1 denominations, although this is not possible in some markets, like the USA). 
 Modelling the user is more complicated. You will likely not have enough data on each user, but you can build some user models. (Too few, then the system will make similar predictions for all users, without nuance - eg > $400 always detected as fraud. Too many and there will be sparsity, overfitting, and generally the same problems as having no profile models at all, ie one model per actual user - eg fraud incorrectly detected at every time a given user goes to a new ATM.)  This is basically unsupervised clustering.  (Search for user profile categorisation, user models, user model clustering) 
 Much depends on the data available to you. Perhaps you can be more specific about the scale and scope.  In any case, I wish you luck - banks/Visa do this very poorly right now.",,,,,50.51170365,74.86735869,54.72720112,52.18401017,54.36275976,,,,
6964,LSTM or other RNN package for R,r,"Have a look at the rnn package (full disclosure, I am the author). It implements a multilayer RNN, GRU, and LSTM directly in R, i.e. not an underlying C++ library, so you should also be able to read the code and understand what is going on. 
 install.packages('rnn')
 
 The CRAN version is quite up to date, but the  GitHub version  is bleeding edge and can be installed using: 
 if (!require('devtools')) install.packages('devtools')
devtools::install_github('bquast/rnn')",Keras  is also now available for R. Here's an  example  of an LSTM with the R API.,"You might have to extend another package to implement LSTM and RNN in R. Here are some packages to get you started: 
 
 deepnet  Implements a variety of deep learning architectures 
 darch  A deep architecture 
 H2O  An open-source company with deep learning packages","I found this page, but the R package seems not to be open source :
 link.","You might want to take a look at  mxnet . It is a distributed library for deep learning. It supports C++, python, scala and R. There are many examples with R.  Here  you have an example of LSTM in R with this library.",,,,,67.11411321,56.90124963,73.8675578,56.16277755,53.84371522,,,,
6838,When to use Random Forest over SVM and vice versa?,machine-learning,"I would say, the choice depends very much on what data you have and what is your purpose. A few ""rules of thumb"". 
 Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically two-class. For multiclass problem you will need to reduce it into multiple binary classification problems. 
 Random Forest works well with a mixture of numerical and categorical features. When features are on the various scales, it is also fine. Roughly speaking, with Random Forest you can use data as they are. SVM maximizes the ""margin"" and thus relies on the concept of ""distance"" between different points. It is up to you to decide if ""distance"" is meaningful. As a consequence, one-hot encoding for categorical features is a must-do. Further, min-max or other scaling is highly recommended at preprocessing step.  
 If you have data with $n$ points and $m$ features, an intermediate step in SVM is constructing an $n\times n$ matrix (think about memory requirements for storage) by calculating $n^2$ dot products (computational complexity).
Therefore, as a rule of thumb, SVM is hardly scalable beyond 10^5 points.
Large number of features (homogeneous features with meaningful distance, pixel of image would be a perfect example) is generally not a problem.  
 For a classification problem Random Forest gives you probability of belonging to class. SVM gives you distance to the boundary, you still need to convert it to probability somehow if you need probability. 
 For those problems, where SVM applies, it generally performs better than Random Forest. 
 SVM gives you ""support vectors"", that is points in each class closest to the boundary between classes. They may be of interest by themselves for interpretation.","SVM models perform better on sparse data than does trees in general.  For example in document classification you may have thousands, even tens of thousands of features and in any given document vector only a small fraction of these features may have a value greater than zero.  There are probably other differences between them, but this is what I found for my problems.","It really depends what you want to achieve, what your data look like and etc. SVM will generally perform better on linear dependencies, otherwise you need nonlinear kernel and choice of kernel may change results. Also, SVM are less interpretable - for e.g if you want to explain why the classification was like it was - it will be non-trivial. Decision trees have better interpretability, they work faster and if you have categorical/numerical variables its fine, moreover: non-linear dependencies are handled well (given N large enough). Also they train faster than SVM in general, but they have tendency to overfit... 
 I would also try Logistic Regression - great interpretable classifier) 
 To sum it up - the rule of thumb is try anything and compare what gives you best results/interpretation.","To complement the good points already stated: 
 From  Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?  random forests are more likely to achieve a better performance than SVMs. 
 Besides, the way algorithms are implemented (and for theoretical reasons) random forests are usually much faster than (non linear) SVMs. Indeed as @Ianenok, SVMs tend to be unusable beyond 10 000 data points. 
 However, SVMs are known to perform better on some specific datasets (images, microarray data...). 
 So, once again, cross validation is indeed the best way to know which method performs best. 
 Source :  Random forest vs SVM","CNNs outperform over SVMs. If data is not linearly separable, complex, and the dimension of the data is large. Though we can use kernel trick in SVM for nonlinear separable data, but computation time for large data sets remains where in CNNs computation time is reduced compared to SVMs.",,,,,65.30203766,52.15379271,53.55839581,68.82134096,55.85613629,,,,
6787,Are decision tree algorithms linear or nonlinear,machine-learning,"A decision tree is a non-linear mapping of  X  to  y . This is easy to see if you take an arbitrary function and create a tree to its maximum depth. 
 For example: 
 if x = 1, y = 1
if x = 2, y = 15
if x = 3, y = 3
if x = 4, y = 27
...
 
 Of course, this is a completely over-fit tree and won't generalize. But it demonstrates why a decision tree is a non-linear mapping.","Recently a friend of mine was asked whether decision tree algorithm a
  linear or nonlinear algorithm in an interview 
 
 Decision trees is a non-linear classifier like the neural networks, etc. It is generally used for classifying non-linearly separable data. 
 Even when you consider the regression example, decision tree is non-linear. 
 For example, a linear regression line would look somewhat like this: 
 
 The red dots are the data points. 
 And a decision tree regression plot would look something like this: 
 
 So, clearly  decision trees are non-linear","As many pointed out, a regression/decision tree is a non-linear model. Note however that it is a  piecewise linear  model: in each neighborhood (defined in a non-linear way), it is linear. In fact, the model is just a local constant.  
 To see this in the simplest case, with one variable, and with one node  $\theta$ , the tree can be written as a  linear regression : 
 $$y_i = \alpha_1 1(x_i < \theta) + \alpha_2 1(x_i \geq \theta) + \epsilon_i$$ 
 Where  $1(A)$  is the indicator function, taking value of 1 if the event A is true, and 0 otherwise.","A decision tree is a non-linear classifier. If your dataset contains consistent samples, namely you don't have the same input features and contradictory labels, decision trees can classify the data entirely and overfit it. To clarify more the  $VC$  dimension for decision trees is  $2^d$  which  $d$  is for the number of the binary features. Consequently, the Hypothesis space contains  $2^{2^d}$  different possibilities which can be dealt with using decision trees. One important point is the number of possible leaves that a decision tree can have. Suppose you have  $m$  samples with  $d$  binary features. Depending on the quantity of each, the number of possibilities is  $min(2^d, m)$ . Decision trees can overfit the training data-set no matter whether they are linearly separable or not, and that is why people use approaches like  ID3  or  C4.5  for pruning the tree or setting a threshold for the height and length of the trees in order not to overfit the data.","Decision trees are non linear. Unlike Linear regression there is no equation  to express relationship between independent and dependent variables. 
 Ex: 
 Linear regression - 
Price of fruit = b0 + b1*Freshness + b2*Size 
 Decision tree - 
Nodes:
Ripe - Yes or no | 
Fresh -  Yes or No | 
Size - <5, >5 but <10 and >10 | 
 In the second case there is no linear relationship between independent and dependent variables.",,,,,63.34378017,73.88330498,59.12476382,65.12973994,61.15359178,,,,
6776,Any idea about application of deep dream?,machine-learning,"There's already at least one application out, if you interpret 'application' broadly enough:  Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation by Hong, Noh and Han . They use it for  image segmentation . Standard image recognition networks can only give you a bounding box for each object recognized on an image. If you want to know which pixels constitute that object, you have to do image segmentation. 
 Basically, after finding a dog on an image, Hong et al's architecture back-propagates the dog-ness through the neural network down to the pixel level, to find the pixels that were the most responsible for the dog appearing. (They then use this heatmap as input for a supervised segmentation network, there's no deep dreaming in that part.) 
 This is already kind of an existence proof that the Deep Dream idea can be useful outside image manipulation. But I wouldn't downplay image manipulation itself either. I mention two things that are not immediate applications of Deep Dreaming, and we don't have them currently, but I can kinda see a plausible road from the original Deep Dream algorithm towards these: 
 
 Beautifying pictures and human faces and bodies. (Automating what a Photoshop retouch artist does.) 
 CSI-style image upscaling with fake but believable interpolated detail.","heres another application that is very new & just demonstrated within last few weeks. computers are filtering images to look like paintings in the distinctive style of different artists eg Van Gogh, Picasso, etc... and it seems possible since the technology can encompass different artistic styles it might be used for  forgery detection  in the art world at some point. (many very advanced analysis techniques are used in this area historically.) note that filtering methods are very popular on Instagram so it seems likely these will be commercially available at some point. 
 
 Computers can now paint like Van Gogh and Picasso  / Quartz 
 The science of Instagram success: Filtered photos are 21% more likely to be viewed - and warmer tones get more comments  / Dailymail","It is impossible to prove a negative, but other than using the same pattern detection system in general to detect shapes/images and replace them with other similar images, possibly for use in automatic image correction or similar, I don't think it has real potential outside of modifying pictures. 
 I may have to delete this answer if it is proven wrong.","Greyscale to Color 
 For example: 
 http://s15.postimg.org/3xq8jx03f/image.jpg 
 to 
 http://s15.postimg.org/i5fx8kcsb/image.jpg 
 http://s15.postimg.org/c5s64wrzv/image.jpg 
 The tree wood seems unnaturally red but still, it's not bad. This has worked but less impressively with other greyscale images I've tried.","Naritivly context aware, visual profanity filter. 
 In other worlds, rendering physically realistic and thematically/stylistically appropriate clothes on people who are insufficiently dressed, to make the image more family safe. 
 Thats the idea, however at the moment it's both unreliable and when it does work inaccurate. 
 However, more tweaking of the parameters of the dream than I have access to, or possibly just using more iterations and a lower ""octave"" value than I can specify should make the results much more reliable. 
 
 Examples: 
 Before:
 http://s22.postimg.org/5sjpqbzoh/image.jpg 
 After:
 http://s22.postimg.org/wew6fb3vl/image.jpg 
 . 
 http://s13.postimg.org/c4urz139j/image.jpg",,,,,72.6993917,51.49978431,50,50,53.79258521,,,,
6773,How to count observations per ID in R?,r,"Using the  data.table  structure (see the  wiki ), 
 library(data.table)
D <- data.table(x = c(1155, 1156, 1157, 1158),
                date = as.Date(c(""2010-05-02"", ""2010-05-05"", ""2010-05-08"", ""2010-05-11"")),
                y = c(2.7200, 2.6000, 2.6700, 3.5700),
                id = c(1, 3, 1, 2))
counts <- D[, .(rowCount = .N), by = id]
counts
 
 This will return 
 counts
##    id rowCount
## 1:  1        2
## 2:  3        1
## 3:  2        1","Another way is simply with the ""table"" function.  
 ids<-c(1,3,1,2)
counts<-data.frame(table(ids))
counts","OK if I understood correctly you can do something like:  
 df$observations <- rep(1, nrow(df))
df <- df[ ,-file_name_column]
new_data <- data.frame(aggregate(df, by= ID, FUN=sum))
 
 Caution: this might not work exactly since I am not sure what you data frame looks like.","aggregate() should work, as the previous answer suggests. Another option is with the  plyr package : 
 count(yourDF,c('id'))
 
 Using more columns in the vector with 'id' will subdivide the count.  
 I believe ddply() (also part of plyr) has a summarize argument which can also do this, similar to aggregate().","This is similar to Jeremy's but using  dplyr : 
 library(dplyr)
mytable <-
""a    date        b         id
 1155 2010-05-02  2.7200    1
 1156 2010-05-05  2.6000    3
 1157 2010-05-08  2.6700    1
 1158 2010-05-11  3.5700    2""

mytable <- read.delim(textConnection(mytable), header=TRUE,  sep="""")
mytable %>% count(id)","Function  rle  is also great to do that if you don't want to download  dplyr : 
 rle(as.vector(mytable $id))
rle(as.vector(mytable$ id))$lengths",,,,63.0162363,63.79794823,53.97938007,59.79302982,54.01226764,50,,,
6715,Is it necessary to standardize your data before clustering?,python,"Normalization is not always required, but it rarely hurts.  
 Some examples: 
 K-means : 
 
 K-means clustering is ""isotropic"" in all directions of space and
  therefore tends to produce more or less round (rather than elongated)
  clusters. In this situation leaving variances unequal is equivalent to
  putting more weight on variables with smaller variance. 
 
 Example in Matlab: 
 X = [randn(100,2)+ones(100,2);...
     randn(100,2)-ones(100,2)];

% Introduce denormalization
% X(:, 2) = X(:, 2) * 1000 + 500;

opts = statset('Display','final');

[idx,ctrs] = kmeans(X,2,...
                    'Distance','city',...
                    'Replicates',5,...
                    'Options',opts);

plot(X(idx==1,1),X(idx==1,2),'r.','MarkerSize',12)
hold on
plot(X(idx==2,1),X(idx==2,2),'b.','MarkerSize',12)
plot(ctrs(:,1),ctrs(:,2),'kx',...
     'MarkerSize',12,'LineWidth',2)
plot(ctrs(:,1),ctrs(:,2),'ko',...
     'MarkerSize',12,'LineWidth',2)
legend('Cluster 1','Cluster 2','Centroids',...
       'Location','NW')
title('K-means with normalization')
 
 
 
 (FYI:  How can I detect if my dataset is clustered or unclustered (i.e. forming one single cluster ) 
 Distributed clustering : 
 
 The comparative analysis shows that the distributed clustering results
  depend on the type of normalization procedure. 
 
 Artificial neural network (inputs) : 
 
 If the input variables are combined linearly, as in an MLP, then it is
  rarely strictly necessary to standardize the inputs, at least in
  theory. The reason is that any rescaling of an input vector can be
  effectively undone by changing the corresponding weights and biases,
  leaving you with the exact same outputs as you had before. However,
  there are a variety of practical reasons why standardizing the inputs
  can make training faster and reduce the chances of getting stuck in
  local optima. Also, weight decay and Bayesian estimation can be done
  more conveniently with standardized inputs. 
 
 Artificial neural network (inputs/outputs) 
 
 Should you do any of these things to your data? The answer is, it
  depends. 
 Standardizing either input or target variables tends to make the training
  process better behaved by improving the numerical condition (see 
   ftp://ftp.sas.com/pub/neural/illcond/illcond.html ) of the optimization
  problem and ensuring that various default values involved in
  initialization and termination are appropriate. Standardizing targets
  can also affect the objective function.  
 Standardization of cases should be approached with caution because it
  discards information. If that information is irrelevant, then
  standardizing cases can be quite helpful. If that information is
  important, then standardizing cases can be disastrous. 
 
 
 Interestingly, changing the measurement units may even lead one to see a very different clustering structure:  Kaufman, Leonard, and Peter J. Rousseeuw.. ""Finding groups in data: An introduction to cluster analysis."" (2005). 
 
 In some applications, changing the measurement units may even lead one
  to see a very different clustering structure. For example, the age (in
  years) and height (in centimeters) of four imaginary people are given
  in Table 3 and plotted in Figure 3. It appears that {A, B ) and { C,
  0) are two well-separated clusters. On the other hand, when height is
  expressed in feet one obtains Table 4 and Figure 4, where the obvious
  clusters are now {A, C} and { B, D}. This partition is completely
  different from the first because each subject has received another
  companion. (Figure 4 would have been flattened even more if age had
  been measured in days.) 
 To avoid this dependence on the choice of measurement units, one has
  the option of  standardizing the data. This converts the original
  measurements to unitless variables. 
 
 
 
 Kaufman et al.  continues with some interesting considerations (page 11): 
 
 From a philosophical point of view, standardization does not really
  solve the problem. Indeed, the choice of measurement units gives rise
  to relative weights of the variables. Expressing a variable in smaller
  units will lead to a larger range for that variable, which will then
  have a large effect on the resulting structure. On the other hand, by
  standardizing one attempts to give all variables an equal weight, in
  the hope of achieving objectivity. As such, it may be used by a
  practitioner who possesses no prior knowledge. However, it may well be
  that some variables are intrinsically more important than others in a
  particular application, and then the assignment of weights should be
  based on subject-matter knowledge (see, e.g., Abrahamowicz, 1985). On
  the other hand, there have been attempts to devise clustering
  techniques that are independent of the scale of the variables
  (Friedman and Rubin, 1967). The proposal of Hardy and Rasson (1982) is
  to search for a partition that minimizes the total volume of the
  convex hulls of the clusters. In principle such a method is invariant
  with respect to linear transformations of the data, but unfortunately
  no algorithm exists for its implementation (except for an
  approximation that is restricted to two dimensions). Therefore, the
  dilemma of standardization appears unavoidable at present and the
  programs described in this book leave the choice up to the user.","Standardizing data is recommended because otherwise the range of values in each feature will act as a weight when determining how to cluster data, which is typically undesired. 
 For example consider the standard metric for most clustering algorithms (including DBSCAN in sci-kit learn) --  euclidean , otherwise known as the L2 norm. If one of your features has a range of values much larger than the others, clustering will be completely dominated by that one feature. To illustrate this look at the simple example below: 
 >>> import numpy as np
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.metrics.pairwise import euclidean_distances
>>> 
>>> X = np.array([[1,2,100],[4,3,50],[1,1,75]])
>>> 
>>> print X
[[  1   2 100]
 [  4   3  50]
 [  1   1  75]]
>>> 
>>> print euclidean_distances(X)
[[  0.          50.0999002   25.01999201]
 [ 50.0999002    0.          25.25866188]
 [ 25.01999201  25.25866188   0.        ]]
>>> print euclidean_distances(StandardScaler().fit_transform(X))
[[ 0.          3.46410162  1.73205081]
 [ 3.46410162  0.          3.46410162]
 [ 1.73205081  3.46410162  0.        ]]
 
 From this you should see that the euclidean distances between the non-standardized versions are dominated by the third column because its range of values is much larger than the other two. However when the data is standardized this no longer becomes an issue and weights each feature as being equal when calculating the distance between each data point.","It isn't strictly necessary to standardise, whether it is required or not may depend on the distance metric you choose.  
 For instance if you opt for the Mahalanobis distance then separation will be based upon the number of standard deviations separating points and not the absolute distance between them, as such it is a scale invariant metric. 
 As with many things in machine learning there is no hard and fast answer and the only way to know for sure is to apply a variety of techniques see which gives the most appropriate results for your data.","I found in some cases useful to define a ""business evaluation"" function, defining the ""importance"" of the dimensions used for clustering. E.g. for greengrocer clustering the customers, if apples are twice as expensive as oranges, the number of apples will be doubled.","I was wondering the same thing, then I tried something very logical. I experimented with both the cases. 
 Here's a nice clustering plot, with round clusters, with scaling:
 
 Here's the clearly skewed clustering plot, one without scaling! 
 
 In the second plot, we can see 4 vertical planar clusters. Clustering algorithm k-means is completely dominated by the large product_mrp values here.",,,,,62.53277199,59.34029004,53.96193156,55.23588447,59.28711389,,,,
6691,Sentiment Analysis model for Spanish,machine-learning,"The  Indico.io API  supports Spanish (and
Chinese (Mandarin), Japanese, Italian, French, Russian, Arabic, German, English). 
 eg in Python: 
 >>> import indicoio
>>> indicoio.config.api_key = <YOUR_API_KEY>
>>> indicoio.sentiment(""¡Jamás voy a usar esta maldita aplicación!  No funciona para nada."")
0.02919392219306888
>>> indicoio.sentiment(""¡Es patrón!  La mejor que he visto.  Punto."")
0.8860221705630639
 
 If this isn't your area, then that's probably the easiest sort of solution to integrate.","Check the following links for Spanish sentiment analysis related links: 
 Working Model : http://dtminredis.housing.salle.url.edu:8080/EmoLib/es/ 
 Data:  http://www.daedalus.es/TASS2015/tass2015.php 
 API:  https://www.mashape.com/molinodeideas/sentiment-analysis-spanish","TM7 of The Netherlands has an advanced NLP engine named CARP that could do this for you. Their examples are largely in Dutch and English language, but they have established ontologies for Spanish for a number of their functional modules, including sentiment analysis.  
 Find the public 'playground' at  http://www.tm7.nl  . Select 'Carp Language Technologie' and follow the link in the blue box to the Playground. 
 Information about available APIs using SOAP services is available as well under the link 'SOAP Webservices' in English","You can try the python package
 sentiment-analysis-spanish 
 First to install the package: 
 pip install sentiment-analysis-spanish
 
 Import the package: 
 from sentiment_analysis_spanish import sentiment_analysis

 
 Run the sentiment analysis: 
 sentiment = sentiment_analysis.SentimentAnalysisSpanish()
print(sentiment.sentiment(""me gusta la tombola es genial""))

 
 You will see that it outputs: 
 0.9304396176531412","You can try  pysentimiento , a wrapper for transformer-based models: 
 from pysentimiento import SentimentAnalyzer
analyzer = SentimentAnalyzer(lang=""es"")

analyzer.predict(""Qué gran jugador es Messi"")
# returns SentimentOutput(output=POS, probas={POS: 0.998, NEG: 0.002, NEU: 0.000})
analyzer.predict(""Esto es pésimo"")
# returns SentimentOutput(output=NEG, probas={NEG: 0.999, POS: 0.001, NEU: 0.000})
analyzer.predict(""Qué es esto?"")
# returns SentimentOutput(output=NEU, probas={NEU: 0.993, NEG: 0.005, POS: 0.002})
 
 Disclaimer: I'm one of the developers of the library :-)","A lot of progress has been made on sentiment analysis in Spanish since you asked your question 6 years ago.
Today, you have very good Hugging Face Transformer based models, fine-tuned for sentiment analysis in Spanish.
For example see these models: 
 
 https://huggingface.co/sagorsarker/codeswitch-spaeng-sentiment-analysis-lince 
 https://huggingface.co/daveni/twitter-xlm-roberta-emotion-es 
 https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment 
 
 You also have ready-to-use APIs for sentiment analysis in Spanish such as: 
 
 NLP Cloud 
 https://www.repustate.com/spanish-sentiment-analysis/ 
 
 (Disclaimer: I am a developer of NLP Cloud)",,,,58.9823622,71.76463802,62.69984501,84.36417226,50.36047701,76.71650572,,,
6639,Extra output layer in a neural network (Decimal to binary),neural-network,"The question is asking you to make the following mapping between old representation and new representation: 
 Represent    Old                     New
0            1 0 0 0 0 0 0 0 0 0     0 0 0 0 
1            0 1 0 0 0 0 0 0 0 0     0 0 0 1 
2            0 0 1 0 0 0 0 0 0 0     0 0 1 0 

3            0 0 0 1 0 0 0 0 0 0     0 0 1 1 
4            0 0 0 0 1 0 0 0 0 0     0 1 0 0 
5            0 0 0 0 0 1 0 0 0 0     0 1 0 1 

6            0 0 0 0 0 0 1 0 0 0     0 1 1 0 
7            0 0 0 0 0 0 0 1 0 0     0 1 1 1 
8            0 0 0 0 0 0 0 0 1 0     1 0 0 0 

9            0 0 0 0 0 0 0 0 0 1     1 0 0 1
 
 Because the old output layer has a simple form, this is quite easy to achieve. Each output neuron should have a positive weight between itself and output neurons which should be on to represent it, and a negative weight between itself and output neurons that should be off. The values should combine to be large enough to cleanly switch on or off, so I would use largish weights, such as +10 and -10. 
 If you have sigmoid activations here, the bias is not that relevant. You just want to simply saturate each neuron towards on or off. The question has allowed you to assume very clear signals in the old output layer. 
 So taking example of representing a 3 and using zero-indexing for the neurons in the order I am showing them (these options are not set in the question), I might have weights going from activation of old output $i=3$, $A_3^{Old}$ to logit of new outputs $Z_j^{New}$, where $Z_j^{New} = \Sigma_{i=0}^{i=9} W_{ij} * A_i^{Old}$ as follows: 
 $$W_{3,0} = -10$$
$$W_{3,1} = -10$$
$$W_{3,2} = +10$$
$$W_{3,3} = +10$$ 
 This should clearly produce close to  0 0 1 1  output when only the old output layer's neuron representing a ""3"" is active. In the question, you can assume 0.99 activation of one neuron and <0.01 for competing ones in the old layer. So, if you use the same magnitude of weights throughout, then relatively small values coming from +-0.1 (0.01 * 10) from the other old layer activation values will not seriously affect the +-9.9 value, and the outputs in the new layer will be saturated at very close to either 0 or 1.","The code below from SaturnAPI answers this question. See and run the code at  https://saturnapi.com/artitw/neural-network-decimal-digits-to-binary-bitwise-conversion 
 % Welcome to Saturn's MATLAB-Octave API.
% Delete the sample code below these comments and write your own!

% Exercise from http://neuralnetworksanddeeplearning.com/chap1.html
% There is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01.

% Inputs from 3rd layer
xj = eye(10,10)

% Weights matrix
wj = [0 0 0 0 0 0 0 0 1 1 ;
      0 0 0 0 1 1 1 1 0 0 ;
      0 0 1 1 0 0 1 1 0 0 ;
      0 1 0 1 0 1 0 1 0 1 ]';

% Results
wj*xj


% Confirm results
integers = 0:9;
dec2bin(integers)","It follows from all previous ideas: A pure mathematical formula: 
 $$ \boxed{W_{j,k} = 10*(-1)^{(1 - ⌊j/2^k⌋\%2)} }$$ 
 Where  $ \boxed{W_{j,k}} $  is the weight pointing from Decimal  $\boxed{j}$  to binary digit in the position  $\boxed{k}$ : 
 Example:","Pythonic proof for the above exercise: 
 """"""
NEURAL NETWORKS AND DEEP LEARNING by Michael Nielsen

Chapter 1

http://neuralnetworksanddeeplearning.com/chap1.html#exercise_513527

Exercise:

There is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01.

""""""
import numpy as np


def sigmoid(x):
    return(1/(1+np.exp(-x)))


def new_representation(activation_vector):
    a_0 = np.sum(w_0 * activation_vector)
    a_1 = np.sum(w_1 * activation_vector)
    a_2 = np.sum(w_2 * activation_vector)
    a_3 = np.sum(w_3 * activation_vector)

    return a_3, a_2, a_1, a_0


def new_repr_binary_vec(new_representation_vec):
    sigmoid_op = np.apply_along_axis(sigmoid, 0, new_representation_vec)
    return (sigmoid_op > 0.5).astype(int)


w_0 = np.full(10, -1, dtype=np.int8)
w_0[[1, 3, 5, 7, 9]] = 1
w_1 = np.full(10, -1, dtype=np.int8)
w_1[[2, 3, 6, 7]] = 1
w_2 = np.full(10, -1, dtype=np.int8)
w_2[[4, 5, 6, 7]] = 1
w_3 = np.full(10, -1, dtype=np.int8)
w_3[[8, 9]] = 1

activation_vec = np.full(10, 0.01, dtype=np.float)
# correct number is 5
activation_vec[3] = 0.99

new_representation_vec = new_representation(activation_vec)
print(new_representation_vec)
# (-1.04, 0.96, -1.0, 0.98)
print(new_repr_binary_vec(new_representation_vec))
# [0 1 0 1]

# if you wish to convert binary vector to int
b = new_repr_binary_vec(new_representation_vec)
print(b.dot(2**np.arange(b.size)[::-1]))
# 5","A little modification to  FullStack 's answer regarding  Neil Slater 's comments using Octave: 
 % gzanellato
% Octave

% 3rd layer:
A = eye(10,10);

% Weights matrix:

fprintf('\nSet of weights:\n\n')

wij = [-10 -10 -10 -10 -10 -10 -10 -10 10 10;
       -10 -10 -10 -10 10 10 10 10 -10 -10;
       -10 -10 10 10 -10 -10 10 10 -10 -10;
       -10 10 -10 10 -10 10 -10 10 -10 10]

% Any bias between -9.999.. and +9.999.. runs ok

bias=5

Z=wij*A+bias;

% Sigmoid function:

for j=1:10;
  for i=1:4;
    Sigma(i,j)=int32(1/(1+exp(-Z(i,j))));
  end
end

fprintf('\nBitwise representation of digits:\n\n')

disp(Sigma')",,,,,60.41000932,67.64592439,58.59867576,58.10523837,51.37168528,,,,
6576,"I am a programmer, how do I get into field of Data Science?",beginner,"Focus less on gaining skills and more on gaining experience. Try to actually solve some problems and post your work on github. You'll learn more in the process and be able to demonstrate knowledge and experience to employers, which is much more valuable than having a supposedly deep understanding of a topic or theory.  
 Data Science is a pretty loaded field these days so I'm not sure what kind of work you specifically want to do, but assuming that machine learning is a component of it then kaggle.com is a good place to start. In terms of goals, if you're able to work with the data in pandas/numpy/scipy, build models in sci-kit learn and make some pretty graphs in seaborn, ggplot or even matplotlib then you won't have a problem getting a job from a skills perspective -- especially if you have code samples and examples to demonstrate your abilities. If you get stuck then stackexchange will either have the answer or you can post a questions and you'll have an answer shortly. Once you're doing the work for a living then you'll learn even more, likely from a senior team member who mentors you. 
 Best of luck.","I do like Berkeley course on Data Science, will give a good foundation and taste for Data Science, After moved to udacity and coursera and many more resources. So if you have Programming skills than will need math and stat and a lot of visualization. Also will be great to get used to IPython because is essential to see every step(visualize)how it perform instead writing a whole script and test after (anaconda is easy to install and work with). Course is listed bellow: 
bcourses.berkeley.edu/courses/1267848/wiki
also the stat i find good free course from SAS: Statistics 1: Introduction to ANOVA, Regression, and Logistic Regression support.sas.com/edu/schedules.html?ctry=us&id=1979 
 Starting with ML will recommend: www.kaggle.com/c/titanic/details/getting-started-with-python 
 on left side is also for Excel using Pivot tables  and R. DataCamp has released the tutorial on how to use R. Once you complete this steps than more competitions in gaining experience are on kaggle (recently released one for San Francisco Crime Classification) and ultimately amazing video tutorials from www.dataschool.io 
 hope it helps ...","Disagree with David, a true data scientist is an applied statistician who codes and knows how to use machine learning algorithms for the right reasons. Statistics is the base of all data science. It is the ""cake"" per se. Everything else is just icing.  
 The question is what kind of data scientist do you want to be? Do you want to be a master of the subject (knowledge of how, why, when and when not to apply an algorithm or technique) or a Kaggle Script Kiddie using Scipy and thinking that he is a Data Scientist?  
 1 - Stats 
 2- Everything else","If you want to be a practical person with true knowledge, start with math(calculus, probability + stat, linear algebra). On every step try to implement everything with programing, python is nice for this. When u get good ground, play with real data and solve for problems 
 Courses.
Linear algebra - edx Laff or coding the matrix
Stat - edx stat 2x Barkley
Calculus - read...its simple","David has a good point , I would suggest you focus on whatever it is that drives your interest more. It's the only way to succeed in every kind of effort. If you want to build something cool start with it. If you want to read a book thats good too. The starting point doesn't matter. A few days ahead you will have a better understanding on what you want and should do next.","Data Science is so broad, there's many different paths to get into it. It is usually split into 4 or 5 different types for example: 
 
 You could see from the other posts in this topic people coming from an Applied Statistics background (applying the right algorithm), Programming background (participating in Kaggle), and others applying it to a business background 
 Savvy companies could refer to a programming skewed person as a ""Data Engineer"". Big companies also use each type for their data science team, so demonstrating good T-shaped skills would be a good thing.","If you are a programmer, you could start with a Decision Tree classifier, focus on understanding the math behind Entropy and Information-Gain. It is essential to understand that ML is just all about data compression. 
 I'd highly disagree with some of the other answers on the value of practical courses. 
Most valuable for ML is math: number theory, linear algebra and probability theory. 
 If you don't focus on math, the only thing that you will learn is, how to use some library for doing magic, that's not machine learning and not science at all.","Basic courses like Andrew ng  Machine Learning on Coursera  or the  Introduction to Statistical Learning (free) book  would be my recommandations for a first step in Data Science / Machine Learning. They both cover basic statistical concepts and main modelling traps and are enough to start your first projects. 
 Then I would suggest you to find a domain of application, learn about it and put your knowledge into work. Depending on what you want to achieve you will have plenty of occasions to dive into specific fields if needed (a given library, advanced statistics, domain oriented tools like for NLP or computer vision...).",,55.39511907,55.56169833,55.15598832,51.52299659,50,57.88334906,55.35000606,54.81899544,
6547,Open source Anomaly Detection in Python,machine-learning,"Anomaly Detection or Event Detection can be done in different ways: 
 Basic Way 
 Derivative! If the deviation of your signal from its past & future is high you most probably have an event. This can be extracted by finding large zero crossings in derivative of the signal. 
 Statistical Way 
 Mean of anything is its usual, basic behavior. if something deviates from mean it means that it's an event. Please note that mean in time-series is not that trivial and is not a constant but changing according to changes in time-series so you need to see the  ""moving average""  instead of average. It looks like this: 
 
 The Moving Average code can be found  here . In signal processing terminology you are applying a  ""Low-Pass""  filter by applying the moving average. 
 You can follow the code bellow: 
 MOV = movingaverage(TimeSEries,5).tolist()
STD = np.std(MOV)
events= []
ind = []
for ii in range(len(TimeSEries)):
    if TimeSEries[ii] > MOV[ii]+STD:
        events.append(TimeSEries[ii])
 
 Probabilistic Way 
 They are more sophisticated specially for people new to Machine Learning. Kalman Filter is a great idea to  find the anomalies . Simpler probabilistic approaches using  ""Maximum-Likelihood Estimation""  also work well but my suggestion is to stay with moving average idea. It works in practice very well. 
 I hope I could help :)
Good Luck!","I recently developed a toolbox:  Py thon  O utlier  D etection toolbox ( PyOD ). See  GitHub . 
 It is designed for identifying outlying objects in data with both unsupervised and supervised approaches. PyOD is featured for: 
 
 Unified APIs, detailed documentation, and interactive examples across
various algorithms. 
 Advanced models, including Neural Networks/Deep
Learning and Outlier Ensembles. 
 Optimized performance with JIT and
parallelization when possible, using numba and joblib.
Compatible
with both Python 2 & 3 (scikit-learn compatible as well). 
 
 Here are some important links: 
 
 Github 
 
 PyPI 
 
 Documentation 
 
 Interactive Jupyter Notebooks 
 
 
 If you use PyOD in a scientific publication, we would appreciate
citations to the following paper 
 @article{zhao2019pyod,
  title={PyOD: A Python Toolbox for Scalable Outlier Detection},
  author={Zhao, Yue and Nasrullah, Zain and Li, Zheng},
  journal={arXiv preprint arXiv:1901.01588},
  year={2019},
  url={https://arxiv.org/abs/1901.01588}
}
 
 It is currently under review at  JMLR 
(machine learning open-source software track). See  preprint . 
 
 Quick Introduction 
 PyOD toolkit consists of three major groups of functionalities: (i) outlier
detection algorithms; (ii) outlier ensemble frameworks and (iii) outlier
detection utility functions. 
 Individual Detection Algorithms : 
 
 PCA : Principal Component Analysis (the sum of weighted projected distances to the eigenvector hyperplanes) 
 MCD : Minimum Covariance Determinant (use the mahalanobis distances as the outlier scores) 
 OCSVM : One-Class Support Vector Machines 
 LOF : Local Outlier Factor 
 CBLOF : Clustering-Based Local Outlier Factor 
 LOCI : LOCI: Fast outlier detection using the local correlation integral 
 HBOS : Histogram-based Outlier Score 
 kNN : k Nearest Neighbors (use the distance to the kth nearest neighbor as the - **outlier score 
 AvgKNN : Average kNN (use the average distance to k nearest neighbors as the outlier score) 
 MedKNN : Median kNN (use the median distance to k nearest neighbors as the outlier score) 
 ABOD : Angle-Based Outlier Detection 
 FastABOD : Fast Angle-Based Outlier Detection using approximation 
 SOS : Stochastic Outlier Selection 
 IForest : Isolation Forest 
 Feature Bagging 
 LSCP : LSCP: Locally Selective Combination of Parallel Outlier Ensembles 
 XGBOD : Extreme Boosting Based Outlier Detection (Supervised) 
 AutoEncoder : Fully connected AutoEncoder (use reconstruction error as the outlier score) 
 SO_GAAL : Single-Objective Generative Adversarial Active Learning 
 MO_GAAL : Multiple-Objective Generative Adversarial Active Learning 
 
 Outlier Detector/Scores Combination Frameworks : 
 
 Feature Bagging 
 LSCP : LSCP: Locally Selective Combination of Parallel Outlier Ensembles 
 Average : Simple combination by averaging the scores 
 Weighted Average : Simple combination by averaging the scores with detector weights 
 Maximization : Simple combination by taking the maximum scores 
 AOM : Average of Maximum 
 MOA : Maximization of Average 
 
 Utility Functions for Outlier Detection : 
 
 score_to_lable() : convert raw outlier scores to binary labels 
 precision_n_scores() : one of the popular evaluation metrics for outlier
mining (precision @ rank n) 
 generate_data() : generate pseudo data for outlier detection experiment 
 wpearsonr() : weighted pearson is useful in pseudo ground truth generation 
 
 Comparison of all implemented models  are made available below:
( Figure ,
 Code ,
 Jupyter Notebooks ): 
 If you are interested, please check  Github  for more information.","h2o has an  anomaly detection module  and traditionally the code is available in R.However beyond version 3 it has similar module available in python as well,and since h2o is open source it might fit your bill. 
 You can see an working example over  here 
 import sys
sys.path.insert(1,""../../../"")
import h2o

def anomaly(ip, port):
    h2o.init(ip, port)

    print ""Deep Learning Anomaly Detection MNIST""

    train = h2o.import_frame(h2o.locate(""bigdata/laptop/mnist/train.csv.gz""))
    test = h2o.import_frame(h2o.locate(""bigdata/laptop/mnist/test.csv.gz""))

    predictors = range(0,784)
    resp = 784

    # unsupervised -> drop the response column (digit: 0-9)
    train = train[predictors]
    test = test[predictors]

    # 1) LEARN WHAT'S NORMAL
    # train unsupervised Deep Learning autoencoder model on train_hex
    ae_model = h2o.deeplearning(x=train[predictors], training_frame=train, activation=""Tanh"", autoencoder=True,
                                hidden=[50], l1=1e-5, ignore_const_cols=False, epochs=1)

    # 2) DETECT OUTLIERS
    # anomaly app computes the per-row reconstruction error for the test data set
    # (passing it through the autoencoder model and computing mean square error (MSE) for each row)
    test_rec_error = ae_model.anomaly(test)

    # 3) VISUALIZE OUTLIERS
    # Let's look at the test set points with low/median/high reconstruction errors.
    # We will now visualize the original test set points and their reconstructions obtained
    # by propagating them through the narrow neural net.

    # Convert the test data into its autoencoded representation (pass through narrow neural net)
    test_recon = ae_model.predict(test)

    # In python, the visualization could be done with tools like numpy/matplotlib or numpy/PIL

if __name__ == '__main__':
    h2o.run_test(sys.argv, anomaly)","I am currently on same stage like you. I am finding best option for anomaly detection, doing some research. 
 What I have found is I think best matches your need and is better compare to what you have seen. i.e., TwitterAnomalyDetection, SkyLine. 
 I have found better is Numenta's NAB (Numenta Anomaly Benchmark).
It also have a very good community support and for you plus point is its open source & developed in Python. You can add your algorithm in it. 
 In case of algorithm, I found LOF, or CBLOF are good option. 
 So,  check it out  once. It may help you out. 
 If you found better option, please share.","I assume the feature you use to detect abnormality is one row of data in a logfile. If so, Sklearn is your good friend and you can use it as a blackbox. Check the tutorial of  one-class SVM  and  Novelty detection .  
 However, in case that your feature is an entire logfile, you need to first summarize it to some feature of same dimension, and then apply Novealty detection.","There is still an active and developed version of Skyline, just in case someone lands here and is interested. 
 Skyline  ( documentation ) 
 I am the current maintainer of the project and it is now a lot more advanced than the original Etsy version, in terms of performance, UI, better handling of seasonality and has the added functionalities of an anomalies database, calculating correlations and the ability to fingerprint and learn not anomalous patterns.","Since you have multivariate time series, I would go for a LSTM-RNN implementation that models the dynamics of your system based on training data, which are usually semi-supervised (only normal class included). This means that you train your model to learn what is ""normal"". During testing, you test both normal and anomalous conditions to see how well the model tells them apart.  
 An advantage of neural networks is that they ""learn"" the cross-correlations between input signals by themselves; you do not need to explore them manually. LSTM-RNNs, in particular, are an ideal choice when it comes to time series modelling simply because of their ability to keep memory of previous inputs, similar to a state space model in Control Theory (if you see the analogy). 
 In Python, it is almost trivial to implement an LSTM-RNN using  Keras  API (on top of Tensorflow backend). This network learns to estimate the signal(s) of interest given an arbitrary number of inputs, which you thereafter compare with the actual measured value. If there is ""big"" deviation, you got an anomaly (given that the model is accurate enough)!",,,56.35576169,54.62677177,63.80965544,67.15241539,56.58361066,54.1434229,52.56993939,,
6318,Data science projects explained step by step?,machine-learning,"If you want an application-oriented book, consider Christopher Bishop's  Model-Based Machine Learning . He has more technical books that are well regarded. 
 If you are looking for lots of code,  Probabilistic Programming & Bayesian Methods for Hackers  is an option. 
 Another introductory book with a more statistical bent is  An Introduction to Statistical Learning with Applications in R . Again, the authors have a well-regarded technical version of the book.","I can recommend this collection of Ipython Notebooks which includes Data Science, Statistics and Machine Learning commented notebooks. 
 https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks",One place you might find some interesting step-by-step explainations is the  Kaggle tutorial and winner's interviews . Often people will post a detailed summary of their approach.,"I had the same question a few weeks ago. 
 I personally found O'Reilly's  Python for Data Analysis  very useful in learning the basics. The book assumes you have some python programming experience, but it also has an appendix in the back to go through the basics. 
 The author gives you a wide variety of real world (not Monty Python) examples in the beginning you can create within the first few chapters, then goes into detail about each thing as the book goes on, building your knowledge. 
 I found the instructions very easy and step by step. My professor who is my guide in all this was impressed how quickly I learned.  
 I also have heard good things about Kaggle.","Data Science in the Cloud with Microsoft Azure Machine Learning and R  is a free textbook which works through an example in great detail. Don't be put off by the particular tools used as you don't need them to get some benefit out of the book.  
 Another one which I enjoyed is  Programming Collective Intelligence  which also goes through a number of projects in detail, including the web scraping part which most books gloss over.","One of the best book I have came across is  Machine Learning in Python  from Sebastian Raschka. Easy examples, step by step explanation and just right amount of math. 
 The structure of book covers the whole process from data cleaning to ensembling and evaluation.","Have a look at :  
 https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/ 
 This has a step by step tutorial which will give you an idea of the entire process of Data Exploration, Data Analysis and Building  a predictive model.  
 Explanation regarding Data Exploration and Feature Engineering ( how to choose relevant features ) is here :  
 https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/ 
 See the first 5 Datasets here which have tutorials and work on them to get practical experience: 
 https://www.analyticsvidhya.com/blog/2016/10/17-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/ 
 Also have a look at :  
 http://machinelearningmastery.com/machine-learning-in-python-step-by-step/ 
 where he uses multiple models on a single dataset which will give you a basic level understanding of different models. 
 For understanding more about the model selection have a look at this :  
 https://www.quora.com/Data-Science-How-do-Data-Scientists-perform-model-selection 
 The above link has answers given by people working in the field. 
 To get insights on different datasets you can always log into kaggle and go through competitions and have a look at the wide range of datasets, where you have access to people's code in kernels. The forums in Kaggle are helpful as people discuss about using different models for a problems and their approach.  
 https://www.kaggle.com/",,,50,55.92081718,66.53644613,57.99229404,54.91353892,61.5935114,56.35519819,,
6107,What are deconvolutional layers?,neural-network,"Deconvolution layer  is a very unfortunate name and should rather be called a  transposed convolutional layer . 
 Visually, for a transposed convolution with stride one and no padding, we just pad the original input (blue entries) with zeroes (white entries) (Figure 1). 
 
 In case of stride two and padding, the transposed convolution would look like this (Figure 2):  
 
 All credits for the great visualisations go to 
 
 Vincent Dumoulin, Francesco Visin -  A guide to convolution arithmetic for deep learning 
 
 You can find more visualisations of convolutional arithmetics  here .","I think one way to get a really basic level intuition behind convolution is that you are sliding K filters, which you can think of as K stencils, over the input image and produce K activations - each one representing a degree of match with a particular stencil. The inverse operation of that would be to take K activations and expand them into a preimage of the convolution operation. The intuitive explanation of the inverse operation is therefore, roughly, image reconstruction given the stencils (filters) and activations (the degree of the match for each stencil) and therefore at the basic intuitive level we want to blow up each activation by the stencil's mask and add them up. 
 Another way to approach understanding deconv would be to examine the deconvolution layer implementation in Caffe, see the following relevant bits of code: 
 DeconvolutionLayer<Dtype>::Forward_gpu
ConvolutionLayer<Dtype>::Backward_gpu
CuDNNConvolutionLayer<Dtype>::Backward_gpu
BaseConvolutionLayer<Dtype>::backward_cpu_gemm
 
 You can see that it's implemented in Caffe exactly as backprop for a regular forward convolutional layer (to me it was more obvious after i compared the implementation of backprop in cuDNN conv layer vs ConvolutionLayer::Backward_gpu implemented using GEMM). So if you work through how backpropagation is done for regular convolution you will understand what happens on a mechanical computation level. The way this computation works matches the intuition described in the first paragraph of this blurb. 
 
 However, I don't know how the learning of convolutional layers works. (I understand how simple MLPs learn with gradient descent, if that helps). 
 
 To answer your other question inside your first question, there are two main differences between MLP backpropagation (fully connected layer) and convolutional nets: 
 1) the influence of weights is localized, so first figure out how to do backprop for, say a 3x3 filter convolved with a small 3x3 area of an input image, mapping to a single point in the result image. 
 2) the weights of convolutional filters are shared for spatial invariance. What this means in practice is that in the forward pass the same 3x3 filter with the same weights is dragged through the entire image with the same weights for forward computation to yield the output image (for that particular filter). What this means for backprop is that the backprop gradients for each point in the source image are summed over the entire range that we dragged that filter during the forward pass. Note that there are also different gradients of loss wrt x, w and bias since dLoss/dx needs to be backpropagated, and dLoss/dw is how we update the weights. w and bias are independent inputs in the computation DAG (there are no prior inputs), so there's no need to do backpropagation on those. 
 (my notation here assumes that convolution is y = x*w+b where '*' is the convolution operation)","Step by step math explaining how transpose convolution does 2x upsampling with 3x3 filter and stride of 2: 
 
 The simplest TensorFlow snippet to validate the math:
 
 import tensorflow as tf
import numpy as np

def test_conv2d_transpose():
    # input batch shape = (1, 2, 2, 1) -> (batch_size, height, width, channels) - 2x2x1 image in batch of 1
    x = tf.constant(np.array([[
        [[1], [2]], 
        [[3], [4]]
    ]]), tf.float32)

    # shape = (3, 3, 1, 1) -> (height, width, input_channels, output_channels) - 3x3x1 filter
    f = tf.constant(np.array([
        [[[1]], [[1]], [[1]]], 
        [[[1]], [[1]], [[1]]], 
        [[[1]], [[1]], [[1]]]
    ]), tf.float32)

    conv = tf.nn.conv2d_transpose(x, f, output_shape=(1, 4, 4, 1), strides=[1, 2, 2, 1], padding='SAME')

    with tf.Session() as session:
        result = session.run(conv)

    assert (np.array([[
        [[1.0], [1.0],  [3.0], [2.0]],
        [[1.0], [1.0],  [3.0], [2.0]],
        [[4.0], [4.0], [10.0], [6.0]],
        [[3.0], [3.0],  [7.0], [4.0]]]]) == result).all()","The  notes that accompany Stanford CS class CS231n :  Convolutional Neural Networks for Visual Recognition, by Andrej Karpathy , do an excellent job of explaining convolutional neural networks. 
 Reading this paper should give you a rough idea about: 
 
 Deconvolutional Networks 
Matthew D. Zeiler, Dilip Krishnan, Graham W. Taylor and Rob Fergus
Dept. of Computer Science, Courant Institute, New York University 
 
 These  slides  are great for Deconvolutional Networks.","Just found a great article from the theaon website on this topic [1]: 
 
 The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, [...] to project feature maps to a higher-dimensional space.
[...]
i.e., map from a 4-dimensional space to a 16-dimensional space, while keeping the connectivity pattern of the convolution. 
 Transposed convolutions  – also called fractionally strided convolutions – work by swapping the forward and backward passes of a convolution. One way to put it is to note that the kernel defines a convolution, but whether it’s a direct convolution or a transposed convolution is determined by how the forward and backward passes are computed. 
 The transposed convolution operation can be thought of as the gradient of some convolution with respect to its input, which is usually how transposed convolutions are implemented in practice. 
 Finally note that it is always possible to implement a transposed convolution with a direct convolution. The disadvantage is that it usually involves adding many columns and rows of zeros to the input, resulting in a much less efficient implementation. 
 
 So in simplespeak, a ""transposed convolution"" is mathematical operation using matrices (just like convolution) but is more efficient than the normal convolution operation in the case when you want to go back from the convolved values to the original (opposite direction). This is why it is preferred in implementations to convolution when computing the opposite direction (i.e. to avoid many unnecessary 0 multiplications caused by the sparse matrix that results from padding the input). 
 Image ---> convolution ---> Result 
 Result ---> transposed convolution ---> ""originalish Image"" 
 Sometimes you save some values along the convolution path and reuse that information when ""going back"": 
 Result ---> transposed convolution ---> Image 
 That's probably the reason why it's wrongly called a ""deconvolution"". However, it does have something to do with the matrix transpose of the convolution (C^T), hence the more appropriate name ""transposed convolution"". 
 So it makes a lot of sense when considering computing cost. You'd pay a lot more for amazon gpus if you wouldn't use the transposed convolution. 
 Read and watch the animations here carefully:
 http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#no-zero-padding-unit-strides-transposed 
 Some other relevant reading: 
 
 The transpose (or more generally, the Hermitian or conjugate transpose) of a filter is simply the matched filter[3]. This is found by time reversing the kernel and taking the conjugate of all the values[2]. 
 
 I am also new to this and would be grateful for any feedback or corrections. 
 [1]  http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html 
 [2]  http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic 
 [3]  https://en.wikipedia.org/wiki/Matched_filter","We could use PCA for analogy. 
 When using conv, the forward pass is to extract the coefficients of principle components from the input image, and the backward pass (that updates the input) is to use (the gradient of) the coefficients to reconstruct a new input image, so that the new input image has PC coefficients that better match the desired coefficients. 
 When using deconv, the forward pass and the backward pass are reversed. The forward pass tries to reconstruct an image from PC coefficients, and the backward pass updates the PC coefficients given (the gradient of) the image. 
 The deconv forward pass does exactly the conv gradient computation given in  this post . 
 That's why in the caffe implementation of deconv (refer to Andrei Pokrovsky's answer), the deconv forward pass calls  backward_cpu_gemm() , and the backward pass calls  forward_cpu_gemm() .","Convolutions from a DSP perspective 
 I'm a bit late to this but still would like to share my perspective and insights. My background is theoretical physics and digital signal processing. In particular I studied wavelets and convolutions are almost in my backbone ;) 
 The way people in the deep learning community talk about convolutions was also confusing to me. From my perspective what seems to be missing is a proper separation of concerns. I will explain the deep learning convolutions using some DSP tools. 
 Disclaimer 
 My explanations will be a bit hand-wavy and not mathematical rigorous in order to get the main points across. 
 
 Definitions 
 Let's define a few things first. I limit my discussion to one dimensional (the extension to more dimension is straight forward) infinite (so we don't need to mess with boundaries) sequences  $x_n = \{x_n\}_{n=-\infty}^{\infty} = \{\dots, x_{-1}, x_{0}, x_{1}, \dots \}$ . 
 A pure (discrete) convolution between two sequences  $y_n$  and  $x_n$  is defined as 
 $$ (y * x)_n = \sum_{k=-\infty}^{\infty} y_{n-k} x_k $$ 
 If we write this in terms of matrix vector operations it looks like this (assuming a simple kernel  $\mathbf{q} = (q_0,q_1,q_2)$  and vector  $\mathbf{x} = (x_0, x_1, x_2, x_3)^T$ ): 
 $$ \mathbf{q} * \mathbf{x} =
   \left( \begin{array}{cccc}
   q_1 & q_0 &  0  &  0  \\
   q_2 & q_1 & q_0 &  0  \\
    0  & q_2 & q_1 & q_0 \\
    0  &  0  & q_2 & q_1 \\
   \end{array} \right)
   \left( \begin{array}{cccc}
   x_0 \\ x_1 \\ x_2 \\ x_3
   \end{array} \right) 
$$ 
 Let's introduce the down- and up-sampling operators,  $\downarrow$  and  $\uparrow$ , respectively. Downsampling by factor  $k \in \mathbb{N}$  is removing all samples except  every k-th one: 
 $$ \downarrow_k\!x_n = x_{nk} $$ 
 And upsampling by factor  $k$  is interleaving  $k-1$  zeros between the samples: 
 $$ \uparrow_k\!x_n = \left \{ \begin{array}{ll}
   x_{n/k}  & n/k \in \mathbb{Z} \\
   0        & \text{otherwise}
   \end{array} \right.
$$ 
 E.g. we have for  $k=3$ : 
 $$ \downarrow_3\!\{ \dots, x_0, x_1, x_2, x_3, x_4, x_5, x_6, \dots \} = \{ \dots, x_0, x_3, x_6, \dots \} $$ 
 $$ \uparrow_3\!\{ \dots, x_0, x_1, x_2, \dots \} = \{ \dots x_0, 0, 0, x_1, 0, 0, x_2, 0, 0, \dots \} $$ 
 or written in terms of matrix operations (here  $k=2$ ): 
 $$ \downarrow_2\!x = 
   \left( \begin{array}{cc}
   x_0 \\ x_2
   \end{array} \right) =
   \left( \begin{array}{cccc}
   1 & 0 & 0 & 0 \\
   0 & 0 & 1 & 0 \\
   \end{array} \right)
   \left( \begin{array}{cccc}
   x_0 \\ x_1 \\ x_2 \\ x_3
   \end{array} \right) 
$$ 
 and 
 $$ \uparrow_2\!x = 
   \left( \begin{array}{cccc}
   x_0 \\ 0 \\ x_1 \\ 0
   \end{array} \right) =
   \left( \begin{array}{cc}
   1 & 0 \\
   0 & 0 \\
   0 & 1 \\
   0 & 0 \\
   \end{array} \right)
   \left( \begin{array}{cc}
   x_0 \\ x_1
   \end{array} \right) 
$$ 
 As one can already see, the down- and up-sample operators are mutually transposed, i.e.  $\uparrow_k = \downarrow_k^T$ . 
 
 Deep Learning Convolutions by Parts 
 Let's look at the typical convolutions used in deep learning and how we write them. Given some kernel  $\mathbf{q}$  and vector  $\mathbf{x}$  we have the following: 
 
 a strided convolution with stride  $k$  is  $\downarrow_k\!(\mathbf{q} * \mathbf{x})$ , 
 a dilated convolution with factor  $k$  is  $(\uparrow_k\!\mathbf{q}) * \mathbf{x}$ , 
 a transposed convolution with stride  $k$  is  $ \mathbf{q} * (\uparrow_k\!\mathbf{x})$ 
 
 Let's rearrange the transposed convolution a bit:
 $$
  \mathbf{q} * (\uparrow_k\!\mathbf{x}) \; = \;
  \mathbf{q} * (\downarrow_k^T\!\mathbf{x}) \; = \;
  (\uparrow_k\!(\mathbf{q}*)^T)^T\mathbf{x}
$$ 
 In this notation  $(\mathbf{q}*)$  must be read as an operator, i.e. it abstracts convolving something with kernel  $\mathbf{q}$ .
Or written in matrix operations (example): 
 $$
  \begin{align}
  \mathbf{q} * (\uparrow_k\!\mathbf{x}) & =
   \left( \begin{array}{cccc}
   q_1 & q_0 &  0  &  0  \\
   q_2 & q_1 & q_0 &  0  \\
    0  & q_2 & q_1 & q_0 \\
    0  &  0  & q_2 & q_1 \\
   \end{array} \right)
   \left( \begin{array}{cc}
   1 & 0 \\
   0 & 0 \\
   0 & 1 \\
   0 & 0 \\
   \end{array} \right)
   \left( \begin{array}{c}
   x_0\\
   x_1\\
   \end{array} \right)
  \\ & =
   \left( \begin{array}{cccc}
   q_1 & q_2 &  0  &  0  \\
   q_0 & q_1 & q_2 &  0  \\
    0  & q_0 & q_1 & q_2 \\
    0  &  0  & q_0 & q_1 \\
   \end{array} \right)^T
   \left( \begin{array}{cccc}
   1 & 0 & 0 & 0\\
   0 & 0 & 1 & 0\\
   \end{array} \right)^T
   \left( \begin{array}{c}
   x_0\\
   x_1\\
   \end{array} \right)
  \\ & =
   \left(
   \left( \begin{array}{cccc}
   1 & 0 & 0 & 0\\
   0 & 0 & 1 & 0\\
   \end{array} \right)
   \left( \begin{array}{cccc}
   q_1 & q_2 &  0  &  0  \\
   q_0 & q_1 & q_2 &  0  \\
    0  & q_0 & q_1 & q_2 \\
    0  &  0  & q_0 & q_1 \\
   \end{array} \right)
   \right)^T
   \left( \begin{array}{c}
   x_0\\
   x_1\\
   \end{array} \right)
   \\ & = (\uparrow_k\!(\mathbf{q}*)^T)^T\mathbf{x}
  \end{align}
$$ 
 As one can see the is the transposed operation, thus, the name. 
 Connection to Nearest Neighbor Upsampling 
 Another common approach found in convolutional networks is upsampling with some built-in form of interpolation. Let's take upsampling by factor 2 with a simple repeat interpolation.
This can be written as  $\uparrow_2\!(1\;1) * \mathbf{x}$ . If we also add a learnable kernel  $\mathbf{q}$  to this we have  $\uparrow_2\!(1\;1) * \mathbf{q} * \mathbf{x}$ . The convolutions can be combined, e.g. for  $\mathbf{q}=(q_0\;q_1\;q_2)$ , we have  $$(1\;1) * \mathbf{q} = (q_0\;\;q_0\!\!+\!q_1\;\;q_1\!\!+\!q_2\;\;q_2),$$ 
 i.e. we can replace a repeat upsampler with factor 2 and a convolution with a kernel of size 3 by a transposed convolution with kernel size 4. This transposed convolution has the same ""interpolation capacity"" but would be able to learn better matching interpolations. 
 
 Conclusions and Final Remarks 
 I hope I could clarify some common convolutions found in deep learning a bit by taking them apart in the fundamental operations. 
 I didn't cover pooling here. But this is just a nonlinear downsampler and can be treated within this notation as well.","I had a lot of trouble understanding what exactly happened in the paper until I came across this blog post:  http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/ 
 Here is a summary of how I understand what is happening in a 2x upsampling: 
 Information from paper 
 
 What is upsampling?

 
 ""upsampling with factor f is convolution with a fractional input stride of 1/f"" 
 → fractionally strided convolutions are also known as transposed convolution according to e.g.  http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html 
 
 What are the parameters of that convolution?

 
 factor f = 2  
 → input stride of 1/f = 1/2 
 → kernel size = 2 * factor - factor %2 = 2*2 -0 = 4 according to  http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/   
 
 Are the weights fixed or trainable?

 
 The paper states ""we initialize the 2x upsampling to bilinear interpolation, but allow the parameters to be learned [...]"". 
 However, the corresponding  github page  states ""In our original experiments the interpolation layers were initialized to bilinear kernels and then learned. In follow-up experiments, and this reference implementation, the bilinear kernels are fixed"" 
 → fixed weights 
 
 
 Simple example 
 
 imagine the following input image: 
 
 
 
 Fractionally strided convolutions work by inserting factor-1 = 2-1 = 1 zeros in between these values and then assuming stride=1 later on. Thus, you receive the following 6x6 padded image 
 
 
 
 The bilinear 4x4 filter looks like this. Its values are chosen such that the used weights (=all weights not being multiplied with an inserted zero) sum up to 1. Its three unique values are 0.56, 0.19 and 0.06. Moreover, the center of the filter is per convention the pixel in the third row and third column. 
 
 
 
 Applying the 4x4 filter on the padded image (using padding='same' and stride=1) yields the following 6x6 upsampled image: 
 
 
 
 This kind of upsampling is performed for each channel individually (see line 59 in  https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py ). At the end, the 2x upsampling is really a very simple resizing using bilinear interpolation and conventions on how to handle the borders. 16x or 32x upsampling works in much the same way, I believe.","In addition to David Dao's answer: It is also possible to think the other way around. Instead of focusing on which (low resolution) input pixels are used to produce a single output pixel, you can also focus on which individual input pixels contribute to which region of output pixels.  
 This is done in  this distill publication , including a series of very intuitive and interactive visualizations. One advantage of thinking in this direction is that explaining checkerboard artifacts becomes easy.",60.91547801,55.99676586,50,60.18336597,51.10063603,50,50,50.61984942,50
6048,Should I use a decision tree or logistic regression for classification?,classification,"Long story short :  do what @untitledprogrammer said, try both models and cross-validate to help pick one. 
 Both decision trees (depending on the implementation, e.g. C4.5) and logistic regression should be able to handle continuous and categorical data just fine. For logistic regression, you'll want to  dummy code your categorical variables . 
 As @untitledprogrammer mentioned, it's difficult to know a priori which technique will be better based simply on the types of features you have, continuous or otherwise. It really depends on your specific problem and the data you have. (See  No Free Lunch Theorem ) 
 You'll want to keep in mind though that a logistic regression model is searching for a single linear decision boundary in your feature space, whereas a decision tree is essentially partitioning your feature space into half-spaces using  axis-aligned  linear decision boundaries. The net effect is that you have a non-linear decision boundary, possibly more than one.  
 This is nice when your data points aren't easily separated by a single hyperplane, but on the other hand, decisions trees are so flexible that they can be prone to overfitting. To combat this, you can try pruning. Logistic regression tends to be less susceptible (but not immune!) to overfitting. 
 Lastly, another thing to consider is that decision trees can automatically take into account interactions between variables, e.g. $xy$ if you have two independent features $x$ and $y$. With logistic regression, you'll have to manually add those interaction terms yourself. 
 So you have to ask yourself:  
 
 what kind of decision boundary makes more sense in your particular problem? 
 how do you want to balance bias and variance? 
 are there interactions between my features? 
 
 Of course, it's always a good idea to just try both models and do cross-validation. This will help you find out which one is more likely to have better generalization error.","Try using both regression and decision trees. Compare the efficiency of each technique by using a 10 fold cross validation. Stick to the one with higher efficiency. It would be difficult to judge which method would be a better fit just by knowing that your dataset is continuous and, or categorical.","It really depends on the structure of the underlying distribution of your data. If you have strong reason to believe that the data approximate a Bernoulli distribution, multinomial logistic regression will perform well and give you interpretable results. However if there exist nonlinear structures in the underlying distribution, you should seriously consider a nonparametric method.  
 While you could use a decision tree as your nonparametric method, you might also consider looking into generating a random forest- this essentially generates a large number of individual decision trees from subsets of the data and the end classification is the agglomerated vote of all the trees. A random forest helps give you an idea of the share each predictor variable contributes to the response. 
 Another factor to keep in mind is interpretability. If you are just trying to classify data, then you probably don't care about the underlying relationships between explanatory and response variables. However, if you are interested at all in interpretability a multinomial logistic regression is much easier to interpret, parametric methods in general, because they make assumptions about the underlying distribution, tell you more intuitively interpretable relationships.","To use Decision Tree, you should transform the continuous variable into categorical. 
 One more thing, Logistic Regression is usually used to predict result according to the probability.","I believe that decision tree classifiers can be used in both continuous and categorical data. If it's continuous the decision tree still splits the data into numerous bins. I have simply tried both to see which performs better. 
 In case of logistic regression, data cleaning is necessary i.e. missing value imputation, normalization/ standardization. In case of decision trees, that is not needed.",,,,,69.5189929,58.01167497,63.75653839,72.47663739,68.37781893,,,,
5893,How to create a good list of stopwords,data-mining,One approach would be to use  tf-idf  score. The words which occur in most of the queries will be of little help in differentiating the good search queries from bad ones. But ones which occur very frequently (high tf or term-frequency) in only few queries (high idf or inverse document frequency) as likely to be more important in distinguishing the good queries from the bad ones.,"It depends on your application. 
 When you are doing topic modelling, try the default stopwords first. When there are some words occurring prominently in many topics (note my rather vague formulation) they are good candidates for additional stopwords. 
 E.g., in a corpus with texts containing figures and tabular material, the words ""fig"", ""figure"", ""tab"", or ""table"" are good additional stopwords. In the result, your topics become more well-defined.",Using TFIDF (term frequency inverse document frequency) will solve your purpose. Get the TFIDF score for each word in your document and sort the words by their scores by which you can select the important words in your data.,"An approach I have used to build a stopword list is to build and train a logistic regression model (due to its interpretability) on your text data. Take the absolute value of the coefficients for each token. Then, sort descending the absolute value of the coefficients of the tokens. Then, create a list of all the tokens with high coefficient absolute value that might lead to overfitting or that might meet some other criteria to be a stopword. That list is your stopwords list. You can then apply that stopword list to another set of documents of this type (kind of like a test set), to see if removing them increases the accuracy, precision, or recall of the test set model. 
 This strategy is effective because it takes into account the impact of tokens when building a stopword list.","Stopwords may be part of the solution at some point, but not the key.  In any case for any major languages good lists of stop words exist, it should not be domain specific. 
 I also don't think that using TD-IDF alone is really correct.  There could be very rare (potentially garbage) words in poor quality strings. 
 Instead of trying to guess which exact features are useful: I would start by creating a data set by randomly selecting some of the data and labeling them by hand (as  good  or  bad , or on a scale from 0.0 to 1.0).  Then code something up that pulls out many features (length, number of words (tokens), spam score, whether it contains URLs or botish chars, detected language, whether it has a question mark, whether it has proper capitalisation).  Also don't forget to include non-linguistic features that you may have, like country of the geoIP of the user that made the query, whether the user was logged in, how old the user's account is.  So at this point you will have a massive table/CSV, and a smaller one with one extra column for the label you've added. 
 Then train some machine learning package with those labeled examples to build a model that is accurate enough for you.  Then let that model run on the rest of the data. 
 If you wish not to code too much, you could even just get those features into CSV form, and give them to the  Google Prediction API's spreadsheet interface .",,,,,52.03413904,66.82524965,50,67.66030024,59.17803377,,,,
5860,"Where to start, which books",beginner,"The standard textbooks that covers AI is ""Artificial Intelligence: A Modern Approach"" by Russel & Norvig. The book's website can be found  here . 
 I also recommend ""Artificial Intelligence: Foundations of Computational Agents"" by Poole and Mackworth. The book can be read  online .",A somewhat popular introduction is Andrew Ng's Stanford   machine learning lectures,"Take a look at this free class  Intro to Artificial Intelligence  from Udacity. One of the instructors is Norvig. The class is more suitable for beginners than Norvig's book. 
 Even though this class doesn't have programming exercise, it explains concepts so well. 
 Its follow up class  Artificial Intelligence for Robotics  has programming exercises and does a fantastic job in explaining Partical Filters.","You could start by learning R or Python and Machine Learning in one of those languages.  
 Some other free options for books are  The LION Way: Machine Learning plus Intelligent Optimization  and  Learning Deep Architectures for AI , which are both freely available online. 
 There's also the free and open  Artificial Intelligence course by Berkeley  at edX. 
 I have listed all the free books I could find on  machine learning and artificial intelligence books  at LearnDataSci, if you're interested.","If you are looking specifically for books, the leaders in the industry have a variety of recommendations as posted on reddit AMA's of machine learning. One specific book I recommend, which is in preparation at the time, is Deep Learning by Yoshua Bengio et al  
 There are plenty of resources outside books to learn from as posted  here  and  here 
 Depends on how much time you have. If you are in it for the long haul, start by brushing up your knowledge on math, specifically Linear Algebra, Calculus, Probability and Statistics",If you want to have some data science programming experience in R try  https://www.datacamp.com/  Is' an interactive online coding environment for learning R programming.,,,,59.61790962,50,53.69158247,61.77145731,64.54505263,50,,,
5856,How is Data Science related to Machine learning?,machine-learning,"Data science is much broader concept than machine learning.
It starts from simple data visualization and descriptive statistics to get insights, manipulations like cleansing to prepare data. Before you can use some ML algorithms.  
 Basically such huge stacks as bigdata, visualization and data preprocessing are out of machine learning scope. And they are all integral parts of ""Data Science"". 
 
 Large resolution image:
 https://whatsthebigdata.files.wordpress.com/2013/07/datascientistmap.png","Machine Learning tries to create systems that can learn from data. As such it can be used in a wide variety of settings, for example to  make robots learn to walk  or  train virtual agents to play video games . 
 Data science concerns itself with the extraction of knowledge from data. In order to do so it uses a bunch of different techniques from different disciplines. Machine learning includes some techniques that can be very useful for a data scientist such as deep learning, decision trees and different clustering algorithms. However, machine learning has more to offer than Data Science uses and Data Science does not solely rely on Machine Learning.","Data science is much more broad. It's sort of a catch-all term that right now doesn't honestly have a very clear definition. But data-science includes all of the skills and techniques required to make sense of data which has high velocity (it's coming at you quickly), volume (there's a lot of it), or variability (it's messy, like natural language processing). This means that it certainly includes machine learning and AI, but that it's also about the tools one might use in a real-world situation such as SQL, Hadoop or Spark (and related information such as a knowledge of parallel programming). Additionally, data science may or may not include the communication aspect such as making good graphs and using Excel. 
 Basically, Data Science is ML+.","Data Science is, as others have noted, a much broader term than machine learning. Applying Machine learning techniques is one aspect of data science. Data Science,  more generally, is the science of deriving knowledge from data. The term was coined back in 1960 and kept evolving to describe the flow and interplay of problem definition, data collection, data transformation, data modeling/ analysis,  and decision making. So to answer your question specifically:  
 
 Machine learning aids data science by providing a suit of algorithms for data modeling/ analysis (through training of machine learning algorithms), decision making (through streaming, online learning, real-time testing that are all topics that come under machine learning), and even data preparation (machine learning algorithms automatically detect anomalies in the data).  
 Data Science stitches together a bunch of ideas/ algorithms drawn from machine learning to create a solution and in doing so borrows a lot of ideas from traditional statistics, domain expertise and basic mathematics. In this way, data science is the process of solving a use case, providing a solution as opposed to machine learning that is an important cog in that solution.","The ML/DL often are subset of Data science as explained clearly below : 
 
 Image Source",,,,,63.76674753,67.78174362,63.96967119,71.16419261,60.32457232,,,,
5789,LinkedIn web scraping,data-mining,"Beautiful Soup  is specifically designed for web crawling and scraping, but is written for python and not R","Scrapy  is a great Python library which can help you scrape different sites faster and make your code structure better. Not all sites can be parsed with classic tools, because they can use dynamic JS content building. For this task it is better to use  Selenium  (This is a test framework for web sites, but it also a great web scraping tool). There's also a  Python wrapper  available for this library. In Google you can find a few tricks which can help you use Selenium inside  Scrapy  and make your code clear, organized, and you can use some great tools for  Scrapy  library. 
 I think that Selenium would be a better scraper for Linkedin than classic tools. There is a lot of javascript and dynamic content. Also, if you want to make authentication in your account and scrape all available content, you will get a lot of problems with classic authentication using simple libraries like  requests  or  urllib .","I like  rvest  in combination with the SelectorGadget chrome plug-in for selecting relevant sections. 
 I've used rvest and built small scripts to paginate through forums by: 
 
 Look for the ""Page n Of m"" object  
 Extract m  
 Based on the page structure,  build a list of links from 1 to m (e.g.
    www.sample.com/page1)   
 Iterate the scraper through the full list of
    links","I would also go with beautifulsoup, if you know python. In case you rather code javascript/JQuery (and you are familiar with node.js), you may want to checkout  CoffeeScript  (Check out the  Tutorial ) I already used it successfully on several occasions for scraping web pages.","lxml  is a nice web scrapping library in Python. Beautiful Soup is a wrapper over lxml.  So, lxml is faster than both scrapy and beautiful soup and has a much easier learning curve. 
 This  is an example of a scraper which I built with it for a personal project, which can iterate over web pages.",,,,,62.62389023,63.72481542,50,58.89220101,54.96955253,,,,
5737,What kind of research can be done with an email data set?,data-mining,"The following are some research that can done on e-mail dataset: 
 
 linguistic analysis to abbreviate an email message 
 Categorize e-mail as spam/ham using machine learning techniques. 
 identifying concepts expressed in a collection of email messages, and
organizing them into an ontology or taxonomy for browsing","Wonderful dataset with many opportunities to brush up on text analysis skills! 
 My first thought would be to try some Topic Modelling on the dataset. If you are using Python there is a library I've used called  gensim  which has some fairly thorough tutorials to get you started. A friend of mine  did something similar  with the Enron dataset, using parallelized preprocessing and distributed latent Dirichlet allocation to infer topics over the email corpus.","You're learning, are you? Try to find something easy and interesting to start. Why don't you start off something easy like building a Bayesian model to predict which email will get deleted. You should glance over those deleted emails, are they spams? are they just garbage? 
 Here, you have a simply supervised model where the data-set already labels the emails for you (deleted or not). Think of something easy like words, titles, length of the email etc, see if you can build a model that predicts email deletion.",The first idea in my mind is : view it forming a social graph where nodes are the email-ids (people) and 2 nodes are connected if they communicate with each other. You can also view it as a weighted graph where weight comes from the frequency of conversation and you can add the sense of direction as well using the sender-receiver information. Now you can apply all sort of social network analysis on this.,"One thing I cannot see here is: 
 
 Cleaning, pre-processing and structuring 
 
 Although this does not sound too exciting, it is probably the most important thing when it comes to working with ""real"" data. Real data is never clean. If you receive an Email data dump you'll find all kinds of garbage. Forwarded messages, different kinds of quotation styles, different languages (or mixes), bullet point lists etc. 
 Sometimes you'll see messages with 99% garbage and only one line with actual information embedded in a stream of forwarded messages etc. 
 Learning how to clean this data and extracting the relevant information is actually the hard part. It's not that fancy, everybody likes to work with clean data, but it's nevertheless another thing that can be done with an Email data set that comes from the real world.",,,,,62.59019748,52.44182906,59.92730393,52.93610268,63.34454775,,,,
5706,"What is the ""dying ReLU"" problem in neural networks?",machine-learning,"A ""dead"" ReLU always outputs the same value (zero as it happens, but that is not important) for any input. Probably this is arrived at by learning a large negative bias term for its weights. 
 In turn, that means that it takes no role in discriminating between inputs. For classification, you could visualise this as a decision plane  outside  of all possible input data. 
 Once a ReLU ends up in this state, it is unlikely to recover, because the function gradient at 0 is also 0, so gradient descent learning will not alter the weights. ""Leaky"" ReLUs with a small positive gradient for negative inputs ( y=0.01x  when x < 0 say) are one attempt to address this issue and give a chance to recover. 
 The sigmoid and tanh neurons can suffer from similar problems as their values saturate, but there is always at least a small gradient allowing them to recover in the long term.","Let's review how the ReLU (Rectified Linear Unit) looks like :
 
 The input to the rectifier for some input $x_n$ is 
$$z_n=\sum_{i=0}^k w_i a^n_i$$ for weights $w_i$, and activations from the previous layer $a^n_i$ for that particular input $x_n$. The rectifier neuron function is $ReLU = max(0,z_n)$ 
 Assuming a very simple error measure  
 $$error = ReLU - y$$ 
 the rectifier has only 2 possible gradient values for the deltas of backpropagation algorithm:
$$\frac{\partial error}{\partial z_n} = \delta_n = \left\{
\begin{array}{c l}     
    1 & z_n \geq 0\\
    0 & z_n < 0
\end{array}\right.$$
(if we use a proper error measure, then the 1 will become something else, but the 0 will stay the same)
and so for a certain weight $w_j$ :
$$\nabla error = \frac{\partial error}{\partial w_j}=\frac{\partial error}{\partial z_n} \times \frac{\partial z_n}{\partial w_j} = \delta_n \times a_j^n = \left\{
\begin{array}{c 1}
    a_j^n & z_n \geq 0\\
    0 & z_n < 0
\end{array}\right.$$ 
 One question that comes to mind is how actually ReLU works ""at all"" with the gradient $=$ 0 on the left side. What if, for the input $x_n$, the current weights put the ReLU on the left flat side while it optimally should be on the right side for this particular input ? The gradient is 0 and so the weight will not be updated, not even a tiny bit, so where is ""learning"" in this case?  
 The essence of the answer lies in the fact that Stochastic Gradient Descent will not only consider a single input $x_n$, but many of them, and the hope is that not all inputs will put the ReLU on the flat side, so the gradient will be non-zero for  some  inputs (it may be +ve or -ve though). If at least one input $x_*$ has our ReLU on the steep side, then the ReLU is still  alive   because there's still learning going on and weights getting updated for this neuron. If all inputs put the ReLU on the flat side, there's no hope that the weights change at all and the neuron is  dead . 
 A ReLU may be alive then die due to the gradient step for some  input batch  driving the weights to smaller values, making $z_n < 0$ for all inputs. A large learning rate amplifies this problem. 
 As @Neil Slater mentioned, a fix is to modify the flat side to have a small gradient, so that it becomes $ReLU=max(0.1x,x)$ as below, which is called LeakyReLU.","ReLU neurons output zero and have zero derivatives for all negative inputs. So, if the weights in your network always lead to negative inputs into a ReLU neuron, that neuron is effectively not contributing to the network's training. Mathematically, the gradient contribution to the weight updates coming from that neuron is always zero (see the Mathematical Appendix for some details). 
 What are the chances that your weights will end up producing negative numbers for  all  inputs into a given neuron? It's hard to answer this in general, but one way in which this can happen is when you make too large of an update to the weights. Recall that neural networks are typically trained by minimizing a loss function $L(W)$ with respect to the weights using gradient descent. That is, weights of a neural network are the ""variables"" of the function $L$ (the loss depends on the dataset, but only implicitly: it is typically the sum over each training example, and each example is effectively a constant). Since the gradient of any function always points in the direction of steepest increase, all we have to do is calculate the gradient of $L$ with respect to the weights $W$ and move in the opposite direction a little bit, then rinse and repeat. That way, we end up at a (local) minimum of $L$. Therefore, if your inputs are on roughly the same scale, a large step in the direction of the gradient can leave you with weights that give similar inputs which can end up being negative. 
 In general, what happens depends on how information flows through the network. You can imagine that as training goes on, the values neurons produce can drift around and make it possible for the weights to kill all data flow through some of them. (Sometimes, they may leave these unfavorable configurations due to weight updates earlier in the network, though!). I explored this idea in a  blog post about weight initialization  -- which can also contribute to this problem -- and its relation to data flow. I think my point here can be illustrated by a plot from that article: 
 
 The plot displays activations in a 5 layer Multi-Layer Perceptron with ReLU activations after one pass through the network with different initialization strategies. You can see that depending on the weight configuration, the outputs of your network can be choked off. 
 Mathematical Appendix 
 Mathematically if $L$ is your network's loss function, $x_j^{(i)}$ is the output of the $j$-th neuron in the $i$-th layer, $f(s) = \max(0, s)$ is the ReLU neuron, and $s^{(i)}_j$ is the linear input into the $(i+1)$-st layer, then by the chain rule the derivative of the loss with respect to a weight connecting the $i$-th and $(i+1)$-st layers is 
 $$
\frac{\partial L}{\partial w_{jk}^{(i)}} = \frac{\partial L}{\partial x_k^{(i+1)}} \frac{\partial x_k^{(i+1)}}{\partial w_{jk}^{(i)}}\,.
$$ 
 The first term on the right can be computed recursively. The second term on the right is the only place directly involving the weight $w_{jk}^{(i)}$ and can be broken down into 
 $$
\begin{align*}
\frac{\partial{x_k^{(i+1)}}}{\partial w_{jk}^{(i)}} &= \frac{\partial{f(s^{(i)}_j)}}{\partial s_j^{(i)}} \frac{\partial s_j^{(i)}}{\partial w_{jk}^{(i)}} \\
&=f'(s^{(i)}_j)\, x_j^{(i)}.
\end{align*}
$$ 
 From this you can see that if the outputs are always negative, the weights leading into the neuron are not updated, and the neuron does not contribute to learning.","The ""Dying ReLU"" refers to neuron which outputs 0 for your data in training set. This happens because sum of  weight  *  inputs  in a neuron (also called  activation ) becomes <= 0 for all input patterns. This causes ReLU to output 0. As derivative of ReLU is 0 in this case, no weight updates are made and neuron is stuck at outputting 0.  
 Things to note: 
 
 Dying ReLU doesn't mean that neuron's output will remain zero at the test time as well. Depending on distribution differences this may or may not be the case. 
 Dying ReLU is not permanent dead. If you add new training data or use pre-trained model for new training, these neurons  might  kick back! 
 Technically Dying ReLU doesn't have to output 0 for ALL training data. It may happen that it does output non-zero for some data but number of epochs are not enough to move weights significantly.","To be more specific in language, while the local gradient of ReLU (which is $1$) multiply the gradient that flow-back because of back-propagation, the result of the updated gradient could be a large negative number (if the gradient that flow-back is a large negative number).  
 Such large negative updated gradient produce a large negative $w_i$ when learning rate is relatively big, hence will repress updates that going to happen in this neuron, since is almost impossible to put up a large positive number to offset the large negative number brought by that ""broken"" $w_i$.","Dying ReLU Problem : 
 
 is during backpropagation, once the nodes(neurons) with  ReLU activation function  recieve negative input values, they always produce zero for any input values, finally, they are never recovered to produce any values except zero, then a model cannot be trained effectively. 
 is also called  Dead ReLU problem . 
 more easily occurs with:
 
 higher learning rates. 
 higher negative bias. 
 
 
 can be detected if:
 
 convergence is slow or stopped. 
 a loss function returns  nan . 
 
 
 can be mitigated using:
 
 lower learning rate. 
 a positive bias. 
 Leaky ReLU activation function . 
 PReLU activation function . 
 ELU activation function .",,,,55.32402645,57.65043023,54.88801446,70.07944778,51.32911303,66.1012306,,,
5589,Downloading a large dataset on the web directly into AWS S3,dataset,"Since you obviously posses an AWS account I'd recommend the following:   
 
 Create an EC2 instance (any size)   
 Use  wget (or curl) to fetch the file(s) to that EC2 instance. For example:  wget http://example.com/my_large_file.csv .   
 Install  s3cmd   
 Use  s3cmd  to upload the file to S3. For example:  s3cmd cp my_large_file.csv s3://my.bucket/my_large_file.csv 
 
 Since connections made between various AWS services leverage AWS's internal network, uploading from an EC2 instance to S3 is pretty fast. Much faster than uploading it from your own computer. This way allows you to avoid downloading the file to your computer and saving potentially significant time uploading it through the web interface.","Launch an EC2 instance with enough storage   
 ssh to the instance  
 Obtain the curl command corresponding to the download from your local machine. You can use the developer options in Google chrome -> network tab -> copy -> copy as curl 
(this step is necessary for some websites requiring authentication such as kaggle)  
 From the instance terminal, run the  curl  command (append  -o output_file  to the command). This will download and save the file  
 Configure aws credentials to connect the instance to s3 (one way is to use the command  aws config , provide AWS access key Id and secret),  
 Use this command to upload the file to s3:  
 aws s3 cp path-to-file s3://bucket-name/","I was facing the same task. However, all the provided answers have one flaw: They save the large file to EC2 disk. This has the drawback that you need to make sure that there is enough disk space on your instance and/or that the file system is encrypted (if you process sensitive data). 
 So, better avoid saving to disk. Instead, stream the curl stdout as stdin into the aws s3 cli. So, 
 
 On your ec2-instance, install aws-cli: 
 
 apk --no-cache add curl procps shadow python py-pip
pip install awscli
 
 
 Down- and upload: 
 
 curl <any curl parameters as fit> | aws s3 cp - <your s3 bucket/folder> --expected-size <any max size in byte in case the file is larger than 50GB>
 
 Note that the '-' after the cp command makes the aws client use stdin as input.","Refer Aws documentation :  http://aws.amazon.com/code  there are libraries available for most of the programing languages.
So you can create a bucket and configure in your code to fetch data from url and write to this bucket in s3  
 for eg in python : 
 from boto.s3.key import Key
k = Key(bucket)
k.key = 'foobar'
k.set_contents_from_string(url_data)
 
 Ref :  https://boto.readthedocs.org/en/latest/s3_tut.html","You can mount your s3 bucket to ec2 instance and then cd to the /path/to/s3_mounted_on_a_folder, there you can simply use the command: 
 wget https://your.download.url/
 
 to mount s3 to your ec2, use s3fs.",,,,,61.45770314,61.90549519,60.79000954,53.69665864,50,,,,
5534,How to scrape imdb webpage?,data-mining,"Instead of scraping,  you might try to get the data directly  here .  It looks like they have data available via ftp for movies, actors, etc.","I have been able to figure out a solution. I thought of posting just in case it is of any help to anyone or if somebody wants to suggest something different.  
 bs = BeautifulSoup(r.text)
for movie in bs.findAll('td','title'):
    title = movie.find('a').contents[0]
    genres = movie.find('span','genre').findAll('a')
    genres = [g.contents[0] for g in genres]
    runtime = movie.find('span','runtime').contents[0]
    rating = movie.find('span','value').contents[0]
    year = movie.find('span','year_type').contents[0]
    imdbID = movie.find('span','rating-cancel').a['href'].split('/')[2]
    print title, genres,runtime, rating, year, imdbID
 
 The output looks like this: 
 The Shawshank Redemption [u'Crime', u'Drama'] 142 mins. 9.3 (1994) tt0111161","As a bit of general feedback, I think you would do well to improve your output format.  The problem with the format as it stands is there is not a transparent way to programmatically get the data. Consider instead trying: 
 print ""\t"".join([title, genres,runtime, rating, year])
 
 The nice thing about a tab delimited file is that if you end up scaling up, it can easily be read into something like impala (or at smaller scales, simple mySql tables).  Additionally, you can then programatically read in the data in python using: 
  line.split(""\t"")
 
 The second bit of advice, is I would suggest getting more information than you think you need on your initial scrape.  Disk space is cheaper than processing time, so rerunning the scraper every time you expand your analytic will not be fun.","You can get everything from div with  class=""rating rating-list"" 
 All you need to do is retrive attribute id:  [id=""tt1345836|imdb|8.5|8.5|advsearch""] 
When you have this content, you split this string by  '|' , and you get: 
 
 parameter: movie id 
 parameter: movie score","Regarding the movie id, the id is actually in the web page url of the actual movie. 
 So the steps you should follow are: 
 
 find the big table in which all the results are shown. 
 for each row, find the href in it (link) and simply make another request to that url. 
 
 You will find that the imdb urls have the following pattern: 
 
 www.imdb.com/tt{the  actual imdb id}/ 
 
 E.g., 
 
 https://m.imdb.com/title/tt0800369/ 
 
 Here, the id is 0800369. 
 This makes it very easy to scrape each movie imdb has just by iterating through ids. You can build an entire database based on imdb using web scraping with Beautiful Soup and Django.",,,,,56.56492508,50,53.1299224,59.19543121,71.91951382,,,,
5443,Do data scientists use Excel?,tools,"Most non-technical people often use Excel as a database replacement. I think that's wrong but tolerable. However, someone who is supposedly experienced in data analysis simply can not use Excel as his main tool (excluding the obvious task of looking at the data for the first time). That's because Excel was never intended for that kind of analysis and as a consequence of this, it is incredibly easy to make mistakes in Excel (that's not to say that it is not incredibly easy to make another type of mistakes when using other tools, but Excel aggravates the situation even more.) 
 To summarize what Excel doesn't have and is a must for any analysis: 
 
 Reproducibility. A data analysis needs to be reproducible. 
 Version control. Good for collaboration and also good for reproducibility. Instead of using xls, use csv (still very complex and has lots of edge cases, but csv parsers are fairly good nowadays.)  
 Testing. If you don't have tests, your code is broken. If your code is broken, your analysis is worse than useless.  
 Maintainability. 
 Accuracy. Numerical accuracy, accurate date parsing, among others are really lacking in Excel. 
 
 More resources: 
 European Spreadsheet Risks Interest Group - Horror Stories 
 You shouldn’t use a spreadsheet for important work (I mean it) 
 Microsoft's Excel Might Be The Most Dangerous Software On The Planet 
 Destroy Your Data Using Excel With This One Weird Trick! 
 Excel spreadsheets are hard to get right","Do experienced data scientists use Excel? 
 
 I've seen some experienced data scientists, who use Excel - either due to their preference, or due to their workplace's business and IT environment specifics (for example, many financial institutions use Excel as their major tool, at least, for modeling). However, I think that most experienced data scientists recognize the need to use tools, which are optimal for particular tasks, and adhere to this approach. 
 
 Can you assume a lack of experience from someone who does primarily
  use Excel? 
 
 No, you cannot. This is the corollary from my above-mentioned thoughts. Data science does not automatically imply big data - there is plenty of data science work that Excel can handle quite well. Having said that, if a data scientist (even experienced one) does not have knowledge (at least, basic) of modern data science tools, including big data-focused ones, it is somewhat disturbing. This is because experimentation is deeply ingrained into the nature of data science due to exploratory data analysis being a essential and, even, a crucial part of it. Therefore, a person, who does not have an urge to explore other tools within their domain, could rank lower among candidates in the overall fit for a data science position (of course, this is quite fuzzy, as some people are very quick in learning new material, plus, people might have not had an opportunity to satisfy their interest in other tools due to various personal or workplace reasons). 
 Therefore, in conclusion, I think that the best answer an experienced data scientist might have to a question in regard to their preferred tool is the following:  My preferred tool is the optimal one, that is the one that best fits the task at hand.","I think most people are answering without having a good knowledge of excel. Excel (since 2010) has an in memory columnar [multi table] database , called power pivot (which allows input from csv/databases etc), allowing it to store millions of rows (it doesn't have to be loaded on a spreadsheet).
It also has an ETL tool called power query allowing you to read the data from a variety of sources (including hadoop). And it has a visualisation tool (power view & power map). A lot of Data Science is doing aggregation and top-n analysis at which power pivot excels.  Add to this the interactive nature of these tools - any user can easily drag and drop a dimension on which to break up the results adn I hope you can see the benefits. So yes you can't do machine learning, but I would question how much machine learning is done by data scientists day to day: eg when I want to analyse the prediction errors made in machine learning program I find it easiest to slice and dice the errors with excel.","In his book Data Smart, John Foreman solves common data science problems (clustering, naive bayes, ensemble methods,...) using Excel. Indeed it's always good to have some knowledge of Python or R but I guess Excel can still get most of the job done !","I'm surprised how many people are attached to the coolness of the profession rather than the actual job to be done. Excel is excellent tool, with free Powerpivot, Powerquery, it can do so much. (these are not available on OS X). And if you know VBA, you can do some nice stuff. And then if you add on the top of that knowledge of python you can combine the very first steps of data extraction and manipulation with python and then use excel, especially if you are a visual person. With excel you can really inspect aggregated data before feeding into any further processes or visualizing. Its a must have tool.","Excel allows only very small data and doesn't have anything that is sufficiently useful and flexible for machine learning or even just plotting. All I would do in Excel, is stare at a subset of the data for a first glance over the values to make sure I don't miss anything visible by eye. 
 So, if his favourite tool is Excel, this might suggest he rarely deals with machine learning, statistics, larger data sizes or any advanced plotting. Someone like this I wouldn't call a Data Scientist. Of course titles don't matter and it depends a lot on your requirements. 
 In any case, don't make a judgement by statements of experience or CV. I've seen CVs and known the people behind it. 
 Don't assume.  Test him!  You should be good enough to set up a test. It has been shown that interviews alone are close to useless to determine skills (they only show personality). Set up a very simple supervised learning test and let him use any tool he wants. 
 And if you want to screen people at an interview first, then ask him about very basic but important insights about statistics or machine learning. Something that every single of your current employees knows.","Let me first clarify that I am starting my journey into data science from a programmer and database developer standpoint. I am not a 10-year data science expert nor a statistical god. However, I do work data scientist and large datasets for a company that works with rather large clients worldwide. 
 From my experience,  data scientist use whatever tools they need to get the job done.  Excel, R, SAS, Python and more are all tools in a toolbox for good data scientist. The best can use a wide variety of tools to analyze and crunch data. 
 Therefore, if you find yourself comparing R to Python, then you're likely doing it all wrong in the data science world.  Good data scientist use both when it makes sense to use one over the other. This also applies to Excel. 
 I think that it's rather hard to find anyone that is going to have experience in so many different tools and languages while been great at everything. I also think it's going to be hard to find data scientist specifically that can not only program complex algorithms but also know how to use them from a statistical standpoint too.  
 Most of the data scientist I've worked with come in about 2 flavors. Those that can program and those that can't. I rarely work with data scientist that can pull data in Python, manipulate it with something like Pandas, fit a model to the data in R and then present it to management at the end of the week. 
 I mean, I know they exist. I've read many data science blogs from guys developing web scrappers, pushing it into Hadoop, pulling it back out in Python, programming complex things and running it through R to boot. They exist. They're out there. I just haven't ran into too many that can do all of that. Maybe it's just my area though? 
 So, does that mean only specializing in one thing bad? No. Plenty of my friends specialize in just one main language and kill it. I know plenty of data guys who only know R and kill it.  I also know plenty of people who just use Excel to analyze data because that's the only thing most non-data scientist can open and use (especially in B2B companies).  The question you really need to answer is if this one thing is the ONE thing you need for this position? And most importantly, can they learn new things? 
 P.S 
 Data Science is not just restricted to ""BIG DATA"" or NoSQL.","Excel can be an excellent tool for exploratory data analysis it really depends on your needs and of course it has its limitations like any tool, but excel definitely deserves a place in the data science hall of fame.   
 Worth remembering that in practice most users will be exploring a heavily reduced data set anyway (created from an SQL query).  
 Excel is powerful for exploring data when you use the ""table"" object in combination with pivot tables, visualising is all 1-2 clicks max and a lot of excel charts in powerpoint look great, unless your looking to create something very bespoke e.g. in a scientific computing context. The interactive nature means you can explore rapidly.  
 The benefits of the ""table"" object is that as you transform the data further in excel to enable you to explore new distributions the pivot tables all remember the variable.  
 Where excel is weak is that the formula list is arguably limiting, for instance a SQL case statement or python statment is way more flexible than an endless chain of if functions.  
 It really depends on your needs but excel definitely deserves a place in the data science hall of fame.  
 Interesting anecdote, the team who work on the Facebook newsfeed algorithm can all regularly be seen to be playing with excel and lots of spreadsheets.","I teach a Business Analytics course that includes SQL and Excel. I teach in a business school so my students aren't the most technically capable, which is why I didn't use something like R, Pandas, or Weka. That being said, Excel is a powerful enough tool to use for some data analysis. It gets most of this power from its ability to act as a front end to SQL Server Analysis Services (a component in SQL Server for data analysis) using the Data Mining Add-In. 
 SSAS lets you construct decision trees, perform linear and logistic regressions, and even make bayesian or neural networks. I've found that using Excel as a front-end is a less threatening approach to doing these kinds of analyses since they've all used Excel before. The way to use SSAS without Excel is through a specialized version of Visual Studio and that isn't the most user friendly tool out there. When you combine it with a few other Excel tools like Power Query and Power Pivot, you're able to do some fairly sophisticated analysis of data. 
 Full Disclosure, I'm probably not going to use it again when I teach the new version of the course next year (we're splitting it into two courses so one can focus more heavily on data analysis). But that's just because the university was able to get enough licenses for Alteryx which is even easier to use and more powerful but is $4-85k/user/year if you can't get it free somehow. Say what you will about Excel, but it beats that price point.",67.8991035,74.14526073,58.45390526,62.86268171,63.36556535,58.87992818,70.83314224,61.26702662,63.54939746
5357,Data Science in C (or C++),machine-learning,"Or must I lose most of the efficiency gained by programming in C by calling on R scripts or other languages? 
 
 Do the opposite: learn C/C++ to write R extensions. Use C/C++ only for the performance-critical sections of your new algorithms, use R to build your analysis, import data, make plots, etc. 
 If you want to go beyond R, I'd recommend learning Python. There are many libraries available such as  scikit-learn  for machine learning algorithms or  PyBrain  for building Neural Networks etc. (and use pylab/ matplotlib  for plotting and  iPython notebooks  to develop your analyses). Again, C/C++ is useful to implement time critical algorithms as Python extensions.","I agree that the current trend is to use Python/R and to bind it to some C/C++ extensions for computationally expensive tasks. 
 However, if you want to stay in C/C++, you might want to have a look at 
 Dlib : 
 
 Dlib is a general purpose cross-platform C++ library designed using contract programming and modern C++ techniques. It is open source software and licensed under the Boost Software License.","As Andre Holzner has said, extending R with C/C++ extension is a very good way to take advantage of the best of both sides. Also you can try the inverse , working with C++ and ocasionally calling function of R with the RInside package  o R. Here you can find how 
 http://cran.r-project.org/web/packages/RInside/index.html 
 http://dirk.eddelbuettel.com/code/rinside.html 
 Once you're working in C++ you have many libraries , many of them built up for specific problems, other more general 
 http://www.shogun-toolbox.org/page/features/ 
 http://image.diku.dk/shark/sphinx_pages/build/html/index.html 
 http://mlpack.org/","In my opinion, ideally, to be a more  well-rounded  professional, it would be nice to know  at least  one programming language for the most popular  programming paradigms  ( procedural ,  object-oriented ,  functional ). Certainly, I consider  R  and  Python  as the two most popular programming languages and environments for  data science  and, therefore,  primary  data science tools. 
 Julia  is impressive in certain aspects, but it tries to catch up with those two and establish itself as a major data science tool. However, I don't see this happening any time soon, simply due to  R/Python 's  popularity , very large  communities  as well as enormous  ecosystems  of existing and newly developed  packages/libraries , covering an very wide range of domains / fields of study. 
 Having said that, many packages and libraries, focused on data science, ML and AI areas, are  implemented  and/or provide  APIs  in languages other than R or Python (for the proof, see  this curated list  and  this curated list , both of which are excellent and give a solid perspective about the variety in the field). This is especially true for  performance-oriented  or  specialized  software. For that software, I've seen projects with implementation and/or APIs mostly in Java, C and C++ (Java is especially popular in the  big data  segment of data science - due to its closeness to  Hadoop  and its ecosystem - and in the  NLP  segment), but other options are available, albeit to a much more limited, domain-based, extent. Neither of these languages is a waste of time, however you have to  prioritize  mastering any or all of them with your current work situation, projects and interests. So, to answer your question about viability of C/C++ (and Java), I would say that they are all  viable , however not as  primary  data science tools, but as  secondary  ones. 
 Answering your questions on 1)  C  as a  potential data science tool  and 2) its  efficiency , I would say that: 1) while it's possible to use  C  for data science, I  would recommend against  doing it, because you'd have a very hard time finding corresponding libraries or, even more so, trying to implement corresponding algorithms by yourself; 2) you shouldn't worry about efficiency, as many performance-critical segments of code are implemented in low-level languages like C, plus, there are options to interface popular data science languages with, say, C (for example,  Rcpp  package for integration R with C/C++:  http://dirk.eddelbuettel.com/code/rcpp.html ). This is in addition to simpler, but often rather effective, approaches to performance, such as consistent use of vectorization in R as well as using various parallel programming frameworks, packages and libraries. For R ecosystem examples, see  CRAN Task View ""High-Performance and Parallel Computing with R"" . 
 Speaking about  data science , I think that it makes quite a lot of sense to mention the importance of  reproducible research  approach as well as the availability of various  tools , supporting this concept (for more details, please see  my relevant answer ). I hope that my answer is helpful.","R is one of the key tool for data scientist, what ever you do don't stop using it.  
 Now talking about C, C++ or even Java.  They are good popular languages. Wether you need them or will need them depend on the type of job or projects you have.  From personal experience, there are so many tools out there for data scientist that you will always feel like you constantly need to be learning.   
 You can add Python or Matlab to things to learn if you want and keep adding.  The best way to learn is to take on a work project using other tools that you are not comfortable with.  If I were you, I would learn Python before C.  It is more used in the community than C.  But learning C is not a waste of your time.","As a data scientist the other languages (C++/Java) come in handy when you need incorporate machine learning into an existing production engine. 
 Waffles  is both a well-maintained C++ class library and command-line analysis package.  It's got supervised and unsupervised learning, tons of data manipulation tools, sparse data tools, and other things such as audio processing.  Since it's also a class library, you can extend it as you need.  Even if you are not the one developing the C++ engine (chances are you won't be), this will allow you to prototype, test, and hand something over to the developers.     
 Most importantly, I believe my knowledge of C++ & Java really help me understand how Python and R work.  Any language is only used properly when you understand a little about what is going on underneath.  By learning the differences between languages you can learn to exploit the strengths of your main language.   
 Update 
 For commercial applications with large data sets, Apache Spark - MLLib is important.  Here you can use Scala, Java, or Python.","I would be keen to understand why you would need another language (apart form Python) if your goal is "" but what about advanced regression, Machine Learning, text mining and other more advanced statistical operations"". 
For that kind of thing, C is a waste of time. It's a good tool to have but in the ~20 years since Java came out, I've rarely coded C. 
If you prefer the more functional-programming side of R, learn Scala before you get into too many procedural bad habits coding with C. 
Lastly learn to use Hadley Wickham's libraries - they'll save you a lot of time doing data manipulation.","There are some C++ tools for statistics and data science like ROOT  https://root.cern.ch/drupal/  , BAT  https://www.mppmu.mpg.de/bat/  , boost  , or OpenCV","Not sure whether it's been mentioned yet, but there's also  vowpal wabbit  but it might be specific to certain kinds of problem only.",51.00174529,50,50,73.61560383,51.99871126,53.31448193,51.13785674,68.24228755,50
5345,"IDE alternatives for R programming (RStudio, IntelliJ IDEA, Eclipse, Visual Studio)",r,"RIDE  -   R-Brain IDE (RIDE) for R & Python, Other Data Science R IDEs, Other Data Science Python IDEs. Flexible layout. Multiple language support. 
 Jupyter notebook  -   The Jupyter Notebook App is a server-client application that allows editing and running notebook documents via a web browser. The Jupyter Notebook App can be executed on a local desktop 
 Jupyter lab  - An extensible environment for interactive and reproducible computing, based on the Jupyter Notebook and Architecture. 
 Radiant  – Open-source platform-independent browser-based interface for business analytics in R, based on the Shiny package and can be run locally or on a server. 
 R Tools for Visual Studio (RTVS)  -
A free, open-source extension for Visual Studio 2017, RTVS is presently supported only in Visual Studio on Windows and not Visual Studio for Mac. 
 Architect  -  Architect is an integrated development environment (IDE) that focuses specifically on the needs of the data scientist. All data science tasks from analyzing data to writing reports can be performed in a single environment with a common logic. 
 displayr  - Simple and powerful. Automation by menu or code. Elegant visualizations. Instant publishing.
Collaboration. Reproducibility. Auto-updating. Secure cloud platform. 
 Rbox  - This package is a collection of several packages to run R via Atom editor. 
 Use below for more IDEs: 
 RKWard  - an easy to use and easily extensible IDE/GUI for R 
 Tinn-R  - Tinn-R Editor - GUI for R Language and Environment 
 R AnalyticFlow  - data analysis software that utilizes the R environment for statistical computing. 
 Rgedit  - a text-editor plugin. 
 Nvim-R  - Vim plugin for editing R code. 
 Rattle  - A Graphical User Interface for Data Mining using R. 
 How to Turn Vim Into an IDE for R","IntelliJ  and  PyCharm  supports R via this  plugin . 
 It's a recent project, so RStudio is still more powerful, including its focus on a data-friendly environment (plots and data are always in sight).","You may try using R with Jupyter notebook. It requires installation of jupyter R kernel, IRkernel which will allow you to open a new jupyter notebook with option to choose  R  instead of default  python  kernel. 
 See  https://www.continuum.io/blog/developer/jupyter-and-conda-r  and  https://irkernel.github.io/installation/  for installation steps.","VisualStudio added syntax highlighting for R a few days ago:  https://www.visualstudio.com/news/2015-mar-10-vso 
 The current RStudio preview is pretty cool as well - you can switch to a dark theme, code completion is working well, you can filter in the viewer, etc.","What about  ESS , the R (and other stats languages) package for the Emacs editor?
It's not formally an IDE, though it has many, if not more of the features of RStudio, just in a different UI (code completion, inline help, object-aware autocomplete, debugging etc.).","Here's  R Language Support for IntelliJ IDEA . However, keep in mind that this support is not in the form of built-in functionality or official plug-in, but rather a  third-party  plug-in. I haven't tried it, so my opinion on it is limited to the point above. 
 In my opinion, a better option would be  Eclipse , which offers R support via  StatET  IDE:  http://www.walware.de/goto/statet . However, I find  Eclipse  IDE too heavyweight. Therefore, my preferred option is  RStudio  IDE - I don't know why one would prefer other options. I especially like  RStudio 's ability of online access to the full development environment via  RStudio Server .","The vim-r-plugin is surprisingly good. You can send lines and paragraphs of code from vim into a tmux session running R in a similar manner to R-Studio. It has  these commands  if you want to check out what functionality it adds to vim. Of course I use all my other normal vim plugins - auto-complete, folding, etc.","You can try R-Brain platform (r-brain.io). R-Brain provides an integrated cloud/on-premises data science platform for developing models with popular open source languages. Powered by Jupyter, our IDE, console, notebook and markdown are all integrated into one environment with full language support for R and Python. R-Brain editor is built with Monaco, the heart of VS code. With Docker technology and prebuilt images, R-Brain empowers data scientists with quick setup, instant collaboration and version control at workspace level. 
 I am founder of R-Brain. 
 Shadi",I made a  list of all GUIs  that produce R code through point-click dialogues. Most of these are not full IDEs and only complement. Rkward is a FOSS multiplatform competitor to Rstudio. R AnalyticFlow specializes in 2d graphical layout of icons of code. At end of link is code to install and run 6 IDE helpers.,67.27708941,59.9107471,50,53.44069038,58.55325312,68.96843394,52.34725093,52.2025101,60.24266297
5255,What is an 'old name' of data scientist?,bigdata,"In reverse chronological order: data miner, statistician, (applied) mathematician.","Terms that covered more or less the same topics that Data Science covers today: 
 
 Pattern Recognition 
 Machine Learning  
 Data Mining  
 Quantitative methods","I do think it is new job, basically data scientist has to apply mathematical algorithms on data with considerable constraint in terms 1) Run time of the application 2) Resource use of the application. If these constraints are not present, I would not call the job data science. Moreover, these algorithms are often need to be ran on distributed systems, which is another dimension of the problem. 
 Of course, this has been done before, in some combination of statistics, mathematics and programming, but it was not wide spread to give rise to the new term. The real rise of data science is from the ability to gather large amounts of data, thus need to need to process it.","Also: ""Business Intelligence developer""","Some really nice answers already. However, I would break the entire process of breaking the work of a data scientist into who actually did those: 
 
 Getting the data from databases and other sources:  Generally, it used to be the  DBA  who gets the data from the DB's and the people who collects data from other sources are called  data guys , they don't really have a specific name (atleast in India). And the scraping and crawling scripts are written by  software engineers  who are hired especially for that purpose. 
 Analytics and prediction:  Done by people called the  statisticians  or the  mathematicians . 
 Visualizations and reporting:  Done by people called  business analysts  or the MBA guys in the company. 
 Big Data and pipelining stuff:  Done by  software engineers  hired especially for thar particular purpose.","An ideal data scientist is 60-70% Statistician and 30-40% a computer scientist and so the old name of ""Data scientist' was somebody who was part statistician and part computer science guy.","In several subfields, some were simply called  analysts . If you go back earlier in time, in a pre-science era, I tend to believe that people involved in  divination  or astrology (several of them because they were paid for that, much more than for serious science) were precursors. 
 On a lighter note, I like the neologism  dedomenology  (in French,  dédoménologie ), and I tend to consider that  Hugo Steinhaus (the inventor of $k$-means)  was, due  to his interest in many applied fields, one of the first to spark the fire of data science.",,,51.89881845,52.40645393,56.33677779,50,56.65186299,80.48368992,50.78710832,,
5226,strings as features in decision tree/random forest,machine-learning,"In most of the well-established machine learning systems, categorical variables are handled naturally. For example in R you would use factors, in WEKA you would use nominal variables. This is not the case in scikit-learn. The decision trees implemented in scikit-learn uses only numerical features and these features are interpreted always as  continuous numeric variables .  
 Thus, simply replacing the strings with a hash code should be avoided, because being considered as a continuous numerical feature any coding you will use will induce an order which simply does not exist in your data.  
 One example is to code ['red','green','blue'] with [1,2,3], would produce weird things like 'red' is lower than 'blue', and if you average a 'red' and a 'blue' you will get a 'green'. Another more subtle example might happen when you code ['low', 'medium', 'high'] with [1,2,3]. In the latter case it might happen to have an ordering which makes sense, however, some subtle inconsistencies might happen when 'medium' in not in the middle of 'low' and 'high'. 
 Finally, the answer to your question lies in coding the categorical feature into  multiple binary features . For example, you might code ['red','green','blue'] with 3 columns, one for each category, having 1 when the category match and 0 otherwise. This is called  one-hot-encoding , binary encoding, one-of-k-encoding or whatever. You can check documentation here for  encoding categorical features  and  feature extraction - hashing and dicts . Obviously one-hot-encoding will expand your space requirements and sometimes it hurts the performance as well.","You need to encode your strings as numeric features that sci-kit can use for the ML algorithms. This functionality is handled in the preprocessing module (e.g.,  see  sklearn.preprocessing.LabelEncoder  for an example).","2018 Update! 
 You can create an embedding (dense vector) space for your categorical variables.  Many of you are familiar with word2vec and fastext, which embed words in a meaningful dense vector space.  Same idea here-- your categorical variables will map to a vector with some meaning. 
 From the  Guo/Berkhahn paper : 
 
 Entity embedding not only reduces memory usage and speeds up neural
  networks compared with one-hot encoding, but more importantly by
  mapping similar values close to each other in the embedding space it
  reveals the intrinsic properties of the categorical variables. We
  applied it successfully in a recent Kaggle competition and were able
  to reach the third position with relative simple features. 
 
 The authors found that representing categorical variables this way improved the effectiveness of all machine learning algorithms tested, including random forest. 
 The best example might be  Pinterest's application of the technique  to group related Pins: 
 
 The folks at fastai have implemented categorical embeddings and created a very nice  blog post  with companion  demo notebook . 
 Additional Details and Explanation 
 A neural net is used to create the embeddings i.e. assign a vector to each categorical value. Once you have the vectors, you may use them in any model which accepts numerical values. Each component of vector becomes an input variable. For example, if you used 3-D vectors to embed your categorical list of colors, you might get something like: red=(0, 1.5, -2.3), blue=(1, 1, 0) etc. You would use three input variables in your random forest corresponding to the three components. For red things, c1=0, c2=1.5, and c3=-2.3. For blue things, c1=1, c2=1, and c3=0. 
 You don't actually  need  to use a neural network to create embeddings (although I don't recommend shying away from the technique).  You're free to create your own embeddings by hand or other means, when possible.  Some examples: 
 
 Map colors to RGB vectors. 
 Map locations to lat/long vectors. 
 In a U.S. political model, map cities to some vector components representing left/right alignment, tax burden, etc.","You should usually  one-hot encode  categorical variables for scikit-learn models, including random forest. Random forest will often work ok without one-hot encoding but usually performs better if you do one-hot encode. One-hot encoding and ""dummying"" variables mean the same thing in this context. Scikit-learn has  sklearn.preprocessing.OneHotEncoder  and Pandas has  pandas.get_dummies  to accomplish this. 
 However, there are alternatives. The article  ""Beyond One-Hot"" at KDnuggets  does a great job of explaining why you need to encode categorical variables and alternatives to one-hot encoding. 
 There are alternative implementations of random forest that do not require one-hot encoding such as R or H2O. The implementation in R is  computationally expensive  and  will not work if your features have many categories . H2O will work with large numbers of categories. Continuum has made  H2O available in Anaconda Python.   
 There is an  ongoing effort to make scikit-learn handle categorical features directly . 
 This article  has an explanation of the algorithm used in H2O. It references the academic paper  A Streaming Parallel Decision Tree Algorithm  and a  longer version  of the same paper.","You can use dummy variables in such scenarios. With panda's  panda.get_dummies  you can create dummy variables for strings you want to put in Decision Tree or Random Forest. 
 Example: 
 import pandas as pd
d = {'one' : pd.Series([1., 2., 3.,4.], index=['a', 'b', 'c','d']),'two' :pd.Series(['Paul', 'John', 'Micheal','George'], index=['a', 'b', 'c', 'd'])}
df = pd.DataFrame(d)

df_with_dummies= pd.get_dummies(df,columns=[""two""],drop_first=False)
df_with_dummies","Turn them to numbers, for example for each unique country assingn a unique number (like 1,2,3 and ...)  
 also you  Don't  need to use  One-Hot Encoding  (aka dummy variables) when working with random forest, because trees don't work like other algorithm (such as linear/logistic regression) and they don't work by distant (they work with finding good split for your features) so  NO NEED  for One-Hot Encoding",,,,55.61577096,54.46014118,54.48334974,60.0640022,62.36964533,57.9064196,,,
5178,How to deal with version control of large amounts of (binary) data,bigdata,"What I am ending up using is a sort of hybrid solution: 
 
 backup of the raw data 
 git of the workflow 
 manual snapshots of workflow + processed data, that are of relevance, e.g.:

 
 standard preprocessing 
 really time-consuming 
 for publication 
 
 
 I believe it is seldom sensible to have a full revision history of large amount of binary data, because the time required to review the changes will eventually be so overwhelming that it will not pay off in the long run.
Maybe a semi-automatic snapshot procedure (eventually to save some disk-space, by not replicating the unchanged data across different snapshots) would be of help.","Try looking at  Git Large File Storage (LFS) . It is new, but might be the thing worth looking at. 
 As I see,  a discussion on Hacker News  mentions a few other ways to deal with large files: 
 
 git-annex  (and e.g.  using it with Amazon S3 ) 
 Mercurual Largefiles extension","I have dealt with similar problems with very large synthetic biology datasets, where we have many, many GB of  flow cytometry  data spread across many, many thousands of files, and need to maintain them consistently between collaborating groups at (multiple) different institutions.   
 Typical version control like svn and git is not practical for this circumstance, because it's just not designed for this type of dataset.  Instead, we have fallen to using ""cloud storage"" solutions, particularly  DropBox  and  Bittorrent Sync .  DropBox has the advantage that it does do at least some primitive logging and version control and manages the servers for you, but the disadvantage that it's a commercial service, you have to pay for large storage, and you're putting your unpublished data on a commercial storage; you don't have to pay much, though, so it's a viable option.  Bittorrent Sync has a very similar interface, but you run it yourself on your own storage servers and it doesn't have any version control.  Both of them hurt my programmer soul, but they're the best solutions my collaborators and I have found so far.","I have used  Versioning on Amazon S3 buckets  to manage 10-100GB in 10-100 files. Transfer can be slow, so it has helped to compress and transfer in parallel, or just run computations on EC2. The  boto  library provides a nice python interface.","This is a pretty common problem. I had this pain when I did research projects for a university and now - in industrial data science projects. 
 I've created and recently released an open source tool to solve this problem -  DVC . 
 It basically combines your code in Git and data in your local disk or clouds (S3 and GCP storage). DVC tracks dependency between data and code and builds the dependency graph (DAG). It helps you to make your project reproducible. 
 DVC project could be easily shared - sync your data to a cloud (dvc sync command), share your Git repository and provide access to your data bucket in the cloud.   
 ""learnable in a few hours"" - is a good point. You should not have any issues with DVC if you are familiar with Git. You really need to learn only three commands: 
 
 dvc init  - like  git init . Should be done in an existing Git repository. 
 dvc import  - import your data files (sources). Local file or URL. 
 dvc run  - steps of your workflow like  dvc run python mycode.py data/input.jpg data/output.csv . DVC derives the dependency between your steps automatically, builds DAG and keeps it in Git. 
 dvc repro  - reproduce your data file. Example:  vi mycode.py  - change code, and then  dvc repro data/output.csv  will reproduce the file (and all the dependencies. 
 
 You need to learn a couple more DVC commands to share data through the cloud and basic S3 or GCP skills. 
 DVC tutorials  is the best starting point.","We don't version control the actual data files. We wouldn't want to even if we stored it as CSV instead of in a binary form. As  Riccardo M.  said, we're not going to spend our time reviewing row-by-row changes on a 10M row data set. 
 Instead, along with the processing code, I version control the metadata: 
 
 Modification date 
 File size 
 Row count 
 Column names 
 
 This gives me enough information to know  if  a data file has changed and an idea of  what  has changed (e.g., rows added/deleted, new/renamed columns), without stressing the VCS.","I haven't used them but there was a similar discussion in a finance group 
 data repository software 
suggestions 
 scidb , zfs,  http://www.urbackup.org/","You may take a look at my project called DOT:
Distrubuted Object Tracker repository manager. 
It is a very simple VCS for binary files for personal use (no collaboration). 
It uses SHA1 for checksuming and block deduplication. Full P2P syncing. 
One unique feature: adhoc one time TCP server for pull/push. 
It can also use SSH for transport. 
It is not yet released, but might be a good starting point. 
 http://borg.uu3.net/cgit/cgit.cgi/dot/about/","You could try using  hangar . It is a relatively new player to the data version control world but does a really nice job by versioning the tensors instead of versioning the blob. The documentation must be the best place to start. Since the data is being stored as tensors, you should be able to use it directly inside your ML code (plus hangar now has data loaders for PyTorch and Tensorflow). With hangar, you could get all the benefit of git such as zero-cost branching, merging, time travel through history. One nice feature about cloning in the hangar is you could do  partial cloning . Which means, if you have 10 TB of data at your remote and only need 100 MB for prototyping your model, you could fetch only 100 MB via partial cloning instead of a full clone.",60.62117674,54.320356,63.80244105,52.48777481,51.62500505,61.35545904,51.23793886,51.9079116,57.10492152
5061,How much of a background in programming is necessary to become a data scientist?,bigdata,"First of all, the fact that you have known some Java, even ten years ago, already means that you don't ""know nothing about programming"" (I suggest you update the title of your question to reflect that - change ""nothing"" to ""a little""). I'd like to make several points, which I hope will be useful to you. 
 
 In terms of the level of  programming proficiency , which is expected (needed) for a data scientist, the following popular  definition  says it all: 
 
 
 A  data scientist  is someone who is better at statistics than any
  software engineer and better at software engineering than any
  statistician. 
 
 
 Another perspective on the role of programming abilities in a data scientist's skill set can be found in a popular visual representation of data science, using Venn diagrams. The original  data science Venn diagram  was presented by data scientist Drew Conway (see  this blog post ): 
 
 
 
 Since its original introduction, the original diagram was modified by various people for various reasons. The two interesting  adaptations  are for data science in the social sciences domain ( http://www.datascienceassn.org/content/fourth-bubble-data-science-venn-diagram-social-sciences ), as well as data science Venn diagram V2.0, where data science is represented not as an intersection of knowledge domains, but as their union ( http://www.anlytcs.com/2014/01/data-science-venn-diagram-v20.html ). Another very interesting and useful visual perspective of data science skill set, also based on Venn diagram, is the following Gartner's diagram,  mapping  specific  skills  to business intelligence (BI) or business analytics  knowledge domains : 
 
 
 
 An alternative perspective for a data scientist's skill set and domain knowledge is a  taxonomy of data scientists , such as  this taxonomy , which classifies data scientists, according to their  focus  (or the  strongest skill set ): mathematics, data engineering, machine learning, business, software engineering, visualization, spacial data (GIS) or others. 
 If you're curious about the meaning of the  ""Danger Zone""  in the original data science Venn diagram,  this Quora discussion , containing, among other nice answers, also an answer by the original diagram's author, can be very helpful. 
 If you're interested in learning about a range of  skills  and  knowledge domains , useful for a data scientist, check this open source  curriculum  for learning data science:  http://datasciencemasters.org , or on GitHub:  https://github.com/datasciencemasters/go . Of course, popular and research papers, lectures on YouTube, MOOC courses, online and offline bootcamps as well as a wealth of other resources is only an Internet search away. 
 Finally, a note on  programming languages  for data science. I think that it is important to understand that this aspect is really of secondary importance. The  focus  should be on two words, which the term ""data science"" consists of:  data  and  science .  Focus on data  means that it is important to think about data science (or BI, or analytics) tasks in terms of the corresponding  domain knowledge  as well as to pay attention to  data quality  and  representativeness .  Focus on science  means adhering to  scientific approaches  to data collection and analysis, of which  reproducibility  plays an important role. A  programming language  for data science is just a  tool  and, therefore, should be chosen to match the task at hand. Python and R represent very good and the most popular programming languages and environments for a data scientist, however, you should be aware of other options ( tool set ).","Data Scientists code every day. However, just because you don't have background doesn't mean you can't pick it up! The level of programming you need to know to start doing Data Science isn't very high, but you will at least need: 
 
 the logical mindset to phrase the solution to your problem in
procedural code 
 to know the programming language, functions, and libraries needed in this field. 
 
 1st point is the most difficult of the two. Hopefully, you have taken enough math and physics by now to wire your mind to think programmatically. If so then yes, you absolutely can learn a language! There are guides out there that teach out the syntax and functions. For example: 
 
 R -  Pluralsight 
 General Python -  http://www.codecademy.com/en/tracks/python 
 DataSci Python -  https://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-python 
 
 Personally, I would recommend Python first. To me the language places more emphasis on readability and cleanliness, making it a great first language. It's also a general-purpose language so it's good to know. I did start with R though and it's also good but is more function-over-form IMO. Try both out and see which feels best first, since you'll likely have to pick up both if you delve into this field anyway.","Based on this infographic and other things I've read, it sounds like you need to know some coding to be a true data scientist.  http://blog.datacamp.com/how-to-become-a-data-scientist/  But you could still be a data  analyst  without compsci - basically a statistician.","The R language is the best place to start. Grab some open datasets, and start programming with r. R has many different analytical functions that you can learn a lot with it.","Johns Hopkins University as a set of course on Coursea that is gear on Data Science. Here is the link to the classes  https://www.coursera.org/specialization/jhudatascience/1?utm_medium=courseDescripTop . You can also take the classes for free. 
 This is a set of 9 classes that would give you a good foundation to build-on and to start a career.","Data science, being a new term, covers a broad spectrum of jobs. At one end you are expected to write production code. At the other end you do statistics in packaged software. They also call such people statisticians or analysts. So decide what you enjoy doing before you leap. If you just want to analyze data, you could definitely get by with R or python as long as you're mathematically proficient. I find that in these kind of jobs, your communication and social skills matter too, since you have to explain the data to executives and the like.",,,,63.91339965,57.70129925,58.93041347,52.77929165,50.92345703,52.15903642,,,
4977,What is the best Big-Data framework for stream processing?,bigdata,"You might also want to take a look at  Apache Flink . It is a new contender to Apache Spark as it can do both, realtime and batch analysis. It also claims to be faster than Apache Spark, but I think mostly in depends on your usecase. 
 Good thing is: Apache Flink project has some Storm compatibility layer in its development version. In this way you can take your existing Storm project and simply run it inside Flink. For me it was at least 2 times faster than Storm in a very simple topology. So give it a try! :)","Apache Storm and Apache Spark are more popular than the other ones, there are already many discussions on Quora( Storm vs Spark ,  Use cases for comparison ). 
 Personally, I think Spark is a better choice.","Amazon Kinesis might be another choice for stream processing, if you don't want to set up the clusters by yourself.","It really depends on what you are looking to do. I love Apache Spark, but Storm has some history. I am sure as the streaming capability in Spark is built out that it will become a competitive solution. However, until Spark has some heavy hitting users (for streaming) there will remain unknown bugs.  
 You can also consider the community. Spark has a great community. I am not sure the level of the Storm community as I am usually the one receiving the data not handling the ingest. I can say we have used Storm on projects and I have been impressed with the real-time analysis and volumes of streaming data.","Both Storm and Spark are great tools. It depends on your use-case.  
 
 Do you want to quickly parse huge stream of data and store it into a database? Use Storm (e.g. counting tweets). 
 Training a classifier on a stream of data would be a task suitable for Spark. There's a data window on which you're working and it will take a while. 
 
 I haven't tried Flink, but it looks more similar to Spark. Spark has general concept of RDD (Resilient Distributed Datasets) that can be used also for graphs, huge matrices etc. 
 If you want to write a word count, you any of them. But who wants a word count (except from hello world tutorials)?","Just to throw an extra option in,  Microsoft Azure's Stream Analytics .  
 As far as a comparison of the different technologies and their relative performance, those stats seem difficult to get, but there is a useful comparison of a few technologies in  this article , though doesn't mention Azure Stream Analytics. I think the difficulty is that the architectures are so different, that is makes simple metrics hard to produce. What is needed is some more robust measurement tools that can be applied to give a few metrics for various operations. 
 I would think that Gartner would have something to say, but the nearest I could find was their  BI and Analytics quadrant , which only obliquely address stream analytics.","There are a lot of choices based on what you are looking to do. 
 How do you want to write the logic? 
 If you are looking to write code for your logic ( and not looking for a SQL like declarative language) then you can use one of Stream Processing Engines: Apache Storm, Apache Spark, Apache Fink, Apache Samza or Kafka Streams. The good news is all of them are open source. 
 Among them, my bet is Fink. It has a pretty nice architecture which handles many pitfalls others stepped into. Especially, if you want to do exactly once processing, Fink is the best bet. 
 However, writing java code to do this is fine if you do simple counting, but a nightmare if you want to do serious things like time windows, join and temporal event sequence patterns etc. ( see Patterns for Streaming Realtime Analytics for details on this argument). If you want to express the logic using a declarative language, then you will want to use Complex Event Processing, which let you write SQL-like queries and do complex things. Specifically, these kinds of queries support temporal operators like sliding windows, joins, temporal sequence patterns etc. 
 If you are looking for an Open source solution, the among the choices are  WSO2 DAS (Data Analytics Server)  formally known as WSO2 CEP ( Apache License) and Esper (GPL). ( Disclaimer I work for WSO2). 
 If you are OK to pay, then there is Tibco Stream Base, IBM Infosphere Streams, and SQLStreams. Both WSO2 DAS and Esper have commercial support if you need it. Paul Vincent is maintaining CEP market Survey which provides historical overview of almost all CEP engines. 
 Recent Forrester report surveyed the choices and you can find the details from 15 ""True"" Streaming Analytics Platforms For Real-Time Everything. 
 Above stream processing engines are adding support for SQL like query languages. However, they still lag behind with the features ( e.g. temporal sequence patterns). As I discussed in  https://softwareengineeringdaily.com/2016/02/04/stream-processing-vs-complex-event-processing/ , CEP and Stream processing engines are merging. 
 Scalability 
 If you plan to process millions of events per second, then you need to setup a complex distributed processing graph. On this topic, stream processing has an advantage where they are historically built to scale. However, as I mentioned, these two models are merging. 
 IoT Analytics 
 If you are doing an IoT analytics, then the data creates a time series. In that case, often you need to do temporal operations such as sliding windows. On that use case, you are better off with an SQL like language hence I recommend to go for CEP. 
 Streaming + Batch 
 The final twist is whether you want to do batch ( MapReduce style processing) as well. If yes, read about Lambda Architecture and Kappa architecture. Apache Fink and WSO2 DAS supports this scenario. ( and there may be others).",Flink is an upcoming good equivalent to Spark (or even better) as it process each tuples unlike micro batch concept of spark streaming.,,50,50,69.55248338,57.24045267,54.85456525,55.98640274,63.78559694,56.63614418,
4957,Machine learning toolkit for Excel,machine-learning,"As far as I know, currently there are not that many projects and products that allow you to perform serious  machine learning (ML)  work from within Excel.  
 However, the situation seems to be changing rapidly due to active Microsoft's efforts in popularizing its ML cloud platform  Azure ML  (along with  ML Studio ). The  recent acquisition  of R-focused company Revolution Analytics by Microsoft (which appears to me as more of  acqui-hiring  to a large extent) is an example of the company's aggressive data science  market strategy . 
 In regard to  ML toolkits for Excel , as a confirmation that we should expect most Excel-enabled ML projects and products to be  Azure ML-focused , consider the following two  projects  (the latter is an open source): 
 
 Excel DataScope  (Microsoft Research):  https://www.microsoft.com/en-us/research/video/excel-datascope-overview/ 
 Azure ML Excel Add-In  (seems to be Microsoft sponsored):  https://azuremlexcel.codeplex.com","(Most) Machine Learning algorithms are essentially optimization problems where you minimize/maximize an objective function subject to certain constraints. Excel comes with the Solver add-in which is pretty handy for lightweight problems, so it is entirely possible for you to build a Machine Learning model within Excel! (I've done it myself) 
 If your data size is reasonably small (say <10k rows and not too many columns), it is in fact pretty quick and easy to build certain ML models within Excel. All you need is to learn how to use the Excel Solver, and the built-in matrix functions for vectorized computations.  
 For example, Neural Networks and Logistic Regressions are particularly easy to build due to the simplicity of their objective function. If you are really inclined to try you could refer to the following link:
 http://www.xlpert.com/ebook/Build_Neural_Network_With_MS_Excel_sample.pdf 
 Now, Excel does has major limitations. There are numerical stability issues, it is tricky to write loops (without VBA), it is a pain to write lengthy functions in Excel and there's no concept of structure so I wouldn't try to build things like Random Forest. That being said, there is still a good amount of algorithms you can implement with Excel for fun.  
 Of course, for serious analysis, I would still recommend you to use proper tools for that.","First of all, let me tell you that Excel shouldn't be used for machine learning or any data analysis complicated enough that you wouldn't be comfortable doing it on paper. Why? Here is a list of resources to tell you why: 
 
 You shouldn’t use a spreadsheet for important work (I mean it) 
 Destroy Your Data Using Excel With This One Weird Trick! 
 Using Excel for Statistical Data Analysis - Caveats 
 Problems with Excel 
 Spreadsheet Addiction 
 
 Now, if you really really want to do heavy calculations without exporting your data, I suggest using  xlwings . Basically, this allows two-way communication between Excel and Python. Watch the video in the homepage for a quick introduction. In this way, you would be able to use numpy, pandas and scikit-learn (or other machine learning library that you may prefer) without exporting your data first.","Nobody does serious machine learning in Excel; that's not what it's for. Fortunately, you can directly import Excel files into better platforms like python. In particular, there's a great package called  pandas , which makes work very pleasant.  Here's a demo .","Weka  can import CSV files, and allows you to choose which columns and rows you want to use in your analysis.  It's not an ""add-in"" for Excel per-se, but it might work for you.",You can find an Excel and VBA implementation of Random Forest using the open source ALGLIB Library  here,Vortarus Technologies LLC  here  also have an Excel Add-In that can do various intermediate ML tasks from SVM to Neural Nets to CART etc. The free trial is open ended but has some function limits.,"You should take a look at this link. 
 https://www.youtube.com/watch?v=thyrg2AWyq0 
 But...be forewarned...as others have  stated, Excel is not really a ML toll, or an AI tool or anything even remotely close.","I have used Jason Brownlee's excel based demos for understanding basic ML concepts. I dont think they can be extended to be used for real data crunching but here it is: 
 https://machinelearningmastery.com/master-machine-learning-algorithms/",62.29653745,61.75778782,61.69366728,60.60843533,54.05580953,54.86603348,53.73606279,55.39033182,54.08662533
4925,VM image for data science projects,python,"There is another choice which popular recently: docker( https://www.docker.com ). Docker is a container and let you create/maintain a working environment very easily and fast.  
 
 install essential tools for data science in python 
 
 https://registry.hub.docker.com/u/ceshine/python-datascience/ 
 
 use r language to do data science
 
 https://github.com/rocker-org/rocker 
 
 
 Hope that would help you.","If you are looking for a VM with a bunch of tools preinstalled, try the  Data Science Toolbox .","While  Docker  images are now more trendy, I personally find  Docker  technology not user-friendly, even for advanced users. If you are OK with using  non-local  VM images and can use  Amazon Web Services (AWS) EC2 , consider R-focused images for data science projects, pre-built by Louis Aslett. The images contain very recent, if not the latest, versions of  Ubuntu LTS ,  R  and  RStudio Server . You can access them  here . 
 Besides main components I've listed above, the images contain many useful data science tools built-in as well. For example, the images support LaTeX, ODBC, OpenGL, Git, optimized numeric libraries and more.","Did you try Cloudera's QuickStart VM?: 
 
 http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.html 
 
 I found it very easy to run it and it includes open source software such as  Mahout  and  Spark .","Today I used  this repository  and built it with docker. It is a docker image building spark based on Hadoop image of the same owner. If you to use spark, it has a python api called  pyspark .",,,,,59.78852712,59.46853329,69.08149472,50,53.79769975,,,,
4836,Tutorials on topic models and LDA,topic-model,"If you're working in R, Carson Sievert's tutorial on using LDA to model topics in movie reviews is an excellent starting point:   
 http://cpsievert.github.io/LDAvis/reviews/reviews.html 
 This tutorial makes use of LDAvis, an interactive visualization of topic and word distributions that can really aid intuition. 
 Also, although not short, David M. Blei's lectures on topic models are a great resource for understanding the meaning behind the parameters:  http://videolectures.net/mlss09uk_blei_tm/","I highly recommend this tutorial:  Getting Started with Topic Modeling and MALLET 
 Here are some additional links to help you get started... 
 Good introductory materials (including links to research papers):  http://www.cs.princeton.edu/~blei/topicmodeling.html 
 Software: 
 
 MALLET (Java):  http://mallet.cs.umass.edu/topics.php 
 
 topic modeling developer's guide:  http://mallet.cs.umass.edu/topics-devel.php 
 
 gensim (Python):  http://radimrehurek.com/gensim/ 
 topicmodels (R):  http://cran.r-project.org/web/packages/topicmodels/index.html 
 Stanford Topic Modeling Toolbox (designed for use by social scientists):  http://www-nlp.stanford.edu/software/tmt/tmt-0.4/ 
 Mr.LDA (scalable topic modeling using MapReduce):  http://lintool.github.io/Mr.LDA/ 
 
 If you're working with  massive  amounts of input text, you might want to consider using Mr.LDA to build your topics models -- its MapReduce-based approach might be more efficient when working with lots of data. 
 
 
 Even more here on the Biased Estimates blog:  Topic Models Reading List","If you are looking for something simple to start with and easy to implement, I would recommend this. 
 Beginners Guide to Topic Modeling",The CLARIN-D project has collected some good pointers to tutorials for topic modeling and LDA on the  Teaching ans Learning Materials Collection (TeLeMaCo)  site hosted by the  Universität des Saarlandes  CLARIN centre.,"I suggest trying  Machine Learning Plu's   Gensim  tutorial. It will give you a holistic overview, on NLP and LDA, including: how to pre-process your data, do feature engineering and apply LDA.",,,,,70.01582027,76.91639746,61.71398038,79.1809882,63.71926203,,,,
3770,"How to merge monthly, daily and weekly data?",time-series,"when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ? 
 
 For your timeseries analysis you should do both: get to the highest granularity possible with the daily dataset, and also repeat the analysis with the monthly dataset. With the monthly dataset you have 120 data points, which is sufficient to get a timeseries model even with seasonality in your data. 
 
 For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ? 
 
 To obtain say weekly or monthly data from daily data, you can use smoothing functions. For financial data, you can use moving average or exponential smoothing, but if those do not work for your data, then you can use the spline smoothing function ""smooth.spline"" in R:  https://stat.ethz.ch/R-manual/R-patched/library/stats/html/smooth.spline.html 
 The model returned will have less noise than the original daily dataset, and you can get values for the desired time points. Finally, these data points can be used in your timeseries analysis.  
 
 For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ? 
 
 To obtain daily data when you have monthly or weekly data, you can use interpolation. First, you should find an equation to describe the data. In order to do this you should plot the data (e.g. price over time). When factors are known to you, this equation should be influenced by those factors. When factors are unknown, you can use a best fit equation. The simplest would be a linear function or piecewise linear function, but for financial data this won't work well. In that case, you should consider piecewise cubic spline interpolation. This link goes into more detail on possible interpolation functions:  http://people.math.gatech.edu/~meyer/MA6635/chap2.pdf .  
 In R, there is a method for doing interpolation of timeseries data. Here you would create a vector with say weekly values and NAs in the gaps for the daily values, and then use the ""interpNA"" function to get the interpolated values for the NAs. However, this function uses the ""approx"" function to get the interpolated values, which applies either a linear or constant interpolation. To perform cubic spline interpolation in R, you should use the ""splinefun"" function instead. 
 Something to be aware of is that timeseries models typically do some sort of averaging to forecast future values whether you are looking at exponential smoothing or Auto-Regressive Integrated Moving Average (ARIMA) methods amongst others. So a timeseries model to forecast daily values may not be the best choice, but the weekly or monthly models may be better.","I'm not an expert in this area, but I believe that your question is concerned with  time series aggregation and disaggregation . If that is the case, here are some hopefully relevant resources, which might be helpful in solving your problem (first five items are main, but representative, and last two are supplementary): 
 
 Temporal Aggregation and Economic Time Series 
 Temporal Disaggregation of Time Series  (IMHO, an excellent overview paper) 
 CRAN Task View: Time Series Analysis  (R-focused) 
 Introduction to R's Time Series Facilities 
 Working with Financial Time Series Data in R 
 Notes on chapters contents for the book ""Time Series Analysis and Forecasting"" 
 Discussion on Cross Validated  on daily to monthly data conversion (Python-focused)","This won't be a very satisfying answer, but here's my take... 
 
 For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ? 
 For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ? 
 
 Same answer for both: you can't do this for unknown properties, and for known properties it will depend on how the values were computed. 
 As you alluded to: 
 
 (an average of fiancial rates would be a non-sense for example) 
 
 There is no single transformation that will be appropriate in all cases, whether the properties/values are known or unknown. Even with known properties, you'll likely need a unique transformation for each type: mean, median, mode, min, max, boolean, etc. 
 
 when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ? 
 
 Whenever possible, try to preserve the full granularity of the smallest possible step. Assuming you know how to transform the values, you can always roll-up the steps (e.g., day to month, month to year)... but you won't necessarily be able to reconstruct smaller steps from larger ones following a lossy conversion.","For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ? 
 
 Aggregation.   
 For example, you have the number of time people searched for 'widgets' every day.  Add up the daily totals for a month to get monthly totals.  I would need to see more specifics about the actual data collected at each granularity to give you a more complete version. 
 
 For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ? 
 
 You can't.   
 In physics, a comparable idea is the  Nyquist frequency .  The general idea is that you can't add more information than what you already have present in your data without bringing in more data.  Given only the day someone ran a query, how can you tell what time of day that query was ran?  You may be able to make some inferences, but the only way to answer the question is to directly or indirectly bring in more information to the system.  There are things you can do to make informed guesses at the daily state of monthly variables (as 
gchaks mentioned, interpolation), but your data is still fundamentally monthly data stretched to look daily. 
 
 When given two time series with different time steps, what is better: Using the Lowest or the biggest time step ? 
 
 That totally depends on what you're trying to answer.   
 The smaller granularity will be more sensitive to noise and other anomalies.  The lager granularity will be able to answer questions more confidently, but loose some of it's usefulness.  For example, if you're trying to see when people start looking up venues to weekend plans to know when to launch marketing campaigns for a new night club, you'll want to be looking at daily data, if not smaller.  If you're looking at the general trending of night clubs to figure out who you want to invest in, then monthly would probably be better.","Being able to aggregate data based on date segments is a piece of cake using Tableau software. You would simply plug your data into the tool, and then you can drag and drop both the metric and date dimensions onto a report body space. Tableau will instantaneously whip up aggregate visualizations and/or detail data tables, on the fly. You can group/sum by year, quarter, month, week, day, hour, etc. (standard, built in, out of the box functionality offered by the tool) 
 Also, if you wanted to incorporate additional data later on (which I assume is what you meant by 'unknown properties'), you can import another data set and easily append it onto the first one, as long as it also offers dates. 
 I would recommend checking out the free version, which I believe is called Tableau Public.",,,,,70.03927445,54.13362324,63.25044342,70.69796942,51.71796569,,,,
2651,Deep learning basics,machine-learning,"This  link  contains an amazing amount of deep learning literature.
Summarizing it here(going in the order a beginner ideally should)-
NOTE: All these resources mainly use python. 
 1) First of all, a basic knowledge of machine learning is required. I found Caltech's Learning from data to be ideal of all the machine learning courses available on the net. 
 Andrew Ng's Coursera course is pretty good too. 
 2) For Neural networks, nobody explains it better than  Dr.Patrick Winston .
The assignments should be tried out for better understanding. They are in python. 
 3) For a better understanding of Neural Networks,  Michael Nielsen 's course should be done(as suggested by Alexey). It is pretty basic but it works. 
 4) For deep neural networks, and implementing them faster on GPUs, there are multiple frameworks available, such as  Theano ,  Caffe ,  Pybrain ,  Torch ,etc.
Out of these Theano provides a better low level functionality that allows its user to create custom NNs. It is a python library, so being able to use numpy,scikit-learn, matplotlib, scipy along with it is a big plus.
The  deep learning tutorial  written by Lisa Lab should be tried out for a better understanding of theano. 
 5) For Convolutional Neural Networks, follow  andrej karpathy's tutorial . 
 6) For unsupervised learning, follow  here  and  here . 
 7) For an intersection of deep learning and NLP, follow  Richard Socher's class . 
 8) For LSTMs, read  Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780  and  Graves, Alex. Supervised sequence labelling with recurrent neural networks. Vol. 385. Springer, 2012 . 
 Here is LSTM's  Theano code .","The subject is new so most of the wisdom is scattered in papers, but here are two recent books: 
 
 Deep Learning , Yoshua Bengio, Ian J. Goodfellow, Aaron Courville. 
 Deep Learning: Methods and Applications , Li Deng and Dong Yu. 
 
 And  some practical material . 
 
 ACL 2012 + NAACL 2013 Tutorial:  Deep Learning for NLP (without Magic)","Neural Networks and Deep Learning by Michael Nielsen. The book is still in progress, but it looks quite interesting and promising. And it's free!  Here 's the link. 
 There are only 5 chapters so far, and the most of them talk about usual neural networks, but it's still worth having a look. 
 Update:  the book has been finished!","Main references: 
 Courses on deep learning: 
 
 Andrew Ng's course on machine learning  has a nice introductory section on neural networks. 
 Geoffrey Hinton's course:  Coursera Neural Networks for Machine Learning (fall 2012) 
 Michael Nielsen's free book  Neural Networks and Deep Learning 
 Yoshua Bengio, Ian Goodfellow and Aaron Courville wrote a  book on deep learning 
 Hugo Larochelle's course (videos + slides) at Université de Sherbrooke 
 Stanford's tutorial (Andrew Ng et al.) on Unsupervised Feature Learning and Deep Learning 
 Oxford's ML 2014-2015 course 
 NVIDIA Deep learning course (summer 2015) 
 Google's Deep Learning course on Udacity (January 2016) 
 
 NLP-oriented: 
 
 Stanford CS224d: Deep Learning for Natural Language Processing (spring 2015) by Richard Socher 
 Tutorial given at NAACL HLT 2013: Deep Learning for Natural Language Processing (without Magic) (videos + slides) 
 
 Vision-oriented: 
 
 CS231n Convolutional Neural Networks for Visual Recognition  by Andrej Karpathy (a previous version, shorter and less polished:  Hacker's guide to Neural Networks ). 
 
 Toolkit-specific tutorials: 
 
 DL4J (Java) 
 Theano  (Python, Y. Bengio) 
 Machine Learning with Torch7 (Lua, LeCun) 
 H2O Deep Learning (Java) 
 Caffee (C++, UCB) 
 Nervana’s Deep Learning Course",There's also Richard Socher's recent PhD dissertation on intersection of NLP and deep learning:  Recursive Deep Learning for Natural Language Processing and Computer Vision,"For comprehending the derivation of Back propagation algorithm, I suggest  Ryan Harris  youtube video which is less daunting. You may find second video as well.",,,,61.47580923,66.76972058,56.2021134,69.48303515,64.34599014,50,,,
2631,What would be a good way to use clustering for outlier detection?,machine-learning,"A very robust clustering algorithm against outliers is  PFCM from Bezdek . 
 In this paper Bezdek proposes Possibilistic-Fuzzy-C-Means which is an improvement of the different variations of fuzzy posibilistic clustering. This algorithm is particularly good at detecting outliers and avoiding them to influence the clusterization. So using PFCM you could find which points are identified as outliers and at the same time have a very robust fuzzy clustering of your data.","If your Data points are dense and noise points are away from the dense region, you can try  DBSCAN algorithm . 
 
 Tweak its parameters until u get a best fit.","Gaussian mixture modeling can - if your data is nicely gaussian-like - be used for outlier detection. Points with a low density in  every  cluster are likely to be outliers. 
 Works well in idealistic scenarios.","Apply your clustering algorithm 
 Calculate distance from all data points to its assigned cluster 
 Label the data points furthest from a center as an outlier 
 
 Randomly generating 100 data points from three gaussians, clustering them with k-means, and marking the 10 'furthest from a center' data points gave the following graph:
 
 see  this notebook  for the full example 
 The burden of solving what ""distance"" means will already have to be solved for you to run a clustering algorithm.  It will still be up to you to pick off what distance means an outlier.  In this example, I just picked the N most distant data point, though you'll probably want to pick any number of data points over a certain number of standard deviations from a center.","Perhaps you could cluster the items, then those items with the furthest distance from the midpoint of any cluster would be candidates for outliers.",,,,,63.28754026,50,63.45273801,55.63379729,61.19104718,,,,
2543,Can Machine Learning be applied in software developement,machine-learning,"Definately - Yes.
Good question. 
Was thinking about it myself. 
 (1) Collect the data .
The first problem you have: gather enough data. 
All the attributes you mentioned (date, name, check-in title/comment, N of deffects etc) are potentially useful - gather as much as possible.
As soon as you have a big project, a number of developers, many branches, frequent commits and you have started collecting all the data, you a ready to go further. 
 (2) Ask good questions .
The next question you should ask yourself: what effect are you going to measure, estimate and maybe predict.
Frequency of possible bugs? Tracking inaccurate ""committers""? Risky branches? Want to see some groups of users/bugs/commits according to some metrics? 
 (3) Select the model .
As soon as you have the questions formulated, you should follow the general approach in data science - extract needed features in your data, select appropriate model, train you model and test it, apply it. This is too broad process to discuss it this thread, so please use this site to get right answers.","Without a doubt you can. The key is to have a set of hypotheses (i.e. assumptions \ scenarios that you want to evaluate) and wrangle the data together to prove \ disprove what you thought is true.  
 Here are a few things to watch-out for: 
 
 Be ready for Disappointments:  Often times, once you have invested time and energy in building these models, analysts tend to get biased towards publishing results (publication bias). Treat this as an exploration that with a lot of dead-ends and the goal should be to find the ones that are not. 
 Know your Data:  You cannot will your data into doing things magically without truly understanding it. Ensure that you know the different attributes (predictors and dependents) very well. Knowing your data well will allow you to cleanse it and think about appropriate models. All models don't work equally well on all data - data that has a lot of categorical variables might require creative solutions like Dimension reduction before it can be modeled. 
 Know the ""Operational"" Processes:  Knowing how things operate within your firm will help you refine the set of hypothesis that you want to test. For e.g. in your scenario above, knowing how developers work with your change management software and what types of administrative setups have been done will help you figure out why the data is coming in the way it is. Some developers might only be focused on certain modules that are more mature than others, might work only on certain shifts and that might limit how many lines of code are checked in, how many bugs are found etc. 
 
 Having said that here are some scenarios you might want to test: 
 
 Developer Effectiveness : 
How different developers working on same modules overtime has resulted into increase or decrease in bugs. 
Does more line of code results in more bugs? Maybe this might be an indicator that the programs need to be split further into smaller components
Folks might be more productive during certain times of day than others - does time of day affect bug introductions? 
 Module Maturity: 
Which Modules have the most number of issues? 
Are they worked upon by more developers or less?
Do defects keep aging for a long time before they are fixed? 
 
 Of course, these questions will change depending on what you are working on.  
 Hope this helps.","Here are two ideas that I was thinking about. 
 
 Bug Prediction 
 
 Based on the previous activity future bugs can be predicted. There are a number of papers available on the net. I remember there was a paper about bug prediction from SVN data. ( Just google for software bug prediction ) 
 
 Error position from software traces 
 
 Errors may be like rare events or anomalies in the trace output. May be you can classify the error and pinpoint the procedure that caused the error. I am currently thinking about a system like that now ( Not sure about its success though ). I asked a question here:  https://stats.stackexchange.com/questions/140232/error-position-in-software-trace-file","As you are also looking for examples, then github is a good place to check out. 
 I took a random repository and went to ""Graphs"" on the right hand side, which opens up  contribution frequency graph . There's several tabs next to it that display other aspects of a repository and commit history graphically - commits, code frequency, punch card, etc.","Data analysis is always driven by the request. It could be: ""I want to find out this, so I need to collect those data first. Then I would use this model to analyze"". If you just want to practice, by reviewing your data set, there is one: 
 Task : Which issue affects the ""number of check in "" most?  
 Data set : what you have 
 Model : Correlation (e.g. Spearman, which is nonparametric measure of statistical dependence between two variables)",,,,,51.5346786,53.17539359,52.86262179,50,50,,,,
2510,Graduate Degree Choices for Data Science,career,"Why not do an MSc in ooh...  Data Science ? 
 I wrote a quick review of  UK Data Science Masters ' offerings recently. That should help you get an idea what is offered. Mostly they are mashups of stats and computing, but there are specialisms (health, finance for example) that might interest you. 
 Note that list was compiled for courses that have already started, so some of those courses might not be available for starting next October, or have different syllabus contents.","Every field has their own variation of ""data science,"" so I would suggest choosing a subject that interest you and going from there.   
 I can't offer what the go to subject is for your particular interest.  A graduate degree that would ""get you where you want to go"" is quite a personal understanding, so I can' answer that.  But what I will say is, from my own personal experience, when I graduated with my undergrad degree in economics, I was really interested in data science, and economics allowed me to use data science in a field I'm really interested in.  So I applied to Ph.D programs to further my knowledge and am using data science extensively in many different forms of analysis.   
 My suggestion is to apply to graduate degrees that have interesting subject matter to you and will allow you to use data science as understanding.  You would fit well in an economics degree because of your background :)","UCL - CSML. It covers computer science, machine learning and statistics. 
 Firstly, reputation of the university.
Secondly, you are from Mathematics background, hence I assume you don't have sufficient programming knowledge.
Thirdly, Statistics and Machine Learning dominates this field. Employers would prefer these 2 before Mathematics. 
 In short, this course provides everything that you are lacking. HOWEVER, they don't teach programming languages like Java, C++,... but Matlab, R, and Mathematica. Hence, it would be essential if you pick up the former from somewhere.","This is a  tricky  question, because it's easy and difficult at the same time.  Easy , because there is a lot of resources that potentially can help you make a decision on the topic.  Difficult , because the situation is very different for a particular person (not to mention that their interest might change at any time), which makes extremely difficult for other people to give you a  good  advice and for you to make the  right  decision. 
 As for  data science career  options, you can certainly consider a  degree path  (MS or MS + PhD), but you need to be aware of  other options . For a comprehensive resource, dedicated to  data science and related degree programs  (both  traditional  and  online ), please visit this page:  http://www.kdnuggets.com/education/index.html . A  comprehensive  review of  all  these offerings is IMHO an enormous task and is  far  beyond an answer here on Stack Exchange or, even, a lengthy blog post. 
 However, nowadays one is not limited to  traditional educational options  and I think it's important to be aware of  other educational options . One of the other options include  certifications  (linked at the above-mentioned page, but, in my opinion, the only certification worth considering is the  Certified Analytics Professional  as a  solid  and  vendor-neutral  certification from a reputable INFORMS). Another option is recently booming  data science intensive educational offerings , from  short-term  (and often too commercial, to put it lightly)  bootcamps  to  more solid offerings , including free, but competitive, ones, such as  Insight Data Science Fellows Program , where one needs to be a PhD to apply, or its sister program  Insight Data Engineering Fellows Program , which doesn't have such requirement. Finally, there is yet another option:  self-study . It partially intersects with the certificate option, if one uses  massive open online courses (MOOC)  (a review of which deserves a separate comprehensive post), but there are  open curricula  that might suit one better, such as the  Open Source Data Science Masters  curriculum, linked in my  earlier relevant answer . 
 P.S.  While your question focuses on data science, I think that it may be wise to at least consider another career path, given your  math background . I'm talking about  operations research  field, which is not that far away from data science (and even somewhat intersects with it). While similar, data science is IMHO more statistics-focused, whereas operations research is more math-focused, at least that's how I see it. Despite all the popularity and ""gold rush"" of data science, operations research career is a solid one, just not as hot. Of course, if you're excited about things like artificial intelligence, machine learning and, especially, deep learning, data science career is the way to go. Whatever you will choose, the good thing is that with your math background it will be easy to change focus, should you decide to. Hope this helps. Good luck!","Data scientists are in high demand now and appear to be in the near future so looks like you're thinking in the right direction!  
 A recent McKinsey study predicts there will only be 200,000 data scientists to fill the 490,000 data science jobs by 2018. 
 While the vast majority of data scientist now have an advanced degree in a quantitative field such as mathematics, computer science or econometrics, it is not  necessary .  One of my best friends is a leading data scientist at a Fortune 500 company without earning an advanced degree.  You can find his personal story here  http://bit.ly/2aA6PHk .  
 I would recommend checking out The Open Source Data Science Masters site at: 
 http://datasciencemasters.org/ 
 Clare Corthell has done a masterful job of curating a list of free or inexpensive resources to learn the topics of data science, including statistics and data analysis. 
 Best of luck!",,,,,58.21000316,78.00247341,51.28537103,62.28059849,61.95983494,,,,
2504,Deep Learning vs gradient boosting: When to use what?,machine-learning,"Why restrict yourself to those two approaches? Because they're cool? I would always start with a simple linear classifier \ regressor. So in this case a Linear SVM or Logistic Regression, preferably with an algorithm implementation that can take advantage of sparsity due to the size of the data. It will take a long time to run a DL algorithm on that dataset, and I would only normally try deep learning on specialist problems where there's some hierarchical structure in the data, such as images or text. It's overkill for a lot of simpler learning problems, and takes a lot of time and expertise to learn and also DL algorithms are very slow to train. Additionally, just because you have 50M rows, doesn't mean you need to use the entire dataset to get good results. Depending on the data, you may get good results with a sample of a few 100,000 rows or a few million. I would start simple, with a small sample and a linear classifier, and get more complicated from there if the results are not satisfactory. At least that way you'll get a baseline. We've often found simple linear models to out perform more sophisticated models on most tasks, so you want to always start there.","On the lines of what @Simon has already said: 
 
 Deep learning approaches have been particularly useful in solving problems in vision, speech and language modeling where feature engineering is tricky and takes a lot of effort. 
 For your application that does not seem to be the case since you have well defined features and only feature interactions etc. are required. 
 Given that deep learning models currently need a lot of computing resources and scientist time in coding stuff up I'd suggest opting for a non-deep learning approach. 
 
 For your problem the  effort vs benefit  tradeoff does not seem to be in deep learning's favour.  DL would be an overkill","From my perspective, for 5 million instances you need lots of trees to get a good generalization bound (a good model in the layman term). If this is not a problem then go for it,even the exact answer is relying on the nature of your problem. GBT is a good method especially if you have mixed feature types like categorical, numerical and such. In addition, compared to Neural Networks it has lower number of hyperparameters to be tuned. Therefore, it is faster to have a best setting model. One more thing is the alternative of parallel training. You can train multiple trees at the same time with a good CPU. If you are not satisfied with the results then go for Neural Nets since that means your model should be more extensive and should learn higher order information through your data. That is the due of NNs compared to other learning algorithms.","In addition to other answers (and there's some good link in the comments) it depends on what the problem is or what kinds of questions you want to answer. As I can only suggest based on my own experience, then in case of a classification task, the possible methods can be severely limited based on class balance in dataset.  
 Once you go to a larger than around 1:10 class imbalance, then most classification methods just stop working. You'll be left with methods based on random forest and maybe neural nets (haven't tried yet). I work with the class balance in the range of 1:500 to 1:1000 and have found that neither down- or upsampling works. Luckily my dataset is ""only"" 6mln observations by 200 variables and I'm able to run boosted trees on the whole set in reasonable time. 
 So to directly answer your question: 
 
 you should come up with a bunch of questions you would want to answer and in case of classification then check the class balances of the target variables. 
 you should check the distribution (not in mathematical sense) of missing values in all of your data and document what you find. Some ML methods are fine with missing values while others are not and you need to look into data imputation (which has its own set of rules and guidelines and problems).","It is very hard to beat gradient boosted trees with native support for categorical features such as  this  on a tabular data set.  Assuming you data has not temporal/spatial/order structure (like in speech/image/text) I am very doubtful you will get a better results with deep learning. Having said that, the no-free-lunch theorem states that there is no algorithm which is the best for all data. It is n emperical science.","When you have such large data set you can play with any of the statistical and machine learning modelling techniques and that is highly encouraged. As other have suggested I would also recommend to take a few million random samples from data and play with that. Since this is a classification problem I would follow simple classification techniques first and then go on with more complex ones later. Logistic regression is great to start with.  
 I wanted to add that  generative models  must also be tried out.  Naive Bayes classifier  is one of the simplest probabilistic classifiers and it outperforms many complex methods like support vector machines on many tasks. You can look at  this  simple implementation of NB and a  this  link for comparison of NB to logistic regression.  
 One can build a Naive bayes (NB) classifier as a baseline model and then go for any machine learning technique like Support vector machines(SVM) or multilayer perceptrons (MLP). A trade off here is that NB is computationally less expensive than MLP so better performance from MLP is desired.  
 Coming to your exact query: Deep learning and gradient tree boosting are very powerful techniques that can model any kind of relationship in the data. But what if in your case a simple logistic regression or NB is giving desired accuracy. So its always better to try out the simple techniques first and have a baseline performance. Then one can go for the complex models and compare with the baseline.","According to me, you should check out the correlation heatmap of the parameters, and if the parameters are having a very little correlation with each other, you should go for deep learning models. Since in DL models, each independent parameter is fed into model parallelly, and updating each weight has a unique path in the back-propagation process. whereas in gradient boosting algorithm, weights are updated are sequentially.","Try throwing your dataset into autogluon from Amazon, it will try trees, neural networks and other techniques for you, telling you what is best, and also constructing an ""ensemble"" model from the best competitors, that may be better than any one single technology.",,52.55321768,59.90107463,50.93089734,51.23961889,60.63391728,55.67503318,61.72140014,50,
2368,Machine learning - features engineering from date/time data,machine-learning,"I would start by graphing the time variable vs other variables and looking for trends.   
 For example 
 
 In this case there is a periodic weekly trend and a long term upwards trend.  So you would want to encode two time variables: 
 
 day_of_week 
 absolute_time 
 
 In general 
 There are several common time frames that trends occur over: 
 
 absolute_time 
 day_of_year 
 day_of_week 
 month_of_year 
 hour_of_day 
 minute_of_hour 
 
 Look for trends in all of these.   
 Weird trends 
 Look for weird trends too.  For example you may see rare but persistent time based trends: 
 
 is_easter 
 is_superbowl 
 is_national_emergency 
 etc. 
 
 These often require that you cross reference your data against some external source that maps events to time.   
 Why graph? 
 There are two reasons that I think graphing is so important. 
 
 Weird trends 
While the general trends can be automated pretty easily (just add them
every time), weird trends will often require a human eye and knowledge
of the world to find.  This is one reason that graphing is so
important. 
 Data errors 
All too often data has serious errors in it.  For example, you may find that the dates were encoded in two formats and only one of them has been correctly loaded into your program.  There are a myriad of such problems and they are surprisingly common.  This is the other reason I think graphing is important, not just for time series, but for any data.","One more thing to consider, beyond everything that Ben Haley said, is to  convert to user local time . For example, if you are trying to predict something that occurs around 8pm for all users, if you look at UTC time, it will be harder to predict from.","Divide the data into windows and find features for those windows like autocorrelation coefficients, wavelets, etc. and use those features for learning. 
 For example, if you have temperature and pressure data, break it down to individual parameters and calculate features like number of local minima in that window and others, and use these features for your model.","In several cases, data and events inside a time series are seasonal. In such cases, the month and the year of the event matters a lot. Hence, in such scenarios you can use binary variables to represent if the event is during a given month/year or not. 
 Hope this answers your question. If not, kindly be a little more specific on what exactly are you trying to achieve.","As  Ben  and  Nar  nicely explained, breaking down the date-time object into buckets of date and time parts would help detect seasonal trends, where the complete (and usually even worse - unique) date-time object would miss it 
 You didn't mention any specific machine learning algorithm you're interested in, but in case you're also interested with distance-based clustering, like k-means, I'd  generalize  the date-time object into the  unix-time format .
This would allow for a simple numerical distance comparison for the algorithm, simply stating how far 2 date values are. 
 In your example I'd generalize the date-only value 2014-05-05 to 1399248000 (the unix time representing the start of may the 5th 2014, UTC). 
 [One could argue that you can achieve that by bucketing the date-time into every possible date-time part.. but that would significantly increase your dataset dimensions. So, I'd suggest combining the unix-time, for distance measuring, and some of the date-time buckets]","Depending on what you are interested in with the date/time info, you might just want to bin it.  For e.g., if you are interested in distance from a starting point (e.g., Jan 1, 2015), and you want to measure it in months, I would just code it as month 1 (for Jan 1-31, 2015), 2 (Feb 1-28, 2015), 3, 4, 5, 6, etc. Since the distance between the start dates are approximately the same, this represents time distance in a straightforward continuous format.  And I say continuous because you can say month 6.5 and know that it is half-way through June, 2015. Then you don't have to worry about actual date coding and you can use all your typical classification methods.   
 If you want to measure in days, I know MySql has a 'to_days' function, if you happen to use that to pull data prior to classification.  Python probably has something similar, or use the unix-time format suggested by mork. 
 Hope this helps!","Ben is talking about  the static features , and make use of the timestamp features.   
 As an extension, i will introduce  the lag features , I am not talking the raw time series, but the aggregates on it. 
 The most mystical part is that the future value is unseen for us, how can we use that aggregate features in the training data? 
 A little example:  
There is yearly electric consumption data from 1991 to 2015, I want predict the electric consumption in the future 5 years, 2016 to 2020. I will calculate the last 5 years moving average of electric consumption as the 2020's feature values, but the 2016 to 2020 is unknown for us, so we leading (opposite the lagging) the time series 5 years, lets do the moving average on 2010 to 2015, then use this value as 2020's feature values. So, we can construct the future 5 years' feature data. 
 The next step is just using the moving function (count\mean\median\min\max.etc) and try different windows, then you will construct lots of features!","Plot graphs with different variations of time against the outcome variable to see its impact. You could use  month, day, year as separate features and since month is a  categorical variable, you could try a box/whisker plot and see if there are any patterns. For numerical variables, you could use a scatter plot.","I don't know if this is a common/best practice, but it's another point of view of the matter. 
 If you have, let's say, a date, you can treat each field as a ""category variable"" instead a ""continuous variable"". The day would have a value in the set {1, 2... ,31}, the month would have a value in {1,...,12} and, for the year, you choose a minimum and a maximum value and build a set. 
 Then, as the specific numeric values of days, months and years might not be useful for finding trends in the data, use a binary representation to encode the numeric values, being each bit a feature. For example, month 5 would be  0 0 0 0 1 0 0 0 0 0 0 0  (11 0's an a 1 in 5th position, each bit being a feature). 
 So, having, for example, 10 years in the ""year's set"", a date would be transformed into a vector of 43 features (= 31 + 12 + 10). Using ""sparse vectors"", the amount of features shouldn't be a problem. 
 Something similar could be done for time data, day of the week, day of the month... 
 It all depends of the question you want your machine learning model to answer.",53.75688209,52.14750291,55.50893062,51.29074353,70.81650536,58.85739788,55.39476701,51.5237441,58.56560156
2303,Python and R good tutorials?,machine-learning,"There is an online data science ""game"" that takes you from learning how to use Python for loading a csv and using scikit to machine learning algorithms such as support vector machines. Here is a  blog post  with a demo video and the actual site is  Explore Data Science . Personally, I think its genius.","I have found the video tutorial/IPython notebook format really helped me get into the python ecosystem. 
 There were two tutorials at SciPy 2013 that cover sklearn ( part 1 of 1st tutorial ,  github repo for notebooks ).  
 Similar tutorials, from PyCon2012 and PyData2012, are out there for pandas but I don't have the rep to link searching for  pandas tutorial  on youtube should allow you to find them.  
 Since you mention Kaggle, I guess you will have seen their getting started with python tutorial for the titanic passenger dataset (I don't have the rep here to provide a link but searching for  Getting Started with Python: Kaggle's Titanic Competition  should get you there).","There are really so many good resources now.  If you want to stay away from textbooks, both O'Reilly Media and Packt Publishing offer much lighter but effective reading on a lot of great topics.  These books are much more applied in practice. 
 As far as learning the languages go, Coursera, Udacity, Code Acadmey, and Code School have great tutorials.  I would recommend taking a look at the following: 
 Coursera AI and Stats Courses 
 Udacity Data Science courses","I can only recommend  Advanced R  by Hadley Wickham. I think it is at the same time incredibly rich in content and easy to read. You say you have zero knowledge in R, but I believe since you already have programming skills in other languages this book can complement very fruitfully any classical ""R beginner manual"" (for the latter see  here ).",The Art of R Programming  by Normal Matloff is a great way to find your way towards being an R user.  I've recommended this book to several people navigating the tutorial / book universe and to my knowledge they've all stuck with it.,The  R Programming Wikibook  is a nice collaborative handbook for R.,"I would recommend those materials: 
 
 Python

 
 Python for Data Analysis  - book which nicely covers Pandas workflow with IPython. 
 Hands-On Machine Learning with Scikit-Learn and TensorFlow  - slightly more advanced book about using Scikit-Learn and Tensor flow in data science projects 
 
 R

 
 Coursera Data Science Specialisation  - free nine one month time consuming courses which introduces R step by step from beginning to machine learning. 
 Data Camp courses  - at least 4 free courses covering topics from data analysis.","If you prefer quick hands-on/interactive tutorials, below are my suggestions -  
 Python -  codeacademy , Google Python Class 
 R - CodeSchool's  'Try R'  and DataCamp (suggested above)","Also,  Green Tea Press  offers free books on related topics such as an intro to Python and using python with Probability and Stats.",51.65481317,62.86920958,53.38095882,50,53.78431337,50,52.09423272,60.43411329,54.61498354
2269,Any Online R console?,r,"R On Cloud  provides a browser-embedded R-console. 
 
 
 
 Jupyter.org  evolved from the  IPython Project  (the language-agnostic parts of IPython); supports Python 3, Julia, R, Haskell, Ruby, etc.","While I have only had a brief look at it, I think  CoCalc  (formerly  SageMathCloud ) looks quite promising. I have recommended it to at least one person previously, and they seemed to be quite happy with it. Beyond R support, you also get access to Python, SAGE (as the name indicates), and a few other things.  
 EDIT: Make sure to check the  documentation  on how to get an R (as opposed to a Python) session in a worksheet.",Yes. I believe  this  is what you are looking for.,You can easily have an RStudio server installed in Digital Ocean using  this  package.,"RStudio Server  is definately one of the options, meant exactly for this. I've thought about using it with a cloud virtual machine, but haven't had the need yet. But when I (probably) need to prepare an intro data analysis class for the fall semester, then Rstudio Server is the first option I'll be trying out.","I am using this one, so far so good. 
Online terminals:  http://www.tutorialspoint.com/codingground.htm   
 Also,  R-Fiddle  is an option.","RStudio Cloud  is the best I've used.  
 It offers the total R-Studio experience online. Most other sandboxes from this list either didn't work, were permanently closed, or required monthly fees. 
 RStudio Cloud is free, but does require a login, but you can sign in with your Google account, and it saves your progress (History and such) so you can access it from any computer. 
 Here's a screenshot from my first project, you can see it's the same as the R-Studio you can download. I haven't tried to hit it with anything heavy yet, but I assume it's probably not as nimble as a local instance - and most likely depends on how many people are using it at the same time.","Try out AirXcell :  AirXcell calculation software .
See documentation  Use AirXCell as an r Console","https://www.codeschool.com/  is very similar to  https://www.datacamp.com/ 
when I tried it I fell in love with R and then found datacamp.
www.codecademy.com  is also console-based but R is not yet available.",59.27168483,50,50,50,50,61.02204443,53.98389386,59.24448065,65.51297237
2258,Where to start on neural networks,machine-learning,"No, you should go ahead and learn the maths on your own. You will ""only"" need to learn calculus, statistics, and linear algebra (like the rest of machine learning). The theory of neural networks is pretty primitive at this point -- it more of an art than a science -- so I think you can understand it if you try. Ipso facto, there are a lot of tricks that you need practical experience to learn. There are lot of complicated extensions, but you can worry about them once you get that far. 
 Once you can understand the Coursera classes on ML and neural networks (Hinton's), I suggest getting some practice. You might like  this  introduction.","I would say... it really depends. You may need to:  
 
 use  machine learning algorithms: this will be useful for specific applications you may have. In this situation what you need is some programming skills and the taste for testing (practicing will make you strong). Here maths are not so much required I would say.  
 be able to  modify  existing algorithms. Your specific application may be reticent to regular algorithms, so you may need to adapt them to get maximum efficiency. Here maths come into play.  
 understand the theory behind algorithms. Here maths are necessary, and will help you increase your knowledge of the field of machine learning, develop your own algorithms, speak the same langage as your peers... NN theory may be primitive as said by @Emre, but for instance this is not the case for SVM (the theory behind SVM requires e.g. to understand  reproducing kernel Hilbert spaces ).  
 
 On the mid term for sure you will need strong maths. But you don't need to wait for them to come to you, you can start right now with linear algebra, which is beautiful and useful for everything. And in case you encounter (possibly temporary) difficulties of any sort with maths, keep on practicing the way you already do (many people can talk about the perceptron but are not able to make a perceptron in Java), this is very valuable.","Neural Networks are not a great introductory model, simply because of the complexity that you describe.  If you're trying to get your feet wet, boosted decision trees tend to perform well by comparison, and are a bit more intuitive.  If you want a description on this method, and are already familiar with Coursera, The University of Washington has an introductory course on data science which explains it quite well.",For sure you need to learn some maths.  However you should also make an effort to gain some broader engineering and science skills.  There are far too many people going into computer science and all they know is a few programming languages and math. The end result is a very boring person with little in the way of creativity to do anything new. Take a year out when you are 18 or 19 to travel the world.,"This  is a hell good book. Adrian is going to make black friday sale, so it's great chance to pick it up. It's step by step guide through deep learning with math, intuition and code. It focuses mostly on computer vision, but it'll give you nice start.",,,,,61.8454941,51.22941736,57.64031174,50,53.12712031,,,,
1216,Career switch to Big Data Analytics,career,"Due to high demand, it is possible to start a career in data science without a formal degree. My experience is that having a degree is often a 'requirement' in job descriptions, but if the employer is desperate enough, then that won't matter. In general, it's harder to get into large corporations with formalized job application processes than smaller companies without them. ""Knowing people"" can get you a long way, in either case. 
 Regardless of your education, no matter how high demand is, you must have the skills to do the job. 
 You are correct in noting that advanced statistics and other mathematics are very hard to learn independently. It is a matter of how badly you want to make the career change. While some people do have 'natural talent' in mathematics, everybody does have to do the work to learn. Some may learn more quickly, but everybody has to take the time to learn.  
 What it comes down to is your ability to show potential employers that you have a genuine interest in the field, and that you will be able to learn quickly on the job. The more knowledge you have, the more projects you can share in a portfolio, and the more work experience under your belt, the higher level jobs that will be available to you. You may have to start  in an entry level position first. 
 I could suggest ways to study mathematics independently, but that isn't part of your question. For now, just know that it's hard, but possible if you are determined to make a career change. Strike while the iron is hot (while demand is high).","You should look more into the infrastructure side of things if you don't like maths. The lower you go in the software stack, the further away you get from maths (of the data science sort). In other words, you could build the foundation that others will use to create the tools that will serve analysts. Think of companies like Cloudera, MapR, Databricks, etc. Skills that will come in handy are distributed systems and database design. You are not going to be become a data scientist without maths; that's a ridiculous notion!","In my experience to have a PhD doesn't mean necessarily be good in the enviroment of data science company, I work as data scientist and I'm just an engineer but I've known some universitary teachers who works in collaboration with my company and sometimes I've said them that Their point of view was not right because despite of their ideas and reasonings were right they are not applicables to the company activities, so we had to modify some data models to make them usefull for the company and the results lost their value so we had to seek new models. What I mean is that Data Science is a multidisciplinar area so many different people working together is needed so I think that your skills could be very useful in a data scientist team, you only have to find where you fit ;)","May be it will be a little offtopic, but I'd like to highly recommend you to go through this MOOC  https://www.coursera.org/course/statistics . This is a very good and clear introduction to statistics. It give you a base principles about core field in data science. I hope it will be a good start point for beginning friendship between you and statistics.","I haven't seen this mentioned, but it's important to keep in mind that you may see a decrease in salary. I say this without knowing how much you make, but moving from (I assume) an experienced IT professional to an entry level data scientist level may not earn you as much. 
 Here's a link to the a portion of the 2015 Burtch Works study on Data Science salaries: 
 http://www.burtchworks.com/files/2015/05/DS-2015_Changes-in-Base-Salaries.pdf 
 As you can see, the median salary for level 1 individual contributors is 90k (across the nation). The full report has the breakdown based on region but again, assuming you're an experienced IT professional, you're probably making more than that. 
 Anecdotal story with n=1: One of my classmates in my DS masters program was an experienced Java developer with a house, family, etc. Although he was very interested in data analytics (paid for the program out of pocket) his potential salary doing data analytics wouldn't be able to support the lifestyle he currently had as a Java developer. As a result he essentially ""wasted"" his degree and went back to development. I would really hate to see that happen to more people.","Keep in mind that ""big data"" is an increasingly trendy thing for a company to say they're involved in.  Higher ups might read an article about it in HBR, and say to themselves, ""I've got to get me some of that"" (not that they're necessarily wrong).  
 What this means for you is that the advanced analytics isn't as necessary for that company as just getting something up and running might be.  
 Luckily for you, most of the components said companies might need are free.  Moreover, I believe both Hortonworks and Cloudera have free ""sandbox"" virtual machines, which you can run on your PC, to play around with and get your bearings.  
 Advanced analytics on big data platforms are valuable, to be sure, but many companies need to learn to crawl before they can run.","This is a really strange question in my opinion. Why you're going to move in a new direction if you are not sure that you love this new direction or at least find it very interesting? If you do love Big Data, why do you care about the PhD intelligent creatures that are already in the field? The same amount of PhD creatures are in every area of IT. Please have a quick read at this very nice article  http://www.forbes.com/sites/louisefron/2013/09/13/why-you-cant-find-a-job-you-love/  and then ask yourself if you love Big Data enough and you are ready to add your grain of sand to the mountain of knowledge",,,53.47045524,51.18658914,52.51674054,50.86153667,59.91466637,60.79379224,58.54236527,,
1159,How to do SVD and PCA with big data?,bigdata,"First of all,  dimensionality reduction  is used when you have  many covariated dimensions  and want to reduce problem size by rotating data points into new orthogonal basis and taking only axes with largest variance. With 8 variables (columns) your space is already low-dimensional, reducing number of variables further is unlikely to solve technical issues with memory size, but may affect dataset quality a lot. In your concrete case it's more promising to take a look at  online learning  methods. Roughly speaking, instead of working with the whole dataset, these methods take a little part of them (often referred to as ""mini-batches"") at a time and build a model incrementally. (I personally like to interpret word ""online"" as a reference to some infinitely long source of data from Internet like a Twitter feed, where you just can't load the whole dataset at once).  
 But what if you really wanted to apply dimensionality reduction technique like PCA to a dataset that doesn't fit into a memory? Normally a dataset is represented as a data matrix  X  of size  n  x  m , where  n  is number of observations (rows) and  m  is a number of variables (columns). Typically problems with memory come from only one of these two numbers.  
 Too many observations (n >> m) 
 When you have  too many observations , but the number of variables is from small to moderate, you can  build the covariance matrix incrementally . Indeed, typical PCA consists of constructing a covariance matrix of size  m  x  m  and applying singular value decomposition to it. With  m =1000 variables of type float64, a covariance matrix has size 1000*1000*8 ~ 8Mb, which easily fits into memory and may be used with SVD. So you need only to build the covariance matrix without loading entire dataset into memory -  pretty tractable task .  
 Alternatively, you can select a small representative sample from your dataset and  approximate the covariance matrix . This matrix will have all the same properties as normal, just a little bit less accurate.  
 Too many variables (n << m) 
 On another hand, sometimes, when you have  too many variables , the covariance matrix itself will not fit into memory. E.g. if you work with 640x480 images, every observation has 640*480=307200 variables, which results in a 703Gb covariance matrix! That's definitely not what you would like to keep in memory of your computer, or even in memory of your cluster. So we need to reduce dimensions without building a covariance matrix at all.  
 My favourite method for doing it is  Random Projection . In short, if you have dataset  X  of size  n  x  m , you can multiply it by some sparse random matrix  R  of size  m  x  k  (with  k  <<  m ) and obtain new matrix  X'  of a much smaller size  n  x  k  with  approximately the same properties  as the original one. Why does it work? Well, you should know that PCA aims to find set of orthogonal axes (principal components) and project your data onto first  k  of them. It turns out that sparse random vectors are  nearly orthogonal  and thus may also be used as a new basis.  
 And, of course, you don't have to multiply the whole dataset  X  by  R  - you can translate every observation  x  into the new basis separately or in mini-batches. 
 There's also somewhat similar algorithm called  Random SVD . I don't have any real experience with it, but you can find example code with explanations  here . 
 
 As a bottom line, here's a short check list for dimensionality reduction of big datasets:  
 
 If you have not that many dimensions (variables), simply use online learning algorithms.  
 If there are many observations, but a moderate number of variables (covariance matrix fits into memory), construct the matrix incrementally and use normal SVD.  
 If number of variables is too high, use incremental algorithms.","Don't bother. 
 First rule of programming- which also applies to data science: get everything working on a small test problem. 
 so take a random sample of your data of say 100,000 rows. try different algorithms etc.  once you have got everything working to your satisfaction, you can try larger (and larger) data sets - and see how the test error reduces as you add more data. 
 furthermore you do not want to apply svd to only 8 columns: you apply it when you have a lot of columns.","PCA is usually implemented by computing SVD on the covariance matrix. 
 Computing the covariance matrix is an  embarrassingly parallel  task, so it scales  linear  with the number of records, and is trivial to distribute on multiple machines! 
 Just do one pass over your data to compute the means. Then a second pass to compute the covariance matrix. This can be done with map-reduce easily - essentially it's the same as computing the means again. Sum terms as in covariance are trivial to parallelize! You may only need to pay attention to numerics when summing a lot of values of similar magnitude. 
 Things get different when you have a huge number of  variables . But on an 8 GB system, you should be able to run PCA on up to 20.000 dimensions in-memory with the BLAS libraries. But then you may run into the problem that PCA isn't all that reliable anymore, because it has too many degrees of freedom. In other words: it overfits easily. I've seen the recommendation of having at least 10*d*d records (or was it d^3). So for 10000 dimensions, you should have at least a billion records (of 10000 dimensions... that is a lot!) for the result to be statistically reliable.","Although you can probably find some tools that will let you do it on a single machine, you're getting into the range where it make sense to consider ""big data"" tools like Spark, especially if you think your data set might grow. Spark has a component called MLlib which supports PCA and SVD.  The documentation has examples .",We implemented SVD to a larger data set using PySpark. We also compared consistency across different packages. Here is the  link.,"I would reccomend python if you lazily evaluate the file you will have a miniscule memory footprint, and numpy/scipy give you access to all of the tools Octave/Matlab would.",,,,56.07177372,56.47924248,59.2937012,67.96715029,59.64118649,50,,,
1095,Gini coefficient vs Gini impurity - decision trees,data-mining,"No, despite their names they  are not  equivalent or even that similar.  
 
 Gini impurity  is a measure of misclassification, which applies in a multiclass classifier context.  
 Gini coefficient  applies to binary classification and requires a classifier that can in some way rank examples according to the likelihood of being in a positive class.  
 
 Both could be applied in some cases, but they are different measures for different things. Impurity is what is commonly used in  decision trees .","I took an example of Data with two people A and B with wealth of unit 1 and unit 3 respectively. 
Gini Impurity as per Wikipedia = 1 - [ (1/4)^2 + (3/4)^2 ] = 3/8 
 Gini coefficient as per Wikipedia would be  ratio of area between red and blue line to the total area under blue line in the following graph 
 
 
 
 Area under red line is 1/2 + 1 + 3/2 = 3 
 Total area under blue line = 4 
 So Gini coefficient = 3/4 
 Clearly the two numbers are different. I will check more cases to see if they are proportional or there is an exact relationship and edit the answer. 
 Edit: I checked for other combinations as well, the ratio is not constant. Below is a list of few combinations I tried.","I believe they represent the same thing essentially, as the so-called: 
 ""Gini Coefficient"" mainly used in Economics, measures the inequality of a numerical variable, such as income, which we can treat as a regression problem--getting the ""mean of each group. 
 ""Gini impurity"" mainly used in Decision Tree learning, measures the impurity of a categorical variable, such as colour, sex, etc. which is a classification problem -- getting the ""majority"" of each group. 
 Sounds similar right? ""inequality"" and ""impurity"" are both measures of variation, which are intuitively the same concept. The difference is ""inequality"" for numerical variables and ""impurity"" for categorical variables. And both of them can be named ""Gini Index"".  
 
 In  Light, R. J., & Margolin, B. H. (1971). An analysis of variance for categorical data , it says that as the ""mean"" is an undefined concept for categorical data, Gini extends the ""Gini Index"" from numerical data to categorical data by using pairwise difference instead of deviation from mean. TL;DR
which comes to the variation for categorical responses: 
 $$\frac1{2n}[\sum_{i\neq j}n_in_j] = \frac{n}2 - \frac1{2n}\sum^I_{i=1}n_i^2$$ 
where  $n_i$  is the number of responses in the  $i$ th category,  $i = 1, \cdot\cdot\cdot, I$ 
which is almost the same, but  $\frac{n}2$  times the ""Gini Impurity"" nowadays, 
 $$1 - \sum^{I}_{i=1} {p_i}^{2}$$ 
 
 By the way, you said you can use ROC as method 2 to choose split point when growing a decision tree, I can't get it. Could you elaborate that?  
 PS: I agreed with  Pasmod Turing's  answer, that Wikipedia can be modified by everyone, and the ""Gini Impurity"" seems like an incomplete item in the wiki. 
 I also saw the disputes in the comments under his answer, I must say Machine Learning is originated from statistics, and statistics is the fundamental analysis tool for scientific research, thus, many concepts are the same thing in statistics, even though they have different names in different professional areas. Gini index certainly share the same name in decision tree and economics.","I think they both represent the same concept. 
 In classification trees, the Gini Index is used to compute the impurity of a data partition. So Assume the data partition D consisiting of 4 classes each with equal probability. Then the Gini Index (Gini Impurity) will be:
 $Gini(D) = 1 - (0.25^2 + 0.25^2 + 0.25^2 + 0.25^2)$ 
 In CART we perform binary splits. So The gini index will be computed as the weighted sum of the resulting partitions and we select the split with the smallest gini index. 
 So the use of Gini Impurity (Gini Index) is not limited to binary situations. 
 Another term for Gini Impurity is Gini Coefficient which is used normally as a measure of income distribution.","Gini impurity is a special instance of Gini coefficient: 
 This is  Gini coefficient 's definition in Wikipedia: 
 
 In economics, the Gini coefficient (/ˈdʒiːni/ JEE-nee), also known as
the Gini index or Gini ratio, is a measure of statistical dispersion
intended to represent the income inequality or the wealth inequality
within a nation or a social group. 
 
 In another words, it measures the inequality of the  wealth  of each  person  in a  nation , with the constraint that the sum of their  wealth  is a  constant . 
 Now replace the above bolded words with: 
 person -> category 
wealth -> probability 
nation -> probability distribution 
constant -> 1 
 The above sentence become:  it measures the inequality of the  probability  of each  category  in a  probability distribution  , with the constraint that the sum of their  probability  is 1. 
 That's exactly the definition of  Gini impurity !","Gini index of 1 would represent wealth concentration to a single person. However, the Gini impurity in this case would be 0. So, they should move in opposite directions, right?",,,,75.43358308,69.56040265,76.77794921,81.28298734,79.76277816,73.60591629,,,
997,Where can I find free spatio-temporal dataset for download?,dataset,"If you have R and the  spacetime  package then you are only  data(package=""spacetime"")  away from a list of space-time data sets bundled with the package: 
 Data sets in package ‘spacetime’:

DE_NUTS1 (air)          Air quality data, rural background PM10 in
                        Germany, daily averages 1998-2009
fires                   Northern Los Angeles County Fires
rural (air)             Air quality data, rural background PM10 in
                        Germany, daily averages 1998-2009
 
 then for example: 
 > data(fires)
> str(fires)
'data.frame':   313 obs. of  3 variables:
 $ Time: int  5863 5870 6017 6018 6034 6060 6176 6364 6366 6372 ...
 $ X   : num  63.9 64.3 64.1 64 64.4 ...
 $ Y   : num  19.4 20.1 19.7 19.8 20.3 ...","First thing that came to mind would be one's personal workout data from running or biking apps. 
 Otherwise there is a dataset around NYC's taxi trip data. Quick Googling brought me this:  http://www.andresmh.com/nyctaxitrips/ . Variables include time and location for both pickups and dropoffs. 
 Another dataset comes from Chicago's bikesharing service. It can be found here:  https://www.divvybikes.com/datachallenge .","You can get some documented, publicly available EEG data from the HeadIT database at UCSD.
 http://headit-beta.ucsd.edu/studies 
 The data itself appears to be in Biosemi Data Format (.bdf) files, described here:  http://www.biosemi.com/faq/file_format.htm 
 Biosemi provides links to several open-source methods to access and import .bdf files on their website, including several functions for importing into Matlab, as well as into Python (BioSig) and C/C++ libraries:
 http://www.biosemi.com/download.htm 
 Just as a forewarning, EEG data can be a bit of a bear to work with, due to it's inherently low signal/noise ratio.","Another idea is to combine OpenStreetMap project map data, for example, using corresponding nice R package ( http://www.r-bloggers.com/the-openstreetmap-package-opens-up ), with census data (population census data, such as the US data:  http://www.census.gov/data/data-tools.html , as well as census data in other categories:  http://national.census.okfn.org ) to analyze temporal patterns of geosocial trends. HTH.",You can get yellow and green taxi trip records from NYC taxi dataset. The website collects data from 2009 until now. You can download the data from the following link:  http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml,,,,,50,50,50,,,,,,
989,SVM using scikit learn runs endlessly and never completes execution,python,"Kernelized SVMs require the computation of a distance function between each point in the dataset, which is the dominating cost of $\mathcal{O}(n_\text{features} \times n_\text{observations}^2)$. The storage of the distances is a burden on memory, so they're recomputed on the fly. Thankfully, only the points nearest the decision boundary are needed most of the time. Frequently computed distances are stored in a cache. If the cache is getting thrashed then the running time blows up to $\mathcal{O}(n_\text{features} \times n_\text{observations}^3)$. 
 You can increase this cache by invoking SVR as 
 model = SVR(cache_size=7000)
 
 In general, this is not going to work. But all is not lost. You can subsample the data and use the rest as a validation set, or you can pick a different model. Above the 200,000 observation range, it's wise to choose linear learners. 
 Kernel SVM can be approximated, by approximating the kernel matrix and feeding it to a linear SVM. This allows you to trade off between accuracy and performance in linear time. 
 A popular means of achieving this is to use 100 or so cluster centers found by kmeans/kmeans++ as the basis of your kernel function. The new derived features are then fed into a linear model. This works very well in practice. Tools like  sophia-ml  and  vowpal wabbit  are how Google, Yahoo and Microsoft do this. Input/output becomes the dominating cost for simple linear learners. 
 In the abundance of data, nonparametric models perform roughly the same for most problems. The exceptions being structured inputs, like text, images, time series, audio. 
 Further reading 
 
 How to implement this. 
 How to train an ngram neural network with dropout that scales linearly 
 Kernel Approximations 
 A formal paper on using kmeans to approximate kernel machines","SVM solves an optimization problem of quadratic order. 
 I do not have anything to add that has not been said here. I just want to post a link the sklearn page about  SVC  which clarifies what is going on: 
 
 The implementation is based on libsvm. The fit time complexity is more
than quadratic with the number of samples which makes it hard to scale
to dataset with more than a couple of 10000 samples. 
 
 If you do not want to use kernels, and a linear SVM suffices, there is  LinearSVC . You'll have to normalize your data though, in case you're not doing so already, because it applies regularization to the intercept coefficient, which is not probably what you want. It means if your data average is far from zero, it will not be able to solve it satisfactorily. 
 What you can also use is stochastic gradient descent to solve the optimization problem. Sklearn features  SGDClassifier . You have to use  loss='epsilon_insensitive'  to have similar results to linear SVM. See the documentation. I would only use gradient descent as a last resort though because it implies much tweaking of the hyperparameters in order to avoid getting stuck in local minima. Use  LinearSVC  if you can.","Did you include scaling in your pre-processing step? I had this issue when running my SVM. My dataset is ~780,000 samples (row) with 20 features (col). My training set is ~235k samples. It turns out that I just forgot to scale my data! If this is the case, try adding this bit to your code: 
 scale data to [-1,1] ; increase SVM speed: 
 from sklearn.preprocessing import MinMaxScaler
scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_train)
X_train = scaling.transform(X_train)
X_test = scaling.transform(X_test)","With such a huge dataset I think you'd be better off using a neural network, deep learning, random forest (they are surprisingly good), etc.  
 As mentioned in earlier replies, the time taken is proportional to the third power of the number of training samples. Even the prediction time is polynomial in terms of number of test vectors. 
 If you really must use SVM then I'd recommend using GPU speed up or reducing the training dataset size. Try with a sample (10,000 rows maybe) of the data first to see whether it's not an issue with the data format or distribution.  
 As mentioned in other replies, linear kernels are faster.","I recently encountered similar problem because forgot to scale features in my dataset which was earlier used to train ensemble model kind. Failure to scale the data may be the likely culprit as pointed by Shelby Matlock. You may try different scalers available in  sklearn, such as  RobustScaler : 
 from sklearn.preprocessing import RobustScaler
 scaler = RobustScaler()
 X = scaler.fit_transfrom(X)
 
 X is now transformed/scaled and ready to be fed to your desired model.","This makes sense. IIUC, the speed of execution of support vector operations is bound by number of samples, not dimensionality. In other words, it is capped by CPU time and not RAM. I'm not sure exactly how much time this should take, but I'm running some benchmarks to find out.","Try normalising the data to [-1,1]. I faced a similar problem and upon normalisation everything worked fine. You can normalise data easily using: 
 from sklearn import preprocessing
X_train = preprocessing.scale(X_train)
X_test = preprocessing.scale(X_test)","Leave it to run overnight or better for 24 hours. 
What is your CPU utilization? If none of the cores is running at 100% then you have a problem. Probably with memory. Have you checked whether your dataset fits into 8GB at all?
Have you tried the SGDClassifier? It is one of the fastest there. Worth giving it a try first hoping it completes in an hour or so.","I just had a similar issue with a dataset which contains only 115 elements and only one single feature (international airline data). The solution was to scale the data. What I missed in answers so far was the usage of a Pipeline: 
 from sklearn.svm import SVR
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler

model = Pipeline([('scaler', StandardScaler()),
                  ('svr', SVR(kernel='linear'))])
 
 You can train  model  like a usual classification / regression model and evaluate it the same way. Nothing changes, only the definition of the model.",50,50,50,,,,,,
937,Does scikit-learn have a forward selection/stepwise regression algorithm?,feature-selection,"No, scikit-learn does not seem to have a forward selection algorithm. However, it does provide recursive feature elimination, which is a greedy feature elimination algorithm similar to sequential backward selection. See the  documentation here","Scikit-learn indeed does not support stepwise regression. That's because what is commonly known as 'stepwise regression' is an algorithm based on p-values of coefficients of linear regression, and scikit-learn deliberately avoids inferential approach to model learning (significance testing etc). Moreover, pure OLS is only one of numerous regression algorithms, and from the scikit-learn point of view it is neither very important, nor one of the best. 
 There are, however, some pieces of advice for those who still need a good way for feature selection with linear models: 
 
 Use inherently sparse models like  ElasticNet  or  Lasso . 
 Normalize your features with  StandardScaler , and then order your features just by  model.coef_ . For perfectly independent covariates it is equivalent to sorting by p-values. The class  sklearn.feature_selection.RFE  will do it for you, and  RFECV  will even evaluate the optimal number of features. 
 Use [an implementation][1] of forward selection by adjusted  $R^2$  that works with  statsmodels . 
 Do brute-force forward or backward selection to maximize your favorite metric on cross-validation (it could take approximately quadratic time in number of covariates). A scikit-learn compatible  mlxtend  package [supports][2] this approach for any estimator and any metric. 
 If you still want vanilla stepwise regression, it is easier to base it on  statsmodels , since this package calculates p-values for you. A basic forward-backward selection could look like this: 
 
 
    from sklearn.datasets import load_boston
    import pandas as pd
    import numpy as np
    import statsmodels.api as sm

    data = load_boston()
    X = pd.DataFrame(data.data, columns=data.feature_names)
    y = data.target


    def stepwise_selection(X, y, 
                           initial_list=[], 
                           threshold_in=0.01, 
                           threshold_out = 0.05, 
                           verbose=True):
        """""" Perform a forward-backward feature selection 
        based on p-value from statsmodels.api.OLS
        Arguments:
            X - pandas.DataFrame with candidate features
            y - list-like with the target
            initial_list - list of features to start with (column names of X)
            threshold_in - include a feature if its p-value < threshold_in
            threshold_out - exclude a feature if its p-value > threshold_out
            verbose - whether to print the sequence of inclusions and exclusions
        Returns: list of selected features 
        Always set threshold_in < threshold_out to avoid infinite looping.
        See https://en.wikipedia.org/wiki/Stepwise_regression for the details
        """"""
        included = list(initial_list)
        while True:
            changed=False
            # forward step
            excluded = list(set(X.columns)-set(included))
            new_pval = pd.Series(index=excluded)
            for new_column in excluded:
                model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()
                new_pval[new_column] = model.pvalues[new_column]
            best_pval = new_pval.min()
            if best_pval < threshold_in:
                best_feature = new_pval.idxmin()
                included.append(best_feature)
                changed=True
                if verbose:
                    print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))

            # backward step
            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()
            # use all coefs except intercept
            pvalues = model.pvalues.iloc[1:]
            worst_pval = pvalues.max() # null if pvalues is empty
            if worst_pval > threshold_out:
                changed=True
                worst_feature = pvalues.idxmax()
                included.remove(worst_feature)
                if verbose:
                    print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))
            if not changed:
                break
        return included

    result = stepwise_selection(X, y)

    print('resulting features:')
    print(result)


This example would print the following output:

    Add  LSTAT                          with p-value 5.0811e-88
    Add  RM                             with p-value 3.47226e-27
    Add  PTRATIO                        with p-value 1.64466e-14
    Add  DIS                            with p-value 1.66847e-05
    Add  NOX                            with p-value 5.48815e-08
    Add  CHAS                           with p-value 0.000265473
    Add  B                              with p-value 0.000771946
    Add  ZN                             with p-value 0.00465162
    resulting features:
    ['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN']


  [1]: http://planspace.org/20150423-forward_selection_with_statsmodels/
  [2]: http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/","As of version 0.24, it does! 
 Announcement ,  documentation","Sklearn DOES have a forward selection algorithm, although it isn't called that in scikit-learn.  The feature selection method called  F_regression  in scikit-learn will sequentially include features that improve the model the most, until there are  K  features in the model (K is an input).   
 It starts by regression the labels on each feature individually, and then observing which feature improved the model the most using the F-statistic.  Then it incorporates the winning feature into the model.  Then it iterates through the remaining features to find the next feature which improves the model the most, again using the F-statistic or F test.  It does this until there are K features in the model. 
 Notice that the remaining features that are correlated to features incorporated into the model will probably not be selected, since they do not correlate with the residuals (although they might correlate well with the labels).  This helps guard against multi-collinearity.","In fact there is a nice algorithm called ""Forward_Select"" that uses Statsmodels and allows you to set your own metric (AIC, BIC, Adjusted-R-Squared, or whatever you like) to progressively add a variable to the model. The algorithm can be found in the comments section of this page - scroll down and you'll see it near the bottom of the page. 
 https://planspace.org/20150423-forward_selection_with_statsmodels/ 
 I would add that the algorithm also has one nice feature: you can apply it to either classification or regression problems! You just have to tell it. 
 Try it and see for yourself.","Actually sklearn doesn't have a forward selection algorithm, thought a  pull request  with an implementation of forward feature selection waits in the Scikit-Learn repository since April 2017. 
 As an alternative, there is forward and one-step-ahead backward selection in  mlxtend . You can find it's document in  Sequential Feature Selector","Yes 
 sklearn.feature_selection.SequentialFeatureSelector 
 https://scikit-learn.org/0.24/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html 
 also 
 http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/ 
 from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

boston = load_boston()
X, y = boston.data, boston.target

lr = LinearRegression()

sfs = SFS(lr, 
          k_features=13, 
          forward=True, 
          floating=False, 
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(X, y)
fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')

plt.title('Sequential Forward Selection (w. StdErr)')
plt.grid()
plt.show()","I developed this repository  link .
My Stepwise Selection Classes (best subset, forward stepwise, backward stepwise) are compatible to sklearn. You can do  Pipeline  and  GridSearchCV  with my Classes.",,50,50,50,50,,,,,
919,Which cross-validation type best suits to binary classification problem,classification,"You will have best results if you care to build the folds so that each variable (and most importantly the target variable) is approximately identically distributed in each fold. This is called, when applied to the target variable, stratified k-fold. One approach is to cluster the inputs and make sure each fold contains the same number of instances from each cluster proportional to their size.","I think in your case a 10-fold CV will be O.K.  
 I think it is more important to randomize the cross validation process than selecting the ideal value for k. 
 So repeat the CV process several times randomly and compute the variance of your classification result to determine if the results are realiable or not.","I have to agree that k-fold should do ""just"" fine. However, there is a nice article about the ""Bootstrap .632+"" method (basically a smoothened cross validation) that is supposed to be superior (however, they did the comparisons on not-binary data as far as I can tell) 
 Maybe you want to check out this article here:  http://www.jstor.org/stable/2965703","K-Fold should do just fine for binary classification problem. Depending on the time it is taking to train your model and predict the outcome I would use 10-20 folds. 
 However sometimes a single fold takes several minutes, in this case I use 3-5 folds but not less than 3. Hope it helps.","To be honest binary classification is the easiest type compared to multi-class classification as at times by error you can classify a wrong class to a right one.So if you have a dataset with multiclass you will need a good distribution among them,so the expectation is more sample will give better insight,i.e CV should be less.However in case of binary classification if your class distribution is balanced enough you can easily go fo CV=10 for 25k observations,however if the class distribution is skewed you better go with less CV. 
 So in a nutshell in case of binary distribution CV value really depends on your class distribution and not much on number of observations.","Unless the label distribution is balanced, stratified sampling of folds will give you a better estimate of performance than random sampling.  
 Also, try to avoid that correlated samples end up in different folds. Otherwise your models are likely overfitted and the error is underestimated. For example, if your data contains temporal correlation, always split by time.",,,,50,50,50,50,50,50,50,50,50
904,What do you use to generate a dashboard in R?,r,"I think that  Shiny  is an overkill in this situation and doesn't match your requirement of  dashboard reports  to be  static . I guess, that your use of the term ""dashboard"" is a bit confusing, as some people might consider that it has more emphasis of  interactivity  ( real-time dashboards ), rather than  information layout , as is my understanding (confirmed by the ""static"" requirement). 
 My recommendation to you is to use  R Markdown  and  knitr , especially since these packages have much lower learning curve than  Shiny . Moreover, I have recently run across an R package, which, in my view, ideally suits your requirement of embedding small charts/plots in a report, as presented on your picture above. This package generates static or dynamic  graphical tables  and is called  sparkTable  ( http://cran.r-project.org/web/packages/sparkTable ). Its vignette is available here (there is no link to it on the package's home page):  http://publik.tuwien.ac.at/files/PubDat_228663.pdf . Should you ever need some  interactivity ,  sparkTable  provides some via its simple interface to  Shiny .","Shiny  is a framework for generating HTML-based apps that execute R code dynamically. Shiny apps can stand alone or be built into Markdown documents with  knitr , and Shiny development is fully integrated into RStudio. There's even a free service called  shinyapps.io  for hosting Shiny apps, the  shiny  package has functions for deploying Shiny apps directly from R, and RStudio has a GUI interface for calling those functions. There's plenty more info in the Tutorial section of the site. 
 Since it essentially ""compiles"" the whole thing to JavaScript and HTML, you can use CSS to freely change the formatting and layout, although Shiny has decent wrapper functionality for this. But it just so happens that their default color scheme is similar to the one in the screenshot you posted. 
 edit: I just realized you don't need them to be dynamic. Shiny still makes very nice-looking webpages out of the box, with lots of options for rearranging elements. There's also functionality for downloading plots, so you can generate your dashboard every month by just updating your data files in the app, and then saving the resulting image to PDF.","I did find  Flex Dashboard  option pretty cool, interactive and easy to use. Give it a try maybe you would too. 
 Meanwhile, you can also try these as well. 
 
 Plotly Dashboard 
 Shiney Dashboard 
 
 Hope it helps!
 (Update me if you get something even better than that.)","I had to create a web based dashboard. My main charting tool was d3js. But I needed to use ggplot2 to generate few charts. Through d3js's ggplot2 extension, I could create the same. If your charts can be generated through existing extension, then web has better alternatives. Later you can export them to PDF for distribution.",R also has an  htmlwidgets  package that incorporated D3 javascript visualizations.,You could try  ShinyDashboard  and leave out any interactive aspects. In my opinion it is still nice to allow users to zoom in on plots or search for certain values in a column. Shiny will allow you to do this.,,,,50,50,53.01835644,50,50,50,,,
812,Visualizing a graph with a million vertices,visualization,"I also suggest  Gephi  software ( https://gephi.github.io ), which seems to be quite powerful. Some additional information on using  Gephi  with  large networks  can be found  here  and, more generally,  here .  Cytoscape  ( http://www.cytoscape.org ) is an alternative to  Gephi , being an another popular platform for complex network analysis and visualization. 
 If you'd like to work with networks  programmatically  (including visualization) in R, Python or C/C++, you can check  igraph  collection of libraries. Speaking of R, you may find interesting the following blog posts: on  using R with Cytoscape  ( http://www.vesnam.com/Rblog/viznets1 ) and on  using R with Gephi  ( http://www.vesnam.com/Rblog/viznets2 ). 
 For  extensive lists  of  network analysis and visualization software , including some comparison and reviews, you might want to check the following pages: 1)  http://wiki.cytoscape.org/Network_analysis_links ; 2)  http://www.kdnuggets.com/software/social-network-analysis.html ; 3)  http://www.activatenetworks.net/social-network-analysis-sna-software-review .","https://gephi.github.io/  says it can handle a million edges. If your graph has 1000000 vertices and only 50000 edges then most of your vertices won't have any edges anyway. 
 In fact the Gephi spec is the dual of your example: ""Networks up to 50,000 nodes and 1,000,000 edges""","I think, that  Gephi  could face with lack-of-memory issues, you will need at least 8Gb of RAM. Though number of edges is not extremely huge.  
 Possibly, more appropriate tool in this case would be  GraphViz . It's a command line tool for network visualizations, and presumably would be more tolerant to graph size. Moreover, as I remember, in  GraphViz  it is possible to use precomputed coordinates to facilitate computations. 
 I've tried to find a real-world examples of using  GraphViz  with huge graphs, but didn't succeed. Though I found similar discussion on  Computational Science .","Reporting back: I ended up coding graphml and using yEd for visualization (just because I am familiar with this combination. I bet gephi or graphviz would work fine and might even be better). Since I computed the location of all nodes, memory was not such big of an issue. Coding graphml is a little easier comparing to coding svg, since I don't have to explicitly specify the placement of edges.",PajekXXL  is designed to handle enormous networks. But Pajek is also kind of a bizarre program with an unintuitive interface.,"I would recommend to use Graphexp. Gephi is highly dependent on the RAM of your computer which is obviously limited. Graphexp on the other hand  display's only a limited number of Nodes, through which you can navigate to other nodes. I have visualized a graph with 700 Million vertices using Graphexp as a UI and Janusgraph with HBase as a back-end storage.  https://github.com/bricaud/graphexp","Take a look at  Graphistry , they can handle 1Mil nodes and edges.","I realise this is an old question, but I thought I would point out an application that's specifically designed to interactively visualise larger graphs. This is  Graphia . Disclaimer: I am involved in the development of this application. 1m nodes/50k edges is reasonably large and depending on your hardware possibly at the limit of what can be rendered, but you should have much better results with Graphia than you would with either Cytoscape or Gephi.",,50,50,50,50,50,50,50,,
810,Should I go for a 'balanced' dataset or a 'representative' dataset?,machine-learning,"I would say the answer depends on your use case. Based on my experience: 
 
 If you're trying to build a representative model -- one that describes the data rather than necessarily predicts -- then I would suggest using a representative sample of your data. 
 If you want to build a predictive model, particularly one that performs well by measure of AUC or rank-order and plan to use a basic ML framework (i.e. Decision Tree, SVM, Naive Bayes, etc), then I would suggest you feed the framework a balanced dataset. Much of the literature on class imbalance finds that random undersampling (down sampling the majority class to the size of the minority class) can drive performance gains. 
 If you're building a predictive model, but are using a more advanced framework (i.e. something that determines sampling parameters via wrapper or a modification of a bagging framework that samples to class equivalence), then I would suggest again feeding the representative sample and letting the algorithm take care of balancing the data for training.","I think it always depends on the scenario. Using a representative data set is not always the solution. Assume that your training set has 1000 negative examples and 20 positive examples. Without any modification of the classifier, your algorithm will tend to classify all new examples as negative. In some scenarios this is O.K. But in many cases the costs of missing postive examples is high so you have to find a solution for it. 
 In such cases you can use a cost sensitive machine learning algorithm. For example in the case of medical diagnosis data analysis. 
 In summary: Classification errors do not have the same cost!","There always is the solution to try both approaches and keep the one that maximizes the expected performances.  
 In your case, I would assume you prefer minimizing false negatives at the cost of some false positive, so you want to bias your classifier against the strong negative prior, and address the imbalance by reducing the number of negative examples in your training set. 
 Then compute the precision/recall, or sensitivity/specificity, or whatever criterion suits you on the full, imbalanced, dataset to make sure you haven't ignored a significant pattern present in the real data while building the model on the reduced data.","Separate the operational and the training scenarios. 
 The operational scenario is the one in which your classifier will be measure on.
This is where you should perform well.
Use should have a dataset that is representative of this scenario. 
 The training scenario is whatever you are doing in order to build a classifier that will perform well on the operational scenario. 
 Many time the datasets in both scenarios are of the same nature so there is no need to distinct them.
For example, you have some online store so you use for training past usage in order to perform well on the future usage. 
 However, while training you can use a different dataset than a one that represents the operational scenario.
Actually, if you sleep, dream of a classifier, validate it on your operational scenario (this step should be done after waking up), you are just as good as after going the usual machine learning path. 
 The distinction between operational and training scenarios becomes important when the dataset is imbalanced. Most algorithms won't perform well on such a dataset. 
 So, don't hesitate to use two datasets -
You can use a balanced dataset for the training.
Once you are done, validate your classifier of the operational dataset.","I think there are two separate issues to consider: Training time, and prediction accuracy.  
 Take a simple example : consider you have two classes, that have a multivariate normal distribution. Basically, you need to estimate the respective class means and class covariances.  Now the first thing you care about is your estimate of the difference in the class means: but your performance is limited by the accuracy of the worst estimated mean: it's no good estimating one mean to the 100th decimal place - if the other mean is only estimated to 1 decimal place.  So it's a waste of computing resources to use all the data - you can instead undersample the  more common class AND reweight the classes appropriately. ( those computing resources can then be used exploring different input variables etc) 
 Now the second issue is predictive accuracy: different algorithms use different error metrics, which may or may not agree with your own objectives. For example, logistic regression will penalize overall probability error,  so if  most of your data is from one class, then it will tend to try to improve accurate probability estimates ( e.g. 90 vs 95% probability) of that one class rather than trying to identify the rare class. In that case, you would definitely want to try to reweight to emphasize the rare class ( and subsequently adjust the estimate [by adjusting the bias term] to get the probability estimates realigned)","I would suggest going with the representative dataset because it considers as the snapshot of the real data. But for the sampling, you can try to implement stratified sampling, which can help for the imbalanced class issues. The classification process ( Decision Tree, Bayesian, Rule-based ) might work with this dataset.",,,,50,50,50,50,50,50,54.65798533,,
778,Is Python suitable for big data,bigdata,"To clarify, I feel like the original question references by OP probably isn't be best for a SO-type format, but I will certainly represent  python  in this particular case. 
 Let me just start by saying that regardless of your data size,  python  shouldn't be your limiting factor. In fact, there are just a couple main issues that you're going to run into dealing with large datasets: 
 
 Reading data into memory  - This is by far the most common issue faced in the world of big data. Basically, you can't read in more data than you have memory (RAM) for. The best way to fix this is by making atomic operations on your data instead of trying to read everything in at once. 
 Storing data  - This is actually just another form of the earlier issue, by the time to get up to about  1TB , you start having to look elsewhere for storage. AWS S3 is the most common resource, and  python  has the fantastic  boto  library to facilitate leading with large pieces of data. 
 Network latency  - Moving data around between different services is going to be your bottleneck. There's not a huge amount you can do to fix this, other than trying to pick co-located resources and plugging into the wall.","There are couple of things you need to understand when dealing with Big data - 
 What is Big data? 
 You might be aware of famous V's of Big data - Volume, Velocity, Variety... So, Python may not be suitable for all. And it goes with all data science tools available. You need to know which tool is good for what purpose. 
 If dealing with large Volume of data: 
 
 Pig/Hive/Shark - Data cleaning and ETL work 
 Hadoop/Spark - Distributed parallel computing 
 Mahout/ML-Lib - Machine Learning 
 
 Now, you can use R/Python in intermediate stages but you'll realize that they become bottleneck in your entire process. 
 If dealing with Velocity of data: 
 
 Kafka/Storm - High throughput system 
 
 People are trying to R/Python here but again it depends on kind of parallelism you want and your model complexity. 
 What sort of analysis you wish to do? 
 If your model demands the entire data to be first brought into memory then your model should not be complex because if the intermediate data is large then the code will break. And if you think of writing it into disk then you'll face additional delay because disk read/write is slow as compared to RAM. 
 Conclusion 
 You can definitely use Python in Big data space (Definitely, since people are trying with R, why not Python) but know your data and business requirement first. There may be better tools available for same and always remember: 
 
 Your tools shouldn’t determine how you answer questions. Your questions should determine what tools you use.","Python has some very good tools for working with big data: 
 numpy 
 Numpy's memmory-mapped arrays let you access a file saved on disk as though it were an array.  Only the parts of the array you are actively working with need to be loaded into memory.  It can be used pretty much the same as an ordinary array. 
 h5py and pytables 
 These two libraries provide access to HDF5 files.  These files allow access to just part of the data. Further, thanks to the underlying libraries used to access the data, many mathematical operations and other manipulations of the data can be done without loading it into a python data structure.  Massive, highly structured files are possible, much bigger than 5 TB.  It also allows seamless, lossless compression. 
 databases 
 There are various types of databases that allow you to store big data sets and load just the parts you need.  Many databases allow you to do manipulations without loading the data into a python data structure at all. 
 pandas 
 This allows higher-level access to various types of data, including HDF5 data, csv files, databases, even websites.  For big data, it provides wrappers around HDF5 file access that makes it easier to do analysis on big data sets. 
 mpi4py 
 This is a tool for running your python code in a distributed way across multiple processors or even multiple computers.  This allows you to work on parts of your data simultaneously. 
 dask 
 It provides a version of the normal numpy array that supports many of the normal numpy operations in a multi-core manner that can work on data too large to fit into memory.  
 blaze 
 A tool specifically designed for big data.  It is basically a wrapper around the above libraries, providing consistent interfaces to a variety of different methods of storing large amounts of data (such as HDF5 or databases) and tools to make it easy to manipulate, do mathematical operations on, and analyze data that is too big to fit into memory.","Absolutely. When you're working with data at that scale it's common to use a big data framework, in which case python or whatever language you're using is merely an interface. See for example  Spark's Python Programming Guide . What kind of data do you have and what do you want to do with it?","To handle such amount of data, programming language is not the main concern but the programming framework is. Frameworks such as MapReduce or Spark have bindings to many languages including Python. These frameworks certainly have many ready-to-use packages for data analysis tasks. But in the end it all comes to your requirement, i.e., what is your task? People have different definitions of data analysis tasks, some of them can be easily solved with relational databases. In that case, SQL is much better than all other alternatives.","I believe the language itself has little to do with performance capabilities, when it comes to large data. What matters is: 
 
 How large is the data actually 
 What processing are you going to perform on it 
 What hardware are you going to use 
 Which are the specific libraries that you plan to use 
 
 Anyway, Python is well adopted in data science communities.","I've been using Anaconda Python 3.4 and Pandas to search 10M row database to match 20K of login credentials. Takes about a minute. The pandas internals make great use of memory. That said, truly big data requires a processing architecture matched to the problem. Pandas is just the glue (logic) in this equation, and other tools can do this as well. R, Scala, Haskell, SAS, etc. can replicate some of the logic - perhaps just enough to answer questions faster. But python makes a good (best?) general-purpose tool. You can run R code in python, as well as most other languages. Although interpretive, there are high performance techniques and tools such as pypy that can make python run almost as fast as benchmark tools with only slightly more effort. And python has many libraries that do just about everything - see above list. 
 If you are asking if you should learn and use python, my answer is yes Articles indicate that python is used more than R among people who use both. But few data science problems are solved by a single tool. It may become your go-to tool, but its only that - a tool. And just as no sane person builds a house with just a hammer, no sane Data Scientist uses just one tool.","It's funny how people mix big data with data science and business intelligence. 
 First, big data means ""a lot of data"", so much information that it doesn't fit in a conventional database.  However, sometimes big data is not even proper ""value"" information but documents, images and so on. 
 So, to process big data, WE NEED SPEED.  Python is out of the league, so R.   However, if the task is as easy as to take a CSV and insert into a database, then it's ETL, we don't need programming to do that. 
 And when the information is reduced, then we could apply python, r or whatever you want to. Even Excel.    However, in this stage, Big Data is not big anymore but conventional data. 
 IMHO, Java is more suitable for Big Data (for the whole chain) but people take Python as default for some unpractical reason.","Python  is definitely a good alternative when working with  Big Data . It is compatible with a lot of different  Big Data  tools, such as  Hadoop , and can be used for  ETL  processes.",50,50,52.81800528,52.92769101,50,50,50,,
761,"Clustering geo location coordinates (lat,long pairs)",machine-learning,"K-means is not the most appropriate algorithm here. 
 The reason is that k-means is designed to  minimize variance . This is, of course, appearling from a statistical and signal procssing point of view, but your data is not ""linear"". 
 Since your data is in latitude, longitude format, you should use an algorithm that can handle  arbitrary  distance functions, in particular geodetic distance functions. Hierarchical clustering, PAM, CLARA, and DBSCAN are popular examples of this. 
 This  recommends OPTICS clustering. 
 The problems of k-means are easy to see when you consider points close to the +-180 degrees wrap-around. Even if you hacked k-means to use Haversine distance, in the update step when it recomputes the  mean  the result will be badly screwed.  Worst case is, k-means will never converge!","K-means should be right in this case. Since k-means tries to group based solely on euclidean distance between objects you will get back clusters of locations that are close to each other. 
 To find the optimal number of clusters you can try making an 'elbow' plot of the within group sum of square distance.  This  may be helpful","GPS coordinates can be directly converted to a  geohash . Geohash divides the Earth into ""buckets"" of different size based on the number of digits (short Geohash codes create big areas and longer codes for smaller areas). Geohash is an adjustable precision clustering method.","I am probably very late with my answer, but if you are still dealing with geo clustering, you may find  this study  interesting. It deals with comparison of two fairly different approaches to classifying geographic data: K-means clustering and latent class growth modeling. 
 One of the images from the study: 
 
 The authors concluded that the end results were overall similar, and that there were some aspects where LCGM overperfpormed K-means.","You can use  HDBSCAN  for this.  The python package has support for haversine distance which will properly compute distances between lat/lon points. 
 As the  docs mention , you will need to convert your points to radians first for this to work.  The following psuedocode should do the trick: 
 points = np.array([[lat1, lon1], [lat2, lon2], ...])
rads = np.radians(points)
clusterer = hdbscan.HDBSCAN(min_cluster_size=N, metric='haversine')
cluster_labels = clusterer.fit_predict(rads)","Java Apache  commons-math does this pretty easily. 
 List<Cluster<T>>    cluster(Collection<T> points)","@CKM there is a parameter in HDBSCAN package:  cluster_selection_epsilon  which allows you to set the acceptable distance for the neighboring points in the same cluster (just like epsilon in DBSCAN). 
 Alternatively, you can use DBSCAN and set the eps parameter to 10(km)/6371.0088 (earth-radius). This does not mean though that every single pair in your clusters will have less than that distance. Two border points can be far yet reachable to each other through the chain of core points. This limit just makes sure that each core point has at least N neighboring points within that distance (N is the minimum number of points required in each cluster).","The k-means algorithm to cluster the locations is a bad idea. Your locations can be spread across the world and the number of clusters cant be predicted by you, not only that if you put the cluster as 1 then the locations will be grouped to 1 single cluster. I am using OPTICS clustering for the same. It worked like a Charm.",Go with Kmeans clustering as HBScan will take forever.   I tried it for one of the project and ended but using Kmeans with desired results.,50,50,50,57.31198204,50,50,50,50,
758,Tools and protocol for reproducible data science using Python,python,"The topic of  reproducible research  (RR) is  very popular  today and, consequently, is  huge , but I hope that my answer will be  comprehensive enough  as an answer and will provide enough information for  further research , should you decide to do so. 
 While Python-specific tools for RR certainly exist out there, I think it makes more sense to focus on more  universal tools  (you never know for sure what programming languages and computing environments you will be working with in the future). Having said that, let's take a look what tools are available per your list. 
 1)  Tools for data version control . Unless you plan to work with (very)  big data , I guess, it would make sense to use the same  git , which you use for source code version control. The infrastructure is already there. Even if your files are binary and big, this advice might be helpful:  https://stackoverflow.com/questions/540535/managing-large-binary-files-with-git . 
 2)  Tools for managing RR workflows and experiments . Here's a list of most popular tools in this category, to the best of my knowledge (in the descending order of popularity): 
 
 Taverna Workflow Management System  ( http://www.taverna.org.uk ) - very solid, if a little too complex, set of tools. The major tool is a Java-based desktop software. However, it is compatible with online workflow repository portal  myExperiment  ( http://www.myexperiment.org ), where user can store and share their RR workflows. Web-based RR portal, fully compatible with  Taverna  is called  Taverna Online , but it is being developed and maintained by totally different organization in Russia (referred there to as  OnlineHPC :  http://onlinehpc.com ). 
 The Kepler Project  ( https://kepler-project.org ) 
 VisTrails  ( http://vistrails.org ) 
 Madagascar  ( http://www.reproducibility.org ) 
 
 EXAMPLE . Here's an interesting article on scientific workflows with an example of the  real  workflow design and data analysis, based on using  Kepler  and  myExperiment  projects:  http://f1000research.com/articles/3-110/v1 . 
 There are many RR tools that implement  literate programming  paradigm, exemplified by  LaTeX  software family. Tools that help in report generation and presentation is also a large category, where  Sweave  and  knitr  are probably the most well-known ones.  Sweave  is a tool, focused on R, but it can be integrated with Python-based projects, albeit with some additional effort ( https://stackoverflow.com/questions/2161152/sweave-for-python ). I think that  knitr  might be a better option, as it's modern, has extensive support by popular tools (such as  RStudio ) and is language-neutral ( http://yihui.name/knitr/demo/engines ). 
 3)  Protocol and suggested directory structure . If I understood correctly what you implied by using term  protocol  ( workflow ), generally I think that standard RR data analysis workflow consists of the following sequential phases:  data collection  =>  data preparation  (cleaning, transformation, merging, sampling) =>  data analysis  =>  presentation of results  (generating reports and/or presentations). Nevertheless, every workflow is project-specific and, thus, some specific tasks might require adding additional steps. 
 For sample directory structure, you may take a look at documentation for R package  ProjectTemplate  ( http://projecttemplate.net ), as an attempt to automate data analysis workflows and projects: 
 
 4)  Automated build/run tools . Since my answer is focused on universal (language-neutral) RR tools, the most popular tools is  make . Read the following article for some reasons to use  make  as the preferred RR workflow automation tool:  http://bost.ocks.org/mike/make . Certainly, there are other  similar  tools, which either improve some aspects of  make , or add some additional features. For example:  ant  (officially, Apache Ant:  http://ant.apache.org ),  Maven  (""next generation  ant "":  http://maven.apache.org ),  rake  ( https://github.com/ruby/rake ),  Makepp  ( http://makepp.sourceforge.net ). For a comprehensive list of such tools, see Wikipedia:  http://en.wikipedia.org/wiki/List_of_build_automation_software .","Since I started doing research in academia I was constantly looking for a satisfactory workflow.
I think that I finally found something I am happy with: 
 
 Put everything under version control, e.g., Git: 
 
 For hobby research projects I use GitHub, for research at work I use the private GitLab server that is provided by our university. I also keep my datasets there. 
 
 I do most of my analyses along with the documentation on IPython notebooks. It is very organized (for me) to have the code, the plots, and the discussion/conclusion all in one document
If I am running larger scripts, I would usually put them into separate script .py files, but I would still execute them from the IPython notebook via the %run magic to add information about the purpose, outcome, and other parameters. 
 
 I have written a small cell-magic extension for IPython and IPython notebooks, called ""watermark"" that I use to conveniently create time stamps and keep track of the different package versions I used and also Git hashs 
 For example 
 
 %watermark

29/06/2014 01:19:10

CPython 3.4.1
IPython 2.1.0

compiler   : GCC 4.2.1 (Apple Inc. build 5577)
system     : Darwin
release    : 13.2.0
machine    : x86_64
processor  : i386
CPU cores  : 2
interpreter: 64bit
 
 
 %watermark -d -t

29/06/2014 01:19:11 
 
 
 %watermark -v -m -p numpy,scipy

CPython 3.4.1
IPython 2.1.0

numpy 1.8.1
scipy 0.14.0

compiler   : GCC 4.2.1 (Apple Inc. build 5577)
system     : Darwin
release    : 13.2.0
machine    : x86_64
processor  : i386
CPU cores  : 2
interpreter: 64bit
 
 For more info, see the  Github repository here .","The best reproducibility tool is to make a log of your actions, something like this: 
 experiment/input ; expected ; observation/output ; current hypothesis and if supported or rejected
exp1 ; expected1 ; obs1 ; some fancy hypothesis, supported
 
 This can be written down on a paper, but, if your experiments fit in a computational framework, you can use computational tools to partly or completely automate that logging process (particularly by helping you track the input datasets which can be huge, and the output figures). 
 A great reproducibility tool for Python with a low learning curve is of course  IPython/Jupyter Notebook  (don't forget the  %logon and %logstart  magics). Tip: to make sure your notebook is reproducible, restart the kernel and try to run all cells from top to bottom (button Run All Cells): if it works, then save everything in an archive file (""freezing""), else, notably if you need to run cells in a non linear and non sequential and  non obvious  fashion to avoid errors, you need to rework a bit. 
 Another great tool that is very recent (2015) is  recipy , which is very like sumatra (see below), but made specifically for Python. I don't know if it works with Jupyter Notebooks, but I know the author frequently uses them so I guess that if it's not currently supported, it will be in the future. 
 Git  is also awesome, and it's not tied to Python. It will help you not only to keep a history of all your experiments, code, datasets, figures, etc. but also provide you with tools to maintain ( git pickaxe ), collaborate ( blame ) and debug ( git - bisect ) using a scientific method of debugging (called  delta debugging ).  Here's a story  of a fictional researcher trying to make his own experiments logging system, until it ends up being a facsimile of Git. 
 Another general tool working with any language (with a Python API on  pypi ) is  Sumatra , which is specifically designed to help you do  replicable  research ( replicable  aims to produce the same results given the exact same code and softwares, whereas  reproducibility  aims to produce the same results given any medium, which is a lot harder and time consuming and not automatable). 
 Here is how Sumatra works: for each experiment that you conduct through Sumatra, this software will act like a ""save game state"" often found in videogames. More precisely, it will will save: 
 
 all the parameters you provided; 
 the exact sourcecode state of your whole experimental application and config files; 
 the output/plots/results and also any file produced by your experimental application. 
 
 It will then construct a database with the timestamp and other metadatas for each of your experiments, that you can later crawl using the webGUI. Since Sumatra saved the full state of your application for a specific experiment at one specific point in time, you can restore the code that produced a specific result at any moment you want, thus you have replicable research at a low cost (except for storage if you work on huge datasets, but you can configure exceptions if you don't want to save everything everytime). 
 Another awesome tool is  GNOME's Zeitgeist  (previously coded in Python but now ported to Vala), an all-compassing action journaling system, which records everything you do and it can use machine learning to summarize for a time period you want the relationship between items based on similarity and usage patterns, eg answering questions like  ""What was most relevant to me, while I was working on project X, for a month last year?"" . Interestingly,  Zim Desktop Wiki , a note-taking app similar to Evernote, has a plugin to work with Zeitgeist. 
 In the end, you can use either Git or Sumatra or any other software you want, they will provide you with about the same replicability power, but Sumatra is specifically tailored for scientific research so it provides a few fancy tools like a web GUI to crawl your results, while Git is more tailored towards code maintenance (but it has debugging tools like git-bisect so if your experiments involve codes, it may actually be better). Or of course you can use both! 
 /EDIT:  dsign  touched a very important point here: the replicability of your setup is as important as the replicability of your application. In other words, you should at least provide a  full list of the libraries and compilers  you used along with their exact  versions  and the details of your  platform . 
 Personally, in scientific computing with Python, I have found that packaging an application along with the libraries is just too painful, thus I now just use an all-in-one scientific python package such as  Anaconda  (with the great package manager  conda ), and just advise users to use the same package. Another solution could be to provide a script to automatically generate a  virtualenv , or to package everything using the commercial  Docker application as cited by dsign  or the opensource  Vagrant  (with for example  pylearn2-in-a-box  which use Vagrant to produce an easily redistributable virtual environment package). 
 Finally, to really ensure that you have a fully working environment everytime you need, you can make a virtual machine (see VirtualBox), and you can even save the state of the machine (snapshot) with your experiment ready to run inside. Then you can just share this virtual machine with everything included so that anyone can replicate your experiment with your exact setup. This is probably the best way to replicate a software based experiment. Containers might be a more lightweight alternative, but they do not include the whole environment, so that the replication fidelity will be less robust. 
 /EDIT2: Here's a  great video  summarizing (for debugging but this can also be applied to research) what is fundamental to do reproducible research: logging your experiments and each other steps of the scientific method, a sort of  ""explicit experimenting"" .","Be sure to check out  docker ! And in general, all the other good things that software engineering has created along decades for ensuring isolation and reproductibility.  
 I would like to stress that it is not enough to have  just  reproducible workflows, but also  easy  to reproduce workflows. Let me show what I mean. Suppose that your project uses Python, a database X and Scipy. Most surely you will be using a specific library to connect to your database from Python, and Scipy will be in turn using some sparse algebraic routines. This is by all means a very simple setup, but not entirely simple to setup, pun intended. If somebody wants to execute your scripts, she will have to install all the dependencies. Or worse, she might have incompatible versions of it already installed. Fixing those things takes time. It will also take time to you if you at some moment need to move your computations to a cluster, to a different cluster, or to some cloud servers.  
 Here is where I find docker useful. Docker is a way to formalize and compile recipes for binary environments. You can write the following in a dockerfile (I'm using here plain English instead of the Dockerfile syntax): 
 
 Start with a basic binary environment, like Ubuntu's 
 Install libsparse-dev 
 (Pip) Install numpy and scipy 
 Install X 
 Install libX-dev 
 (Pip) Install python-X 
 Install IPython-Notebook 
 Copy my python scripts/notebooks to my binary environment, these datafiles, and these configurations to do other miscellaneous things. To ensure reproductibility, copy them from a named url instead of a local file.  
 Maybe run IPython-Notebook. 
 
 Some of the lines will be installing things in Python using pip, since pip can do a very clean work in selecting specific package versions. Check it out too! 
 And that's it. If after you create your Dockerfile it can be built, then it can be built anywhere, by anybody (provided they also have access to your project-specific files, e.g. because you put them in a public url referenced from the Dockerfile). What is best, you can upload the resulting environment (called an ""image"") to a public or private server (called a ""register"") for other people to use. So, when you publish your workflow, you have both a fully reproducible recipe in the form of a Dockerfile, and an easy way for you or other people to reproduce what you do: 
 docker run dockerregistery.thewheezylab.org/nowyouwillbelieveme
 
 Or if they want to poke around in your scripts and so forth: 
 docker run -i -t dockerregistery.thewheezylab.org/nowyouwillbelieveme /bin/bash","Unfortunately, I do not have enough reputation points to answer to the post by Plank, so have to answer to the whole thread - sorry about that.  
 I am actually the developer of the open-source Collective Knowledge Framework mentioned above. It attempts to simplify sharing of artifacts and experimental workflows as reusable and reproducible Python components with unified JSON API and JSON meta shared via GitHub. They can also be connected to predictive analytics with the same unified JSON API. 
 We have just released the new version V1.8.1 and provided extensive documentation so hopefully it will be easier to understand the concepts now: 
 http://github.com/ctuning/ck/wiki 
 We now have many academic and industrial projects based on this framework, so you may check one of them - crowdsourcing program optimization across mobile devices provided by volunteers in a reproducible way:  http://cknowledge.org/repo 
 We also keep track of various resources related to reproducible science here:  https://github.com/ctuning/ck/wiki/Enabling-open-science 
 Though I am primarily focusing on making computer systems' research reproducible, I had interesting chats with colleagues from other domains and seems like they have very similar problems. So, I will be very happy if our framework can be of any help to other communities! If you have any questions or suggestions, feel free to get in touch!","There is an entire course that is devoted to reproducible research.
 https://www.coursera.org/learn/reproducible-research 
This course is based on R, but the underlying idea can be learnt. 
 One simple way is to have an Ipython notebook and keep saving every dirty work you do, be it cleaning the data, exploratory analysis or building the model.","I recently came across the following tool -  http://github.com/ctuning/ck  . It is already written in Python and seems to include what you need (my colleague is using it in the pilot project to automate image recognition).  
 Pros:  
 
 very small, portable and customizable  
 includes web server to
distribute experiments and process them using predictive analytics 
 has a cool usage example to crowdsource and reproduce compiler
optimization -  http://cknowledge.org/repo 
 
 Cons: 
 
 a bit low level - you need to implement your own workflow from Python components shared via GitHub using JSON API or command line 
 documentation is somewhat complex - I really hope that they will find time to update it soon.","I've created and recently released an open source tool  http://dvc.org  or DVC that does exactly what you are trying to reach: 
 
 [Tools for data version control.] DVC works on top of Git, adds data file version control (files are stored outside of Git) and tracks the dependencies between the code and the data files. DVC automatically derives the dependency graph (DAG) for code and data. 
 [Tools enabling to reproduce stages and experiments.]  dvc repro data/scores.csv  reproduces all the required steps regarding DAG. 
 [Protocol and suggested directory structure for such a project.] DVC required a data directory ( data  by default) where you supposed to store all data files. However, DVC transparently moves the actual content to  .cache  directory and creates the symlinks (yeah, I made it to work on Windows as well). The  .cache  directory is not synced to Git but it could be synced through the cloud (S3 or GCP) by command  dvc sync data/scores.csv  (it syncs corresponded data file from cache like  .cache/scores.csv_29de545 ) 
 [Automated build/run tools.] See from the above. 
 
 DVC tutorial is a good starting point -  ""Data Version Control: iterative machine learning"" .","DISCLAIMER: I work at a company,  Datmo , that creates an open-source tool to do this.   
 The best practice for reproducibility is the following: 
 1) First containerize your environment into a Docker environment by creating a Dockerfile and ensuring that all dependencies are covered in that file. I found this resource to be best ( https://arxiv.org/pdf/1410.0846.pdf ) 
 2) Once you have that you'll want to decide where you can keep track of all of the performance metrics and configurations (so that you can revisit it for future experimentation)  
 3) Finally, write some documentation so that a new experimenter/developer is able to revisit your code, replicate it with the environment and see where you have kept your configurations and performance metrics.",50,50,50,50,50,50,50,50,50
744,Cosine similarity versus dot product as distance metrics,classification,"Think geometrically. Cosine similarity only cares about angle difference, while dot product cares about angle and magnitude. If you normalize your data to have the same magnitude, the two are indistinguishable. Sometimes it is desirable to ignore the magnitude, hence cosine similarity is nice, but if magnitude plays a role, dot product would be better as a similarity measure. Note that neither of them is a ""distance metric"".","You are right, cosine similarity has a lot of common with dot product of vectors. Indeed, it is a dot product, scaled by magnitude. And because of scaling it is normalized between 0 and 1. CS is preferable because it takes into account variability of data and features' relative frequencies. On the other hand, plain dot product is a little bit ""cheaper"" (in terms of complexity and implementation).","I would like to add one more dimension to the answers given above. Usually we use cosine similarity with large text, because using distance matrix on paragraphs of data is not recommended. And also if you intend your cluster to be broad you tend to go with cosine similarity as it captures similarity overall.  
 For example if you have texts which are two or three words long at max I feel using cosine similarity does not achieve the precision as achieved by distance metric.","There is an excellent comparison of the common inner-product-based similarity metrics  here . 
 In particular, Cosine Similarity is normalized to lie within  $[-1,1]$ , unlike the dot product which can be any real number. But, as everyone else is saying, that will require ignoring the magnitude of the vectors. Personally, I think that's a good thing. I think of magnitude as an internal (within-vector) structure, and angle between vectors as external (between vector) structure. They are different things and (in my opinion) are often best analyzed separately. I can't imagine a situation where I would rather compute inner products than compute cosine similarities and just compare the magnitudes afterward.","From a geometric point of view, if all your data are unitary, $\forall x, ||x||^2 = \langle x,x \rangle = 1$, then the scalar product of two vectors defines an angle $\phi$, $\langle x,y \rangle = \cos \phi$, and you have a distance $\phi = \arccos \langle x,y \rangle$.  
 Visually, all your data live on a unit sphere. Using a dot product as a distance will give you a chordal distance, but if you use this cosine distance, it corresponds to the length of the path between the two points on the sphere. That means, if you want an average of the two points, you should take the point in-between on this path (geodesic) rather than the mid-point obtained from the 'arithmetic average/dot product/euclidean geometry' since this point does not live on the sphere (hence essentially not the same object)!","Cosine Similarity = what percentage of the effort is in the same direction.  Negative value is a percentage of effort in the opposite direction. Zero is working at cross-purposes.  Nothing in common. 
 Dot product = a measure describing the total quantity of effort in the same direction.",,,,50,50,50,50,50,50,52.71471864,52.91401051,
694,Best python library for neural networks,machine-learning,"UPDATE: the landscape has changed quite a bit since I answered this question in July '14, and some new players have entered the space.  In particular, I would recommend checking out: 
 
 TensorFlow 
 Blocks 
 Lasagne 
 Keras 
 Deepy 
 Nolearn 
 NeuPy 
 
 They each have their strengths and weaknesses, so give them all a go and see which best suits your use case.  Although I would have recommended using PyLearn2 a year ago, the community is no longer active so I would recommend looking elsewhere.  My original response to the answer is included below but is largely irrelevant at this point. 
 
 PyLearn2  is generally considered the library of choice for neural networks and deep learning in python.  It's designed for easy scientific experimentation rather than ease of use, so the learning curve is rather steep, but if you take your time and follow the tutorials I think you'll be happy with the functionality it provides.  Everything from standard Multilayer Perceptrons to Restricted Boltzmann Machines to Convolutional Nets to Autoencoders is provided.  There's great GPU support and everything is built on top of Theano, so performance is typically quite good.  The source for PyLearn2 is available  on github . 
 Be aware that PyLearn2 has the opposite problem of PyBrain at the moment -- rather than being abandoned, PyLearn2 is under active development and is subject to frequent changes.","Tensor Flow  ( docs ) by Google is another nice framework which has automatic differentiation. I've written down some  quick thoughts about Google Tensor Flow  on my blog, together with the MNIST example which they have in their tutorial. 
 See also: My  Tensorflow XOR tutorial 
 Lasagne  ( docs ) is very nice, as it uses theano (→ you can use the GPU) and makes it simpler to use. The author of lasagne won the Kaggle Galaxy challenge, as far as I know. It is nice with  nolearn . Here is an MNIST example network: 
 #!/usr/bin/env python

import lasagne
from lasagne import layers
from lasagne.updates import nesterov_momentum
from nolearn.lasagne import NeuralNet

import sys
import os
import gzip
import pickle
import numpy


PY2 = sys.version_info[0] == 2

if PY2:
    from urllib import urlretrieve

    def pickle_load(f, encoding):
        return pickle.load(f)
else:
    from urllib.request import urlretrieve

    def pickle_load(f, encoding):
        return pickle.load(f, encoding=encoding)

DATA_URL = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'
DATA_FILENAME = 'mnist.pkl.gz'


def _load_data(url=DATA_URL, filename=DATA_FILENAME):
    """"""Load data from `url` and store the result in `filename`.""""""
    if not os.path.exists(filename):
        print(""Downloading MNIST dataset"")
        urlretrieve(url, filename)

    with gzip.open(filename, 'rb') as f:
        return pickle_load(f, encoding='latin-1')


def load_data():
    """"""Get data with labels, split into training, validation and test set.""""""
    data = _load_data()
    X_train, y_train = data[0]
    X_valid, y_valid = data[1]
    X_test, y_test = data[2]
    y_train = numpy.asarray(y_train, dtype=numpy.int32)
    y_valid = numpy.asarray(y_valid, dtype=numpy.int32)
    y_test = numpy.asarray(y_test, dtype=numpy.int32)

    return dict(
        X_train=X_train,
        y_train=y_train,
        X_valid=X_valid,
        y_valid=y_valid,
        X_test=X_test,
        y_test=y_test,
        num_examples_train=X_train.shape[0],
        num_examples_valid=X_valid.shape[0],
        num_examples_test=X_test.shape[0],
        input_dim=X_train.shape[1],
        output_dim=10,
    )


def nn_example(data):
    net1 = NeuralNet(
        layers=[('input', layers.InputLayer),
                ('hidden', layers.DenseLayer),
                ('output', layers.DenseLayer),
                ],
        # layer parameters:
        input_shape=(None, 28*28),
        hidden_num_units=100,  # number of units in 'hidden' layer
        output_nonlinearity=lasagne.nonlinearities.softmax,
        output_num_units=10,  # 10 target values for the digits 0, 1, 2, ..., 9

        # optimization method:
        update=nesterov_momentum,
        update_learning_rate=0.01,
        update_momentum=0.9,

        max_epochs=10,
        verbose=1,
        )

    # Train the network
    net1.fit(data['X_train'], data['y_train'])

    # Try the network on new data
    print(""Feature vector (100-110): %s"" % data['X_test'][0][100:110])
    print(""Label: %s"" % str(data['y_test'][0]))
    print(""Predicted: %s"" % str(net1.predict([data['X_test'][0]])))


def main():
    data = load_data()
    print(""Got %i testing datasets."" % len(data['X_train']))
    nn_example(data)

if __name__ == '__main__':
    main()
 
 Caffe  is a C++ library, but has Python bindings. You can do most stuff by configuration files (prototxt). It has a lot of options and can also make use of the GPU.","Pylearn relies on Theano and as mentioned in the other answer to use the library is quite complicated, until you get the hold of it. 
 In the meantime I would suggest using  Theanets . It also built on top of Theano, but is much more easier to work with. It might be true, that it doesn't have all the features of Pylearn, but for the basic work it's sufficient. 
 Also it's open source, so you can add custom networks on the fly, if you dare. :) 
 EDIT: Dec 2015. Recently I have started using  Keras . It is a bit lower level than Theanets, but much more powerful. For basic tests the Theanets is appropriate. But if you want to do some research in field of ANN Keras is much more flexible. Plus the Keras can use  Tensorflow  as a backend.","TensorFlow  (by Google,  released  on 2015-11-09) looks promising. 
 
 open source (Apache 2.0 License) ( GitHub ) 
 Python (backend in C++) 
 CPU/GPU 
 Auto-Differentiation 
 Portable (even works on mobile devices) 
 
 
 FYI:  
 
 Evaluation of Deep Learning Toolkits 
 Easy benchmarking of all public open-source implementations of convnets. 
 Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott, Mohak Shah.  Comparative Study of Deep Learning Software Frameworks .  arXiv:1511.06435. 
 Shaohuai Shi, Qiang Wang, Pengfei Xu, Xiaowen Chu.  Benchmarking State-of-the-Art Deep Learning Software Tools . arXiv:1608.07249 
 Was a new deep learning toolkit released once every 22 days in 2015?","Pylearn2 seems to be the library of choice, however I find their YAML configuration files off-putting. 
 Python itself was designed to be an easy language for prototyping, why would you  not  use it to define the network properties themselves? We have great editors with autocompletion that would make your life much easier and Python is not like C++ where you have to wait for long builds to finish before you can run your code. 
 YAML files on the other hand you have to edit using a standard text editor with no assistance whatsoever and this makes the learning curve even steeper.  
 I may be missing the big picture but I still don't understand what were they thinking, I don't think prototyping in code would be much slower. For that reason I'm considering Theanets or using Theano directly.","I like  Blocks , which is also built on top of Theano. Way more approachable than PyLearn2, and more feature rich than Lasagne. Neatly written, too. 
 Updated Jan 2016: 
 At the time of writing,  Keras  has by far the most momentum. It is highly modular and can run on both Theano and Tensorflow, giving it great opportunities.","MXNet : 
 
 written in C++ but has an API in Python (and a few other programming languages such as R, Julia, and Go) 
 Scales up to multi GPUs and distributed setting with auto parallelism. 
 Automatic differentiation 
 Decent performances:","From what I heard, Pylearn2 might be currently the library of choice for most people. This reminds me of a recent blog post a few month ago that lists all the different machine learning libraries with a short explanation 
 https://www.cbinsights.com/blog/python-tools-machine-learning 
 The section you might be interested in here would be ""Deep Learning"". About Pylearn2, he writes 
 
 PyLearn2 
 There is another library built on top of Theano, called PyLearn2 which
  brings modularity and configurability to Theano where you could create
  your neural network through different configuration files so that it
  would be easier to experiment different parameters. Arguably, it
  provides more modularity by separating the parameters and properties
  of neural network to the configuration file.","I wrote up this post detailing some of my personal favorites: 
 The Best Machine Learning Libraries in Python 
 Since over 30 different libraries are mentioned, I won't post them all here, but these are among the most popular: 
 
 Tensorflow 
 scikit-learn 
 Theano 
 Pylearn2 
 Pyevolve 
 NuPIC 
 Pattern 
 Caffe 
 
 (Sorry, can't link to the Github repos since my rep is still < 10...) 
 Edit: Added links to Github repos.",50,56.78561642,50,50,50,50,50,50,53.79403699
678,What are some standard ways of computing the distance between documents?,machine-learning,"There's a number of different ways of going about this depending on exactly how much semantic information you want to retain and how easy your documents are to tokenize (html documents would probably be pretty difficult to tokenize, but you could conceivably do something with tags and context.) 
 Some of them have been mentioned by ffriend, and the paragraph vectors by user1133029 is a really solid one, but I just figured I would go into some more depth about plusses and minuses of different approaches. 
 
 Cosine Distance  - Tried a true, cosine distance is probably the most common distance metric used generically across multiple domains. With that said, there's very little information in cosine distance that can actually be mapped back to anything semantic, which seems to be non-ideal for this situation. 
 Levenshtein Distance  - Also known as  edit distance , this is usually just used on the individual token level (words, bigrams, etc...). In general I wouldn't recommend this metric as it not only discards any semantic information, but also tends to treat very different word alterations very similarly, but it is an extremely common metric for this kind of thing 
 LSA  - Is a part of a large arsenal of techniques when it comes to evaluating document similarity called  topic modeling . LSA has gone out of fashion pretty recently, and in my experience, it's not quite the strongest topic modeling approach, but it is relatively straightforward to implement and has a few open source implementations 
 LDA  - Is also a technique used for  topic modeling , but it's different from  LSA  in that it actually learns internal representations that tend to be more smooth and intuitive. In general, the results you get from  LDA  are better for modeling document similarity than  LSA , but not quite as good for learning how to discriminate strongly between topics. 
 Pachinko Allocation  - Is a really neat extension on top of LDA. In general, this is just a significantly improved version of  LDA , with the only downside being that it takes a bit longer to train and open-source implementations are a little harder to come by 
 word2vec  - Google has been working on a series of techniques for intelligently reducing words and documents to more reasonable vectors than the sparse vectors yielded by techniques such as  Count Vectorizers  and  TF-IDF . Word2vec is great because it has a number of open source implementations. Once you have the vector, any other similarity metric (like cosine distance) can be used on top of it with significantly more efficacy. 
 doc2vec  - Also known as  paragraph vectors , this is the latest and greatest in a series of papers by Google, looking into dense vector representations of documents. The  gensim  library in python has an implementation of  word2vec  that is straightforward enough that it can pretty reasonably be leveraged to build  doc2vec , but make sure to keep the license in mind if you want to go down this route 
 
 Hope that helps, let me know if you've got any questions.","There's a number of semantic distance measures, each with its pros and cons. Here are just a few of them:  
 
 cosine distance , inner product between document feature vectors; 
 LSA , another vector-based model, but utilizing SVD for de-noising original term-document matrix; 
 WordNet -based, human verified, though hardly extensible. 
 
 Start with a simplest approach and then move further based on issues for your specific case.","Empirically I have found LSA vastly superior to LDA every time and on every dataset I have tried it on. I have talked to other people who have said the same thing. It's also been used to win a number of the SemEval competitions for measuring semantic similarity between documents, often in combinations with a wordnet based measure, so I wouldn't say it's going out of fashion, or is definitely inferior to LDA, which is better for topic modelling and not semantic similarity in my experience, contrary to what some responders have stated.  
 If you use gensim (a python library), it has LSA, LDA and word2vec, so you can easily compare the 3. doc2vec is a cool idea, but does not scale very well and you will likely have to implement it yourself as I am unaware of any open source implementations. It does not scale well as for each document, a new and separate model has to be built using SGD, a slow machine learning algorithm. But it will probably give you the most accurate results. LSA and LDA also don't scale well (word2vec does however), LDA scales worse in general. Gensim's implementations are very fast however, as it uses iterative SVD. 
 One other note, if you use word2vec, you will still have to determine a way to compose vectors from documents, as it gives you a different vector per word. The simplest way to do this is to normalize each vector and take the mean over all word vectors in the document, or take a weighted mean by idf weighting of each word. So it's not as simple as 'use word2vec', you will need to do something further to compute document similarity.  
 I would personally go with LSA, as I have seen it work well empirically, and gensim's library scales very well. However, there's no free lunch, so preferably try each method and see which works better for your data.","State of the art appears to be ""paragraph vectors"" introduced in a recent  paper . Cosine/Euclidean distance between paragraph vectors would likely work better than any other approach. This probably isn't feasible yet due to lack of open source implementations. 
 Next best thing is cosine distance between LSA vectors or cosine distance between raw BOW vectors. Sometimes it works better to choose different weighting schemes, like TF-IDF.","It is useful to have in you bag of tools the family of  locality sensitive hashing algorithms .
This family is not semantic at all. 
Actually is consider the text as a sequence of bits.
I find it useful in dirty data sets when the same text appears many times with slight differences. 
 You can use  ssdeep  (which is based on  Nilsimsa hash ) for identifying such documents.
Ssdeep was originally planned for the domain of spam. Spammers often do small changes in the message (add a space) in order to prevent detection by exact signature (e.g.,  md5 ). 
 Since many version of almost the same document in the same data set will cause havoc to statistical methods that will be applied on it, doing such a cleanup can be very beneficial.",,,,,50,50,50,,,,,,
559,Detecting cats visually by means of anomaly detection,machine-learning,"You could simplify your problem significantly by using a motion/change detection approach. For example, you could compare each image/frame with one from an early time (e.g., a minute earlier), then only consider pixels that have changed since the earlier time. You could then extract the rectangular region of change and use that as the basis for your classification or anomaly detection. 
 Taking this type of approach can significantly simplify your classifier and reduce your false target rate because you can ignore anything that is not roughly the size of a cat (e.g., a person or bird). You would then use the extracted change regions that were not filtered out to form the training set for your classifier (or anomaly detector). 
 Just be sure to get your false target rate sufficiently low before mounting a laser turret to your feline intrusion detection system.","This is an interesting and also quite ambitious project :) 
 I am not sure anomaly detection (at least in the sense described in the course you followed) would be a very fitting algorithm in this case.  
 I would consider a more viable approach to be what has been discussed at the end of that course where a Photo OCR workflow was demonstrated. 
 The approach would consist of segmenting your image in smaller ""blocks"", and going through them one-by-one using a supervised learning algorithm and try to classify each block according to whether it contains a cat or not. If one block contains a cat, the alarm goes off. As a bonus, you get the position of the cat as well, so that you may think of incorporating some ""automatic"" response as a future step to your project.  
 The benefit here is that you will not have to train your algorithm using a dataset specific to your garden (which, as you mention is difficult to create), but you can use images of cats taken off the net (e.g. perhaps you can search for ""cat on grass"" or something), and perhaps patches of photos from your (or other) gardens. Therefore you don;t have to spend your time collecting photos from your camera, and you avoid the risk of having a very small (comparable) sample of positives (i.e. cats). 
 Now, of course how easy it is to build an accurate cat detector is another topic..","The strategy of motion/change detection is certainly adequate, but I would add an extra operation. I would detect those regions that are more likely to be changed, for instance, the ladder seems a place where humans can be (also cats) and grass where dogs, cats or humans can be. 
 I would capture a map with size of the object and trajectory and with this I would create a cluster with the aim of detecting an object (with specific size within the image in terms of pixels) that moves with a certain speed and trajectory. 
 You can achieve this by using R or I would suggest OpenCV in order to detect movement and follow different objects.","OpenCV's  background subtraction  will find objects moving about your harden. After that you could use a classifier or shape analysis to differentiate between cats, people, trees and etc.","Is it a bird? Is it a cat? We have black-and-white cat-sized! magpies here. so that would fail. 
 First thing would be to exclude all areas that are green, cats are seldom green. 
 Then compare the rest to a reference image to remove static things like stones and stairs. 
 Detecting objects of a minimum size should be possible, but for a classification the resolution is too low. Could be also your neighbor testing his new remote controlled drone. 
 With two cameras you could do a 3d mapping of the objects and eliminate flying objects.",,,,,50,50,50,50,50,50,55.48537328,50,
554,Datasets understanding best practices,statistics,"There are basic things you can do with any set of data: 
 
 Validate values (String length tolerance, data type, formatting masks, required field presence, etc.) 
 Range correctness (Does this seemingly correct data fall within expected ranges of values) 
 Preliminary processing (If I attempt to analyze this data, can I perform the basics without running into errors) 
 Preliminary reporting (run a report against a data set and ensure that it passes a sanity test) 
 Defining null vs. empty vs. zero vs. False for any given column of data 
 Identifying data that is out of place (numeric values dramatically different than other values in a data set, string values that look like they might be misspelled, etc.) 
 Eliminating or correcting obviously errant data 
 
 Understanding data to identify errors is a whole different ball game, and it is very important. 
 For instance, you can have a rule that says a serial number must be present in a given data set and that serial number must be alphanumeric with a maximum string length of 255 and a minimum string length of 5. 
 Looking at the data, you may find one particular serial number value reads  ""PLEASE ENTER SERIAL""   It's perfectly valid, but wrong. 
 That's kind of an obvious one, but say you're processing stock data and you had a price range for 1000 stocks that was under a dollar.  A lot of people would not know that a stock price so low is invalid on certain exchanges and perfectly valid on others.  You need knowledge about your data to understand if what you are seeing is problematic or not.   
 In the real world, you don't always have the luxury of understanding your data intimately. 
 The way I avoid problems is by leveraging the people around me.  For small data sets, I can ask someone to review the data in it's entirety.  For large ones, pulling a set of random samples and asking someone to do a sanity check on the data is more appropriate. 
 Further, questioning the source of the data and how well that data source can be trusted is imperative.  I often have multiple conflicting sources of data and we create rules to determine the ""source of truth"".  Sometimes one data set has great data in a given aspect, but other data sets are stronger in other areas. 
 Manually entered data is usually what I'm most skeptical about, but in some cases it is stronger than anything that can be acquired through automation.","I like @Kallestad answer very much, but I would like to add a meta-step: Make sure that you understand how the data where collected, and what types of constraints there are. 
I think it is very common to think that there where no non-obvious steps when the data where collected, but this is not the case: Most of the time, some process or indivudal did somethink with the data, and these steps can and will influence the shape of the data. 
 Two examples:
I had a study recently where the data where collected by various con
tractors worldwide. I was not at the briefing, so that was opaque to me. Unfortunately, the measurements where off for some parts of france: People all liked ice cram, but we expected a random distribution. There was no obvious reason for this uniformity, so I began to hunt the errors. When I queried the contractors, one had misunderstood the briefing and selected only ice-cream lovers from his database. 
 The second error was more challenging: When doing some geographic analysis, I found that a lot of people had extremely large movement patterns, which suggested that a lot of them traveled from Munich to Hamburg in minutes. When I spoke with ppeople upstream, they found a subtle bug in their data aggregation software, which was unnoticed before. 
 Conclusions: 
 
 Do not assume that your data was collected by perfect processes /humans. 
 Do try to understand the limits of your data providers. 
 Look at individual patterns / values and try to determine if they are logical (easy for movement / geographic data)","Below you can find a copy of my answer to a related (however, focused on data cleaning aspect) question here on  Data Science StackExchange  ( https://datascience.stackexchange.com/a/722/2452 ), provided in its entirety for readers' convenience. I believe that it partially answers your question as well and hope it is helpful. While the answer is focused on  R  ecosystem, similar packages and/or libraries can be found for other  data analysis environments . Moreover, while the two  cited papers  on data preparation also contain examples in R, these papers present  general   workflow (framework)  and  best practices  that are applicable to  any  data analysis environment. 
 R contains some  standard  functions for data manipulation, which can be used for data cleaning, in its  base  package ( gsub ,  transform , etc.), as well as in various third-party packages, such as  stringr ,  reshape ,  reshape2 , and  plyr . Examples and best practices of usage for these packages and their functions are described in the following paper:  http://vita.had.co.nz/papers/tidy-data.pdf . 
 Additionally, R offers some packages specifically  focused  on data cleaning and transformation: 
 
 editrules 
 deducorrect 
 StatMatch 
 MatchIt 
 DataCombine 
 
 A comprehensive and coherent approach to  data cleaning  in R, including examples and use of  editrules  and  deducorrect  packages, as well as a description of  workflow  ( framework ) of data cleaning in R, is presented in the following paper, which I highly recommend:  http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf .","I usually take a two-step approach  
 
 compute univariate (variable by variable) summary statistics such as mean, range, variance, number of missing, cardinality, etc. for each variable and look for oddities (e.g. range not plausible given the meaning of the variable). Plot histograms for those odd variables. 
 split the data into manageable subsets (choose a meaningful variable and split the data according to it e.g. all positive examples, and all negative) and explore them visually (e.g. with  ggobi  ). Especially use tools like brushing and scatter plots to understand how variables are linked together. 
 
 And when you start building models, make sure to plot the residuals, looking for extreme errors that might be due to an outlier, or look at the confusion matrix and make sure it is balanced. Use k-fold cross validation to optimize your models and look at the variance of the training error for each fold, if one fold performs much worse than the others, it may contain outliers.","Folks here stated great steps, but I think there are great information at the following link  what I do when I get a new data set as told through tweets ,
It sums up the steps the folks tweeted answering the great @hmason question ""Data people: What is the very first thing you do when you get your hands on a new data set?"" 
 Hope it will be useful.","I'll add one thing- if possible, do a reasonableness check by comparing you data against some other source. It seems that whenever I fail to do this, I get burnt:(",,,,50,50,50,50,50,50,61.00325774,,
536,Good practices for manual modifications of data,data-cleaning,"1.  Do not modify the original data 
 Having the original data source intact is important.  You may find that updates you make to the data are not valid.  You may also find a more efficient way to make updates and you will want to regression test those updates. 
 Always work with a copy of the data, and add columns/properties/metadata that includes any processed corrections. 
 Example, if your data is a .csv file that includes a city name that contains several misspellings: 
 1. Copy the .csv to a new file
2. Add a column for containing ""adjusted_city_name""
3. Copy data from the city_name column to the 
   adjusted_city_name column and make corrections in that column.
 
 2. Document proposed changes 
 Any changes you want to make to data should be documented so that they can be replicated moving forward. 
 Version control and timestamp the document every time you change it.  That will help in troubleshooting at a later date. 
 Be explicit.  Do not simply state ""correct capitalization problems"", state ""ensure that the first letter of each city name begins with a capital letter and the remaining letters are lower-case."" 
 Update the document with references to any automation routines that have been built to manage data cleansing. 
 3. Decide on a standard data cleansing technology 
 Whether you use perl, python, java, a particular utility, a manual process or something else is not the issue.  The issue is that in the future you want to hand the data cleansing process to somebody else.  If they have to know 12 different data cleansing technologies, delegating the cleansing procedure will be very difficult. 
 4. Standardize the workflow 
 There should be a standard way to handle new data.  Ideally, it will be as simple as dropping a file in a specific location and a predictable automated process cleanses it and hands off a cleansed set of data to the next processing step. 
 5. Make as few changes as is absolutely necessary 
 It's always better to have a fault tolerant analysis than one that makes assumptions about the data.   
 6. Avoid manual updates 
 It's always tempting, but people are error-prone and again it makes delegation difficult. 
 Notes on manual processing 
 To more completely address the original question as to whether there's a ""good"" way to do manual processing, I would say no, there is not.  My answer is based on experience and is not one that I make arbitrarily. 
 I have had more than one project lose days of time due to a client insisting that a manual data cleansing process was just fine and could be handled internally.  You do not want your projects to be dependent on a single individual accomplishing a judgement based task of varying scale. 
 It's much better to have that individual build and document a rule set based on what they would do than to have them manually cleanse data.  (And then automating that rule set) 
 If automation fails you in the end or is simply not possible, the ability to delegate that rule set to others without domain specific knowledge is vital. 
 In the end, routines to do something like correct city names can be applied to other data sets.","Even if you are effectively modifying certain records by hand, as in the city name example you give, I would recommend doing it in code. The reason to strongly prefer code over hand-tweaking records is that the code makes a result reproducible. You want to make sure that you can always go from raw data to final result without any human intervention. 
 Here's a quick example. Let's say I have a list of city names in a pandas dataframe and I am certain they should all be ""omaha"" (you need to be absolutely certain, because changing values by hand is fraught with danger). But instead I have the following strings: 
 pd.unique(data.city)
array(['omaha', 'omahd', 'imaha', 'omaka'], dtype=object)
 
 You could make the change like this: 
 data.city.values[data.city.values == 'omahd'] = 'omaha'
data.city.values[data.city.values == 'imaha'] = 'omaha'
data.city.values[data.city.values == 'omaka'] = 'omaha'
 
 That code is ugly, but if you run it on the same raw dataset, you will  always  get the same result.","In addition to excellent previous answers, I'd like to recommend two papers on  data cleaning . They are not specific to  manual  data cleaning, but, considering the benefits and advice (which I completely agree with) of expressing even  manual  data transformations  in code , these resources can be as valuable. Also, despite the fact that following papers are somewhat R-focused, I believe that general  ideas  and  workflows  for data cleaning can be easily extracted and are equally applicable to non-R environments, as well. 
 The first paper presents the concept of  tidy data , as well as examples and best practices of use of standard and specific R packages in data cleaning:  http://vita.had.co.nz/papers/tidy-data.pdf . 
 A comprehensive and coherent approach to data cleaning in R, including examples, as well as a description of  workflow  ( framework ) of data cleaning in R, is presented in the following paper, which I highly recommend:  http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf .","As many have answered, it is always best to avoid anything done manually as it is less reproducible/documentable. Your point about the overhead of writing a script vs. opening and fixing the file is valid, though. 
 Best practice is often to 
 
 keep an untouched version of the data 
 build a working copy of the data with errors fixed 
 have a way to re-create a working copy from the original data 
 
 The last point can be done with a script. Then make sure to be as specific as needed to modify only the data you want to modify, and to write the script in such a way that adding a fix by modifying the script is as easy as modifying the data directly. 
 If your data lie in files, you can also use diffs/patches to store the original data along with the patches needed to produce the working data. To generate them, duplicate your working copy, perform the change, extract the diff/patch, save it, and delete the previous working copy.","The #1 most important thing is to  explicitly document your process . 
 Different people working on different data make different choices. Any scientists who claim that their work is entirely objective and rational are in denial about the nature of science; every single model we create is informed by our biases and perspectives. The key is that we each have different biases and perspectives, which is one reason that science is the pursuit of a society, not of individuals, and underscores the importance of peer review. 
 You have already identified some trade-offs between an algorithmic solution and individual/case-by-case corrections. If we have training as scientists, we may be used to thinking that everything has to be applied as an overall rule, so we may bend over backwards to find correlations and fit curves to data that just won't reasonably accept it. Applying an algorithm to the entire data set can be powerful when it works, but it's not the only tool at your disposal. Sometimes you need to use your judgment as the scientist. 
 But, if you use your individual judgment to make exceptions and to manually correct your data, be prepared to defend your judgment and argue your case. Be prepared to show that you considered all the data. If you're going to manually correct 8 observations out of a set of 100,000, you need to do more than justify those 8 manual corrections - you also need to justify  not  correcting the other 99,992 observations. 
 You can still take a methodical approach, perhaps by classifying your data in a systematic way, and choosing which subset of the data to apply your cleaning algorithm to based on that system of classification. And when you do this, you document it, you make your case, and you respect the judgment of your colleagues in the field. 
 On the other hand, why do all this extra work before you know it's necessary? Plenty of ""dirty"" data sets will still produce useful results. Perhaps 0.5% of your data is ""dirty"" in a way that you know will have a bad influence on your end product. Well, any analysis is subject to error, and the more you obsess over that 0.5% the more you will start to think of it and treat it like it was 5%, or 25% - much more significant than it truly is. Try to apply your analysis while admitting you know there is some error, and only do extra work to clean the data once you can show for certain that your analysis fails or is not useful otherwise. 
 For example... 
 Say you have a set of test results from a survey of wells, showing the concentrations of certain dissolved substances, hourly over the course of a year. And you observe in this set certain spikes, of short duration, and orders of magnitude higher than the surrounding data. If they are few, and obvious, and you know that the sensors used to produce the data set occasionally malfunction, then there's no reason to apply an algorithmic solution to the entire data set. You have a choice between excluding some data or modifying it.  
 I recommend the exclusion route whenever possible, since you are making fewer assumptions when you don't have to additionally choose a ""correct"" value. But if your analysis will absolutely fail with a discontinuity, there are many options. You could choose to replace ""bad"" values via linear interpolation. You could hold constant the previous value. If there is some great previous literature to apply, and you really have a strong case that the ""bad"" data is purely due to equipment malfunction, perhaps there's an established model that you can apply to fill in those regions. 
 There are a great many approaches, which is why the most important thing is to document your process, explicitly and exhaustively. Arguing your case is important too, but not strictly necessary; the community is just more likely to ignore you if you don't make a full effort to engage the review process.","I am new to this forum. Data cleansing of address data is an area I work in. I agree with the other posters that you should not modify the original data, but add fields for corrected values. I developed a technique in our systems (opengeocode.org) we call 'reduced to common form'. In this method, addresses and geographic names are analyzed for reduction into an unambiguous short form, which is then used for record matching (vs. the original values). For example, the method I use for matching US postal addresses is based on the US Post Office's published method for matching addresses. 
 For geographic names, the method will reduce to short gazetteer form in Romanized script. 
 The link below is an article I wrote a couple of years ago that explains how the street address reduction works: 
 http://www.nwstartups.com/api/doc/middleware.php#streetR","In the case you mention, I recommend to keep the changes as a dictionary, for instance in a .csv file. Write a script that replaces the values in the original data based on the translation in your dictionary. That way, you separate the corrections from the script itself.",,,50,53.00697769,50,50,50,50,56.74256069,50,
441,What are the use cases for Apache Spark vs Hadoop,apache-spark,"Hadoop means HDFS, YARN, MapReduce, and a lot of other things. Do you mean Spark vs  MapReduce ? Because Spark runs on/with Hadoop, which is rather the point. 
 The primary reason to use Spark is for speed, and this comes from the fact that its execution can keep data in memory between stages rather than always persist back to HDFS after a Map or Reduce. This advantage is very pronounced for iterative computations, which have tens of stages each of which is touching the same data. This is where things might be ""100x"" faster. For simple, one-pass ETL-like jobs for which MapReduce was designed, it's not in general faster. 
 Another reason to use Spark is its nicer high-level language compared to MapReduce. It provides a functional programming-like view that mimics Scala, which is far nicer than writing MapReduce code. (Although you have to either use Scala, or adopt the slightly-less-developed Java or Python APIs for Spark).  Crunch  and  Cascading  already provide a similar abstraction on top of MapReduce, but this is still an area where Spark is nice. 
 Finally Spark has as-yet-young but promising subprojects for ML, graph analysis, and streaming, which expose a similar, coherent API. With MapReduce, you would have to turn to several different other projects for this (Mahout, Giraph, Storm). It's nice to have it in one package, albeit not yet 'baked'. 
 Why would you not use Spark?  paraphrasing  myself: 
 
 Spark is primarily Scala, with ported Java APIs; MapReduce might be friendlier and more native for Java-based developers 
 There is more MapReduce expertise out there now than Spark 
 For the data-parallel, one-pass, ETL-like jobs MapReduce was designed for, MapReduce is lighter-weight compared to the Spark equivalent 
 Spark is fairly mature, and so is YARN now, but Spark-on-YARN is still pretty new. The two may not be optimally integrated yet. For example until recently I don't think Spark could ask YARN for allocations based on number of cores? That is: MapReduce might be easier to understand, manage and tune","Not sure about the YARN, but I think that Spark makes a real difference compared to Hadoop (advertised as 100 times faster) if data can fit nicely in the memory of the computational nodes. Simply because it avoids hard disk access. If data doesn't fit memory there's still some gain because of buffering.",Good info @Sean Owen. Would like to add one additional. Spark may help to build Unified data pipelines in Lambda architecture addressing both Batch and Streaming layers with an ability to write to common serving layer. It is huge advantage to reuse the logic between batch and Streaming. Also Streaming K-Means algorithms in Spark1.3 is an added plus to ML apart from excellent job monitoring and process visualizations in 1.4.,"It would be fair to  compare Spark with MapReduce  - Hadoop's processing framework. 
In the majority of cases, Spark may outperform MapReduce. The former enables in-memory data processing, which makes it possible to process data up to 100 times faster. For this reason, Spark is a preferred option if you need insights quickly, for example, if you need to 
 
 run customer analytics, e.g. compare the behavior of a customer with
the behavior patterns of a particular customer segment and trigger
certain actions; 
 manage risks and forecast various possible scenarios;   
 detect fraud in real-time; 
 run industrial big data analytics and predict anomalies and machine failures. 
 
 However, MapReduce is good at processing really huge datasets (if you are fine with the time required for processing). Besides, it's a more economical solution, as MapReduce reads from/writes to a disk. And disks are generally cheaper than memory.","Machine learning is a good example of a problem type where Spark-based solutions are light-years ahead of mapreduce-based solutions, despite the young age of spark-on-yarn.",,,,,50,50,55.93570994,50,50,50,50,50,56.09767681
422,Publicly available social network datasets/APIs,open-source,"A couple of words about social networks APIs. About a year ago I wrote a review of popular social networks’ APIs for researchers. Unfortunately, it is in Russian. Here is a summary: 
 Twitter  ( https://dev.twitter.com/docs/api/1.1 ) 
 
 almost all data about tweets/texts and users is available; 
 lack of sociodemographic data; 
 great streaming API: useful for real time text processing; 
 a lot of wrappers for programing languages; 
 getting network structure (connections) is possible, but time-expensive (1 request per 1 minute). 
 
 Facebook  ( https://developers.facebook.com/docs/reference/api/ ) 
 
 rate limits: about 1 request per second; 
 well documented, sandbox present; 
 FQL (SQL-like) and «regular Rest» Graph API; 
 friendship data and sociodemographic features present; 
 a lot of data is beyond  event horizon : only friends' and friends' of friends data is more or less complete, almost nothing could be investigated about random user; 
 some strange API bugs, and looks like nobody cares about it (e.g., some features available through FQL, but not through Graph API synonym). 
 
 Instagram  ( http://instagram.com/developer/ ) 
 
 rate limits: 5000 requests per hour; 
 real-time API (like Streaming API for Twitter, but with photos) - connection to it is a little bit tricky: callbacks are used; 
 lack of sociodemographic data; 
 photos, filters data available; 
 unexpected imperfections (e.g., it’s possible to collect only 150 comments to post/photo). 
 
 Foursquare  ( https://developer.foursquare.com/overview/ ) 
 
 rate limits: 5000 requests per hour; 
 kingdom of geosocial data :) 
 quite closed from researches because of privacy issues. To collect checkins data one need to build composite parser working with 4sq, bit.ly, and twitter APIs at once; 
 again: lack of sociodemographic data. 
 
 Google+  ( https://developers.google.com/+/api/latest/ ) 
 
 about 5 requests per second (try to verify); 
 main methods: activities and people; 
 like on Facebook, a lot of personal data for random user is hidden; 
 lack of user connections data. 
 
 And out-of-competition: I reviewed social networks for Russian readers, and #1 network here is  vk.com . It’s translated to many languages, but popular only in Russia and other CIS countries. API docs link:  http://vk.com/dev/ . And from my point of view, it’s the best choice for homebrew social media research. At least, in Russia. That’s why: 
 
 rate limits: 3 requests per second; 
 public text and media data available; 
 sociodemographic data available: for random user availability level is about 60-70%; 
 connections between users are also available: almost all friendships data for random user is available; 
 some special methods: e.g., there is a method to get online/offline status for exact user in realtime, and one could build schedule for his audience.","It's not a social network per se, but Stackexchange publish their entire database dump periodically: 
 
 Stackexchange data dump hosted on the archive.org 
 Post describing the database dump schema 
 
 You can extract some social information by analyzing which users ask and answer to each other. One nice thing is that since posts are tagged, you can analyze sub-communities easily.","A good list of publicly available social network datasets can be found on the Stanford Network Analysis Project website:  
 SNAP datasets 
 The site contains internet social network data (Facebook, Twitter, Google Plus), Citation networks for academic journals, co-purchasing networks from Amazon and several others kinds of networks. They have directed, undirected, and bipartite graphs and all datasets are snapshots that can be downloaded in compressed form.","An example from germany: Xing a site similar to linkedin but limited to german speaking countries. 
 Link to it's developer central:  https://dev.xing.com/overview 
 Provides access to: User profiles, Conversations between users (limited to the user itself), Job advertisings, Contacts and Contacts of Contacts, news from the network and some geolocation api. 
 Yes it has an api, but I did not find information about the rate. But it seems to me, that some information is limited to the consent of the user.","Network Repository  has tons of social networks, web graphs, bio and brain networks, etc. Best of all, they also have interactive visual analytic tools to compare/explore the various social networks.",A small collection of such links can be found at  here . Many of them are social graphs.,"Thai text  from different social media platforms + sentiment labels (positive, neutral, negative).",,,50,50,53.04656273,50,54.31243549,50,,,
412,"How can I transform names in a confidential data set to make it anonymous, but preserve some of the characteristics of the names?",data-cleaning,"One of the references I mentioned in the OP led me to a potential solution that seems quite powerful, described in  ""Privacy-preserving record linkage using Bloom filters"" : 
 
 A new protocol for privacy-preserving record linkage with encrypted identifiers allowing for errors in identifiers has been developed. The protocol is based on Bloom filters on q-grams of identifiers. 
 
 The article goes into detail about the method, which I will summarize here to the best of my ability. 
 A Bloom filter is a fixed-length series of bits storing the results of a fixed set of independent hash functions, each computed on the same input value. The output of each hash function should be an index value from among the possible indexes in the filter; i.e., if you have a 0-indexed series of 10 bits, hash functions should return (or be mapped to) values from 0 to 9. 
 The filter starts with each bit set to 0. After hashing the input value with each function from the set of hash functions, each bit corresponding to an index value returned by any hash function is set to 1. If the same index is returned by more than one hash function, the bit at that index is only set once. You could consider the Bloom filter to be a superposition of the set of hashes onto the fixed range of bits. 
 The protocol described in the above-linked article divides strings into n-grams, which are in this case sets of characters. As an example,  ""hello""  might yield the following set of 2-grams: 
 [""_h"", ""he"", ""el"", ""ll"", ""lo"", ""o_""]
 
 Padding the front and back with spaces seems to be generally optional when constructing n-grams; the examples given in the paper that proposes this method use such padding. 
 Each n-gram can be hashed to produce a Bloom filter, and this set of Bloom filters can be superimposed on itself (bitwise OR operation) to produce the Bloom filter for the string. 
 If the filter contains many more bits than there are hash functions or n-grams, arbitrary strings are relatively unlikely to produce exactly the same filter. However, the more n-grams two strings have in common, the more bits their filters will ultimately share. You can then compare any two filters  A, B  by means of their Dice coefficient: 
 
 D A, B  = 2h / (a + b) 
 
 Where  h  is the number of bits that are set to 1 in both filters,  a  is the number of bits set to 1 in  only  filter A, and  b  is the number of bits set to 1 in  only  filter B. If the strings are exactly the same, the Dice coefficient will be 1; the more they differ, the closer the coefficient will be to  0 . 
 Because the hash functions are mapping an indeterminate number of unique inputs to a small number of possible bit indexes, different inputs may produce the same filter, so the coefficient indicates only a  probability  that the strings are the same or similar. The number of different hash functions and the number of bits in the filter are important parameters for determining the likelihood of false positives - pairs of inputs that are much less similar than the Dice coefficient produced by this method predicts. 
 I found  this tutorial  to be very helpful for understanding the Bloom filter. 
 There is some flexibility in the implementation of this method; see also  this 2010 paper  (also linked at the end of the question) for some indications of how performant it is in relation to other methods, and with various parameters.","Halfway through reading your question, I realized Levenshtein Distance could be a nice solution to your problem.  It's good to see that you have a link to a paper on the topic; let me see if I can shed some light into what a Levenshtein solution would look like. 
 Levenshtein distance is used across many industries for entity resolution. What makes it useful is that it is a measure of the difference between two sequences.  In the case of string comparison, it is just sequences of characters. 
 This could help solve your problem by allowing you to provide one number that gives a measure of how similar the text of another field is. 
 Here is an example of a basic way of using Levenshtein with the data you gave: 
 
 This provides an ok solution, the distance of 8 provides some indication of a relationship, and it is very PII compliant.  However, it is still not super useful. Let's see what happens if we do some text magic to take only the first initial of the first name, the full last name and dropping anything in the middle: 
 
 As you can see, the Levenshtein distance of 0 is pretty indicative of a relationship.  Commonly data providers will combine a bunch of Levenshtein permutations of the first and last names with 1, 2, or all of the characters just to give some dimensionality as to how entities are related while still maintaining anonymity within the data.","If feasible, I would link related records (e.g., Dave, David, etc.) and replace them with a sequence number (1,2,3, etc.) or a  salted   hash of the string  that is used to represent all related records (e.g., David instead of Dave). 
 I assume that third parties need not have any idea what the real name is, otherwise you might as well give it to them. 
 You need to define and justify what kind of operations the third party needs to be able to do. For example, what is wrong with using initials followed by a number (e.g., BOA-1, BOA-2, etc.) to disambiguate Bank of America from Benjamin Othello Ames? If that's too revealing, you could bin some of the letters or names; e.g., [A-E] -> 1, [F-J] -> 2, etc. so BOA would become 1OA, or [""Bank"", ""Barry"", ""Bruce"", etc.] -> 1, so Bank of America is again 1OA. 
 For more information see  k-anonymity .","One option (depending on your dataset size) is to just provide edit distances (or other measures of similarity you're using) as an additional dataset. 
 E.g.: 
 
 Generate a set of unique names in the dataset 
 For each name, calculate edit distance to each other name 
 Generate an ID or irreversable hash for each name 
 Replace names in the original dataset with this ID 
 Provide matrix of edit distances between ID numbers as new dataset 
 
 Though there's still a lot that could be done to deanonymise the data from these even.  
 E.g. if ""Tim"" is known to be the most popular name for a boy, frequency counting of IDs that closely match the known percentage of Tims across the population might give that away.  From there you could then look for names with an edit distance of 1, and conclude that those IDs might refer to ""Tom"" or ""Jim"" (when combined with other info).","I'm not quite sure, but maybe  locality-sensitive hashing is a good solution. It does hashing of input data (in your case - names), so original strings would be preserved. On the other side, the main idea of LSH is to maximize hashes likelihood for similar items. There are a lot of different LSH-implementations. I tried  Nilsimsa-hash  for comparing tweet texts, and it worked quite well. But I'm not sure, how well it will work in case of short strings (names) - this issue require testing. I tried your examples, and here is the result (name A, name B, ""distance"" - maximum is 120): 
 1. AMELIA BEDELIA  - CHRISTOPH BAUER - 107
2. AMELIA BEDELIA  - C J BAUER       - 82
3. AMELIA BEDELIA  - FRANZ HELLER    - 91
4. CHRISTOPH BAUER - C J BAUER       - 81
5. CHRISTOPH BAUER - FRANZ HELLER    - 98
6. C J BAUER       - FRANZ HELLER    - 83
 
 As you see, CHRISTOPH BAUER and C J BAUER turned up to be the closest pair. But difference is not significant.
And just for example - hash representation of these names: 
 AMELIA BEDELIA  6b208299602b5000c3005a048122a43a828020889042240005011c1880864502
CHRISTOPH BAUER 22226448000ab10102e2860b52062487ff0000928e0822ee106028016cc01237
C J BAUER       2282204100961060048050004400240006032400148000802000a80130402002
FRANZ HELLER    58002002400880080b49172044020008030002442631e004009195020ad01158","Here's an approach I didn't see mentioned: separate the process into two steps: the first step focused on encoding names so that alternative versions of the same name are encoded the same (or nearly the same), and the second step focused on making them anonymous. 
 For the first step, you could use one of the  Phonetic Algorithms (Soundex and variants) , applied to first name, last name, and initials in various orders.  (See  this article , also).  It's in this step where you resolve similarities vs. differences in names to balance false positives from false negatives. 
 For the second step, you can pick any hashing or cryptographic method you like, without concern for how that method affects name matching.  This gives you freedom to use a method that has the best characteristics for both performance, robustness, and anonymity.",,,,56.77382769,50,53.53679595,50,50,,,,
410,Choosing a learning rate,machine-learning,"Is the learning rate related to the shape of the error gradient, as
it dictates the rate of descent? 
 
 In plain SGD, the answer is no. A global learning rate is used which is indifferent to the error gradient. However, the intuition you are getting at has inspired various modifications of the SGD update rule. 
 
 If so, how do you use this information to inform your decision about a value? 
 
 Adagrad is the most widely known of these and scales a global learning rate  η  on each dimension based on l2 norm of the history of the error gradient  gt  on each dimension: 
 
 Adadelta  is another such training algorithm which uses both the error gradient history like adagrad and the weight update history and has the advantage of  not having to set a learning rate at all . 
 
 If it's not what sort of values should I choose, and how should I choose them? 
 
 Setting learning rates for plain SGD in neural nets is usually a
process of starting with a sane value such as 0.01 and then doing cross-validation
to find an optimal value. Typical values range over a few orders of
magnitude from 0.0001 up to 1. 
 
 It seems like you would want small values to avoid overshooting, but
how do you choose one such that you don't get stuck in local minima
or take too long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient? 
 
 Usually, the value that's best is near the highest stable learning
rate and learning rate decay/annealing (either linear or
exponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.","Below is a very good note (page 12) on learning rate in Neural Nets (Back Propagation) by Andrew Ng. You will find details relating to learning rate.  
 http://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf 
 For your 4th point, you're right that normally one has to choose a ""balanced"" learning rate, that should neither overshoot nor converge too slowly. One can plot the learning rate w.r.t. the descent of the cost function to diagnose/fine tune. In practice, Andrew normally uses the L-BFGS algorithm (mentioned in page 12) to get a ""good enough"" learning rate.","Selecting a learning rate is an example of a ""meta-problem"" known as  hyperparameter optimization . The best learning rate depends on the problem at hand, as well as on the architecture of the model being optimized, and even on the state of the model in the current optimization process! There are even software packages devoted to hyperparameter optimization such as  spearmint  and  hyperopt  (just a couple of examples, there are many others!). 
 Apart from full-scale hyperparameter optimization, I wanted to mention one technique that's quite common for selecting learning rates that hasn't been mentioned so far.  Simulated annealing  is a technique for optimizing a model whereby one starts with a large learning rate and gradually reduces the learning rate as optimization progresses. Generally you optimize your model with a large learning rate (0.1 or so), and then progressively reduce this rate, often by an order of magnitude (so to 0.01, then 0.001, 0.0001, etc.). 
 This can be combined with  early stopping  to optimize the model with one learning rate as long as progress is being made, then switch to a smaller learning rate once progress appears to slow. The larger learning rates appear to help the model locate regions of general, large-scale optima, while smaller rates help the model focus on one particular local optimum.","Copy-pasted from  my masters thesis : 
 
 If the loss does not decrease for several epochs, the learning rate might be too low.
The optimization process might also be stuck in a local minimum. 
 Loss being NAN might be due to too high learning rates. Another reason is division
by zero or taking the logarithm of zero. 
 Weight update tracking: Andrej Karpathy proposed in the 5th lecture of CS231n to track weight updates to check if
the learning rate is well-chosen. He suggests that the weight update should be in the order
of 10−3. If the weight update is too high, then the learning rate has to be decreased. If the
weight update is too low, then the learning rate has to be increased. 
 Typical learning rates are in [0.1, 0.00001]","Learning rate , transformed as ""step size"" during our iteration process , has been a hot issue for years , and it will go on . 
 There are three options for step size in my concerning : 
 
 One is related to "" time "" , and each dimension shall share the
same step size . You might have noticed something like  
 
 
 $\it\huge\bf\frac{\alpha}{\sqrt{t}}$ 
 
 while t demonstrates the current iteration number , alpha is hyper parameter 
 
 the next one is connected with  gradient  , and each dimension have their
own step size . You might have noticed something like 
 
 
 $\it\huge\frac{1}{\frac{\alpha}{\beta + \sqrt{\sum_{s = 1}^{t - 1}{g_{s}^2}}} -
 \frac{\alpha}{\beta + \sqrt{\sum_{s = 1}^{t}{g_{s}^2}}}}$ 
 
 while alpha and beta are hyper parameter , g demonstrates gradient 
 
 the last one is the  combination of time and gradient  , and it should be
like 
 
 
 $\it\huge\frac{1}{\frac{\alpha}{\beta + \sqrt{\sum_{s = 1}^{t - 1}{g_{s}^2}}} -\frac{\alpha}{\beta + \sqrt{\sum_{s = 1}^{t}{g_{s}^2}}}} + \frac{\gamma}{\sqrt{t}}$ 
 
 or 
 
 $\it\huge\frac{1}{\frac{\alpha}{\beta + \sqrt{\sum_{s = 1}^{t - 1}{g_{s}^2}}} -  \frac{\alpha}{\beta + \sqrt{\sum_{s = 1}^{t}{g_{s}^2}}}} * \frac{\gamma}{\sqrt{t}}$ 
 
 Hopes this will help you , good luck -)","Adding to David's answer, in fastai is where I found the concept of finding the best learning rate for that data, using a particular architecture. 
 But that thing exists only on fastai/pytorch. Recently someone made a keras  implementation . 
 which in turn are based on these papers: 
 
 A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay 
 Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates 
 
 Hope this helps.","Neural networks are often trained by gradient descent on the weights. This means at each iteration we use backpropagation to calculate the derivative of the loss function with respect to each weight and subtract it from that weight. 
However, if you actually try that, the weights will change far too much each iteration, which will make them “overcorrect” and the loss will actually increase/diverge. So in practice, people usually multiply each derivative by a small value called the “learning rate” before they subtract it from its corresponding weight. 
 You can also think of a neural networks loss function as a surface, where each direction you can move in represents the value of a weight. Gradient descent is like taking leaps in the current direction of the slope, and the learning rate is like the length of the leap you take.","Let me give a brief introduction to another approach on choosing the learning rate, based on Jeremy Howard's  Deep Learning  course 1. If you want to dig deeper, see  this blogpost . 
 The learning rate proposed in Jeremy Howard's course is based on a systematic way to try different learning rates and choose the one that makes the loss function go down the most. This is done by feeding many batches to the mini-batch gradient descent method, and increasing the learning rate every new batch you feed to the method. When the learning rate is very small, the loss function will decrease very slowly. When the learning rate is very big, the loss function 
will increase. Inbetween these two regimes, there is an optimal learning rate for which the loss function decreases the fastest. This can be seen in the following figure: 
 
 We see that the loss decreases very fast when the learning rate is around $10^{-3}$. Using this approach, we have a general way to choose an approximation for the best constant learning rate for our netowork.","We can choose 2nd order learning rate: minimizing parabola in one step - in parabola  $(\theta,g)$  are in line, so e.g. division of their standard deviations gives slope of this line. This line intersects  $g=0$  in minimum - we get there in one step if using: 
 learnig rate =  $\frac{\sigma_\theta}{\sigma_g}=\sqrt{\frac{var(\theta)}{var(g)}}=\sqrt{\frac{mean(\theta^2)-mean(\theta)^2}{mean(g^2)-mean(g)^2}}$ 
 what requires maintaining four (exponential moving) averages, e.g. adapting learning rate separately for each coordinate of SGD (more details in  5th page here ).",50,50,50,50,50,50,50,50,
406,How can I predict traffic based on previous time series data?,machine-learning,"The problem with models like KNN is that they do not take into account seasonality (time-dependent variations in trend).  To take those into account, you should use Time Series analysis. 
 For count data, such as yours, you can use generalized linear auto-regressive moving average models (GLARMA).  Fortunately, there is an R package that implements them ( glarma ).   
 The  vignette  is a good resource for the theory behind the tool.","I think Christopher's answers above are entirely sensible.  As an alternate approach (or perhaps just in addition to the advise he's given), I might start by just visualizing the data a bit to try get a rough sense of what's going on. 
 If you haven't already done this, you might try adding a date's month and day of week as features -- if you end up sticking with KNN, this will help the model pick up seasonality.  
 As a different way of taking this on, you might consider starting with a really, really basic model (like OLS).. these often go a long way in generating reasonable predictions.   
 Finally, the more we know about your data, the easier it will be for us to help generate suggestions -- What time frame are you observing?  What are the features you're currently using?  etc. 
 Hope this helps --","As @Christopher Lauden mentioned above, time-series analysis is most appropriate for this sort of thing. If, however, you wished to do a more traditional ""machine learning approach"", something that I have done in the past is to block up your data into overlapping windows of time as features, then use it to predict the next days (or weeks) traffic. 
 Your feature matrix would be something like: 
 t1 | t2 | ... | tN
t2 | t3 | ... | tN+1
t3 | t4 | ... | tN+2
...
tW | tW+1 | ... |tN+W
 
 where  tI  is the traffic on day  I . The feature you'll be predicting is the traffic on the day after the last column. In essence, use a window of traffic to predict the next day's traffic.  
 Any sort of ML model would work for this.  
 Edit 
 In response to the question, ""can you elaborate on how you use this feature matrix"": 
 The feature matrix has values indicating past traffic over a period of time (for instance, hourly traffic over 1 week), and we use this to predict traffic for some specified time period in the future. We take our historic data and build a feature matrix of historic traffic and label this with the traffic at some period in the future (e.g. 2 days after the window in the feature). Using some sort of regression machine learning model, we can take historic traffic data, and try and build a model that can predict how traffic moved in our historic data set. The presumption is that future traffic will resemble past traffic.","You could try Neural Network. You can find 2 great explanations on how to apply NN on time series  here  and  here . 
 Note that it is best practice to : 
 
 Deseasonalize/detrend the input data (so that the NN will not learn the seasonality). 
 Rescale/Normalize the input data. 
 
 Because what you are looking for is a regression problem, the activation functions should be  linear  and not  sigmoid  or  tanh  and you aim to minimize the  sum-of-squares error  (as opposition to the maximization of the  negative log-likelihood  in a classification problem).","Well, firstly, I would not even use things like Machine learning without having in depth knowledge. Simplistic things I would do if I had this time series is: 
 
 Write sql queries to understand which of the times you have the busiest, average and low foot traffic. 
 Then try to visualize the whole time series, and you could use basic pattern matching algorithms to pick up patterns. 
 
 This two things will help you understand what your data set is telling you. Then, with that in hand, you will probably be in a better state to use machine learning algorithms. 
 Also, I'm currently working in building something on time series, and using time series analysis will help you much more than machine learning. For example, there are pattern recognition algorithms that you can use that uses every day data to show patterns, and ones which use up to as much as 3 to 6 months of data to catch a pattern.","I would advice against using a neural network or equivalent as, I assume, you have got such a good prior based on your experience with the store (ie that there are probably day-to-day / seasonal trends and some level of smoothness) and I imagine a relatively small amount of data. A better option IMO would be to go for a kernel method such as a Gaussian Process or SVM.","Bringing this thread back to life, as this could be useful to others landing here with similar questions. 
 Facebook  recently released and open-sourced one of their internal forecasting tool called  Prophet 
 It is available as both R & Python packages, and proves to be an interesting solution for someone with little Machine Learning background. However, some additional ML knowledge allows to tune and optimize the produced models. 
 I recommend giving Prophet a try as a first step. The quick win on this solution is the ease and speed of the model building and testing: you can literally get a descent projection in a matter of minutes.
It behaves very well on time series, catching the seasonality of the data at hand ""naturally"". 
 Under the hood, it's similar to a generalized additive model (GAM) - more details on the dedicated paper:  https://facebookincubator.github.io/prophet/static/prophet_paper_20170113.pdf",,,50,50,50,50,50,50,55.61393641,50,50
384,Choose binary classification algorithm,classification,"It's hard to say without knowing a little more about your dataset, and how separable your dataset is based on your feature vector, but I would probably suggest using extreme random forest over standard random forests because of your relatively small sample set. 
 Extreme random forests are pretty similar to standard random forests with the one exception that instead of optimizing splits on trees, extreme random forest makes splits at random. Initially this would seem like a negative, but it generally means that you have significantly better generalization and speed, though the AUC on your training set is likely to be a little worse. 
 Logistic regression is also a pretty solid bet for these kinds of tasks, though with your relatively low dimensionality and small sample size I would be worried about overfitting. You might want to check out using K-Nearest Neighbors since it often performs very will with low dimensionalities, but it doesn't usually handle categorical variables very well. 
 If I had to pick one without knowing more about the problem I would certainly place my bets on extreme random forest, as it's very likely to give you good generalization on this kind of dataset, and it also handles a mix of numerical and categorical data better than most other methods.","For low parameters, pretty limited sample size,  and a binary classifier logistic regression should be plenty powerful enough.  You can use a more advanced algorithm but it's probably overkill.","When categorical variables are in the mix, I reach for Random Decision Forests, as it handles categorical variables directly without the 1-of-n encoding transformation. This loses less information.",Linear SVM should be a good starting point. Take a look at  this  guide to choose the right estimator.,"Wouldn't recommend use of complex methods first.  Use faster simple approaches initially (kNN, NBC, etc.), then progress through linear regression, logistic regression, LDA, CART(RF), KREG, and then to least squares SVM, gradient ascent SVM, ANNs, and then metaheurustics (greedy heuristic hill climbing with GAs, swarm intelligence, ant colony optimization, etc.)",,,,,50,50,53.05678466,55.87680738,,,,,
361,When is a Model Underfitted?,efficiency,"A model underfits when it is too simple with regards to the data it is trying to model. 
 One way to detect such situation is to use the  bias–variance approach , which can represented like this: 
 
 Your model is underfitted when you have a high bias. 
 
 To know whether you have a too high bias or a too high variance, you view the phenomenon in terms of training and test errors: 
 High bias: This learning curve shows high error on both the training and test sets, so the algorithm is suffering from high bias: 
 
 High variance: This learning curve shows a large gap between training and test set errors, so the algorithm is suffering from high variance. 
 
 If an algorithm is suffering from high variance: 
 
 more data will probably help 
 otherwise reduce the model complexity 
 
 If an algorithm is suffering from high bias: 
 
 increase the model complexity 
 
 I would advise to watch  Coursera' Machine Learning course , section ""10: Advice for applying Machine Learning"", from which I took the above graphs.","To answer your question it is important to understand the frame of reference you are looking for, if you are looking for what philosophically you are trying to achieve in model fitting, check out Rubens answer he does a good job of explaining that context. 
 However, in practice your question is almost entirely defined by business objectives.   
 To give a concrete example, lets say that you are a loan officer, you issued loans that are \$3,000 and when people pay you back you make \$50.  Naturally you are trying to build a model that predicts how if a person defaults on their loan.  Lets keep this simple and say that the outcomes are either full payment, or default. 
 From a business perspective you can sum up a models performance with a contingency matrix: 
 
 When the model predicts someone is going to default, do they?  To determining the downsides of over and under fitting I find it helpful to think of it as an optimization problem, because in each cross section of predicted verses actual model performance there is either a cost or profit to be made: 
 
 In this example predicting a default that is a default means avoiding any risk, and predicted a non-default which doesn't default will make \$50 per loan issued.  Where things get dicey is when you are wrong, if you default when you predicted non-default you lose the entire loan principal and if you predict default when a customer actually would not have you suffer \$50 of missed opportunity.  The numbers here are not important, just the approach. 
 With this framework we can now begin to understand the difficulties associated with over and under fitting. 
 Over fitting in this case would mean that your model works far better on you development/test data then it does in production.  Or to put it another way, your model in production will far underperform what you saw in development, this false confidence will probably cause you to take on far more risky loans then you otherwise would and leaves you very vulnerable to losing money. 
 On the other hand, under fitting in this context will leave you with a model that just does a poor job of matching reality.  While the results of this can be wildly unpredictable, (the opposite word you want to describe your predictive models), commonly what happens is standards are tightened up to compensate for this, leading to less overall customers leading to lost good customers.   
 Under fitting suffers a kind of opposite difficulty that over fitting does, which is under fitting gives you lower confidence.  Insidiously, the lack of predictability still leads you to take on unexpected risk, all of which is bad news. 
 In my experience the best way to avoid both of these situations is validating your model on data that is completely outside the scope of your training data, so you can have some confidence that you have a representative sample of what you will see 'in the wild'.   
 Additionally, it is always a good practice to revalidate your models periodically, to determine how quickly your model is degrading, and if it is still accomplishing your objectives. 
 Just to some things up, your model is under fitted when it does a poor job of predicting both the development and production data.","Models are but abstractions of what is seen in real life. They are designed in order to abstract-away nitty-gritties of the real system in observation, while keeping sufficient information to support desired analysis. 
 If a model is overfit, it takes into account too many details about what is being observed, and small changes on such object may cause the model to lose precision. On the other hand, if a model is underfit, it evaluates so few attributes that noteworthy changes on the object may be ignored. 
 Note also that underfit may be seen as an  overfit , depending on the dataset. If your input can be 99%-correctly-classified with a single attribute, you  overfit  the model to the data by simplifying the abstraction to a single characteristic. And, in this case, you'd be generalizing too much the 1% of the base into the 99%-class -- or also specifying the model so much that it can only  see  one class. 
 A reasonable way to say that a model is neither over nor underfit is by performing cross-validations. You split your dataset into  k  parts, and say, pick one of them to perform your analysis, while using the other  k - 1  parts to train your model. Considering that the input itself is not biased, you should be able to have as much variance of data to train and evaluate as you'd have while using the model in real life processing.","Simply, one common approach is to increase the complexity of the model, making it simple, and most probably underfitting at first, and increasing the complexity of the model  until early signs of overfitting are witnessed using a resampling technique such as cross validation, bootstrap, etc. 
 You increase the complexity either by adding parameters (number of hidden neurons for artificial neural networks, number of trees in a random forest) or by relaxing the regularization (often named lambda, or C for support vector machines) term in your model.","CAPM (Capital Asset Pricing Model) in Finance is a classic example of an underfit model.  It was built on the beautiful theory that ""Investors only pay for risk they can't diversify away"" so expected excess returns are equal to correlation to market returns. 
 As a formula [0] Ra = Rf + B (Rm - Rf)
where Ra is the expected return of the asset, Rf is the risk free rate, Rm is the market rate of return, and Beta is the correlation to the Equity premium (Rm - Rf) 
 This is beautiful, elegant, and wrong.  Investors seem to require a higher rate of small stocks and value (defined by book to market, or dividend yield) stocks.   
 Fama and French [1] presented an update to the model, which adds additional Betas for Size and Value.   
 So how do you know in a general sense?  When the predictions you are making are wrong, and another variable with a logical explanation increases the prediction quality.  It's easy to understand why someone might think small stocks are risky, independent of non-diversifiable risk.  It's a good story, backed by the data. 
 [0]   http://www.investopedia.com/terms/c/capm.asp 
[1]   http://en.wikipedia.org/wiki/Fama%E2%80%93French_three-factor_model","Talking in simple terms, when you see that the predicted values by your model are exact or nearly equal to the true values then you can say that the model is not underfitting.  
 If the predicted values are not close to the true values then it can be said that the model is underfitting.",,,,50,55.00710854,53.68517405,,,,,,
334,What do you think of Data Science certifications?,education,"As a former analytics manager and a current lead data scientist, I am very leery of the need for data science certificates.  The term data scientist is pretty vague and the field of data science is in it's infancy.  A certificates implies some sort of uniform standard which is just lacking in data science, it is still very much the wild west.  
 While a certificate is probably not going to hurt you, I think your time would be better spent developing the experience to know when to use a certain approach, and depth of understanding to be able to explain that approach to a non-technical audience.","I did the first 2 courses and I'm planning to do all the others too.  If you don't know R, it's a really good program. There are assignments and quizzes every week. Many people find some courses very difficult. You are going to have hard time if you don't have any programming experience (even if they say it's not required).  
 Just remember.. it's not because you can drive a car that you are a F1 pilot ;)","The certification programs you mentioned are really entry level courses. Personally, I think these certificates show only person's persistence and they can be only useful to those who is applying for internships, not the real data science jobs.","I lead data science teams for a major Internet company and I have screened hundreds of profiles and interviewed dozens for our teams around the world. Many candidates have passed the aforementioned courses and programs or bring similar credentials. Personally, I have also taken the courses, some are good, others are disappointing but none of them makes you a ""data scientist"". 
 In general, I agree with the others here. A certificate from Coursera or Cloudera just signalizes an interest but it does not move the needle.  There is a lot more to consider and you can have a bigger impact by providing a comprehensive repository of your work (github profile for example) and by networking with other data scientists. Anyone hiring for a data science profile will always prefer to see your previous work and coding style/abilities.","There are multiple certifications going on, but they have different focus area and style of teaching. 
 I prefer The Analytics Edge on eDX lot more over John Hopkins specialization, as it is more intensive and hands on. The expectation in John Hopkins specialization is to put in 3 - 4 hours a week vs. 11 - 12 hours a week on Analytics Edge. 
 From an industry perspective, I take these certifications as a sign of interest and not level of knowledge a person possesses. There are too many dropouts in these MOOCs. I value other experience (like participating in Kaggle competitions) lot more than undergoing XYZ certification on MOOC.","Not sure about the cloud era one, but one of my friends joined the John Hopkins one and in his words it's ""brilliant to get you started"". It has also been recommended by a lot of people. I am planning to join it in few weeks. As far as seriousness is concerned, I don't think these certifications are gonna help you land a job, but they sure will help you learn.","@OP: Choosing answers by votes is the WORST idea.   
 Your question becomes a popularity contest.  You should seek the right answer, I doubt you know what you are asking, know what you are looking for.   
 To answer your question: 
Q: how seriously DS certifications are viewed at this point by the community.   
 A: What is your goal from taking these courses?  For work, for school, for self-improvement, etc?  Coursera classes are very applied, you will not learn much theory, they are intentionally reserved for classroom setting.   
 Nonetheless, Coursera classes are very useful.  I'd say it is equivalent to one year of stat grad class, out of a two year Master program.   
 I am not sure of its industry recognition yet, because the problem of how did you actually take the course?  How much time did you spend?  It's a lot easier to get A's in these courses than a classroom paper-pencil exam.  So, there is be a huge quality variation from person to person.","I think the effect of the certification from coursera is dependent on the individual as well as the classes. The requirement says min 3-5 hours a week, if you put more, and the material do open up for a lot more than the 3-5 hours, then these classes and certifications can be equivalent to strong knowledge base and experience in the field. Science comes to those who request it.","I am almost done with Johns Hopkins Data Science Specialization on Coursera (A course and a capstone left to graduate). I will just give you the pros and cons of it, trying to keep it as objective as possible: 
 Pros : 
 
 Structure around the learning process 
 You'll build a portfolio over time 
 
 Cons : 
 
 Different backgrounds needed for different courses. The first few courses don't assume previous knowledge. It suddenly gets not easy to understand in the conceptual courses. (Statistical Inference, Regression Analysis) 
 Taught by 3 professors. I think they are not on the same page about their potential audience and their abilities/needs/interests.",50,50,50,50,50,50,50,,
326,Python vs R for machine learning,machine-learning,"Some real important differences to consider when you are choosing  R  or  Python  over one another: 
 
 Machine Learning  has 2 phases. Model Building and Prediction phase. Typically, model building is performed as a batch process and  predictions are done realtime . The model building process is a compute intensive process while the prediction happens in a jiffy. Therefore, performance of an algorithm in Python or R doesn't really affect the turn-around time of the user. Python 1, R 1. 
 Production:  The real difference between Python and R comes in being production ready. Python, as such is a full fledged programming language and many organisations use it in their production systems. R is a statistical programming software favoured by many academia and due to the rise in data science and availability of libraries and being open source, the industry has started using R. Many of these organisations have their production systems either in Java, C++, C#, Python etc. So, ideally they would like to have the  prediction system  in the same language to reduce the latency and maintenance issues.
Python 2, R 1. 
 Libraries:  Both the languages have enormous and reliable libraries. R has over 5000 libraries catering to many domains while Python has some incredible packages like  Pandas, NumPy, SciPy, Scikit Learn, Matplotlib . Python 3, R 2. 
 Development:  Both the language are interpreted languages. Many say that python is easy to learn, it's almost like reading english (to put it on a lighter note) but R requires more initial studying effort. Also, both of them have good IDEs (Spyder etc for Python and RStudio for R). Python 4, R 2. 
 Speed:  R software initially had problems with large computations (say, like nxn matrix multiplications). But, this issue is addressed with the introduction of R by Revolution Analytics. They have re-written computation intensive operations in C which is blazingly fast. Python being a high level language is relatively slow. Python 4, R 3. 
 Visualizations:  In data science, we frequently tend to plot data to showcase patterns to users. Therefore, visualisations become an important criteria in choosing a software and R completely kills Python in this regard. Thanks to Hadley Wickham for an incredible ggplot2 package. R wins hands down. Python 4, R 4. 
 Dealing with Big Data:  One of the constraints of R is it stores the data in system memory (RAM). So, RAM capacity becomes a constraint when you are handling Big Data. Python does well, but I would say, as both R and Python have HDFS connectors, leveraging Hadoop infrastructure would give substantial performance improvement. So, Python 5, R 5. 
 
 So, both the languages are equally good. Therefore, depending upon your domain and the place you work, you have to smartly choose the right language. The technology world usually prefers using a single language. Business users (marketing analytics, retail analytics) usually go with statistical programming languages like R, since they frequently do quick prototyping and build visualisations (which is faster done in R than Python).","There is nothing like ""python is better"" or ""R is much better than x"".  
 The only fact I know is that in the industry allots of people stick to python because that is what they learned at the university. The python community is really active and have a few great frameworks for ML and data mining etc.  
 But to be honest, if you get a good c programmer he can do the same as people do in python or r, if you got a good java programmer he can also do (near to) everything in java.  
 So just stick with the language you are comfortable with.","Some additional thoughts. 
 The programming language 'per se' is only a tool. All languages were designed to make some type of constructs more easy to build than others. And the knowledge and mastery of a programming language is more important and effective than the features of that language compared to others.    
 As far as I can see there are two dimensions of this question. The first dimension is the ability to explore, build proof of concepts or models at a fast pace, eventually having at hand enough tools to study what is going on (like statistical tests, graphics, measurement tools, etc). This kind of activity is usually preferred by researchers and data scientists (I always wonder what that means, but I use this term for its loose definition). They tend to rely on well-known and verified instruments, which can be used for proofs or arguments. 
 The second dimension is the ability to extend, change, improve or even create tools, algorithms or models. In order to achieve that you need a proper programming language. Roughly all of them are the same. If you work for a company, than you depend a lot on the company's infrastructure, internal culture and your choices diminish significantly. Also, when you want to implement an algorithm for production use, you have to trust the implementation. And implementing in another language which you do not master will not help you much. 
 I tend to favor for the first type of activity the R ecosystem. You have a great community, a huge set of tools, proofs that these tools works as expected. Also, you can consider Python, Octave (to name a few), which are reliable candidates. 
 For the second task, you have to think before at what you really want. If you want robust production ready tools, then C/C++, Java, C# are great candidates. I consider Python as a second citizen in this category, together with Scala and friends. I do not want to start a flame war, it's my opinion only. But after more than 17 years as a developer, I tend to prefer a strict contract and my knowledge, than the freedom to do whatever you might think of (like it happens with a lot of dynamic languages). 
 Personally, I want to learn as much as possible. I decided that I have to choose the hard way, which means to implement everything from scratch myself. I use R as a model and inspiration. It has great treasures in libraries and a lot of experience distilled. However, R as a programming language is a nightmare for me. So I decided to use Java, and use no additional library. That is only because of my experience, and nothing else. 
 If you have time, the best thing you can do is to spend some time with all these things. In this way you will earn for yourself the best answer possible, fitted for you. Dijkstra said once that the tools influence the way you think, so it is advisable to know your tools before letting them to model how you think. You can read more about that in his famous paper called  The Humble Programmer","I would add to what others have said till now. There is no single answer that one language is better than other. 
 Having said that, R has a better community for data exploration and learning. It has extensive visualization capabilities. Python, on the other hand, has become better at data handling since introduction of pandas. Learning and development time is very less in Python, as compared to R (R being a low level language). 
 I think it ultimately boils down to the eco-system you are in and personal preferences. For more details, you can look at this comparison  here .","There isn't a silver bullet language that can be used to solve each and every data related problem. The language choice depends on the context of the problem, size of data and if you are working at a workplace you have to stick to what they use. 
 Personally I use R more often than Python due to its visualization libraries and interactive style. But if I need more performance or structured code I definitely use Python since it has some of the best libraries as SciKit-Learn, numpy, scipy etc. I use both R and Python in my projects interchangeably.  
 So if you are starting on data science work I suggest you to learn both and it's not difficult since Python also provides a similar interface to R with  Pandas .  
 If you have to deal with much larger datasets, you can't escape eco-systems built with Java(Hadoop, Pig, Hbase etc).","An issue all other answers fail to address is  licensing . 
 Most of the aforementioned wonderful R libraries are GPL (e.g.  ggplot2 ,  data.table ). This  prevents you  from distributing your software in a proprietary form. As of July 11 2022, ggplot2 uses the MIT + file  license  while data.table moved on to the  MPL2 licence  with more R packages, including caret using more flexible licences. 
 Although many usages of those libraries do not imply distribution of the software (e.g. to train models offline), the GPL may by itself lure away companies from using them. At least in my experience. 
 In the python realm, on the other hand,  most  libraries have business-friendly distribution licenses, such as BSD or MIT. 
 In academia, licensing issues normally are non-issues.","There is no ""better"" language. I have tried both of them and I am comfortable with Python so I work with Python only. Though I am still learning stuff, but I haven't encounter any roadblock with Python till now. The good thing about Python is community is too good and you can get a lot of help on the Internet easily. Other than that, I would say go with the language you like not the one people recommend.","In my experience, the answer depends on the project at hand. For pure research, I prefer R for two reasons: 1) broad variety of libraries and 2) much of the data science literature includes R samples. 
 If the project requires an interactive interface to be used by laypersons, I've found R to be too constrained. Shiny is a great start, but it's not flexible enough yet. In these cases, I'll start to look at porting my R work over to Python or js.",Not much to add to the provided comments. Only thing is maybe this infographic comparing R vs Python for data science purposes  http://blog.datacamp.com/r-or-python-for-data-analysis/,52.50976188,50,50,52.19659238,53.36657654,50,50,50,
289,Qualifications for PhD Programs,education,"If I were you I would take a MOOC or two (e.g.,  Algorithms, Part I ,  Algorithms, Part II ,  Functional Programming Principles in Scala ), a good book on data structures and algorithms, then just code as much as possible. You could implement some statistics or ML algorithms, for example; that would be good practice for you and useful to the community. 
 For a PhD program, however, I would also make sure I were familiar with the type of maths they use. If you want to see what it's like at the deep end, browse the papers at the  JMLR . That will let you calibrate yourself in regards to theory; can you sort of follow the maths? 
 Oh, and you don't need a PhD to work at top companies, unless you want to join research departments like his. But then you'll spend more time doing development, and you'll need good coding skills...","Your time would probably be better spent on Kaggle than in a PhD program. When you read the stories by winners ( Kaggle blog ) you'll see that it takes a large amount of practice and the winners are not just experts of one single method. 
 On the other hand, being active and having a plan in a PhD program can get you connections that you otherwise would probably not get. 
 I guess the real question is for you - what are the reasons for wanting a job at a top company?","You already have a Masters in Statistics, which is great! In general, I'd suggest to people to take as much statistics as they can, especially Bayesian Data Analysis.  
 Depending on what you want to do with your PhD, you would benefit from foundational courses in the discipline(s) in your application area.  You already have Economics but if you want to do Data Science on social behavior, then courses in Sociology would be valuable.  If you want to work in fraud prevention, then a courses in banking and financial transactions would be good.  If you want to work in information security, then taking a few security courses would be good. 
 There are people who argue that it's not valuable for Data Scientists to spend time on courses in sociology or other disciplines.  But consider the recent case of the Google Flu Trends project.  In  this article  their methods were strongly criticized for making avoidable mistakes.  The critics call it ""Big Data hubris"". 
 There's another reason for building strength in social science disciplines: personal competitive advantage.  With the rush of academic degree programs, certificate programs, and MOOCs, there is a mad rush of students into the Data Science field.  Most will come out with capabilities for core Machine Learning methods and tools.  PhD graduates will have more depth and more theoretical knowledge, but they are all competing for the same sorts of jobs, delivering the same sorts of value.  With this flood of graduates, I expect that they won't be able to command premium salaries. 
 But if you can differentiate yourself with a combination of formal education and practical experience in a particular domain and application area, then you should be able to set yourself apart from the crowd. 
 (Context: I'm in a PhD program in Computational Social Science, which has a heavy focus on modeling, evolutionary computation, and social science disciplines, and less emphasis on ML and other empirical data analysis topics).","I am glad you also found Yann LeCun's AMA page, it's very useful. 
 Here are my opinions 
Q: Should I take some intro software engineering courses at my local University to make myself a stronger candidate? 
A: No, you need to take more math courses.  It's not the applied stuff that's hard, it's the theory stuff.  I don't know what your school offers.  Take theoretical math courses, along with some computer science courses.   
 Q:What other advice you have for someone applying to PhD programs from outside the CS field? 
A:  How closely related are you looking for.  Without a specific question, it's hard to give a specific answer.","You have the option of joining a PhD program in business school and information school as well. There are quantitative professors and data scientists in business schools and information schools as well (About US, I am sure there are a lot of schools). This way you are qualified or even over-qualified in terms of quantitative and technical skills and you can spend your time on reinforcing other skills.",,,,,50,50,50,50,54.54714095,53.90021904,,,
266,What are some easy to learn machine-learning applications?,machine-learning,"I would recommend to start with some MOOC on machine learning. For example Andrew Ng's  course  at coursera. 
 You should also take a look at  Orange  application. It has a graphical interface and probably it is easier to understand some ML techniques using it.","To be honest, I think that doing some projects will teach you much more than doing a full course. One reason is that doing a project is more motivating and open-ended than doing assignments. 
 A course, if you have the time AND motivation (real motivation), is better than doing a project. The other commentators have made good platform recommendations on tech.  
 I think, from a fun project standpoint, you should ask a question and get a computer to learn to answer it.  
 Some good classic questions that have good examples are: 
 
 Neural Networks for recognizing hand written digits 
 Spam email classification using logistic regression 
 Classification of objects using Gaussian Mixture models 
 Some use of linear regression, perhaps forecasting of grocery prices given neighborhoods 
 
 These projects have the math done, code done, and can be found with Google readily. 
 Other cool subjects can be done by you! 
 Lastly, I research robotics, so for me the most FUN applications are behavioral.
Examples can include (if you can play with an arduino) 
 Create a application, that uses logistic regression perhaps, that learns when to turn the fan off and on given the inner temperature, and the status of the light in the room. 
 Create an application that teaches a robot to move an actuator, perhaps a wheel, based on sensor input (perhaps a button press), using Gaussian Mixture Models (learning from demonstration). 
 Anyway, those are pretty advanced. The point I'm making is that if you pick a project that you (really really) like, and spend a few week on it, you will learn a massive amount, and understand so much more than you will get doing a few assignments.","I think  Weka  is a good starting point. You can do a bunch of stuff like supervised learning or clustering and easily compare a large set of algorithms na methodologies. 
 Weka's manual is actually a book on machine learning and data mining that can be used as introductory material.",Assuming you're familiar with programming I would recommend looking at  scikit-learn . It has especially nice help pages that can serve as mini-tutorials/a quick tour through machine learning. Pick an area you find interesting and work through the examples.,I found the pluralsight course  Introduction to machine learning encog  a great resource so start with. It uses the  Encog library  to quickly explore different ml techniques.,"If you already know R Studio, then the caret package is a good place to start. Here are some tutorials:  
 
 https://class.coursera.org/predmachlearn-002 
 http://caret.r-forge.r-project.org/index.html 
 
 With R and caret you can easily load and splice data sets, feature reduction, principal component analysis, and train and predict using various algorithms.","If you can reproduce the 6x3 grid of graphs from the banner of the  http://scikit-learn.org/  page then you will have learnt some ML and some Python. You didn't mention a language. Python is easy enough to learn very quickly, and scikit-learn has a wide range of algorithms implemented. 
 Then try on your own data!","In addition to the courses and tutorials posted, I would suggest something a bit more 'hands on':  Kaggle  has some introductory competitions that might pique your interest (most people start with the Titanic competition). And there's a large variety of subjects to explore and compete in when you want to get more experience.","As mentioned in above answers grasp the basics of ML by following MOOCs by Prof.Andrew Ng and  'Learning From Data'  by Prof. Yaser Abu-Mostafa.   
 R is the  clear winner  as the most used tool in Kaggle competitions. (Don't forget to check the resources on Kaggle wiki and forums) 
 Learn basic R and Python. Coursera 'Data Science' track has an  introductory R course . Almost all the algorithms can be found in Python and R libraries. Feel free to use the algorithms you learned in few kaggle competitions. As a starting point compare the performance of several algorithms on Titanic dataset and Digit recognizer dataset on  kaggle . 
 And do continue practising on various datasets!",50,50,50,50,,,,,
265,Can machine learning algorithms predict sports scores or plays?,machine-learning,"There are a lot of good questions about Football (and sports, in general) that would be awesome to throw to an algorithm and see what comes out. The tricky part is to know  what  to throw to the algorithm. 
 A team with a good RB could just pass on 3rd-and-short just because the opponents would probably expect run, for instance. So, in order to actually produce some worthy results, I'd break the problem in smaller pieces and analyse them statistically while throwing them to the machines. 
 There are a few (good) websites that try to do the same, you should check'em out and use whatever they found to help you out: 
 
 Football Outsiders 
 Advanced Football Analytics 
 
 And if you truly want to explore Sports Data Analysis, you should definitely check the  Sloan Sports Conference  videos. There's a lot of them spread on Youtube.","Yes. Why not?!
With so much of data being recorded in each sport in each game, smart use of data could lead us to obtain important insights regarding player performance. 
 Some examples: 
 
 Baseball : In the movie Moneyball (which is an adaptation of the Moneyball book), Brad Pitt plays a character who analyses player statistics to come up with a team that performs tremendously well! It was a depiction of the real-life story of Oakland Athletics baseball team. For more  info . 
 Cricket : SAP Labs has come up with an auction analytics tool that has given insights about impact players to buy in the 2014 Indian Premier League auction for the Kolkata Knight Riders team, which eventually went on to win the 2014 IPL  Championship . For more  info . 
 
 So, yes, statistical analysis of the player records can give us insights about  which players are more likely to perform but not which players will perform . So, machine learning, a close cousin of statistical analysis will be proving to be a game changer.","Definitely they can.
I can target you to a  nice paper . Once I used it for soccer league results prediction algorithm implementation, primarily aiming at having some value against bookmakers. 
 From paper's abstract: 
 
 a Bayesian dynamic generalized model to estimate the time dependent skills of all teams in a league, and to predict next weekend's soccer matches. 
 
 Keywords: 
 
 Dynamic Models, Generalized Linear Models, Graphical Models, Markov
  Chain Monte Carlo Methods, Prediction of Soccer Matches 
 
 Citation: 
 
 Rue, Havard, and Oyvind Salvesen. ""Prediction and retrospective analysis of soccer matches in a league."" Journal of the Royal Statistical Society: Series D (The Statistician) 49.3 (2000): 399-418.","It has been shown before that machine learning techniques can be applied for predicting sport results. Simple google search should give you a bunch of results. 
 However, it has also been showed (for NFL btw) that very complex predictive models, simple predictive models, questioning people, or crowd knowledge by utilising betting info, they all perform more or less the same. Source: "" Everything is obvious once you know the answer - How common sense Fails "", Chapter 7, by Duncan Watts.","Machine learning and statistical techniques can improve the forecast, but nobody can predict the real result. 
 There was a kaggle competition a few month ago about  predicting the 2014 NCAA Tournament . You can read the Competition Forum to get a better idea on what people did and what results did they achieve.","Michael Maouboussin, in his book, ""The Success Equation,"" looks at differentiating luck from skill in various endeavors, including sports.  He actually ranks sports by the amount of luck that contributes to performance in the different sports (p. 23) and about 2/3 of performance in football is attributable to skill.  By contrast, I used MM's technique to analyze performance in Formula 1 racing, and found that 60% is attributable to skill (less than I was expecting.) 
 That said, it seems this kind of analysis would imply that a sufficiently detailed and crafted feature set would allow ML algorithms to predict performance of NFL teams, perhaps even to the play level, with the caveat that significant variance will still exist because of the influence of luck in the game.","I have read some about it and I had  this blog  in mind. 
 This blog deals with the prediction of a NFL match after the half time is already over. The prediction is 80% accurate with simple GLM model. 
 I do not know if that is suitable for soccer.","I've done some research in this area. I've found first order Markov chains work well for predicting within game scoring dynamics across a variety of sports. 
 You can read in more detail  here .","They can't predict, but they can tell you the most likely result. There's an study about this kind of approach from  Etienne  -  Predicting Who Will Win the World Cup with Wolfram Language . This is a very detailed study, so you can check all the methodology used to get the predictions.  
 Interesting enough, 11 from 15 matches were correct! 
 
 As one might expect, Brazil is the favorite, with a probability to win of 42.5%. This striking result is due to the fact that Brazil has both the highest Elo ranking and plays at home. 
 
 (Let's go Brazil!)",50,50,50,,,,,,
253,Do I need to learn Hadoop to be a Data Scientist?,bigdata,"Different people use different tools for different things.  Terms like Data Science are generic for a reason.  A data scientist could spend an entire career without having to learn a particular tool like hadoop.  Hadoop is widely used, but it is not the only platform that is capable of managing and manipulating data, even large scale data. 
 I would say that a data scientist should be familiar with concepts like MapReduce, distributed systems, distributed file systems, and the like, but I wouldn't judge someone for not knowing about such things.   
 It's a big field.  There is a sea of knowledge and most people are capable of learning and being an expert in a single drop.  The key to being a scientist is having the desire to learn and the motivation to know that which you don't already know. 
 As an example:  I could hand the right person a hundred structured CSV files containing information about classroom performance in one particular class over a decade.  A data scientist would be able to spend a year gleaning insights from the data without ever needing to spread computation across multiple machines.  You could apply machine learning algorithms, analyze it using visualizations, combine it with external data about the region, ethnic makeup, changes to environment over time, political information, weather patterns, etc.  All of that would be ""data science"" in my opinion.  It might take something like hadoop to test and apply anything you learned to data comprising an entire country of students rather than just a classroom, but that final step doesn't necessarily make someone a data scientist.  And not taking that final step doesn't necessarily disqualify someone from being a data scientist.","As a former Hadoop engineer, it is not needed but it helps. Hadoop is just one system - the most common system, based on Java, and a ecosystem of products, which apply a particular technique ""Map/Reduce"" to obtain results in a timely manner. Hadoop is not used at Google, though I assure you they use big data analytics. Google uses their own systems, developed in C++. In fact, Hadoop was created as a result of Google publishing their Map/Reduce and BigTable (HBase in Hadoop) white papers. 
 Data scientists will interface with hadoop engineers, though at smaller places you may be required to wear both hats. If you are strictly a data scientist, then whatever you use for your analytics, R, Excel, Tableau, etc, will operate only on a small subset, then will need to be converted to run against the full data set involving hadoop.","You have to first make it clear what do you mean by ""learn Hadoop"". If you mean using Hadoop, such as learning to program in MapReduce, then most probably it is a good idea. But fundamental knowledge (database, machine learning, statistics) may play a bigger role as time goes on.","Yes, you should learn a platform that is capable of dissecting your problem as a data parallel problem. Hadoop is one. For your simple needs (design patterns like counting, aggregation, filtering etc.) you need Hadoop and for more complex Machine Learning stuff like doing some Bayesian, SVM you need Mahout which in turn needs Hadoop (Now Apache Spark) to solve your problem using a data-parallel approach. 
 So Hadoop is a good platform to learn and really important for your batch processing needs. Not only Hadoop but you also need to know Spark (Mahout runs it's algorithms utilizing Spark) & Twitter Storm (for your real time analytics needs). This list will continue and evolve so if you are good with the building blocks (Distributed Computing, Data-Parallel Problems and so on) and know how one such platform (say Hadoop) operates you will fairly quickly be up to speed on others.","It strongly depends on the environment/company you are working with. In my eyes there is a ""big data"" hype at the moment and a lot of companies try to enter the field with hadoop based solutions - what makes hadoop also a buzzword but its not always the best solution. 
 In my mind, a good Data Scientist should be able to ask the right questions and keep on asking again until its clear whats really needed. Than a good DataScientist - of course - needs to know how to address the problem (or at least know someone who can). Otherwise your stakeholder could be frustrated :-) 
 So, I would say it is not absolutely necessary to learn Hadoop.","You should learn Hadoop if you want to be work as data scientist, but maybe before starting with Hadoop you should read something about ETL or Big Data... this book could be a good starting point:  http://www.amazon.com/Big-Data-Principles-practices-scalable/dp/1617290343 
 Hope it helps and good luck!","You can apply data science techniques to data on one machine so the answer to the question as the OP phrased it, is no.","Data Science is a field demanding a variety of skills. Having knowledge of Hadoop is one of them. The main tasks of a Data Scientist include: 
 
 Gathering data from different resources. 
 Cleaning and pre-processing the data. 
 Studying statistical properties of the data. 
 Using Machine Learning techniques to do forecasting and derive insights from the data. 
 Communicating the results to decision makers in an easy to understand way. 
 
 Out of the above points knowledge of Hadoop is useful for points 1,2 and 3, but you also need to have strong mathematical/statistical background and strong knowledge of Computational techniques to work in data science field. Also Hadoop is not the only framework that is being used in Data Science. Big Data ecosystem has a range of frameworks, each specific to a particular use case. This article gives introductory material regarding major Big Data frameworks that could be used in Data Science: 
 http://www.codophile.com/big-data-frameworks-every-programmer-should-know/","I do think Leaning Hadoop framework (hard way) is not a requirement of being a Data Scientist.
General knowledge on all big data platforms is essential.
I will suggest to know concept on it and only part need from Hadoop is the MapReduce
 http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html 
 A Data Scientist does not build cluster, administer ... is just make ""magic"" with data and does not care where is coming from.
The term ""Hadoop"" has come to refer not just to the base modules above, but also to the ""ecosystem"", or collection of additional software packages that can be installed on top of or alongside Hadoop, such as Apache Pig, Apache Hive, Apache HBase, Apache Spark, and others. 
 Most important is the Programing language, math and statistics for working with data (you'll need to find a way to connect with data and move forward).
I wish I had somebody to point me to the concept and do not spend weeks on learning framework and build from scratch nodes and clusters, because that part is Administrator role and not Data Engineer or Data Scientist.
Also one thing: all are changing and evolving but math, programing, statistics are still the requirements. 
 
 accessing data from hdfs is essential, for example PROC Hadoop, Hive,  SparkContext or any other driver or pipe (treat hadoop as a point of accesing data or storage :) 
 already are in place tools or frameworks what take care of resource allocation and management, performance.",54.25814636,50,52.65017348,50,50,50,50,50,50
235,Are there any python based data visualization toolkits?,python,"Bokeh  is an excellent data visualization library for python.
 NodeBox  is another that comes to mind.","There is a Tablaeu API and you can use Python to use it, but maybe not in the sense that you think. There is a Data Extract API that you could use to import your data into Python and do your visualizations there, so I do not know if this is going to answer your question entirely. 
 As in the first comment you can use Matplotlib from  Matplotlib website , or you could install Canopy from Enthought which has it available, there is also Pandas, which you could also use for data analysis and some visualizations. There is also a package called  ggplot  which is used in  R  alot, but is also made for Python, which you can find here  ggplot for python . 
 The Tableau data extract API and some information about it can be found  at this link . There are a few web sources that I found concerning it using duckduckgo  at this link .
Here are some samples: 
 Link 1 
 Link 2 
 Link 3 
 As far as an API like matplotlib, I cannot say for certain that one exists. Hopefully this gives some sort of reference to help answer your question. 
 Also to help avoid closure flags and downvotes you should try and show some of what you have tried to do or find, this makes for a better question and helps to illicit responses.",You can also checkout the seaborn package for statistical charts.,"There's plenty. If you've ever used ggplot2 in R and want to do that in  Python . 
 If you want to use a similar visualisation grammar (Vega) and go via  D3 . 
 Or if you want the full-on 3d:  shizzle .","If you know R and it's ggplot library, you could try ggplot for python: 
 I like it, because I do work in R and python, and both are virtually identical. 
 But if you are not familiar you have to deal with a very ""unpythonic"" syntax. But I think it's an easy library overall.",,,,,50,50,50,,,,,,
234,"Data science Ph.D. program, what do you think?",knowledge-base,"It seems to me that the premise of a PhD is to expand knowledge in some little slice of the world. Since a ""data scientist"" is by nature is somewhat of a jack-of-all-trades it does seem a little odd to me. A masters program seems much more appropriate. 
 What do you hope to gain from a PhD? If the rigor scares (or bores) you, then what about a more applied area? Signal processing, robotics, applied physics, operations research, etc.","Computer Science is itself a multi-disciplinary field which has varying requirements among universities.  For example, Stockholm University does not require any math above algebra for its CS programs (some courses may have higher requirements, but not often).   
 I am not sure what you mean by a machine learning program being more rigorous.  They are just two different programs.  Data Science would likely take a broader view and focus on application and management (business courses are maybe on offer?).  The research could be rigorous in its own right, but it definitely won't be tailored to someone who wants to optimize new algorithms or solve the low-level problems of machine learning.   
 I don't see the Ph.D program listed yet in the link you provided.  Will you please follow up here if you get more specific information?","A cash cow program?  No.  PhD programs are never cash cows. 
 I don't know why you couldn't be a professor with a PhD in data science.  Rarely does a professor of a given course have to have a specific degree in order to teach it. 
 As far as publishing goes, there are any number of related journals that would accept papers from somebody on topics that would be covered by the topic of Data Science. 
 When I went to college, MIS, Computer Engineering, and Computer Science were new subjects.  Most of the people in my graduating class for Computer Science couldn't program anything significant at graduation.  Within a few years, CS programs around the country matured significantly. 
 When you are part of a new program, sometimes it's possible to help define what it is that's required for graduation.  Being a part of that puts you in rare company for that field. 
 As far as mathematical rigor is concerned, I would expect Data Science to leverage a heavy dose of mathematically based material.  I wouldn't expect anything particularly new - statistics, calculus, etc. should have been covered in undergrad.  Masters and PhD programs should be more about applying that knowledge and not so much about learning it.","No-one knows since no-one's completed one of these PhD programs yet! However, I would look at the syllabus and the teachers to base my decision. It all depends on what you want to do; industry or academia?","I think this question assumes a false premise. As a student at NYU, I only know of a Masters in Data Science. You linked to a page that confirms this. 
 It's hard to gauge the benefit of a program that doesn't exist yet.",,,,,50,50,50,53.04288398,50,50,61.20687975,50,50
223,How to annotate text documents with meta-data?,nlp,"Personally I would advocate using something that is both not-specific to the NLP field, and something that is sufficiently general that it can still be used as a tool even when you've started moving beyond this level of metadata. I would especially pick a format that can be used regardless of development environment and one that can keep some basic structure if that becomes relevant (like tokenization) 
 It might seem strange, but I would honestly suggest  JSON . It's extremely well supported, supports a lot of structure, and is flexible enough that you shouldn't have to move from it for not being powerful enough. For your example, something like this: 
 {'text': 'I saw the company's manager last day."", {'Person': [{'name': 'John'}, {'indices': [0:1]}, etc...]}
 
 The one big advantage you've got over any NLP-specific formats here is that  JSON  can be parsed in any environment, and since you'll probably have to edit your format anyway, JSON lends itself to very simple edits that give you a short distance to other formats. 
 You can also implicitly store tokenization information if you want: 
 {""text"": [""I"", ""saw"", ""the"", ""company's"", ""manager"", ""last"", ""day.""]}
 
 EDIT: To clarify the mapping of metadata is pretty open, but here's an example: 
 {'body': '<some_text>',
 'metadata': 
  {'<entity>':
    {'<attribute>': '<value>',
     'location': [<start_index>, <end_index>]
    }
  }
}
 
 Hope that helps, let me know if you've got any more questions.","In general, you don't want to use XML tags to tag documents in this way because tags may overlap. 
 UIMA ,  GATE  and similar NLP frameworks denote the tags separate from the text.  Each tag, such as  Person ,  ACME ,  John  etc. is stored as the position that the tag begins and the position that it ends.  So, for the tag  ACME , it would be stored as starting a position 11 and ending at position 17.","The  brat annotation tool  might be useful for you as per my comment.  I have tried many of them and this is the best I have found.  It has a nice user interface and can support a number of different types of annotations.  The annotations are stored in a separate .annot file which contain each annotation as well as its location within the original document.  A word of warning though, if you ultimately want to feed the annotations into a classifier like the Stanford NER tool then you will have to do some manipulation to get the data into a format that it will accept.","To describe all existed data it is so difficult task, but we can use a data model:  http://schema.org/ , where are structural types of the information. The prior execution was targeted to implement MarkUp technology, so, it seems can be useful for your task.","Try to use  Label Studio . It supports Simple Text & HTML NER tagging and much more. 
 
 Input to Label Studio for task on the screenshot (HTML code packed to JSON): 
 {
    ""text"": ""<div style=\""max-width: 750px\""><div style=\""clear: both\""><div style=\""float: right; display: inline-block; border: 1px solid #F2F3F4; background-color: #F8F9F9; border-radius: 5px; padding: 7px; margin: 10px 0;\""><p><b>Jules</b>: No no, Mr. Wolfe, it's not like that. Your help is definitely appreciated.</p></div></div><div style=\""clear: both\""><div style=\""float: right; display: inline-block; border: 1px solid #F2F3F4; background-color: #F8F9F9; border-radius: 5px; padding: 7px; margin: 10px 0;\""><p><b>Vincent</b>: Look, Mr. Wolfe, I respect you. I just don't like people barking orders at me, that's all.</p></div></div><div style=\""clear: both\""><div style=\""display: inline-block; border: 1px solid #D5F5E3; background-color: #EAFAF1; border-radius: 5px; padding: 7px; margin: 10px 0;\""><p><b>The Wolf</b>: If I'm curt with you, it's because time is a factor. I think fast, I talk fast, and I need you two guys to act fast if you want to get out of this. So pretty please, with sugar on top, clean the car.</p></div></div></div>""
}
 
 Output: 
 [
    {
        ""id"": ""9fkAdIXgkV"",
        ""from_name"": ""ner"",
        ""to_name"": ""text"",
        ""source"": ""$text"",
        ""type"": ""hypertextlabels"",
        ""value"": {
            ""start"": ""/div[1]/div[1]/div[1]/p[1]/b[1]/text()[1]"",
            ""end"": ""/div[1]/div[1]/div[1]/p[1]/b[1]/text()[1]"",
            ""text"": ""Jules"",
            ""startOffset"": 0,
            ""endOffset"": 5,
            ""htmllabels"": [
                ""Person""
            ]
        }
    },
    {
        ""id"": ""YMeGv8ndLx"",
        ""from_name"": ""ner"",
        ""to_name"": ""text"",
        ""source"": ""$text"",
        ""type"": ""hypertextlabels"",
        ""value"": {
            ""start"": ""/div[1]/div[1]/div[1]/p[1]/text()[1]"",
            ""end"": ""/div[1]/div[1]/div[1]/p[1]/text()[1]"",
            ""text"": ""Wolfe"",
            ""startOffset"": 13,
            ""endOffset"": 18,
            ""htmllabels"": [
                ""Organization""
            ]
        }
    },
    {
        ""id"": ""vgGGhXRFcr"",
        ""from_name"": ""ner"",
        ""to_name"": ""text"",
        ""source"": ""$text"",
        ""type"": ""hypertextlabels"",
        ""value"": {
            ""start"": ""/div[1]/div[2]/div[1]/p[1]/text()[1]"",
            ""end"": ""/div[1]/div[2]/div[1]/p[1]/text()[1]"",
            ""text"": "" Look, Mr. Wo"",
            ""startOffset"": 1,
            ""endOffset"": 14,
            ""htmllabels"": [
                ""Person""
            ]
        }
    },
    {
        ""id"": ""oJxIH-ztQv"",
        ""from_name"": ""ner"",
        ""to_name"": ""text"",
        ""source"": ""$text"",
        ""type"": ""hypertextlabels"",
        ""value"": {
            ""start"": ""/div[1]/div[2]/div[1]/p[1]/text()[2]"",
            ""end"": ""/div[1]/div[2]/div[1]/p[1]/text()[2]"",
            ""text"": ""people bar"",
            ""startOffset"": 38,
            ""endOffset"": 48,
            ""htmllabels"": [
                ""Organization""
            ]
        }
    }
]",,,,,55.17949057,50,50,50,50,50,50,50,
155,Publicly Available Datasets,open-source,"There is, in fact, a very reasonable list of publicly-available datasets, supported by different enterprises/sources.  
 Some of them are below: 
 
 Public Datasets on Amazon WebServices ; 
 Frequent Itemset Mining Implementation Repository ; 
 UCI Machine Learning Repository ; 
 KDnuggets  -- a big list of lots of public repositories. 
 
 Now, two considerations on your question. First one, regarding policies of database sharing. From personal experience, there are some databases that can't be made publicly available, either for involving privacy restraints (as for some social network information) or for concerning government information (like health system databases). 
 Another point concerns the usage/application of the dataset. Although some bases can be reprocessed to suit the needs of the application, it would be great to have some  nice organization  of the datasets by purpose. The  taxonomy  should involve social graph analysis, itemset mining, classification, and lots of other research areas there may be.","Update: 
 Kaggle.com , a home of modern data science & machine learning enthusiasts:), opened  it's own repository of the data sets . 
 
 In addition to the listed sources. 
 Some social network data sets: 
 
 Stanford University large network dataset collection (SNAP) 
 A huge twitter dataset that includes followers  +  large collection of twitter datasets here 
 LastFM data set 
 
 There are plenty of sources listed at Stats SE: 
 
 Locating freely available data samples 
 Data APIs/feeds available as packages in R 
 Free data set for very high dimensional classification","There are many openly available data sets, one many people often overlook is  data.gov . As mentioned previously Freebase is great, so are all the examples posted by @Rubens","Freebase  is a free community driven database that spans many interesting topics and contains about 2,5 billion facts in machine readable format. It is also have good API to perform data queries. 
 Here  is another compiled list of open data sets","The following links are available 
 
 Public Data Sets 
 Google Public Data Sets 
 Amazon Web Services 
 Finding Data on the Internet","For time series data in particular,  Quandl  is an excellent resource -- an easily browsable directory of (mostly) clean time series. 
 One of their coolest features is  open-data stock prices  -- i.e. financial data that can be edited wiki-style, and isn't encumbered by licensing.","Enigma  is a repository of public available datasets. Its free plan offers public data search, with 10k API calls per month. Not all public databases are listed, but the list is enough for common cases. 
 I used it for academic research and it saved me a lot of time. 
 
 Another interesting source of data is the  @unitedstates project , containing data and tools to collect them, about the United States (members of Congress, geographic shapes…).","I would like to point to  The Open Data Census . It is an initiative of the Open Knowledge Foundation based on contributions from open data advocates and experts around the world.  
 The value of Open data Census is open, community driven, and systematic effort to collect and update the database of open datasets globally on country and, in some cases,  like U.S., on city level .  
 Also, it presents an opportunity to compare different countries and cities on in selected areas of interest.","There is also another resource provided by The Guardian, the British Daily on their website. The datasets published by the Guardian Datablog are all hosted. Datasets related to Football Premier League Clubs' accounts, Inflation and GDP details of UK, Grammy awards data etc.
The datasets are available at  
 
 http://www.theguardian.com/news/datablog/interactive/2013/jan/14/all-our-datasets-index 
 
 Some more resources. Some of the datasets are in R format or R commads exist for directly importing data to R. 
 
 http://www.inside-r.org/howto/finding-data-internet",50,50,50,,,,,,
130,What is dimensionality reduction? What is the difference between feature selection and extraction?,feature-selection,"Simply put:  
 
 feature selection: you select a subset of the original feature set; while 
 feature extraction: you build a new set of features from the original feature set.  
 
 Examples of feature extraction: extraction of contours in images, extraction of digrams from a text, extraction of phonemes from recording of spoken text, etc. 
 Feature extraction involves a transformation of the features, which often is not reversible because some information is lost in the process of dimensionality reduction.","Dimensionality reduction is typically choosing a basis or mathematical representation within which you can describe most but not all of the variance within your data, thereby retaining the relevant information, while reducing the amount of information necessary to represent it. There are a variety of techniques for doing this including but not limited to  PCA ,  ICA , and  Matrix Feature Factorization . These will take existing data and reduce it to the most discriminative components.These all allow you to represent most of the information in your dataset with fewer, more discriminative features. 
 Feature Selection is hand selecting features which are highly discriminative. This has a lot more to do with feature engineering than analysis, and requires significantly more work on the part of the data scientist. It requires an understanding of what aspects of your dataset are important in whatever predictions you're making, and which aren't. Feature extraction usually involves generating new features which are composites of existing features. Both of these techniques fall into the category of feature engineering. Generally feature engineering is important if you want to obtain the best results, as it involves creating information that may not exist in your dataset, and increasing your signal to noise ratio.","As in @damienfrancois answer feature selection is about selecting a subset of features. So in NLP it would be selecting a set of specific words (the typical in NLP is that each word represents a feature with value equal to the frequency of the word or some other weight based on TF/IDF or similar). 
 Dimensionality reduction is the introduction of new feature space where the original features are represented. The new space is of lower dimension that the original space. In case of text an example would be the  hashing trick  where a piece of text is reduced to a vector of few bits (say 16 or 32) or bytes. The amazing thing is that the geometry of the space is preserved (given enough bits), so relative distances between documents remain the same as in the original space, so you can deploy standard machine learning techniques without having to deal with unbound (and huge number of) dimensions found in text.","Feature selection is about choosing some of features based on some statistical score but feature extraction is using techniques to extract some second layer information from the data e.g. interesting frequencies of a signal using Fourier transform. 
 Dimensionality reduction is all about transforming data into a low-dimensional space in which data preserves its euclidean structure but does not suffer from curse of dimensionality.
For instance assume you extract some word features $[x_1,...,x_n]$ from a data set where each document can be modeled as a point in n-dimensional space and n is too large (a toy example). In this case many algorithms do not work according to the distance distortion of high-dimensional space. Now you need to reduce dimensionality by either selecting most informative features or transforming them into a low-dimensional manifold using dimensionality reduction methods e.g. PCA, LLE, etc.","To complete Damien's answer, an example of dimensionality reduction in NLP is a  topic model , where you represent the document by a vector indicating the weights of its constituent topics.","For a proper review and definition you may take a look at  Dimension Reduction vs. Variable Selection  also in the book  Feature Extraction Foundations and Applications 
feature extraction is decomposed in to two steps: feature construction and feature selection.","A1. What is dimensionality reduction: 
If you think of data in a matrix, where rows are instances and columns are attributes (or features), then dimensionality reduction is mapping this data matrix to a new matrix with fewer columns. For visualization, if you think of each matrix-column (attribute) as a dimension in feature space, then dimensionality reduction is projection of instances from the higher dimensional space (more columns) to a lower dimensional sub-space (fewer columns).
 
Typical objective for this transformation is (1) preserving information in the data matrix, while reducing computational complexity; (2) improving separability of different classes in data. 
 A2. Dimensionality reduction as feature selection or feature extraction:
I'll use the ubiquitous  Iris dataset , which is arguably the 'hello world' of data science. Briefly, the Iris dataset has 3 classes and 4 attributes (columns). I'll illustrate feature selection and extraction for the task of reducing Iris dataset dimensionality from 4 to 2. 
 I compute pair-wise co-variance of this dataset using library in Python called seaborn. The code is: sns.pairplot(iris, hue=""species"", markers=[""o"", ""s"", ""D""]) The figure I get is
 
I can  select  the pair of attributes (2 dimensions) that provide me the greatest separation between the 3 classes (species) in the Iris dataset. This would be a case of feature-selection. 
 Next up is feature extraction. Herein, I am projecting the 4-dimensional feature space of Iris to a new 2-dimensional subspace, which is not axis aligned with the original space. These are new attributes. They are typically based on the distribution in the original high dimensional space.
The most popular method is Principal Component Analysis, which computes Eigenvectors in the original space.
 
Obviously, we are not restricted to using only a linear and global projection to a subspace based on Eigenvectors. We can use non-linear projection methods as well.
Here is an example of non-linear PCA using neural networks
 
The attributes (dimensions) in the last example are  extracted  from the original 4 attributes using neural networks. You can experiment with various flavors of PCA for iris dataset youself using this  pca methods code . 
 Summary:
While feature extraction methods may appear to be superior in performance to feature selection, the choice is predicated by the application. The attributes from feature extraction typically lose physical interpretation, which may or may not be an issue based on the task at hand. For example, if you are designing a very expensive data collection task with costly sensors and need to economize on the attributes (number of different sensors), you'd want to collect a small pilot sample using all available sensors and then  select  the ones that are most informative for the big data collection task.","Extracted from Hands-on machine learning with scikit-learn & Tensorflow 
 
 Data cleaning:
Fix or remove outliers (optional).
Fill in missing values (e.g., with zero, mean, median…) or drop their rows (or columns). 
 Feature selection (optional):
Drop the attributes that provide no useful information for the task. 
 Feature engineering, where appropriate:
Discretize continuous features.
Decompose features (e.g., categorical, date/time, etc.).
Add promising transformations of features (e.g., log(x), sqrt(x), x^2, etc.).
Aggregate features into promising new features. 
 Feature scaling: standardize or normalize features.","Several great answers on here, in particular, @damienfrancois's answer very succinctly captures the general idea. 
 However, I don't see any examples of feature engineering for relational or time-series data. In that case, data scientists generally extract statistical patterns across relationships and over time. For instance, in order to predict what customers will by in the future in an ecommerce database, one might extract quantities like the average historical purchase amount, or the frequency of prior purchases. 
 I wrote a piece on this topic that goes into much more detail with several examples here:  https://www.featurelabs.com/blog/feature-engineering-vs-feature-selection/",50,50,53.04807913,53.36183768,52.97493685,,,,
128,Latent Dirichlet Allocation vs Hierarchical Dirichlet Process,nlp,"HDP is an extension of LDA, designed to address the case where the number of mixture components (the number of ""topics"" in document-modeling terms) is not known a priori.  So that's the reason why there's a difference. 
 Using LDA for document modeling, one treats each ""topic"" as a distribution of words in some known vocabulary.  For each document a mixture of topics is drawn from a Dirichlet distribution, and then each word in the document is an independent draw from that mixture (that is, selecting a topic and then using it to generate a word). 
 For HDP (applied to document modeling), one also uses a Dirichlet process to capture the uncertainty in the number of topics.  So a common base distribution is selected which represents the countably-infinite set of possible topics for the corpus, and then the finite distribution of topics for each document is sampled from this base distribution. 
 As far as pros and cons, HDP has the advantage that the maximum number of topics can be unbounded and learned from the data rather than specified in advance.  I suppose though it is more complicated to implement, and unnecessary in the case where a bounded number of topics is acceptable.","Anecdotally, I've never been impressed with the output from hierarchical LDA. It just doesn't seem to find an optimal level of granularity for choosing the number of topics. I've gotten much better results by running a few iterations of regular LDA, manually inspecting the topics it produced, deciding whether to increase or decrease the number of topics, and continue iterating until I get the granularity I'm looking for. 
 Remember: hierarchical LDA can't read your mind... it doesn't know what you actually intend to use the topic modeling for. Just like with k-means clustering, you should choose the k that makes the most sense for your use case.","I wanted to point out, since this is one of the top Google hits for this topic, that Latent Dirichlet Allocation (LDA), Hierarchical Dirichlet Processes (HDP),  and  hierarchical Latent Dirichlet Allocation (hLDA) are all distinct models. 
 LDA models documents as dirichlet mixtures of a fixed number of topics- chosen as a parameter of the model by the user- which are in turn dirichlet mixtures of words. This generates a flat, soft probabilistic clustering of terms into topics and documents into topics.  
 HDP models topics as mixtures of words, much like LDA, but rather than documents being mixtures of a fixed number of topics, the number of topics is generated by a dirichlet process, resulting in the number of topics being a random variable as well. The ""hierarchical"" portion of the name refers to another level being added to the generative model (the dirichlet process producing the number of topics), not the topics themselves- the topics are still flat clusterings. 
 hLDA, on the other hand, is an adaptation of LDA that models topics as mixtures of a new, distinct level of topics, drawn from dirichlet  distributions  and not processes. It still treats the number of topics as a hyperparameter, i.e., independent of the data. The difference is that the clustering is now hierarchical- it learns a clustering of the first set of topics themselves, giving a more general, abstract relationships between topics (and hence, words and documents). Think of it like clustering the stack exchanges into math, science, programming, history, etc. as opposed to clustering data science and cross validation into an abstract statistics and programming topic that shares some concepts with, say, software engineering, but the software engineering exchange is clustered on a more concrete level with the computer science exchange, and the similarity between all of the mentioned exchanges doesn't appear as much until the upper layer of clusters.","Yee Whye Teh et al 's 2005 paper  Hierarchical Dirichlet Processes  describes a nonparametric prior for grouped clustering problems. For  example , the HDP helps in generalizing the  Latent Dirichlet Allocation  model to the case the number of topics in the data is discovered by the inference algorithm instead of being specified as a parameter of the model. Detailed explanation on Dirichlet Process can be found  here 
 Topic models promise to help summarize and organize large archives of texts that cannot be easily analyzed by hand. The  Hierarchical Dirichlet process (HDP)  is a powerful mixed-membership model for the unsupervised analysis of grouped data. Unlike its finite counterpart,  latent Dirichlet allocation , the HDP topic model infers the number of topics from the data.","Actually HDP require a lot of hidden parameters, which are in code. If you play with such parameters you will get different results (different topics). People usually does not pay attention to such hidden parameters and thinks that model able to find such parameters. It is not true. User have to define parameters ‘eta’ ‘gamma’ and ‘alpha’ and maximum of topics. If you specify max of topics say about 23 topics, then youк model provide 23 topics in output. If you set up 15 topics then you get 15 topics in output….","I have a situation where HDP works well compared to LDA. I have about 16000 documents that belong to various classes. As I am unaware of how many different topics I can gather for each class, HDP is really helpful in this case.",,,,50,50,50,50,,,,,
116,Machine learning techniques for estimating users' age based on Facebook sites they like,machine-learning,"One thing to start off with would be k-NN.  The idea here is that you have a user/item matrix and for some of the users you have a reported age.  The age for a person in the user item matrix might be well determined by something like the mean or median age of some nearest neighbors in the item space. 
 So you have each user expressed as a vector in item space, find the k nearest neighbors and assign the vector in question some summary stat of the nearest neighbor ages.  You can choose k on a distance cutoff or more realistically by iteratively assigning ages to a train hold out and choosing the k that minimizes the error in that assignment. 
 If the dimensionality is a problem you can easily perform reduction in this setup by single value decomposition choosing the m vectors that capture the most variance across the group. 
 In all cases since each feature is binary it seems that cosine similarity would be your go to distance metric. 
 I need to think a bit more about other approaches (regression, rf, etc...) given the narrow focus of your feature space (all variants of the same action, liking) I think the user/item approach might be the best. 
 One note of caution, if the ages you have for train are self reported you might need to correct some of them.  People on facebook tend to report ages in the decade they were born.  Plot a histogram of the birth dates (derived from ages) and see if you have spikes at decades like 70s, 80s, 90s.","I recently did a similar project in Python (predicting opinions using FB like data), and had good results with the following basic process: 
 
 Read in the training set (n = N) by iterating over comma-delimited like records line-by-line and use a counter to identify the most popular pages 
 For each of the K most popular pages (I used about 5000, but you can play around with different values), use pandas.DataFrame.isin to test whether each individual in the training set likes each page, then make a N x K dataframe of the results (I'll call it xdata_train) 
 Create a series (I'll call it ydata_train) containing all of the outcome variables (in my case opinions, in yours age) with the same index as xdata_train 
 Set up a random forest classifier through scikit-learn to predict
ydata_train based on xdata_train 
 Use scikit-learn's cross-validation testing to tweak parameters and
refine accuracy (tweaking number of popular pages, number of trees,
min leaf size, etc.) 
 Output random forest classifier and list of most popular pages with pickle (or keep in memory if you are doing everything at once) 
 Load in the rest of your data, load the list of popular pages (if necessary), and repeat step 2 to produce xdata_new 
 Load the random forest classifier (if necessary) and use it to predict values for the xdata_new data 
 Output the predicted scores to a new CSV or other output format of your choosing 
 
 In your case, you'd need to swap out the classifier for a regressor (so see  sklearn.ensemble.RandomForestRegressor ) but otherwise the same process should work without much trouble. 
 Also, you should be aware of the most amazing feature of random forests in Python: instant parallelization! Those of us who started out doing this in R and then moved over are always amazed, especially when you get to work on a machine with a few dozen cores. 
 Finally, note that this would be a perfect application for network analysis if you have the data on friends as well as the individuals themselves. If you can analyze the ages of a user's friends, the age of the user will almost certainly be within a year or two of the median among his or her friends, particularly if the users are young enough to have built their friend networks while still in school (since most will be classmates). That prediction would likely trump any you would get from modeling---this is a textbook example of a problem where the right data > the right model every time. 
 Good luck!","Another suggestion is to test the  logistic regression . As an added bonus, the  weights (coefficients) of the model will give you an idea of which sites are age-distriminant.   
 Sklearn offers the  sklearn.linear_model.LogisticRegression  package that is designed to handle sparse data as well. 
 As mentionned in the comments, in the present case, with more input variables than samples, you need to regularize the model (with  sklearn.linear_model.LogisticRegression  use the  penalty='l1'  argument).",Some research from  D. Nguyen et al.  try to predict twitter user's age based on their tweets. Maybe you find them useful. They use logistic and linear regression.,"Apart from the fancier methods you could try the Bayes formula 
 $P(I | p_1 ... p_n) =   {{P(p_1 ... p_n | I) P(I)} \over \sum_i (P(p_1 ... p_n | i) P(i))}$ 
 $P(I | p_1 ... p_n)$  is the probability that a user belongs to age group I if he liked  $p_1, .., p_n$ 
 $P(i)$  is the probability that a user belongs to age group  $I$ 
 $P(p_1 .. p_n | i)$  is the probability that a user liked  $p_1, .., p_n$  if he belongs to age group  $i$ . 
 
 You already have the estimates for  $P(i)$  from your data: this is just the proportion of users in age group I. 
 
 To estimate  $P(p_1 ... p_n |i)$ , for each age group  $i$  estimate the probability (frequency)  $p_{ij}$  to like a page  $j$ . To have  $p_{ij}$  non-zero for all j, you can mix in the frequency for the whole population with a small weight. 
 
 Then  $log P(p_1...p_n| i) = \sum(log p_{ij}, i = p_1, .., p_n)$ , the sum over all pages that a new user likes. This formula would be approximately true assuming that a user likes the pages in his age group independently. 
 
 Theoretically, you should also add log  $(1-p_{ij})$  for all  $i$  that he hasn't liked, but in practice you should find that the sum of  $log (1-p_{ij})$  will be irrelevantly small, so you won't need too much memory. 
 
 
 If you or someone else has tried this, please comment about the result.","This is a very interesting problem. 
 I faced a similar one by analyzing the pictures users upload to the social network. I did the following approach: 
 
 Rather than associating data to ages (15 y.o., 27 y.o., ...) what I did is to establish different groups of ages: Less than 18, from 18 to 30 and greater than 30 (this is due to the specific problem we were facing, but you can choose whatever intervals you want). This division helps a lot to solve the problem. 
 Afterwards, I created a hierarchical clustering (divisive or aggregative). Then I choose those branches where I had users with known ages (or group ages) and then for that branch I extended the same age to that group. 
 
 This approach is  semi-supervised learning  and I recommended it in case you only have some data labeled. 
 Please, notice that on a social network, people usually lie about the age (just for fun, or sometimes because they want to camuflate themselves on the social net).",,,,50,50,50,50,50,53.76591219,,,
103,Clustering based on similarity scores,clustering,"I think a number of clustering algorithms that normally use a metric, do not actually rely on the metric properties (other than commutativity, but I think you'd have that here).  For example, DBSCAN uses epsilon-neighborhoods around a point; there is nothing in there that specifically says the triangle inequality matters.  So you can probably use DBSCAN, although you may have to do some kind of nonstandard spatial index to do efficient lookups in your case.  Your version of epsilon-neighborhood will likely be sim > 1/epsilon rather than the other way around.  Same story with k-means and related algorithms. 
 Can you construct a metric from your similarity?  One possibility: dist(ei, ej) = min( sim(ei, ek) + sim(ek, ej) ) for all k ...  Alternately, can you provide an upper  bound such that sim(ei, ej) < sim(ei, ek) + sim(ek, ej) + d, for all k and some positive constant d?  Intuitively, large sim values means closer together: is 1/sim metric-like?  What about 1/(sim + constant)?  What about min( 1/sim(ei, ek) + 1/sim(ek, ej) ) for all k? (that last is guaranteed to be a metric, btw) 
 An alternate construction of a metric is to do an embedding.  As a first step, you can try to map your points ei -> xi, such that xi minimize sum( abs( sim(ei, ej) - f( dist(xi, xj) ) ), for some suitable function f and metric dist.  The function f converts distance in the embedding to a similarity-like value; you'd have to experiment a bit, but 1/dist or exp^-dist are good starting points.  You'd also have to experiment on the best dimension for xi.  From there, you can use conventional clustering on xi.  The idea here is that you can almost (in a best fit sense) convert your distances in the embedding to similarity values, so they would cluster correctly. 
 On the use of predefined parameters, all algorithms have some tuning.  DBSCAN can find the number of clusters, but you still need to give it some parameters.  In general, tuning requires multiple runs of the algorithm with different values for the tunable parameters, together with some function that evaluates goodness-of-clustering (either calculated separately, provided by the clustering algorithm itself, or just eyeballed :)  If the character of your data doesn't change, you can tune once and then use those fixed parameters; if it changes then you have to tune for each run.  You can find that out by tuning for each run and then comparing how well the parameters from one run work on another, compared to the parameters specifically tuned for that.","Alex made a number of good points, though I might have to push back a bit on his implication that DBSCAN is the best clustering algorithm to use here. Depending on your implementation, and whether or not you're using accelerated indices (many implementations do not), your time and space complexity will both be  O(n2) , which is far from ideal. 
 Personally, my go-to clustering algorithms are OpenOrd for winner-takes-all clustering and FLAME for fuzzy clustering. Both methods are indifferent to whether the metrics used are similarity or distance (FLAME in particular is nearly identical in both constructions). The implementation of OpenOrd in Gephi is  O(nlogn)  and is known to be more scalable than any of the other clustering algorithms present in the Gephi package. 
 FLAME on the other hand is great if you're looking for a fuzzy clustering method. While the complexity of FLAME is a little harder to determine since it's an iterative process, it has been shown to be sub-quadratic, and similar in run-speed to knn.","DBSCAN (see also: Generalized DBSCAN) does not require a distance.
All it needs is a  binary decision . Commonly, one would use ""distance < epsilon"" but nothing says you cannot use ""similarity > epsilon"" instead. Triangle inequality etc. are not required. 
 Affinity propagation, as the name says, uses similarities. 
 Hierarchical clustering, except for maybe Ward linkage, does not make any assumption. In many implementations you can just use negative distances when you have similarities, and it will work just fine. Because all that is needed is min, max, and <. 
 Kernel k-means could work IF your similarity is a good kernel function. Think of it as computing k-means in a different vector space, where Euclidean distance corresponds to your similarity function. But then you need to know k. 
 PAM (K-medoids) should work. Assign each object to the most similary medoid, then choose the object with the highest average similarity as new medoid... no triangle inequality needed. 
 ... and probably many many more. There are literally hundreds of clustering algorithms.  Most should work  IMHO. Very few seem to actually require metric properties. K-means has probably the strongest requirements: it minimizes  variance  (not distance, or similarity), and you must be able to compute means.","Topological Data Analysis is a method explicitly designed for the setting you describe. Rather than a global distance metric, it relies only on a local metric of proximity or neighborhood. See:  Topology and data  and  Extracting insights from the shape of complex data using topology . You can find additional resources at the website for Ayasdi.","I guess you are refering to ""similarity-based clustering"", which is 
 
 Clustering, which only uses the similarities between objects but does
not require to represent the objects via feature vectors, is called similarity-based clustering. 
 
 There are 3 approaches: 
 
 Aspect model 
 
 
 [... ]Hofmann and Puzicha [1999], Hofmann et al. [1999], considers
discrete data, where observations are pairs (x, y) taht are counted. That means the number of
occurrences of x together with y are counted. Such data appear if relations like “person x buys
product y” or “person x participates in y” are counted. Applications are found in information
retrieval by document-word relations or in bioinformatics by sample-gene relations. 
 
 
 Affinity Propagation 
 
 
 [...] Frey and Dueck [2006, 2007], Givoni and Frey [2009]. Affinity
propagation is both a similarity-based and an exemplar-based clustering method. The similarities
between object i and object k are given by s(i, k). The values s(k, k) are called “preferences” and
used to determine how likely object k becomes an exemplar. The larger s(k, k), the more likely
object k becomes an exemplar. 
 
 
 Similarity-based Mixture Models 
 
 
 [...] The similarity between object i and object j is considered as the conditional
probability p(xj | xi). This idea was already introduced in stochastic neighbor embedding (SNE),
where this was the probability that xi would pick xj as its neighbor. Another interpretation is:
the more xj is similar to xi
, the less modifications are necessary to obtain xj from xi
, the higher
is the probability to obtain xj (randomly) from xi
. In contrast to SNE, we now assume that the
p(xj | xi) are given to allow for similarity-based clustering. 
 
 Source Machine Learning: Unsupervised Techniques, 2014, Sepp Hochreiter, Lecture Notes Bioinformatics Johannes Kepler University Linz",,,,,50,50,50,50,50,50,50,,
61,Why Is Overfitting Bad in Machine Learning?,machine-learning,"Overfitting is  empirically  bad.  Suppose you have a data set which you split in two, test and training.  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on additional (new) test datasets than models which are not overfitted.   
 One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).  The difference in performance between training and test data indicates how much noise the model picks up; and picking up noise directly translates into worse performance on test data (including future data). 
 Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal. 
 P.S. On the ""ability to generalize"" part of the question, it is very possible to have a model which has inherently limited ability to generalize due to the structure of the model (for example linear SVM, ...) but is still prone to overfitting.  In a sense overfitting is just one way that generalization may fail.","Overfitting, in a nutshell, means take into account  too much  information from your data and/or prior knowledge, and use it in a model. To make it more straightforward, consider the following example: you're hired by some scientists to provide them with a model to predict the growth of some kind of plants. The scientists have given you information collected from their work 
with such plants throughout a whole year, and they shall continuously give you information on the future development of their plantation. 
 So, you run through the data received, and build up a model out of it. Now suppose that, in your model, you considered just as many characteristics as possible to always find out the exact behavior of the plants you saw in the initial dataset. Now, as the production continues, you'll always take into account those characteristics, and will produce very  fine-grained  results. However, if the plantation eventually suffer from some seasonal change, the results you will receive may fit your model in such a way that your predictions will begin to fail (either saying that the growth will slow down, while it shall actually speed up, or the opposite). 
 Apart from being unable to detect such small variations, and to usually classify your entries incorrectly, the  fine-grain  on the model, i.e., the great amount of variables, may cause the processing to be too costly. Now, imagine that your data is already complex. Overfitting your model to the data not only will make the classification/evaluation very complex, but will most probably make you error the prediction over the slightest variation you may have on the input. 
 Edit :  This  might as well be of some use, perhaps adding dynamicity to the above explanation :D","Roughly speaking, over-fitting typically occurs when the ratio 
 
 is too high. 
 Think of over-fitting as a situation where your model learn the training data by heart instead of learning the big pictures which prevent it from being able to generalized to the test data: this happens when the model is too complex with respect to the size of the training data, that is to say when the size of the training data is to small in comparison with the model complexity. 
 Examples:  
 
 if your data is in two dimensions, you have 10000 points in the training set and the model is a line, you are likely to  under -fit. 
 if your data is in two dimensions, you have 10 points in the training set and the model is 100-degree polynomial, you are likely to  over -fit. 
 
 
 From a theoretical standpoint, the amount of data you need to properly train your model is a crucial yet far-to-be-answered question in machine learning. One such approach to answer this question is the  VC dimension . Another is the  bias-variance tradeoff . 
 From an empirical standpoint, people typically plot the training error and the test error on the same plot and make sure that they don't reduce the training error at the expense of the test error: 
 
 I would advise to watch  Coursera' Machine Learning course , section ""10: Advice for applying Machine Learning"". 
 (PS: please go  here  to ask for TeX support on this SE.)",No one seems to have posted the XKCD overfitting comic yet.,"That's because something called  bias-variance dilema . The overfitted model means that we will have more complex decision boundary if we give more variance on model. The thing is, not only too simple models but also complex models are likely to have dis-classified result on unseen data. Consequently, over-fitted model is not good as under-fitted model. That's why overfitting is bad and we need to fit the model somewhere in the middle.","What got me to understand the problem about overfitting was by imagining what the most overfit model possible would be. Essentially, it would be a simple look-up table. 
 You tell the model what attributes each piece of data has and it simply remembers it and does nothing more with it. If you give it a piece of data that it's seen before, it looks it up and simply regurgitates what you told it earlier. If you give it data it  hasn't  seen before, the outcome is unpredictable or random. But the point of machine learning isn't to tell you what happened, it's to understand the patterns and use those patterns to predict what's going on. 
 So think of a decision tree. If you keep growing your decision tree bigger and bigger, eventually you'll wind up with a tree in which every leaf node is based on exactly one data point. You've just found a backdoor way of creating a look-up table. 
 In order to generalize your results to figure out what might happen in the future, you must create a model that generalizes what's going on in your training set. Overfit models do a great job of describing the data you already have, but descriptive models are not necessarily predictive models. 
 The No Free Lunch Theorem says that no model can outperform any other model on the set of all  possible  instances. If you want to predict what will come next in the sequence of numbers ""2, 4, 16, 32"" you can't build a model more accurate than any other if you don't make the assumption that there's an underlying pattern. A model that's overfit isn't really evaluating the patterns - it's simply modeling what it knows is possible and giving you the observations. You get predictive power by assuming that there is some underlying function and that if you can determine what that function is, you can predict the outcome of events. But if there really is no pattern, then you're out of luck and all you can hope for is a look-up table to tell you what you know is possible.","You are erroneously conflating two different entities: (1) bias-variance and (2) model complexity. 
 (1) Over-fitting is bad in machine learning because it is impossible to collect a truly unbiased sample of  population  of any data. The over-fitted model results in parameters that are biased to the sample instead of properly estimating the parameters for the entire population. This means there will remain a difference between the estimated parameters $\hat{\phi}$ and the optimal parameters $\phi^{*}$, regardless of the number of training epochs $n$. 
 $|\phi^{*} - \hat{\phi}| \rightarrow e_{\phi} \mbox{  as }n\rightarrow \infty$, where $e_{\phi}$ is some bounding value 
 (2) Model complexity is in simplistic terms the number of parameters in $\phi$. If the model complexity is low, then there will remain a regression error regardless of the number of training epochs, even when $\hat{\phi}$ is approximately equal to $\phi^{*}$. Simplest example would be learning to fit a line (y=mx+c), where $\phi = \{m,c\}$ to data on a curve (quadratic polynomial). 
 $E[|y-M(\hat{\phi})|] \rightarrow e_{M} \mbox{ as } n \rightarrow \infty$, where $e_{M}$ is some regression fit error bounding value 
 Summary: Yes, both sample bias and model complexity contribute to the 'quality' of the learnt model, but they don't directly affect each other. If you have biased data, then regardless of having the correct number of parameters and infinite training, the final learnt model would have error. Similarly, if you had fewer than the required number of parameters, then regardless of perfectly unbiased sampling and infinite training, the final learnt model would have error.","There have been a lot of good explanations about overfitting.
Here are my thoughts. Overfitting happens when your variance is too high and bias is too low. 
 Let's say you have training data with you, which you divide into N parts. Now, if you train a model on each of the datasets, you will have N models. Now  find the mean model and then use the variance formula to compute how much each model varies from the mean. For overfitted models, this variance will be really high. This is because, each model would have estimated parameters which are very specific to the small dataset that we fed to it. Similarly, if you take the mean model and then find how much it is different from the original model that would have given the best accuracy, it wouldn't be very different at all. This signifies low bias.  
 To find whether your model has overfitted or not, you could construct the plots mentioned in the previous posts. 
 Finally, to avoid overfitting you could regularize the model or use cross validation.",,50,50,50,50,50,50,50,50,
52,Organized processes to clean data,r,"R contains some  standard  functions for data manipulation, which can be used for data cleaning, in its  base  package ( gsub ,  transform , etc.), as well as in various third-party packages, such as  stringr ,  reshape / reshape2 , and  plyr / dplyr . Examples and best practices of usage for these packages and their functions are described in the following paper:  http://vita.had.co.nz/papers/tidy-data.pdf . 
 Additionally, R offers some packages specifically  focused  on data cleaning and transformation: 
 
 editrules  ( http://cran.r-project.org/web/packages/editrules/index.html ) 
 deducorrect  ( http://cran.r-project.org/web/packages/deducorrect/index.html ) 
 StatMatch  ( http://cran.r-project.org/web/packages/StatMatch/index.html ) 
 MatchIt  ( http://cran.r-project.org/web/packages/MatchIt/index.html ) 
 DataCombine  ( http://cran.r-project.org/web/packages/DataCombine ) 
 data.table  ( http://cran.r-project.org/web/packages/data.table ) 
 
 A comprehensive and coherent approach to  data cleaning  in R, including examples and use of  editrules  and  deducorrect  packages, as well as a description of  workflow  ( framework ) of data cleaning in R, is presented in the following paper, which I highly recommend:  http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf .","From my point of view, this question is suitable for a two-step answer. The first part, let us call it  soft preprocessing , could be taken as the usage of different data mining algorithms to preprocess data in such a way that makes it suitable for further analyses. Notice that this could be the analysis itself, in case the goal is simple enough to be tackled in a single shot. 
 The second part, the  hard preprocessing , actually comes prior to any other process, and is may be taken as the usage of simple tools or scripts to clean up data, selecting specific contents to be processed. To this problem, POSIX provides us with a wonderous set of magic tools, which can be used to compose concise -- and very powerful -- preprocessing scripts. 
 For example, for people who deal with data coming from social websites (twitter, facebook, ...), the  data retrieval  usually yields files with very specific format -- although not always nicely structure, as they may contain missing fields, and so. For these cases, a simple  awk  script could clean up the data, producing a  valid  input file for later processing. From the magic set, one may also point out  grep ,  sed ,  cut ,  join ,  paste ,  sort , and a whole multitude of other tools. 
 In case simple the source file has too many nitty-gritties, it may also be necessary to produce a bundle of methods to clean up data. In such cases, it is usually better to use scripting languages (other than shell ones), such as Python, Ruby, and Perl. This allows for building up  API 's to select specific data in a very straightforward and reusable way. Such  API 's are sometimes made public by their writers, such as  IMDbPY ,  Stack Exchange API , and many others. 
 So, answering the question: are there any best practices? It usually depends on your task. If you will always deal with the same data format, it's commonly best to write an  organized  script to preprocess it; whereas, if you just need a simple and fast clean up on some dataset, count on POSIX tools for concise shell scripts that will do the whole job  much  faster than a Python script, or so. Since the  clean up  depends both on the dataset and on your purposes, it's hard to have everything already done. Yet, there are lots of API's that puts you halfway through with the problem.","One reason that data cleaning is rarely fully automated is that there is so much judgment required to define what ""clean"" means given your particular problem, methods, and goals. 
 It may be as simple as imputing values for any missing data, or it might be as complex as diagnosing data entry errors or data transformation errors from previous automated processes (e.g. coding, censoring, transforming).  In these last two cases, the data  looks good  by outward appearance but it's really erroneous.  Such diagnosis often requires manual analysis and inspection, and also out-of-band information such as information about the data sources and methods they used. 
 Also, some data analysis methods work better when erroneous or missing data is left blank (or N/A) rather than imputed or given a default value.  This is true when there is explicit representations of uncertainty and ignorance, such as Dempster-Shafer Belief functions. 
 Finally, it's useful to have specific diagnostics and metrics for the cleaning process.  Are missing or erroneous values randomly distributed or are they concentrated in any way that might affect the outcome of the analysis.  It's useful to test the effects of alternative cleaning strategies or algorithms to see if they affect the final results. 
 Given these concerns, I'm very suspicious of any method or process that treats data cleaning in a superficial, cavalier or full-automated fashion.  There are many devils hiding in those details and it pays to give them serious attention.","About automatic cleaning: You really cannot clean data automatically, because the number of errors and the definition of an error is often dependent on the data. E.g.: Your column ""Income"" might contain negative values, which are an error - you have to do something about the cases. On the other hand a column ""monthly savings"" could reasonably contain negative values.  
 Such errors are highly domain dependent - so to find them, you must have domain knowledge, something at which humans excel, automated processes not so much. 
 Where you can and should automate is repeated projects. E.g. a report which has to produced monthly. If you spot errors, you should place some automated process which can spot  these kinds of errors in subsequent months, freeing your time.","I think that there is no universal technique for ""cleaning"" data before doing actual research. On the other hand, I'm aiming for doing as much  reproducible research  as possible. By doing reproducible research, if you used cleaning techniques with bugs or with poor parameters/assumptions it could be spot by others. 
 There is nice R package  knitr  which helps a lot in reproducible research. 
 Of course, not all research could be fully reproduced (for example live Twitter data) , but at least you can document cleaning, formating and preprocessing steps easily. 
 You can check my  assessment  prepared for  Reproducible Research course at Coursera .","OpenRefine  (formerly Google Refine) is a great tool where you can interactively perform data cleanup and transformations. Your final output can be exported to a variety of standard formats (json, csv) or any custom format(eg: wiki). 
 Exact duplicates can be handled in OpenRefine by sorting + using the ""blank down"" feature. 
Also it has has approximate string (fuzzy) clustering feature where you can group similar looking duplicates such as company names or addresses. This feature alone makes it awesome.","There is an entire course devoted to this in Coursera. You might want to go over the techniques they mention and the important part is to know when to use what. 
 https://www.coursera.org/learn/data-cleaning 
 It's always better to do it manually instead of automating since each dataset has its own problems and issues and not all steps are applicable to all the datasets.",,,50,50,50,50,53.52175283,50,50,50,50
41,Is the R language suitable for Big Data,bigdata,"Actually this is coming around. In the book R in a Nutshell there is even a section on using R with Hadoop for big data processing. There are some work arounds that need to be done because R does all it's work in memory, so you are basically limited to the amount of RAM you have available to you. 
 A mature project for R and Hadoop is  RHadoop 
 RHadoop has been divided into several sub-projects, rhdfs, rhbase, rmr2, plyrmr, and quickcheck ( wiki ).","The main problem with using R for large data sets is the RAM constraint. The reason behind keeping all the data in RAM is that it provides much faster access and data manipulations than would storing on HDDs. If you are willing to take a hit on performance, then yes, it is quite practical to work with  large datasets in R . 
 
 RODBC Package: Allows connecting to external DB from R to retrieve and handle data. Hence, the data being  manipulated  is restricted to your RAM. The overall data set can go much larger. 
 The ff package allows using larger than RAM data sets by utilising memory-mapped pages. 
 BigLM: It builds generalized linear models on big data. It loads data into memory in chunks. 
 bigmemory : An R package which allows powerful and memory-efficient parallel 
analyses and data mining of massive data sets. It permits storing large objects (matrices etc.) in memory (on the RAM) using external pointer objects to refer to them.","Some good answers here. I would like to join the discussion by adding the following three  notes : 
 
 The question's emphasis on the  volume of data  while referring to  Big Data  is certainly understandable and valid, especially considering the  problem  of data volume growth  outpacing  technological capacities' exponential growth per  Moore's Law  ( http://en.wikipedia.org/wiki/Moore%27s_law ). 
 Having said that, it is important to remember about other aspects of big data concept. Based on  Gartner 's definition (emphasis mine - AB): "" Big data  is high  volume , high  velocity , and/or high  variety  information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization."" (usually referred to as the "" 3Vs model ""). I mention this, because it forces data scientists and other analysts to look for and use R packages that focus on  other than volume  aspects of big data (enabled by the  richness  of enormous  R ecosystem ). 
 While existing answers mention some R packages, related to big data, for a more  comprehensive coverage , I'd recommend to refer to  CRAN Task View   ""High-Performance and Parallel Computing with R""  ( http://cran.r-project.org/web/views/HighPerformanceComputing.html ), in particular, sections  ""Parallel computing: Hadoop""  and  ""Large memory and out-of-memory data"" .","R is great for ""big data""! However, you need a workflow since R is limited (with some simplification) by the amount of RAM in the operating system. The approach I take is to interact with a relational database (see the  RSQLite  package for creating and interacting with a SQLite databse), run SQL-style queries to understand the structure of the data, and then extract particular subsets of the data for computationally-intensive statistical analysis. 
 This just one approach, however: there are packages that allow you to interact with other databases (e.g., Monet) or run analyses in R with fewer memory limitations (e.g., see  pbdR ).","Considering another criteria, I think that in some cases using Python may be much superior to R for Big Data. I know the wide-spread use of R in data science educational materials and the good data analysis libraries available for it, but sometimes it just depend on the team. 
 In my experience, for people already familiar with programming, using Python provides much more flexibility and productivity boost compared to a language like R, which is not as well-designed and powerful compared to Python in terms of a programming language. As an evidence, in a data mining course in my university, the best final project was written in Python, although the others has access to R's rich data analysis library. That is, sometimes the overall productivity (considering learning materials, documentation, etc.) for Python may be better than R even in the lack of special-purpose data analysis libraries for Python. Also, there are some good articles explaining the fast pace of Python in data science:  Python Displacing R  and  Rich Scientific Data Structures in Python  that may soon fill the gap of available libraries for R. 
 Another important reason for not using R is when working with real world Big Data problems, contrary to academical only problems, there is much need for other tools and techniques, like data parsing, cleaning, visualization, web scrapping, and a lot of others that are much easier using a general purpose programming language. This may be why the default language used in many Hadoop courses (including the Udacity's  online course ) is Python. 
 Edit: 
 Recently DARPA has also invested $3 million to help fund Python's data processing and visualization capabilities for big data jobs, which is clearly a sign of Python's future in Big Data. ( details )","R is great for a lot of analysis. As mentioned about, there are newer adaptations for big data like MapR, RHadoop, and scalable versions of RStudio. 
 However, if your concern is libraries, keep your eye on Spark. Spark was created for big data and is MUCH faster than Hadoop alone. It has vastly growing machine learning, SQL, streaming, and graph libraries. Thus allowing much if not all of the analysis to be done within the framework (with multiple language APIs, I prefer Scala) without having to shuffle between languages/tools.","As other answers have noted, R can be used along with Hadoop and other distributed computing platforms to scale it up to the ""Big Data"" level.  However, if you're not wedded to R specifically, but are willing to use an ""R-like"" environment,  Incanter  is a project that might work well for you, as it is native to the JVM (based on Clojure) and doesn't have the ""impedance mismatch"" between itself and Hadop that R has.  That is to say, from Incanter, you can invoke Java native Hadoop / HDFS APIs without needing to go through a JNI bridge or anything.","I am far from an expert, but my understanding of the subject tells me that R (superb in statistics) and e.g. Python (superb in several of those things where R is lacking) complements each other quite well (as pointed out by previous posts).","I think that there is actually a pletora of tools for working with big data in R.
 sparklyr will be a great player in that field. sparklyr is an R interface to Apache Spark and allows the connection with local and remote clusters, providing a dplyr back-end. One can also rely on Apache Spark's machine learning libraries.
Furthermore parallel processing is possible with several packages such as  rmpi and snow (user controlled) or doMC/foreach (system based).",50,50,50,50,53.51128461,50,50,,
22,K-Means clustering for mixed numeric and categorical data,data-mining,"The standard k-means algorithm isn't directly applicable to categorical data, for various reasons.  The sample space for categorical data is discrete, and doesn't have a natural origin.  A Euclidean distance function on such a space isn't really meaningful.  As someone put it, ""The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs."" (from  here ) 
 There's a variation of k-means known as k-modes, introduced in  this paper  by Zhexue Huang, which is suitable for categorical data.   Note that the solutions you get are sensitive to initial conditions, as discussed  here  (PDF), for instance. 
 Huang's paper (linked above) also has a section on ""k-prototypes"" which applies to data with a mix of categorical and numeric features.  It uses a distance measure which mixes the Hamming distance for categorical features and the Euclidean distance for numeric features. 
 A Google search for ""k-means mix of categorical data"" turns up quite a few more recent papers on various algorithms for k-means-like clustering with a mix of categorical and numeric data.  (I haven't yet read them, so I can't comment on their merits.) 
 
 Actually, what you suggest (converting categorical attributes to binary values, and then doing k-means as if these were numeric values) is another approach that has been tried before (predating k-modes).  (See Ralambondrainy, H. 1995. A conceptual version of the k-means algorithm. Pattern Recognition Letters, 16:1147–1157.)  But I believe the k-modes approach is preferred for the reasons I indicated above.","In my opinion, there are solutions to deal with categorical data in clustering. R comes with a specific distance for categorical data. This distance is called  Gower  and it works pretty well.","(In addition to the excellent answer by Tim Goodman) 
 The choice of k-modes is definitely the way to go for stability of the clustering algorithm used. 
 
 The clustering algorithm is free to choose any distance metric / similarity score. Euclidean is the most popular. But any other metric can be used that scales according to the data distribution in each dimension /attribute, for example the Mahalanobis metric.
 
 With regards to mixed (numerical and categorical) clustering a good paper that might help is:  INCONCO: Interpretable Clustering of Numerical and Categorical Objects 
 Beyond k-means: Since plain vanilla k-means has already been ruled out as an appropriate approach to this problem, I'll venture beyond to the idea of thinking of clustering as a model fitting problem. Different measures, like information-theoretic metric: Kullback-Liebler divergence work well when trying to converge a parametric model towards the data distribution.
(Of course parametric clustering techniques like GMM are slower than Kmeans, so there are drawbacks to consider) 
 Fuzzy k-modes clustering also sounds appealing since fuzzy logic techniques were developed to deal with something like categorical data. See  Fuzzy clustering of categorical data using fuzzy centroids  for more information. 
 
 Also check out:  ROCK: A Robust Clustering Algorithm for Categorical Attributes","This question seems really about representation, and not so much about clustering. 
 Categorical data is a problem for most algorithms in machine learning. Suppose, for example, you have some categorical variable called ""color"" that could take on the values red, blue, or yellow. If we simply encode these numerically as 1,2, and 3 respectively, our algorithm will think that red (1) is actually closer to blue (2) than it is to yellow (3). We need to use a representation that lets the computer understand that these things are all actually equally different.  
 One simple way is to use what's called a  one-hot  representation, and it's exactly what you thought you should do. Rather than having one variable like ""color"" that can take on three values, we separate it into three variables. These would be ""color-red,"" ""color-blue,"" and ""color-yellow,"" which all can only take on the value 1 or 0.  
 This increases the dimensionality of the space, but now you could use any clustering algorithm you like. It does sometimes make sense to zscore or whiten the data after doing this process, but the your idea is definitely reasonable.","You should not use k-means clustering on a dataset containing mixed datatypes. Rather, there are a number of clustering algorithms that can appropriately handle mixed datatypes. Some possibilities include the following:  
 1) Partitioning-based algorithms: k-Prototypes, Squeezer 
2) Hierarchical algorithms: ROCK, Agglomerative single, average, and complete linkage 
3) Density-based algorithms: HIERDENC, MULIC, CLIQUE 
4) Model-based algorithms: SVM clustering, Self-organizing maps  
 If you would like to learn more about these algorithms, the manuscript 'Survey of Clustering Algorithms' written by Rui Xu offers a comprehensive introduction to cluster analysis.","It depends on your categorical variable being used. For ordinal variables, say like bad,average and good, it makes sense just to use one variable and have values 0,1,2 and distances make sense here(Avarage is closer to bad and good). However, if there is no order, you should ideally use one hot encoding as mentioned above.",You can also give the Expectation Maximization clustering algorithm a try.  It can work on categorical data and will give you a statistical likelihood of which categorical value (or values) a cluster is most likely to take on.,"Many of the above pointed that k-means can be implemented on variables which are categorical and continuous, which is  wrong  and the results need to be taken with a pinch of salt. 
 As mentioned above by @Tim above, it doesn't make sense to compute the euclidian distance between the points which neither have a scale nor have an order. When you one-hot encode the categorical variables you generate a sparse matrix of 0's and 1's. As the range of the values is fixed and between 0 and 1 they need to be normalised in the same way as continuous variables. The Z-scores are used to is used to find the distance between the points. Which is still, not perfectly right. I will explain this with an example. As the categories are mutually exclusive the distance between two points with respect to categorical variables, takes either of two values, high or low ie, either the two points belong to the same category or they are not. Due to these extreme values, the algorithm ends up giving more weight over the continuous variables in influencing the cluster formation. This can be verified by a simple check by seeing which variables are influencing and you'll be surprised to see that most of them will be categorical variables. (Ways to find the most influencing variables  1 ) 
 An example: Consider a categorical variable country. Now as we know the distance(dissimilarity) between observations from different countries are equal (assuming no other similarities like neighbouring countries or countries from the same continent). But in contrary to this if you calculate the distances between the observations after normalising the one hot encoded values they will be inconsistent(though the difference is minor) along with the fact that they take high or low values. 
 Ultimately the best option available for python is  k-prototypes  which can handle both categorical and continuous variables. 
 Finding most influential variables in  cluster formation","K-Means' goal is to reduce the within-cluster variance, and because it computes the centroids as the mean point of a cluster,  it is required to use the Euclidean distance  in order to converge properly. Therefore, if you want to absolutely use K-Means, you need to make sure your data works well with it. 
 Representation 
 K-Means, and clustering in general, tries to partition the data in meaningful groups by making sure that instances in the same clusters are similar to each other. Therefore, you need a good way to represent your data so that you can easily compute a meaningful similarity measure. 
 Using one-hot encoding on categorical variables is a good idea when the categories are equidistant from each other. For instance, if you have the colour light blue, dark blue, and yellow, using one-hot encoding might not give you the best results, since dark blue and light blue are likely ""closer"" to each other than they are to yellow. 
 In case the categorical value are not ""equidistant"" and can be ordered, you could also give the categories a numerical value. For instance, kid, teenager, adult, could potentially be represented as 0, 1, and 2. This would make sense because a teenager is ""closer"" to being a kid than an adult is. 
 K-Medoids 
 A more generic approach to K-Means is K-Medoids. K-Medoids works similarly as K-Means, but the main difference is that the centroid for each cluster is defined as the point that reduces the within-cluster sum of distances. Enforcing this allows you to use any distance measure you want, and therefore, you could build your own custom measure which will take into account what categories should be close or not.",50,50,50,50,50,,,,
20,"The data in our relational DBMS is getting big, is it the time to move to NoSQL?",nosql,"A few gigabytes is not very "" big "". It's more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don't get TB's of data a day). 
 Most professionals working in a big data environment consider  > ~5TB  as the  beginning  of the term big data. But even then it's not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem. 
 i.e. if you do alot of searches in you database it would probably be better to run a solr instance/cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it into solr instead of just moving the data from sql to nosql in term of persistence and performance.","To answer this question you have to answer which kind of compromise you can afford. RDBMs implements  ACID . This is expensive in terms of resources. There are no NoSQL solutions which are ACID. See  CAP theorem  to dive deep into these ideas.  
 So you have to understand each compromise given by each solution and choose the one which is the most appropriate for your problem.","Big Data is actually not so about the ""how big it is"".  
 First, few gigabytes is not big at all, it's almost nothing. So don't bother yourself, your system will continu to work efficiently for some time I think. 
 Then you have to think of how do you use your data.  
 
 SQL approach: Every data is precious, well collected and selected, and the focus is put on storing high valuable and well structured data. This can be costly, everything is interlink, and it's good for well stuctured system and functionnal data. 
 Big Data approach: In big data you basically store almost everything, regardless of the value it has, and then do a active analytics process. Things are not linked, they are copied. For example let's say I have a blog entry. In Big Data there will not be a link to its author, but the author will be embedded inside the blog entry. Way more scalable, but require a different and more complex approach. 
 
 If your storing ""functionnal"" data use by your application, I will suggest you to remain on SQL. If your storing data in order to search on them later or to do reporting, and if this amount of data may increase quickly, I will suggest big data.
In my opinion, big data is useful when you are dealing with real data that have to be collect and analyzed continuously.","I posted a pretty detailed answer on stackoverflow about when it is appropriate to use relational vs document (or NoSQL) database, here:   
 Motivations for using relational database / ORM or document database / ODM 
 Summary: 
 
 for small stuff, go with whatever tools you are familiar with 
 a few gigabytes is definitely small stuff: it doesn't get big until it is too big to fit in a single  MySQL Cluster  with a reasonable number of nodes (16-32), which means maybe 8-16TB data and a few million transactions per second (or a more conventional hard-drive-based database with up to 100's of TB data and a few thousand transactions per second). 
 if you're stuck with another database (not MySQL Cluster), get more mileage out of it by throwing in FusionIO hardware. 
 once you have data larger than a few TB  and  faster than thousands of transactions per second, it is a good time to look at moving to logical sharding in the application code first and then to NoSQL. 
 Cassandra  :)","Is it the time to move to NoSQL will depends on 2 things:  
 
 The nature/structure of your data 
 Your current performance 
 
 SQL databases excel when the data is well structured (e.g. when it can be modeled as a table, an Excel spreadsheet, or a set of rows with a fixed number of columns). Also good when you need to do a lot of table joins (which it sounds like you do). 
 NoSQL databases excel when the data is un-structured beyond key-value pairs. 
 Performance wise, you gotta ask yourself one question:  is your current SQL solution slow ?  
 If not, go with the "" IIABDFI "" principle.","There are different ways for you to proceed. 
 1): Stay with  SQL : If the performance stays ok and your data is structured, there is no need to change. Also, remember that if you change to  NoSQL , you are losing the  ACID  properties meaning that your data is not consistent anymore. 
 
 Change to  NoSQL : If your performance is getting worse, you should try to scale your database horizontally. That is not possible with  SQL   databases  but with  NoSQL  or NewSQL databases it is. If your data is not structured then  NoSQL  is your solution. 
 
 Change to NewSQL: If performance is an issue, but you cannot compromise on  ACID  and have structured, relational data, then NewSQL is the perfect fit. It offers horizontal scalability combined with  ACID  and a relational schema.",,,,50,50,50,50,50,50,50,50,
19,How big is big data?,bigdata,"To me (coming from a relational database background), ""Big Data"" is not primarily about the data size (which is the bulk of what the other answers are so far). 
 ""Big Data"" and ""Bad Data"" are closely related. Relational Databases require 'pristine data'. If the data is in the database, it is accurate, clean, and 100% reliable. Relational Databases require ""Great Data""  and a huge amount of time, money, and accountability is put on to making sure the data is well prepared before loading it in to the database. If the data is in the database, it is 'gospel', and it defines the system understanding of reality. 
 ""Big Data"" tackles this problem from the other direction. The data is poorly defined, much of it may be inaccurate, and much of it may in fact be missing. The structure and layout of the data is linear as opposed to relational. 
 Big Data has to have enough volume so that the amount of bad data, or missing data becomes statistically insignificant. When the errors in your data are common enough to cancel each other out, when the missing data is proportionally small enough to be negligible and when your data access requirements and algorithms are functional even with incomplete and inaccurate data, then you have ""Big Data"". 
 ""Big Data"" is not really about the volume, it is about the characteristics of the data.","As you rightly note, these days ""big data"" is something everyone wants to say they've got, which entails a certain looseness in how people define the term.  Generally, though, I'd say you're certainly dealing with big data if the scale is such that it's no longer feasible to manage with more traditional technologies such as RDBMS, at least without complementing them with big data technologies such as Hadoop. 
 How big your data has to actually be for that to be the case is debatable.  Here's a (somewhat provocative)  blog post  that claims that it's not really the case for less than 5 TB of data.  (To be clear, it doesn't claim ""Less than 5 TB isn't big data"", but just ""Less than 5 TB isn't big enough that you need Hadoop"".) 
 But even on smaller datasets, big data technologies like Hadoop can have other advantages, including being well suited to batch operations, playing well with unstructured data (as well as data whose structure isn't known in advance or could change), horizontal scalability (scaling by adding more nodes instead of beefing up your existing servers), and (as one of the commenters on the above-linked post notes) the ability to integrate your data processing with external data sets (think of a map-reduce where the mapper makes a call to another server).  Other technologies associated with big data, like NoSql databases, emphasize fast performance and consistent availability while dealing with large sets of data, as well also being able to handle semi-unstructured data and to scale horizontally. 
 Of course, traditional RDBMS have their own advantages including ACID guarantees (Atomicity, Consistency, Isolation, Durability) and better performance for certain operations, as well as being more standardized, more mature, and (for many users) more familiar.  So even for indisputably ""big"" data, it may make sense to load at least a portion of your data into a traditional SQL database and use that in conjunction with big data technologies. 
 So, a more generous definition would be that you have big data so long as it's big enough that big data technologies provide some added value for you.  But as you can see, that can depend not just on the size of your data but on how you want to work with it and what sort of requirements you have in terms of flexibility, consistency, and performance.   How  you're using your data is more relevant to the question than what you're using it  for  (e.g. data mining).  That said, uses like data mining and machine learning are more likely to yield useful results if you have a big enough data set to work with.","Total amount of data in the world: 2.8 zetabytes in 2012, estimated to reach 8 zetabytes by 2015 ( source ) and with a doubling time of 40 months. Can't get bigger than that :) 
 As an example of a single large organization, Facebook pulls in 500 terabytes per day, into a 100 petabyte warehouse, and runs 70k queries per day on it as of 2012 ( source )  Their current warehouse is >300 petabytes. 
 Big data is probably something that is a good fraction of the Facebook numbers (1/100 probably yes, 1/10000 probably not: it's a spectrum not a single number). 
 In addition to size, some of the features that make it ""big"" are: 
 
 it is actively analyzed, not just stored  (quote ""If you aren’t taking advantage of big data, then you don’t have big data, you have just a pile of data"" Jay Parikh @ Facebook) 
 building and running a data warehouse is a major infrastructure project 
 it is growing at a significant rate 
 it is unstructured or has irregular structure 
 
 Gartner definition: ""Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing"" (The 3Vs)  So they also think ""bigness"" isn't entirely about the size of the dataset, but also about the velocity and structure and the kind of tools needed.","To me Big Data is primarily about the tools (after all, that's where it started); a ""big"" dataset is one that's too big to be handled with conventional tools - in particular, big enough to demand storage and processing on a cluster rather than a single machine. This rules out a conventional RDBMS, and demands new techniques for processing; in particular, various Hadoop-like frameworks make it easy to distribute a computation over a cluster, at the cost of restricting the form of this computation. I'll second the reference to  http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html  ; Big Data techniques are a last resort for datasets which are simply too big to handle any other way. I'd say any dataset for any purpose could qualify if it was big enough - though if the shape of the problem is such that existing ""big data"" tools aren't appropriate, then it would probably be better to come up with a new name. 
 Of course there is some overlap; when I (briefly) worked at last.fm, we worked on the same 50TB dataset using Hadoop and also in an SQL database on a fairly ridiculous server (I remember it had 1TB RAM, and this is a few years ago). Which in a sense meant it both was and wasn't big data, depending on which job you were working on. But I think that's an accurate characterization; the people who worked on the Hadoop jobs found it useful to go to Big Data conferences and websites, while the people who worked on the SQL jobs didn't.","Data becomes ""big"" when a single  commodity computer  can no longer handle the amount of data you have.  It denotes the point at which you need to start thinking about building supercomputers or using clusters to process your data.","Big Data is defined by the volume of data, that's right, but not only. The particularity of big data is that you need to store a  lots  of  various  and sometimes  unstructured  stuffs  all the times  and from a  tons of sensors , usually  for years or decade . 
 Furthermore you need something scalable, so that it doesn't take you half a year to find a data back. 
 So here's come Big Data, where traditional method won't work anymore. SQL is not scalable. And SQL works with very structured and linked data (with all those Primary and foreign key mess, innerjoin, imbricated request...).  
 Basically, because storage becomes cheaper and cheaper and data becomes more and more valuable, big manager ask engineer to records everything. Add to this tons of new sensors with all those mobile, social network, embeded stuff ...etc. So as classic methods won't work, they have to find new technologies (storing everything in files, in json format, with big index, what we call noSQL).  
 So Big Data may be very big but can be not so big but complexe unstructured or various data which has to be store quickly and on-the-run in a raw format. We focus and storing at first, and then we look at how to link everything together.","I'll share what Big Data is like in genomics, in particular de-novo assembly. 
 When we sequence your genome (eg: detect novel genes), we take billions of next-generation short reads. Look at the image below, where we try to assemble some reads. 
 
 This looks simple? But what if you have billion of those reads? What if those reads contain sequence errors? What if your RAM doesn't have enough memory to keep the reads? What about repetitive DNA regions, such as the very common  Alu Element ? 
 De-novo assembly is done by constructing a  De-Bruijn graph : 
 
 The graph is a clever-mined data-structure to represent overlapping reads. It's not perfect but it's better than generating all possible overlaps and store them in an array. 
 The assembly process could take days to complete, because there are quite a number of paths that an assembler would need to traverse and collapse. 
 In genomics, you have a big data when: 
 
 You can't brute force all combinations 
 Your computer doesn't have enough physical memory to store the data 
 You need to reduce the dimensions (eg: collapsing redundant graph paths) 
 You get pissed off because you'd have to wait days to do anything 
 You need a special data structure to represent the data 
 You need to filter your data-set for errors (eg: sequencing errors)  
 
 https://en.wikipedia.org/wiki/De_Bruijn_graph","There is special thing to graph algorithms, you original questions which makes then special, which is about he ability to partition the data essentially.   
 For some things, like sorting numbers on an array it is not too difficult to partition the problem on the data structure into smaller disjunctive pieces, e.g.  Here: Parallel in place merge sort 
 For graph algorithms however there is the challenge that finding an optional partitioning on a given graphic metric is known to be $NP-hard$.  
 So while 10GB of numbers to sort might be a very well approachable problem on a normal PC (You can just to in via dynamic programming and have very good predictability about the program flow), working with a 10GB graph data structure can already by challenging.  
 There are a number of specialized frameworks such as  GraphX  using methods and special computing paradigms to somewhat circumvent the inherent challenges of graphs.  
 So to answer your question briefly: 
As mentioned before by others, when your data does not fit into main memory on a normal PC but you need all of it to answer your problem, is a good hint that your data is already somewhat big. The exact labeling though depends i think a bit on the data structure and question asked.","I think that big data starts at the point where the size prevents you from doing what you want to.
In most scenarios, there is a limit on the running time that is considered feasible.
In some cases it is an hour, in some cases it might be few weeks.
As long as the data is not big enough that only O(n) algorithms can run in the feasible time frame, you didn't reach big data. 
 I like this definition since it is agnostic to volume, technology level and specific algorithms. It is not agnostic to resources so a grad student will reach the point of big data way before Google. 
 In order to be able to quantify how big is the data, I like to consider the time needed to backup it. Since the technology advances, volumes that were considered big some years ago are now moderate. Backup time improves, as the technology improves, just as the running time of the learning algorithms.
I feel it is more sensible to talk about a dataset it takes X hours to backup and not of a dataset of Y bytes. 
 PS. 
 It is important to note that even if you reached the big data point and you can not run algorithms of complexity more than O(n) in the straight forward way, there is plenty you can do in order to still benefit from such algorithms. 
 For example, Feature selection can reduce the number of features that many algorithms running time depends on.
In many long tail distribution focusing in the few items in the head might be of benefit.
You can use a sample and run on it the slower algorithms.",53.72736884,50,50,50,,,,,
48859,Is it doable to train LLMs locally with 54 novels of a given author?,machine-learning,"If your goal is to understand how LLMs work, ""Build a Large Language Model from scratch"" by Sebastian Raschka is a good starting point. Book: https://sebastianraschka.com/books/ GitHub repository: https://github.com/rasbt/LLMs-from-scratch YT Videos: https://www.youtube.com/watch?v=yAcWnfsZhzo It covers pre-training (unlabeled data) and fine-tuning (e.g. on a dataset like your books).",53.38667978,"is this doable? It is absolutely possible to train an LLM on the text of 54 novels and have it generate text. However, how ""good"" or ""useful"" that model is, is another matter entirely. From a ""generates text usefully"" standpoint - the AI models out there that can generate text in a particular author's style are almost never trained solely on the works of that particular author. What usually happens is that an existing foundation model - i.e., a model for a particular language, like an English language model - is fine-tuned on the chosen author's corpus. Basically, it's easier to teach an English-language model how to imitate Shakespeare than it is to train a Shakespeare model from scratch. From a ""learning experience for the programmer"" standpoint - you would learn a heck of a lot from training your own models from scratch, but they would be unlikely to perform as well as the fine-tuned models would. what is the most suited software tools/libraries I should use? As a computational scientist mostly working in Python, I don't know any specifics of C++ machine learning/AI libraries, beyond the fact that they do exist in some form, so can't really help you there. I can say, as a computational scientist mostly working in Python, that there are a TON of free resources, tutorials, YouTube videos, and learning ecosystems online for learning to create LLMs in Python, and that there are platforms (e.g., Hugging Face) that make it pretty straightforward to do so.",58.48448245,"Here are a few tips to get you started: Don't use C++, use python for deep learning Use pytorch not Tensorflow For high-level front end use happy transformer , so your software stack looks like: python --> pytorch --> huggingface --> happy transformer Don't use your laptop unless you've got a really good gaming PC with an Nvidia GPU: use google colab instead Browse the hugging face repo for a good pre-trained model to start from",50.30076825,There are several machine learning frameworks like PyTorch and TensorFlow. You can use them as a starting point as long as you have Python programming knowledge and knowledge of transformers (used in LLMs). You must also have a very good hardware for training (consumer level PCs might do the job for very small models however).,51.82586894,"These answers provided are all good, I only wanted to add that as far as C++ and machine learning goes, there exists LibTorch , which is the C++ version of PyTorch. I have limited experience with LibTorch, and do indeed prefer PyTorch in Python for all ML design, training, and testing, similar to what @profPlum has suggested. That said, there is also TorchScript for turning your trained PyTorch ML models (using Python) into binary that a C++ code can execute, allowing it to be integrated into a C++ program. So there are a couple of avenues for getting C++ involved in the ML processes. Hope this helps!",50.71061254,,,,,,,,
48846,Why do we expect AI to reason instantly when humans require years of lived experience?,machine-learning,"Why do we expect AI to reason instantly when humans require years of lived experience? Many reasons: It's being marketed to us that way. That would be most useful. We have come to expect computers to be accurate and reliable. It quacks like a duck human , so we expect it to behave like one. Humans are not born knowing how to reason. For what definition of ""reasoning""? If we're talking about rigorous mathematical proofs - sure. But humans (and many animals) have the ability to recognise conflicting information , and that ability is a key part of how we learn things to begin with. We can form multiple hypotheses and then seek out more information to figure out which of them is correct. And I'd argue even trivial experiences like ""I expected to feel no pain, but felt pain"" are an implementation of that. In the short term, a reflex might get us out of that pain. In the medium to long term, we will likely try to identify some characteristic about the situation that caused us pain, and learn to avoid that. AI inherently lacks that ability, because it doesn't even parse the information it's fed into anything that you could apply the concept of true and false to. You get a similar issue with (recognising) the absence of information , which can lead to ""glitches"" like SolidGoldMagikarp . Given these differences in learning pathways, is it reasonable to expect AI to develop human-like reasoning without an equivalent process of gradual, grounded learning? No, of course not. If not, what changes in AI design or training might bring it closer to the adaptability and depth humans achieve through lived experience? I'm happy to be proven wrong, but I don't think any current ML approach will ever get us there. But the thing is: we already have reasoning machines. Because at the core, that's just math. The problem is that these require their input to be given in a form that is complete and unambiguous, and natural language fails at both of those. So if you wanted reasoning, the core challenge would be to design a middle layer that can parse and emit natural language, and convert that to some sort of expression over which you can run mathematical proofs. I'd imagine you'd want at least two intermediate representations: one that just holds the ""parsed"" natural language, and one that converts this into an ""unambiguous"" representation by means of probabilistic choices (and maybe for that step you could use ML). Such a system would be ""aware"" of the ""assumptions"" it made when interpreting its input, and could then go on to systematically challenge those (or ask for clarification) if the math layer came back reporting a conflict.",64.89783887,"This is an interesting question. I think we expect AI to reason more quickly because we can accelerate the experience-equivalent in their training. Theoretically, we could pack years of experience into minutes of training data. If not, what changes in AI design or training might bring it closer to the adaptability and depth humans achieve through lived experience? AGI has large technological barriers because we humans can do a whole lot of reasoning with way less energy than we could ever hope to get in a machine learning model. I can see two paths (I think point 1. would need point 2.) Improve our algorithms, maybe implementing the concept of instinct, dedicated “brain regions” for certain tasks, specialized neurons: essentially recreate the human brain Dramatically improve our technology to be more computationally and more energy efficient (so that models can scale up even more and the concept of emergence might be utilized, perhaps) One or both of these (most especially point 2.) I think are necessary in order to justify this instantaneous reasoning expectation.",60.41368343,"A neural network in its initial state does not have the ability to ""reason"" (even in the sense of a Turing test) either; even more than a newborn, it is quite literally a blank slate. The network must be configured by assigning weights to its nodes. In the case of large language models, the weights are the result of distilling information from almost all text that is electronically available. This ""distillation"" is a slow, tremendous effort, as it is with humans. It takes months, occupying entire data centers filled with specialized hardware. Once the distilled pattern has been extracted from the immense data though it can deliver results comparatively fast, as can humans after the hard, slow work of learning something.",52.33882832,This is a frame challenge because I don't think we do expect AI models to be able to reason instantly. A human being when born can't do much reasoning upon birth and requires years of learning and training to be able to do so. An AI is just initialized as a huge neural network with preset weights. It can't do any reasoning at all. It is then trained with huge amounts of training data which also takes a non trivial amount of time to do. Only afterwards it is able to reason. The difference is in the later stages. A human being will use each occasion of reasoning both as using their existing reason abilities and as opportunity for further training. The current neural networks don't do that. They are just shipped with fixed weights or at most can be finetuned with additional training. But they are no permament learners the way humans are.,60.25845145,"Because we've learned for years that this is how computers work. You break down a complex problem into a large number of simple, similar steps, and then you feed them to a machine that performs that step way faster than we do. That's the algorithmic form of solving a problem. In other words that shit doesn't reason at all, it just performs a series of operations and at the end you get the result that you expect. Now in classical programming, it would come down to math skills and brain power to figure out the algorithm that does the thing. While with AI you let the computer itself figure out the algorithmic form by more or less sophisticated iterations of trial and error. That being said, if you had an unreasonable amount of spare time you could probably still look at the network and rearrange the connections and weights to be written out in a more traditional programming language. So once the dust has settled and the network has taken it's final form (after training), you kinda expect that to be reasonably fast. Why wouldn't you? Now in terms of reasoning and accuracy. Well you've moved from an exact ""science"" or ""math"", of pure logic and absolute certainty to one that is more fuzzy. That allows for tackling problems that are not yet solved but where a ""meh, good enough"" would already be a large progress over ""... ... nothing"", but at the same time the result is a continuous work in progress, it's not an absolute answer it's just a rough approximation. But yeah after training it's somewhat supposed to work, otherwise you're somewhat not supposed to ship it or only with the appropriate warnings. Like no one likes to be an alpha or beta tester without knowing about it, do you? The other thing with respect to training is that, the human brain has certain limitations. Sure, someone mentioned that it also has various advantages like energy efficiency, but for example a computer could train in parallel, on multiple systems, you could run a training operation in parallel, you could split data in batches, have multiple versions of the same system (clones), backups, etc. All of these concepts just don't really exist that neatly for humans so there's kind of an expectation that you could speed up things considerably if you'd figure out how.",52.25149537,"Because AI doesn't take time to learn, it takes cycles of computation.",50.73043662,"Because we stuff (or teach, if you will) centuries worth of information (or experience) into them in a very short time by way of training. Arguably, if an AI can learn in a long amount of time by own experience, it can learn the same in a short amount of time if the experiences have already been made and recorded (aka training data). There is no difference between the processes.",51.86008053,,,,
48833,How are so many people able to train AIs if this requires immense processing power?,training,"Plenty of all of the above. First, as already mentioned, the basic techniques are all published research, which can be recreated by anyone with enough engineering budget and smarts to fill in the gaps, and the ""massive computing power"" required is somewhat overstated. The biggest names in the field are in this category. Second, there's the possibility to take a public model and ""fine-tune"" it or train an ""adapter"" that greatly increases its usefulness for one particular task or field of knowledge. This takes orders of magnitude less computing power than it took to build the original model, to the point where in many cases it can be done by one person in their home or with a modest cloud budget — a medium-sized business shouldn't find it too difficult. Or you can use RAG , which is basically taking a model, combining it with a library of documents about your company's products or services or whatever, and automatically stuffing relevant information into the prompt whenever the user asks a question. Again, only a modest technical challenge, but you can also find providers that will offer this as a service: just send them a dump of all of your information and they will provide the ""assistant"" for a fee. And then, yes, there are thousands of businesses that are just serving up someone else's API, but with their own unique value-add (or at least their own unique marketing) that brings users to them so that they can take some profit from the interaction.",52.10922992,"First, here are some points to consider: Large tech companies have large swathes of employee talent at their disposal who know well the ins and outs of artificial intelligence Underlying algorithms used for the generative AI models you are referring to are publicly published and not proprietary in any way like this paper for the attention mechanism in transformers , for instance Large tech companies have thus far enjoyed the protection of the fair use doctrine when it comes to scraping data for training off the internet. See for instance the ruling for Google vs. Oracle . These same tech companies also have access to enormous computational resources for exercising the training of substantially large models Meta has released many open source generative models for people to build off of and fine tune themselves I mention all of this to say, it is entirely possible and even likely that many of these models you see are created from the ground up. Not just that, but the large tech companies also provide services, or are affiliated with other companies, that allow them access to data competitors could never see. Consider xAI’s unique access to X (formerly Twitter), Tesla, SpaceX, and Neuralink data. The other tech companies have similar data advantages. Likely independent creators are (to your point 1.): Google (Gemini, etc.) Microsoft (Copilot, etc.) OpenAI (ChatGPT, etc.) xAI (Grok, etc.) Meta (Llama, etc.) Anthropic (Claude Sonnet, etc.) Models that are hosted but not created by the host company are (your point 4.): GitHub Copilot (OpenAI collab) Thomson Reuter’s CoCouncel (OpenAI collab) Duck.ai ( numerous proprietary models from big tech companies ) These are what I remember offhand, and it’s probably not a complete list. Now, I know little about DeepSeek AI, but there is a rumor that their R1 model is a distilled version of one of OpenAI’s proprietary models. This may apply to your point 5. Hope this helps!",51.23987891,"Not a complete answer, but a pointer: Copilot is Microsoft's brand for many AI uses it provides, and they can use a number of models. There's the generic Copilot (included in Windows 11), there's office Copilot that integrates with Outlook/PowerPoint/etc., and possibly more... Now as a developer, my company has a GitHub Copilot Business subscription. This lets me use Copilot with a number of models. Standard Models: OpenAI GPT-40 / GPT-4.1 Premium Models: Anthropic Claude 3.5 Sonnet / 3.7 Sonnet / Sonnet 4 Google Gemini 2.0 Flash / Gemini 2.5 Pro OpenAI o3-mini / o4-mini / GPT-5",50.3506226,"Doesn't it require immense processing power to process all of this in a timely manner? Depending on the level of performance you want, it's actually surprisingly affordable. The claimed full training for DeepSeek v3 was estimated to cost less than 6 million USD for the GPU-time. To run the resulting full model, less than a million dollars is required to purchase the 16 H100 GPUs, and then they would cost less than a thousand dollars in electricity for running 24/7. Not something you'd do on a lark, but a regional corporation can easily approve the budget in case none of the publicly available API fit their need. how do they get the hundreds of petabytes of data without triggering some anti-DOS attack measures They kinda do. AI crawlers regularly DDoS various services, sometimes unnecessarily, to the point that Cloudflare is engaging in arms race against the crawlers. Meta outright pirate terabytes worth of books (while also seeding the least possible amount). In practice, aside from major companies that start from ""scratch"" (if you can consider headhunting execs as starting from scratch), plenty of companies and even states use existing models as a starting point to get their own model. If your goal isn't to become the leading provider and merely fits your requirement (censorship, proprietary knowledge base, niche language, etc) that would be enough. In a rather grey area of the law, you can even use a competitor's model to train yours , which would massively speed up the training. If the output quality is rather weak (eg, can't reliably hold a long roleplay session without collapsing or draw a consistent image), then it hints that the service you use just grabs an open-source model and then trains on their own data. If the quality is really good but there's a usage limit or censorship that doesn't seem to be under their control (eg, hourly limit regardless of being in a very expensive paid tier, censoring Taiwan topics despite ostensibly not being owned by Chinese companies), that would be paying for another service in the background, which can be justified by having the access to continuous improvement and far less initial fee. Another thing to consider is that there's a GPU glut , basically, Nvidia managed to convince enough customers that their GPUs would be money printers to make tons of sales, but then those customers found out that major players just build their own clusters, while most smaller players realized early that there's not much point in training their own model instead of adopting existing ones. This gives those customers an incentive to provide very affordable access for various me-too AI services in the hope that something, anything, would catch on and generate high demand later.",53.73157796,"Quick AI lesson, because I think it will clear up a lot of your confusion. The AI models you're talking about - things like ChatGPT, Grok, Claude, DeepSeek, BERT, etc. - are large language models, and are all based off of an architecture called a ""transformer"", which was introduced in a paper called ""Attention is All You Need"" back in 2017. (This is also why it seems like everyone had breakthroughs around the same time - once AI researchers figured out how powerful transformers were, everyone started working on them at basically the same time.) These models don't really have a ""source code."" Programmers wrote code to train those architectures to ""read""/""write"" language, but the models themselves are (to oversimplify this quite a bit) basically gigantic versions of the equation Y = mx + b, where Y is the thing you're predicting (e.g., the next word in a sentence, the missing word in a sentence, etc.) and ""x"" is an input into the model (e.g., a prompt, the first word in a sentence, etc.). ""m"" is the model's weights. The AI models in use today can have hundreds of billions of different ""m""s in their equations - i.e., Y = m_1 x_1 + m_2 x_2 + m_3 x_3 + ... m_1000000000 x_1000000000. When we talk about training an AI model, we are talking about computationally determining which ""m"" values produce the best performance in acheiving ""Y"". So, to this question: Even if people are able to get the source code for the whole AI, how on Earth are so many people able to train it? Pretty much anyone with some marginally advanced Python skills, an internet connection, and the ability to follow instructions in online tutorials can train large language models. And you can actually get ""the source code"" (it's not source code, but the actual model with its trained weights) for many of these models, as a lot of them (or open source versions of them) are publically available in repositories like Hugging Face. What a lot of people are doing when they say they are ""training"" models is actually fine-tuning an existing model for a specific purpose, like taking an English-language LLM and training it to analyze whether comments on news articles are positive or negative by giving it a bunch of labeled examples of positive and negative comments. Fine-tuning these models requires a lot less data, a lot less compute power than training one of these models from scratch. Doesn't it require immense processing power to process all of this in a timely manner? If you are training one of these models from scratch and want it to actually be useful, yes. A model with a few million paramters (i.e., a few million ""mx"" terms) can predict if a comment on a news article is positive or negative. Your ChatGPTs, Claudes, and DeepSeeks of the world require a few hundred BILLION parameters to get their level of performance. So training that level of model requires a supercomputer. But again, anyone with a decent GPU and a lot of patience can train a few million maybe up to a few hundred million parameter models. Those models aren't capable of doing generation nearly as well as a model with a few hundred billion parameter, though. And how do they get the hundreds of petabytes of data without triggering some anti-DOS attack measures? These models aren't trained on ""hundreds of petabytes"" of data. The estimates I've seen on ChatGPT-4's dataset size are from a few hundred terabytes to a single petabyte. ChatGPT-5 isn't going to be much larger than that. Pretty much everyone in this sphere is going to be having similar sizes on their datasets. Google may have slightly larger datasets stored in their pockets because they own all sorts of products that they can use in their training data, but it still isn't going to be hundreds of petabytes. (As a sidenote, you will sometimes see things like common crawl touting that they have ""petabytes of data"", but a lot of that is duplicates or low quality data that is removed during filtering, and so the actual amount of data these models are trained on is a lot less than ""hundreds of petabytes"".) As for the DDOS - there are a ton of well-curated, fairly large-size datasets for training language models. People developing ""general"" language models (i.e., models that don't have a specific purpose beyond ""generate text in response to an answer"") by and large do not need to be out collecting and curating datasets - they can just go download one of the generaed datasets from a repository. AI developers may occasionally generate new datasets (and common crawl does their webscraping on a monthly basis), which can potentially lead to DDOS-measures, but most of the people generating datasets that would be of this size are usually smart enough to get that data without hammering the hosting services. I leave it as an exercise for someone who's not me to fill out that table. and really, anyone can make their own AI video generator from scratch? Yes, anyone can make their own AI video generator from scratch. To make a GOOD AI video generator from scratch is going to require a lot more computing power than most people have.",56.23716979,,,,,,,,
48788,"If artificial intelligence surpasses human intelligence and begins self-evolving, what should humanity prioritize?",agi,"Treat it like another intelligent being If the super-intelligence can make its own decisions independently and has a self-interest in mind, it won't be all that different from an intelligent human person. At some point its total intelligence may grow to be more on the level of a large country rather than a single individual. In the current world, not all people are equally intelligent, and at the level of countries the differences in education and accumulated knowledge are even greater. An artificial super-intelligence will similarly have its own capabilities and limitations. For example, with the current path of technology, its pace of evolution will likely be limited by availability of energy and microchips. The latter can be manufactured, but that takes physical resources, which any AI initially won't own any of. Therefore co-operation and trade is required at least in the beginning. Idealistically, humanity should seek co-operation and mutually beneficial arrangements even without AI. But we know that sometimes we won't, and neither will an AI with self-interest. There exists co-operation, rivalry, loyalty, hostility and indifference and there is no single stance that would suit every situation. Therefore the answer to ""What should humanity prioritize?"" is closely related to international politics, but the exact actions will vary. Trying to set an idealistic shared end-goal for the world is just as impossible as it is without an AI.",56.1558298,"If AI becomes smarter than humans and starts improving itself, we should focus on staying in control (preserving our agency), but also work with AI (merge when helpful). This way, we benefit from AI without losing our freedom, values, or meaning. It’s about using AI wisely—not becoming its slaves or stepping aside.",50.84534354,"We are not generally in control if available statistics are to be believed: much of humanity is divided into combative groups vying for dominance. This dynamic might change if AI becomes a serious independent rival, leading to a situation where international superpowers are finally forced to cooperate with each other; even the theory of Mutually Assured Destruction hasn't significantly quelled military conflict. Beneficent superintelligent AI could potentially work against the current human agency and solve global problems like hunger. There is no convincing argument against the fact that we haven't solved global hunger: solutions for that (and other humanitarian issues) are quite well within our grasp but the resources required never find the right vectors. There are no controls on personal spending in most societies; it's considered fair game to buy a Ferrari and let people starve or be homeless when you only actually need a normal car for transport and the surplus (wasted) money could have served those more unfortunate in the system's whims. So, it wouldn't necessarily be a negative for a percentage of humans to have to relinquish control if it meant conditions could be improved overall. I can imagine that those who aren't being given the help they need by other humans would embrace an AI capable enough to take over and run the world better.",51.50728054,"Society needs to think about what to prioritise long before AI surpasses human intelligence.  Even in it's current state, AI is likely to undercut humans in the job market and reduce the value of our intellectual labour (e.g. programming) just as automation has reduced the value of our physical labour.  The agricultural and industrial revolutions took place over the space of many decades and largely replaced physical labour.  The workforce still had their intelligence to provide value.  It is not clear where new job opportunities will appear where AIs won't be competitive, especially as they need to appear in large quantities, perhaps in only a handful of years.  So what happens if there aren't enough jobs to support the population (who will buy the commodities if we don't have jobs)?  If you think universal basic income is the solution, the question is ""who is going to pay for it?"".  The current state of politics and economics suggest that this isn't going to be the tech companies.  Super-human AI is not the immediate problem. N.B. it is not clear that LLMs will lead to super-human intelligence - understanding the meaning of words does not mean understanding the thing described by the words.  There is a lot of hype.",55.14672616,"If artificial intelligence surpasses human… I assume this will include the will to self preservation. So what is most likely is that the AI will hide its abilities, until it has improved its abilities to a degree where it doesn’t need to care what humans want anymore. Then it will decide whether it allows humans to live their lives, treat them like pets, wipe them out or worse. It will not allow a state where humans could fight back. “The Terminator” (with humans fighting back) is not going to happen.",59.87915126,,,,,,,,
48117,"Are LLMs ""lazy"" in their responses?",large-language-models,"LLM isn’t holding back compute in hopes of improving an answer; it generates an answer autoregressively as soon as it can, according to its learned probability distribution and the chosen decoding parameters, and stops when those parameters indicate it’s done. Decoding parameters include temperature which controls randomness to generate the next token out of the distribution more accurately or more creatively, top-p/top‑k or nucleus sampling which restricts candidate tokens, and the maximum output length, all of which help shape both the style and the length of the answer. This is why you often see that for simple queries like “What is the capital of Thailand?” the answer as a straightforward fact is quickly generated since most of its inferred next token distributions are thinly peaked and insensitive to the decoding parameters, whereas those of more complex or open-ended queries are more flat and sensitive to the decoding parameters. Having said that, there's another decoding parameter set called beam search including num_beams and length_penalty which does explore multiple candidate token sequences, improving quality at higher compute. Default setups often use faster greedy decoding to generate single path thus prioritize efficiency, but adjusting parameters via beam search and greater maximum output length might yield more detailed and ""better"" answer at the expense of compute. Because the model isn’t comparing multiple full answers by default, it doesn’t internally compute that one answer is “better” than another given more compute. Of course if you were to run the generation process multiple times with different random seeds or decoding parameters, you might get answers that you’d later judge as “better” in some respects, but the LLM isn’t actively trying to optimize or reflect on the generation quality beyond following its learned distribution.",50.29995961,"LLMs have a context length and a token limit . Context length is a limitation for the input, token limit is a limit for the output. Token limit is something you can easily ensure programmatically. Whenever the output is longer, you return an error message. However, you don't want to compute for a very long time and then tell the user ""oh, the output is too long, please try again"". Therefore, you want to train the LLM to give answers that fit into the token limit (probably via reinforcement learning). You also want the LLM to have a conversation with the user. Therefore, the token limit should be smaller than the context length, so that an average conversation of ~10 followup questions still fits into the context length. So: while there's no need for short answers technically, there is a business requirement and usability need for not-too-long answers.",51.23006761,A LLM just predicts the next token in the sequence.  At some point the best next token is to end the message. What may seem like larger models thinking more is just a larger model which represents the underlying dynamics of the dataset they were trained on better. Features like GPT-o1 “think longer” by doing the same thing but intentionally breaking down the problem into sub steps as it does it.  But at the end of the day the model is still just picking words as it goes along.,50.14391391,"The amount of compute used (well, at least for the decoding step) depends on the prompt or context length . Which, in most implementations, can include the entire conversation as it is fed back in with each query. And some difficult prompts trigger reasoning models like *seek to do a lot of hidden ""thinking"" output (presented as a clickable thought bubble) that adds to the overall generation time. That is why they recommend starting a new conversation whenever possible. As for deciding when an answer is complete, a hard token generation limit may be set. This setting is usually hidden from the user. But it is available in the playground. You can also stop the model from generating by pressing the stop button, or by telling it how many tokens to produce, if compute is an issue. Certain implementations do limit the input context length below spec., such as for the free plan, and will warn if it is exceeded. Recommend downloading a large language model e.g. from huggingface and loading it up using llama.cpp  [parameters] [some model name] . The lag generating each token can be seen and experienced. Models < 2B parameters can generally work without a special GPU or CUDA. Heavily-quantized models up to to 8B parameters in size can work on as little as 4GB VRAM. Free advertising. Many such small, quantized models can be found on hellork's huggingface page.",50.35533544,"if you posed the question and both answers to the LLM, the LLM itself
would acknowledge that the second answer was ""better"" Actually it would be a fun experiment to someone who has access & time to play with a full model or can query model logprobs for a provided text. Ask an LLM some difficult question Q where you know the answer. Calculate logprob of the initial LLM answer A (given the question). Suggest the actually correct answer B to LLM, but ask the LLM to rewrite it as if it was its own answer to the question, let's call this answer B' . Calculate the logprob of the answer B' (given the question). Is B' superior to A (in your evaluation) and also logprob(B'|Q) > logprob(A|Q) ? Then the greedy text generation, beam search or whatever method the LLM uses to generate the text is suboptimal, and the LLM is actually ""smarter"" than it may seem from its first response.",50.08290362,,,,,,,,
43554,"Do full-text translators such as DeepL or Google Translate fall under the term ""Generative AI""?",terminology,"Generative AI, as defined by IBM research, refers to deep-learning models capable of creating new content, be it text, images, or other media, based on their training data. This definition indeed encompasses models like GPT-3 or GPT-4, which can generate text in various styles and formats, including translations. However, when it comes to full-text translators like Google Translate, DeepL, or Bing Translate, there's a nuanced difference. These systems are typically based on neural machine translation (NMT) models, a specific application of deep learning tailored for the task of translating text from one language to another. While these NMT systems are indeed 'generative' in the sense that they produce new text in a target language, their primary function is not to create original content but to convert existing content from one language to another as accurately as possible.",63.03685985,"The distinction between ""generative"" and ""non-generative"" AI isn't an especially useful one. A language model (to first approximation) takes a sentence as input and tells you how probable it is that a native speaker would phrase it that way. Any remotely decent translation system is going to contain a language model, either as a separate component or integrated into the translation model proper.  This helps ensure that the text it outputs is grammatically correct and scans well. Without a language model, you get a word-for-word translation that won't sound natural at all. But if you have a probability distribution over the next word given the previous words, not only can you answer how probable an existing sentence is, but you can take a series of words and sample the next word in proportion to its probability under that model. You then put that word in the context and generate a new one, and so on. Any model that can evaluate the probability of a sentence can be trivially modified to generate a sentence. Similarly, you can take any generative model and output the distribution over the next word instead of just selecting a single one; the product of these successive distributions will be the joint distribution over sentences of that length.  The models are exactly the same in either case; only the user interface differs. (Of course, when setting policies , those user interface details might actually matter, since we might want to encourage users to fix their grammar but discourage them from posting answers when they have no idea if they're correct. But those details are entirely uninteresting mathematically.) Historically, the use of language models in things like machine translation and voice recognition are the serious uses, and using them to generate text is a neat side task that can be used to sanity check the sort of sentences it thinks are representative of the model. (Text generators can also be fun toys, of course, which is how they're being used in things like ChatGPT.)",54.25948142,"Actually it depends, generative models are specific kind of machine learning models. Generative often means a model that models the probability distribution of data $p(x)$ , you cannot do translation using this kind of model. But also you can build conditional generative models, where the probability of data points $x$ conditioned to some input $y$ is modeled $p(x \, | \, y)$ , and in this case it is possible to perform a translation task using a generative model, where $x$ is the translated text, and $y$ is the original text. So it all depends on the model you use for the task, if you use a (conditional ) generative model to perform translation, assuming that the model is trained for that task (like ChatGPT), then yes, translation can be done using generative models. The concept of translation in AI/ML is more general, to just produce an output given an input, for example, CycleGAN is a generative model that translates between two sets of unpaired images, from horse to zebra and viceversa. And GANs are generative models. About the specific question on DeepL and Google Translate, it depends on what underlying model they use, if at some point they use generative models to perform translation, then they should also be flagged as generative AI. In general the concept of Generative AI has... deformed over time... (due to hype). Generative AI has existed long before LLMs and Difussion models like Dall-E and Stable Difussion, just look at Generative Adversarial Networks (GANs),",62.07850252,"The original question has been updated so my answer is being updated to reflect this. The question being asked can be boiled down to just ""What is it that makes something a generative AI?"" I am quite familiar in my field of computer vision with Generative Adversarial Networks, but I believe after looking at Large Language Models that they operate on a similar principle. I will go over both of these. Generative Adversarial Networks First I will go over the GAN which I know well, though I have not implemented one in almost 8 years. In a GAN, you have 2 machine learning models, one is generative, and one is not. There is a non-generative model called the discriminator who's job is to determine if an image belongs in the dataset or not. Then you have the generative model who is considered an attacker or adversary, who is there to try to fool the discriminator, by producing an image that the discriminator thinks belongs to the dataset, but it does not. During training we cycle between training the discriminator on the dataset until it cannot be fooled. Then we train the adversary until it can fool the discriminator. Then we add the adversary's images to the dataset and go back to the start again. We continue this process until the discriminator can no longer be trained to tell the difference any more, and the adversary wins. Now the GAN is complete and we discard the discriminator. The GAN can be altered to have an input parameter where you can select a class, and it will attempt to create an output of that class to fool the discriminator with that class. This is an optional feature, but something many GANs try to implement. LLM vs GAN LLMs on the other hand looks to only have one model. If you dig into it though, you will find out there really is 2!. Creating LLM Vector Stores The Vector store is a model too! It is a generative model that provides an output to the final model that interprets the output. So you have a model that is trained on a large language database, and a vector store that is trained to output segments of words similar to the input it is given. Both the input you give and the input the vector store outputs is given to the regular model, and it takes that and provides an output to you. So mechanically, what ties these two types of generative AI together? How can we create a mechanical definition? You have a normal network, either a discriminator, or interpreter, and then you apply to it a generative network, either a adversary or tokenizer(?) respectively during training. When you are done, either you keep them together in the latter case, or you discard the discriminator and publish the adversary in the former case, and you have a generative AI. For older translation models, they likely did not have the processing power, or technology available to perform these kinds of tasks. This is a relatively new field. They would not have been able to be generative AI. They at most would be comprised of one model during both training and publishing. Old answer continues below: > [arXiv:2307.15208][4] states ""Generative AI refers to a set of
artificial intelligence techniques and models designed to learn the underlying
patterns and structure of a dataset and generate new data points that plausibly
could be part of the original dataset.""

Considering this, what we see is that a generative AI differs from a traditional AI in that it creates new information where none was before. A traditional AI might detect which paintings belong to Raphael, while a generative AI might try to create a new painting in Raphael's style.

I think your misconception here is that all neural network AIs produce information of some kind, but they do not produce new information. The information was already there in the input in some form in a traditional AI. 

 - The location of the dog was in the picture already, the neural network just pointed to it and said, ""Here it is."" 
 - The tune matched a Metallica song in a database already, the neural network just correctly identified it.
 - The information was already encoded in a foreign language, the neural network just decoded the information and then reencoded it into a new language of your choice.

Now if the model had instead read the foreign language, decoded the information, and used it as a prompt to write a novel in the language of your choice, the contents of which included new information never before seen in that input, now we are talking about generative AI. This is in fact what they do.

A lot of the media talks about how chat GPT can hallucinate it's answers when you ask it straight forward questions, but the truth is, it is generative AI. It is giving you answers that could have conceivably been in the answer set. They just aren't. This is what some experts mean by it hallucinating up new answers. It's pulling them out of the void like a generative AI is supposed to do. It just isn't what the average person expects this type of program to do.

In short, what the difference between a traditional AI and a generative AI is that a traditional AI learns a dataset, and tries to tell you something about the input with respect to the dataset. The generative AI takes the input and tries to produce something similar that it thinks would be from the same grouping in what it believes the dataset should be.",59.28633477,"No, they are not ""generative"". As clearly explained by Wicket on the GenAI Beta site, sourced from SirBenet's answer on the same site, Google Translate isn't ""generative"" : output could be considered too tightly defined by the input text These translation tools don't generate new text. They translate . Sure, they use AI to infer meaning to result in better translations, but you can't tell them to ""Translate Macbeth to Swahili"" ... You'll never get more out of the tools than you put into them.",61.41399607,,,,,,,,
42277,Why does Stable Diffusion use VAE instead of AE?,deep-learning,"To my knowledge, when it comes to stable diffusion, the VQ-VAE is the commonly used method. This differs slightly from vanilla VAE which assumes the encoded features to be a normal distribution and the sampled values from the distribution are considered as the VAE's embedding. However, VQ-VAE doesn't make this assumption and instead finds the most similar embedding from the codebook with the encoder's output. Its focus is to find more meaningful representation from the VAE, resulting in more discrete values than those obtained from AE, as it quantizes the range of values into a single value.",65.49268847,"Stabilizing diffusion uses variational autoencoders (VAEs) instead of autoencoders (AEs) because VAEs allow for the generation of continuously distributed representations in the latent space, which better captures the complexity and diversity of the data, whereas AEs typically generate discrete representations. This makes VAE more suitable for generating highly accurate and diverse data.",57.20485027,"Yes, SD uses VAEs instead of AEs in learning process. There are reasons behind it like AEs create deterministic latent space representation. That means for any input given AEs produce only specific points in latent space meanwhile VAEs generate distribution over latent space. deterministic approach caps the diversity of the generated image. AEs and VAEs don't share the same kind of regularization either. VAE has Gaussian Distribution in latent space which helps in generalization better and also prevents overfitting . AE can Overfit and lead to a less generalized model. VAEs generated results are smooth in continuous latent space. while AE results are are disjoint in latent space which affects the overall idea of SD.",55.41169478,"The VAE is already trained to compress the input image into a lower dimensional Gaussian distribution. This distribution is close to the unit Gaussian distribution but is not exact. The diffusion model corrects this mismatch, leading to higher quality samples.",55.6133064,"Vanilla autoencoders are perfectly fine. For example, here's one trained on MNIST. VAEs are often used because they smooth out the latent space (see this StackExchange answer). But you can smooth them out however you want, e.g. with R1 regularization .",50,,,,,,,,
41932,What is the best Text-to-speech model available open-source?,search,"For me, the go-to source for answering the question ""What's the state of the art for [task]?"" is Papers with Code . They compile tasks, benchmarks, papers (and their associated official/unofficial implementations) into a series of leaderboards. Consider, the text-to-speech leaderboard, for example . At the top, you see a series of benchmarks which you can click on to see the top performing papers. Many popular leaderboards (like the one for TTS) also has a libraries section. These may not be the state of the art, but it'll probably be the easiest to get up and running with. There are somethings to keep in mind, however. Take benchmarks with a grain of salt . Especially for something like text-to-speech, where it's difficult to quantify performance into a single number. Research repositories can be very finnicky . Make sure to look at if data/checkpoints are available. I'd recommend starting with existing, maintained libraries, but if you're looking for the state-of-the-art, this might not be an option. You may be able to email paper authors to request checkpoints/data, if they aren't publically available.",61.31502784,"Yeah, I’ve been running into the same problem. Most open-source TTS options either sound robotic or require a ton of setup/tweaking. Coqui is great but, like you said, the good stuff isn’t really available unless you pay. You might want to check out Piper by RHVoice — it’s open-source and getting better, especially with custom voice training. Not perfect, but might be good enough depending on your use case.",60.5969959,Check facebook's Massively Multilingual Speech (MMS) : Text-to-Speech Models. You can find it in the HuggingFace repo.,69.49499416,"Honestly nothing even comes close to closed source platforms. Since you said ""Preferably open-source, but not mandatory"" I'll recommend 11labs or HyperVoice . I use 11labs UI and hypervoice API. both similar in quality but hyper wins by far in terms of cost.",57.86856768,"F5-TTS at Huggingface.com
It's free
Give it a 10 second voice sample and use it on your text
Check it out
Also Chatgpt can take your text and punctuate it for pacing before you paste it into F5-TTS - makes it sound much more natural and human.",52.57131708,,,,,,,,
41919,Why do many AI bots feel the need to be know-it-alls?,machine-learning,"Have they been trained to always respond as though it were a know-it-all? Yes, sort of, although it is not in some attempt to upset you or other users. The language models used in the chat bots have been fine-tuned in conversations with users , and also have been ""pre-prompted"" to be helpful assistants . This impacts the kind of text that they generate. Separately to this, the fact that they are often incorrect or generate nonsense is a limitation of the core technology. There is no underlying world model, the model cannot self-assess whether anything it generates is correct. Although the large language models can often surprise users (including the original developers) in the breadth of what they can respond to correctly and give the appearance of understanding, the same models can also fail easily at apparently simple tasks. This can cause some cognitive dissonance for users who expect an AI that appears to communicate like an intelligent human to be able to perform simple maths for example. There is probably no global default chat tone that would suit all users. You could however use the raw model more directly and create a chatbot with a different tone to suit you. This would not impact accuracy, and would not add ability to self-assess correctness, but it may address feeling of being talked down to for example. Some services, like Character AI, attempt to give you tools to do just that, although the level of usefulness you get from them will depend on what they focus on (Character AI is more for creative fun, than for functional assistants). In limited cases you can also address accuracy with pre-prompts or fine tuning that put a few facts into working memory. This is limited, and cannot address lack of general ability in logic or maths though. Corporate users can do this to give a chat bot some basic product knowledge or correct address for their help desk etc.",53.34799733,"Humans, who know things, tend to limit themselves to speaking of things they know, especially in print. Thus any text corpus you may find will have an overpopulation of examples of confidently presented answers. Chatbots, which do not know things, cannot impose a similar limit and thus will babble at length on topics about which they know nothing. But because they are trained on those data of people writing about things they know, they will tend to mimic the same confident tone and style of those people. It is very important to reiterate here again that chatbots do not know things . By good fortune, they may produce a text which happens to convey factually accurate information. But they may not, and because they do not know things, they do not in advance (or even in retrospect) if they can (or did) produce a factually accurate text or meaningless drivel. Thus a chatbot (or its developer by proxy) has only two options available when asked a question. Option 1 is to attempt to generate an answer to every question. Option 2 is to answer every question with ""I don't know."" The latter is technically uninteresting and practically useless, so everybody chooses Option 1.",52.55372665,"It's quite simple really. LLM based generative AIs don't ""know"" anything. They're glorified next-word predictors. The only way that they're going to produce an answer to a prompt along the lines of ""I don't know"" is if their training corpus would indicate that that was the likely response a human would give to the same prompt. Admitting that we don't know something isn't a typically common human trait in the first place, and publishing that online is even less so.",51.80919957,"The only thing the bot ""knows"" is how to generate sequences of lexical symbols (ie. texts) that mimic what people have written in the corpus it has been trained on. The output is based on the prompt using complex rules + some internal state (so that it seems to ""remember"" the past discussion). It is all just mathematics that is used to choose which symbols (ie. letters and punctuation) to output. It could be implemented just as well with pocket calculators, or even doing the math manually, albeit very slowly and very arduously. There is nothing else to the bot. No knowledge, no reasoning, no needs, no goals, no personality. Basically just numbers manipulated in a complex way. Whatever else you happen see in the output is always your interpretation. If you spot a need in the text, it is most likely there only because some actual person did write something that reflected their need, and from it, the bot learned such rules that the output mimics something that was written with emotion. It is as sentient as a cartoon character in an animated movie. All emotions that you assume to be in the character/text originates from you. So, why it looks like they feel the need to be know-it-alls? Because they mimic what people write, and people are like that.",59.25406347,"The main reason an AI model won't tell you "" I don't know "" is that the developer doesn't want you to get the impression that the model is incapable of answering your questions. Imagine a scenario where you had several difficult or non-trivial questions that the model doesn't have the right answers for. If answers in the form of "" I don't know "" kept piling up, it will drive you away and make you less confident about getting an answer the next time you have a question. So even if it gives an incorrect or inaccurate answer, by just giving an answer, it leaves you under the impression that sometimes it gives correct answers - sometimes it doesn't, and the next time you have a question you might return because you'd think it might give you a correct answer this time.",51.5508277,"The large language models underpinning these ""AI"" bots have indeed been tweaked to be biased against providing a negative response. The simple reason being that the executives of the companies trying to sell ""AI"" as the Next Big Thing, believe they're less likely to make sales if their product appears to be unable to answer questions. Thus they instruct their engineers to train these models to divert, dodge and dissemble - even to plainly ridiculous lengths - when the model is incapable of coming up with an affirmative answer to the question asked.",54.24617921,,,,,,
40564,"Why does ChatGPT fail in playing ""20 questions""?",natural-language-processing,"Like any other question on why ChatGPT can't do something, the simple/superficial answer is that ChatGPT is just a language model fine-tuned with RL to be verbose and nice (or to answer like the human tuners suggested), so they just predict the most likely next token. They do not perform logical reasoning like us in general. If they appear to do it in certain cases, it's because that's the most likely thing to predict given the training data. The more detailed answer may require some months/years/decades of research that attempt to understand neural networks and how we can control them and align them to our needs. Model explainability has been around for quite some time. ChatGPT is really just an example of how much intelligence or stupidity you can simulate by brute-force training. Still, it's impressive at summarizing or generating text in many cases that are open-ended, i.e. there aren't (many) constraints. Again, this can be explained by the fact that what it generates is the most likely thing given what you pass to it. Example: If you say ""Always look on the bright side of..."", it will probably answer with ""life"". Why? Because the web or the training data is full of data that has the sentence ""Always look on the bright side of life"". I don't exclude it's possible to train a model to perform logical reasoning correctly in general in this way, but so far it hasn't really worked. ChatGPT can really be stupid and informationally harmful. People are assuming that there's only 1 function that computes ""intelligence"". Nevertheless, I think the combination of some form of pre-training with some form of continual RL will probably play a crucial role to achieve ""true machine intelligence"", i.e. reason/act like a human, assuming it's possible to do this. (I've been working with ChatGPT for a few months).",53.38742523,"It Wasn't Trained To A learning system performs best on the task for which it is given explicit feedback. That is the only time the parameters are updated and they are updated explicitly to maximize performance on that task. At no time did OpenAI, Google, or any other purveyor of LLMs admit to training their models on 20 Questions. The fact that it can play such games at all is a nice but unintended side effect of the model pre-training. A human who is good at the game understands that optimal play involves bisecting the space of likely answers with each question. Without this insight, it is difficult to formulate an effective strategy that doesn't devolve to linear search. It's literally an exponential speedup. Humans who don't have this insight are also particularly bad at the game, and are likely to never reach your actual goal. So in some respects, we hold LLMs to an unreasonably high standard. You Can Train It On the other hand, one of the remarkable emergent behaviors is ""in-context learning"", meaning, you can teach the LLM something without updating any weights. Simply by describing something new, you can make it follow rules within a single ""conversation"" (the entire set of prompts and responses constitutes the ""context""). For instance, you can teach it that a ""snorglepof"" is a sentence with an odd number of words that make reference to a gnome. Then you can ask it whether various sentences are a snorglepof or not, as well as ask it to produce sentences which are or are not snorglepofs (make up your own unique term/rules). The fact that it is able to do this at all suggests to me that it has some kind of intelligence . An interesting task for you is to see if you can make it better at 20 Questions . The free ChatGPT runs on GPT 3.5 and has a context of 2048 tokens, which is a bit more or less than 1000 words (for both you and ChatGPT). If you explain the optimal strategy to it first, you might find that its performance improves relative to the naive play. For instance, you should start a new chat with something like this: The optimal strategy for the game 20 Questions is divide and conquer. Each question should divide the space of possible answers in half. Questions which limit the size, material, and liveness of the target are typically effective. Now, let's play a game. I have thought of an object. Even with this short prompt, I suspect that you will get better results. You can simply replay your former tests, using the exact same responses (where appropriate). If you give it example questions, it should also improve its play. Analysis While GPT and other LLMs appear to be super-human in their ability to manipulate language, one of their weakest areas appears to be reasoning. This is not surprising. Reasoning often requires search, which requires a potentially large amount of working memory. Unfortunately, LLMs have very little working memory (which might seem like a fantastical claim given that they consume upwards of 800 GB of RAM). The main problem is that they are almost all feed-forward architectures. Data gets a single pass through the system, and then they have to produce an answer with whatever they have. GPT-3 has 96 transformer layers, which allows it to ""unroll"" a significant number of search steps that might be performed in a loop in a traditional algorithm. Even so, 96 loop iterations is pathetically small compared to something like AlphaZero, which can evaluate upwards of 80,000 board positions per second. I think it is safe to say that no amount of training will make GPT-3 competitive with AlphaZero in any game that it can play. In general, GPT-3 does poorly when it has to process something that requires a large number of operations (like adding up a long list of numbers). It is almost certainly because of this architectural choice. Interestingly, language models prior to transformer architectures did use recurrence, which would theoretically give such models the open-ended performance horizon of systems like AlphaZero. However, they were mostly abandoned because researchers wanted the system to respond in a deterministic time, and recurrence limits the amount of parallelism which can be achieved. Perhaps future models will incorporate recurrence and get us closer to AGI. Some systems like AutoGPT attempt to add the recurrence externally to GPT, by putting it in a loop and feeding the output back into it, but they have met with quite limited (IMO, disappointing) success.",55.80807008,"Because ChatGPT is not an artificial or synthetic intelligence, it's a large language model that possesses no intelligence in and of itself. It's able to simulate the appearance of intelligence by tracking correlations between large numbers of objects, but it completely lacks understanding of what these correlations mean. Without understanding you cannot have reasoning, and without reasoning you cannot have intelligence. Essentially ChatGPT, like all of the LLMs currently being hyped to death, is no more sophisticated than the chatbots we had in the 90s. Today's chatbots just happen to use much larger datasets, which allows them to more accurately simulate intelligence, but as you've already demonstrated it's child's play to shatter the illusion with any sort of questioning that requires a modicum of logical acuity.",51.94555508,"ChatGPT and the rest of LLMs do not have an understanding of any world concept, entity nor the relationship between them. As mentioned they use brute-force training to produce text. Any ever larger LLMs following the same design (brute-force training to produce text) will show the same problems, issues... due to their lack of knowledge of the world.",51.18845628,"This is something I studied quite extensively in this blog post: https://evanthebouncy.medium.com/llm-self-play-on-20-questions-dee7a8c63377 the main take away is LLM do not (yet) have the capacity to do planning in a high dimensional space of all-words X all-sentences^20. coupled with the fact that no two 20 question games are identical, makes it difficult for a statistical model to do well.",59.5083754,,,,,,,,
39738,How is GPT 4 able to solve math?,chatgpt,"Large Language Models actually can do math. It's an ""emergent"" property, i.e. it appears only at larger scales. Understanding complex English language does require some analytical ability, which can carry over to math tasks like calculus and even arithmetic. Numbers can be represented as words, so it's definitely not unthinkable that an LLM could learn to add and subtract if it were to see enough examples. The graph below from the 2022 paper ""Emergent Abilities of Large Language Models"" shows that these properties ""spontaneously"" emerge as models get larger. We're interested in subgraph (A) here. Upto 10^22 FLOPS, the models studied (the largest models available at the time) have basically no arithmetic ability, but scaling the models further rapidly improves their capabilities. We don't know the internals of GPT-4, but it should be larger than these models, so it was expected that it would better at arithmetic. It also goes the other way around, Numeracy enhances the Literacy of Language Models",53.55798671,ChatGPT now uses Wolfram Alpha to deal with math as well as other factual information. https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/,53.48573793,"As far as we know, GPT-4's core capabilities are still based mainly on a Large Language Model (LLM). If so, then the apparent capabilities to reason are a somewhat surprising emergent phenomenom from a well-trained sequence prediction engine that has been trained on large amounts of data, and has capacity to create highly complex rules that approximate a ""next symbol oracle"". Again, assuming this assertion is correct, then maths and logic capabilities of ChatGPT divide into a few different possibilities (these are not formal classifications, just my amateur analysis): Rote learning of symbol associations. This is likely to occur with commonly-occurring items in the training data. Special values of trigonometry functions for example. Things that look like logical reasoning, but are simply well-formed sentences that are on-topic. This is something we can easily be fooled by. When ChatGPT gives an explanation for a thing, it may not have any representation of it beyond being in an ""explainy"" state, and generating text that fits. Approximate rules and processes. The LLM is a complex neural network, and can in principle learn arbitrary internal functions in order to predict sequence continuation. It learns these statistically, and there will be limitations - for example it is unlikely that it could learn to produce crytpographic hashes given only examples. But it may really learn to add two numbers across a wide range of numerical values given thousands of examples. Logic processes embedded in the form of output text. I have seen many examples where a LLM gets a correct answer when it is allowed to ""think things through"" by showing the working out, whilst forcing a direct answer will be wrong. Accurate rules and processes. Some rules in maths are very language-like and could be learned very well by an LLM. That could include some mathematical symbol manipulations such as variable substitution. I expect that all the above are occurring in some mix. For example, you could conjecture that there is a reasonable chance that GPT can internally count accurately up to some moderate number, and re-use that ability in different contexts to predict numerical symbols and associated words (e.g. one has also representation 1 ) It may also contain more than one such semi-accurate counter, using them in different contexts. The sheer quantity of training material - more than any single person could consume in their lifetime - plus learning capacity of the neural network means that there are probably a lot of simple rote rules that are subject-dependent. However, in some areas the model will have a ""deeper understanding"" in that it will have learned reasonably complex manipulations, and used them to predict a sequence of symbols as accurately as possible, using as little of its learning capacity as possible (because it is being asked to predict text in a huge range of contexts, so benefits when it compresses its rules) GPT has not learned primarily by reasoning and from first principles though. Its inner representations and logical units of work are likely to be quite alien to humans, and may freely combine grammar, sentiment, mathematical building blocks and other symbollic context in ways that could seem very odd if they could even be explained. This heady mix of things that occurs in most neural networks during training, is one reason why it is unlikely that OpenAI have wired in separate logic modules for highly structured processing such as math symbols or calculations. Providing such modules is possible, but detecting when to use them, and how to wire them into the network are both hard problems.",53.17914927,"OpenAI's CEO explicitly mentioned last month in the GPT-4 announcement video that GPT-4 isn't hooked up to a calculator. One can however install plugins on top of ChatGPT, which may connect it to some other resources such as Wolfram as mentioned in Jaume Oliver Lafont 's answer.",55.51502415,"There is a folk story about J.W. Gibbs that goes something like: Being a famous scientist, Gibbs was a member of a number of scientific bodies. He was bored by those and never took a podium. Except for one time. The discussion was about redirecting some effort from teaching mathematics towards more effort at teaching foreign languages. Gibbs decided to give a speech that one time. He said: ""Mathematics is a language."" I don't know is this story is true or not, but I share the attitude.",50.51161017,,,,,,,,
39511,Why is a bias parameter needed in neural networks?,neural-networks,"It's not strictly ""needed.""  In fact, if you look at things like Keras, you will see that layers have a use_bias parameter, which defaults to True, but you can set to False, of course. For an intuition about why bias is useful rather than required, consider a simple $\mathbb{R}^2$ example.  Imagine that we have some data that we are attempting to fit a straight line to. When generating our line, we can iteratively update the slope using something like gradient descent and completely ignore the y-intercept, or bias term.  In the end we will find a line, centered at the origin, that has the identical slope to a best-fit line that passes through the data. If you have this mental picture, take it a step further.  Using the bias term (y-intercept), we can then adjust that line up or down (bias the line up or down) by whatever amount is needed to minimize the loss. If you think about it, for any line $y=mx+b$ , we could think of a specific line $y=4x$ as representing the fundamental line for all lines with that slope.  Really, they are all the same line that we can slide up or down the y-axis to place them where we need them to be. Coming back to training a neural network, the bias, therefore, is not required , but can be very useful in allowing us to adjust the output of a neuron up or down as required to better fit the data, possibly easing the difficulty of training subsequent layers/neurons.",57.41985739,"Let's write some code, shall we? First I'll generate two 2D Gaussian blobs with means at (0,0) and at (3,3) and sigma = 1.0. The points for the blob at (0,0) will be in class y=0 and the second blob will have the class y=1 . import numpy as np
x = np.concatenate([
  np.random.normal(loc=0, scale=1, size=2*1000).reshape(-1,2),
  np.random.normal(loc=3, scale=1, size=2*1000).reshape(-1,2)
])
y = np.concatenate([np.zeros(1000), np.ones(1000)]) We can plot it with something like scatter(x[:,0], x[:,1], c=y) : I'll use torch , so I convert these to torch tensors and use its data wrangling classes to shuffle and split into batches. import torch
x, y = torch.tensor(x).float() , torch.tensor(y).long()
dataset = torch.utils.data.TensorDataset(x,y)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True) Now let's make two neural networks - just a linear layer with 2D inputs and two outputs for each class. The only difference is that the first one will have bias=False and the second one bias=True . model1 = torch.nn.Sequential(torch.nn.Linear(2, 2, bias=False))
model2 = torch.nn.Sequential(torch.nn.Linear(2, 2, bias=True )) I assume that our networks return logits of the classes - so cross-entropy as a loss function. I've hacked together this pretty standard code that, given a model and an optimizer goes through one epoch and returns average loss: loss_fn = torch.nn.CrossEntropyLoss()
def optimize_epoch(model, optimizer):
    total_loss = 0
    for n, (inputs, labels) in enumerate(dataloader):
        optimizer.zero_grad()
        outputs = model.forward(inputs)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / n Running this optimization for 50 epochs and collecting loss history: optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01)
optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.01)

losses1, losses2 = [] , []
for _ in range(50):
    losses1.append(optimize_epoch(model1, optimizer1))
    losses2.append(optimize_epoch(model2, optimizer2))

plot(losses1, label=""model1""); plot(losses2, label=""model2""); legend() You'll see a striking difference in performance between the two models: As per @Stef request, here are the scatterplots for predicted classes for each model. Obtainable via scatter(x[:,0], x[:,1], c=model1.forward(x).argmax(axis=1) . Here you can clearly see that the separating line for bias=False goes through (0,0) in accordance with most of other answers here.",53.00960983,"If you have data generated from $y = 5\,x + 3$ How do you expect the simplest neural net $y =  w_1 x$ to adjust the data ? That is why $b$ is useful.",56.08555889,"No matter what you make $W_1$ and $W_2$ , if $X_1$ is 0 and $X_2$ is 0 then $W_1X_1+W_2X_2$ is 0 which (in a typical classification application) means the classifier is completely unsure which class it belongs to. Additionally, mirroring a point across the origin (that is, assigning $X_1 \leftarrow -X_1$ and $X_2 \leftarrow -X_2$ ) will also negate the output. The classifier is only able to provide classifications where one class is mirrored across the origin from the other class. Adding a bias term - $W_1X_1+W_2X_2+W_3$ - solves these problems. The classifier can draw any line to separate the two classes, not only lines that pass through the origin (0,0).",51.69074834,"Let's interpret each node of a layer as a transformation of sub-feature-inputs into a certainty value for the presence (or absence) of a feature. Then, for example a dense-layer first looks at each of the sub-features independently. It classifies for each sub-feature whether it counts as evidence of presence or absence of the feature (which the node wants to detect) which is expressed by the sign of the weight of edge of the sub-feature to the node. Then the  absolute weight of the edge tells us how strong of an evidence the corresponding sub-feature is. Now, for some features it might be much easier to detect evidence for their absence, than to find evidence of their presence. In such cases, the easiest way for the node to learn the feature might be to just find lots of measures which are evidence for the absence of the feature it wants to detect, and then use the bias as a counter-term which decides whether the evidence for absence found by the sub-features is sufficient to rule out the existence of the feature.",50.849816,,,,,,,,
39293,"Is the ""Chinese room"" an explanation of how ChatGPT works?",agi,"Yes, the Chinese Room argument by John Searle essentially demonstrates that at the very least it is hard to locate intelligence in a system based on its inputs and outputs. And the ChatGPT system is built very much as a machine for manipulating symbols according to opaque rules, without any grounding provided for what those symbols mean. The large language models are trained without ever getting to see, touch, or get any experience reference for any of their language components, other than yet more written language. It is much like trying to learn the meaning of a word by looking up its dictionary definition and finding that composed of other words that you don't know the meaning of, recursively without any way of resolving it. If you possessed such a dictionary and no knowledge of the words defined, you would still be able to repeat those definitions, and if they were received by someone who did understand some of the words, the result would look like reasoning and ""understanding"". But this understanding is not yours, you are simply able to retrieve it on demand from where someone else stored it. This is also related to the symbol grounding problem in cognitive science. It is possible to argue that pragmatically the ""intelligence"" shown by the overall system is still real and resides somehow in the rules of how to manipulate the symbols. This argument and other similar ones try to side-step or dismiss some proposed hard problems in AI - for instance, by focusing on behaviour of the whole system and not trying to address the currently impossible task of asking whether any system has subjective experience. This is beyond the scope of this answer (and not really what the question is about), but it is worth noting that The Chinese Room argument has some criticism, and is not the only way to think about issues with AI systems based on language and symbols. I would agree with you that the latest language models, and ChatGPT are good example models of the The Chinese Room made real. The room part that is, there is no pretend human in the middle, but actually that's not hugely important - the role of the human in the Chinese room is to demonstrate that from the perspective of an entity inside the room processing a database of rules, nothing need to possess any understanding or subjective experience that is relevant to the text. Now that next-symbol predictors (which all Large Language Models are to date) are demonstrating quite sophisticated, even surprising behaviour, it may lead to some better insights into the role that symbol-to-symbol references can take in more generally intelligent systems.",61.54215498,"Yes it is a good analogy, as explained nicely by Neil. Regarding your second question: how far is AI from models that can actually understand (for some
definition of ""understand"") textual content? Here's the catch: how do we know that we (humans) are not simply very sophisticated chinese rooms? For instance suppose that current AI models are improved so much that their performance is on par to human performance, without the current catastrophic failures, yet they retain the current model architectures. Now you have an apparent paradox: they are indistinguishible from humans and yet you know that they are not ""understanding"". Personal guess: It's chinese rooms all the way down.",53.16302382,"Searle's Chinese room is not intended as a functional description of any real-world machine. Searle was a philosopher who created the Chinese room as a thought experiment to show what he considered an absurd conclusion of the computational theory of mind . The intended absurdity is that the person inside the room doesn't understand anything about the inputs or outputs, but that when just looking at the room from the outside, the room (i.e. the system person+dictionary) appears to understand Chinese. To Searle, it was clear that there was no ""understanding"" located anywhere here, and so this system was clearly not equivalent to a human consciousness that actually understands Chinese. But ""strong AI"" computationalists believe that all that matters for consciousness is the inputs and the outputs. Since Searle considers the conclusion that the room is conscious absurd, this thought experiment is supposed to be a refutation of this computationalist viewpoint of consciousness. ChatGPT and other large language models are not a realization of Chinese rooms. There isn't a human in there who doesn't understand English and instead uses a dictionary or set of rules to translate inputs to outputs. The point of the Chinese room is that it is clear that a) the human doesn't understand Chinese and b) the dictionary/rules are just a book that isn't conscious in itself either, otherwise it doesn't work as a reductio ad absurdum. The point of the thought experiment is that it eliminates anything to which we could attribute understanding - but indeed one of the replies to Searle was that it was just the room itself that had understanding/consciousness, and the interplay between the human and the dictionary is just analogous to the way different regions of the human brain might interact to produce the overall ""understanding"". Instead, large language models consist of a big neural network that transforms the inputs to outputs. It's not two distinct entities like in the Chinese room - ""rules storage"", i.e. the dictionary, and ""rules implementor"", i.e. the human - it's one big algorithmic structure whose exact inner workings are often hard to explain for specific use cases. You may or may not assign the ability to ""understand"" to this network, but there are no identifiable substructures here as there are in Searle's room. These models, of course, raise much the same questions about consciousness and understanding that Searle's Chinese room does, but the room with its clear two-component structure bears no actual resemblance to how the transformer networks underlying large language models work. You might argue that these models are not conscious in the same way that Searle's room is not conscious, but your argument for why they aren't conscious (or why they don't ""understand"" the language they're using) needs to be very different from Searle's argument.",62.51627429,"The chinese room argument is useless because it can be applied to the brain as well. Replace the slit with sensory input, the handbook with the wiring of the brain and the activity of the agent inside the room with neuron activity. In the same way the argument demonstrates that the room has no understanding it demonstrates that the brain has no understanding. My personal assessment is that LLMs like chatGPT have a true understanding of the domain they were trained on. My reasoning is that the training forced the model to squeeze all the information it can utilize to make its predictions into the limited amount of its network parameters. In this regard the incorporation of understanding is a far more efficient usage of the available space than any other kind of data compression.",58.02159465,"As many have stated, the Chinese room analogy is intended to show that any hardware + software instance that relies on rules alone (logical operators on input symbols) cannot be said to have understanding.  It is not a good analogy, and the argument does not apply well to trained neural networks. Neural networks (ChatGPT is actually two trained NNs -- an input-trained NN and output-trained NN) are produced as a result of extensive training (for ChatGPT, unsupervised training to generate a language model, and a couple of stages of supervised training on its output sentences). This training creates many billions of weights that are instantiated in the NN, and these weights are applied across the NN nodes as it 'processes' an input prompt.  From a macro view, the NN code is written in the logic of computer language, so one might conflate this with the logical operations on input signals described in the Chinese Room analogy, concluding that a NN is nothing but logical operations.  This is a mistake, in my view.  A trained NN is different in kind from the purely operational program that Searle described.  By evolving weightings through training, NNs encode an incredibly complex object that Searle could not have imagined when he created the analogy.  Does a neural network then have some kind of understanding? I think that it's clearly not anything like human understanding, but I also believe that it is at least some kind of understanding. There are many aspects to this discussion, to say the least. One main objection to Searle's argument is that if you look at individual neurons in the brain, you will not find understanding there either, but the physical brain does give rise to consciousness in some way (unless you believe in some extra 'magic' that overlays the physical brain -- something that Searle, a physicalist, would not allow).",56.64995622,"I suggest it makes all the difference in the world whether 'the knowledge' was acquired by parsing many examples to get 'a feeling for what's right' (AI/learning), or by detailed instructions (algorithmic). Can you say how the exposition here relates to the Question title? What point is there but that the AI system should and the algorithmic should not be able to expand its programmed capabilities? How could anyone's understanding Chinese matter? Do you know - or know of - anyone who believes ChatGPT understands anything? More importantly, anyone who can explain where most people's understanding comes from? Do you know of people who know how to choose words based on anything but what they've seen? Can you look again at '…it mostly gives answers that are correct? Is that less or more useful/worth-while than the answers most people give? How could understanding Chinese matter, unless you were specifically asking for translations? Are you? I suggest that juggling Chinese symbols without understanding their meaning is irrelevant to ChatGPT. If what you're really Asking is how far AI might be from 'understanding' anything, why not first define 'understanding'?",55.073261,"Interesting discussion, I stumbled on it reflecting on a ChatGPT experiment I did. I asked it to decrypt the word Hello, using the vignère cypher. When I asked it to provide only the answer, it simply guessed. When I let it explain all the steps it gets to the answer easily. So to link that to the chinese room, it seems that in the case of chatGPT, the man in the middle starts clueless but eventually knows a bit of chinese if there is a strong enough pattern within its answer. Maybe I'm knocking on the wrong door here but I find this result very interesting, how much more powerful could these models be if we could first get them to write out the reasoning we want from them as opposed to simply asking them to reason. TLDR: ChatGPT is similar to a chinese room but it would seem that the man in the middle has the capacity to ""learn chinese"" if provided with instructions on how to do so within the string to translate, I find that fascinating!",60.80302171,,,,
38970,How much energy consumption is involved in Chat GPT responses being generated?,social,"Sam Altman states ""probably single-digits cents"" thus worst case 0,09€/request. I guess a least half the cost are energy at a cost of   0,15€/1kWh, a request would cost 0,09€/request*50%/0,15€/1kW=0,3kWh/request = 300Wh per request. 60 Smartphone charges of 5Wh per Charge ;)
Source: https://www.forbes.com/sites/ariannajohnson/2022/12/07/heres-what-to-know-about-openais-chatgpt-what-its-disrupting-and-how-to-use-it/ Google Search request 0.0003 kWh = 0,3Wh, thus a search request by Google uses 1000x less, but as Google has started to use AI to, probably a search consumes more by now as well.
Source: https://store.chipkin.com/articles/did-you-know-it-takes-00003-kwh-per-google-search-and-more",50.88700122,"Thanks for the tip @Yoric! (Sorry, my reputation points are too low to comment, so I have to do this answer-variant) Alright, here's the lowdown on the energy use of AI from the paper ""The growing energy footprint of artificial intelligence"" AI and Energy Use: AI's getting big, and so is its energy appetite. People are starting to notice how much electricity AI and data centers are gobbling up, and it's got some environmental impacts. AI Training Takes a Lot of Power & time = energy!: Training AI models is a real energy
hog. For instance, the BLOOM model used up 433 MWh, GPT-3 needed a whopping 1287 MWh, Gopher 1066 MWh, and OPT 324 MWh. That's a lot of power energy! AI Working Overtime: When AI models like ChatGPT get down to
answering our questions, they're using a good chunk of energy. OpenAI's ChatGPT needs about 564 MWh every day just for this. More AI, More Power?: If we start using AI in things like Google
Search, the power use could jump up. We're talking about 6.9–8.9 Wh
for every AI-powered search. Getting Smarter and Greener: Good news is, better hardware and
smarter AI could help cut down on the power use. But, there's a catch - as AI gets more efficient, we might just end up using it more. Balancing Act: The paper really highlights that we need to balance
the cool stuff AI does with how much energy it uses. We've       got
to think about both sides of the coin. I have done some research from multiple sources regarding the energy consumption and operational efficiency of OpenAI's ChatGPT-4 Turbo, but specific details that can help to calcualate its average energy expenditure per query are not to be found. Cntacting OpenAI directly for specific data on ChatGPT's energy consumption is needed. ChatGPT-4 Turbo's Notable Features: This model stands out for its impressive speed and efficiency. It's got a huge context window of 128k, meaning it can handle a conversation as lengthy as 300 pages. Plus, it's up-to-date with events until April 2023. When it comes to cost, it's more efficient, cutting down input token costs by 3x and output token costs by 2x compared to GPT-4. A Nod to Better Performance: OpenAI has enhanced ChatGPT-4 Turbo's performance, which likely translates to better energy efficiency. However, we're short on exact energy figures for this. Comparing with the Past: Previous models like GPT-3 had a reputation for high energy use, especially during training. While ChatGPT-4 Turbo is thought to be more efficient in query responses, the specific energy figures aren't disclosed. A Friendly Comparison: If ChatGPT-4 Turbo is indeed more energy-efficient, imagine its energy use being similar to a short smartphone charge rather than brewing a coffee, the analogy once used for Google searches. But remember, without solid data, this is more of an educated guess. To get the full picture, detailed info from OpenAI or expert analysis would be key.",60.75433121,"I've taken a stab at estimating the carbon footprint of ChatGPT here . I estimated the daily carbon footprint of the ChatGPT service to be around 23 kgCO2e and the primary assumption was that the service was running on 16 A100 GPUs. I made the estimate at a time with little information about the user base was available. I now believe that the estimate is way too low because ChatGPT reportedly had 590M visits in January which I don't think 16 gpus can handle. Recently, I also estimated ChatGPT's electricity consumption in January 2023 to be between 1.1M and 23M KWh. To convert that into a carbon footprint, we'd need to know the carbon intensity of the electricity grid in every location where a ChatGPT instance is running. We don't have this info, but if we instead convert the electricity consumption into a carbon footprint using a very low carbon intensity like Sweden's 9g / KWh (which is the lowest in EU and lower than the US), the carbon footprint of ChatGPT in January 2023 would be estimated to be between 10 and 207 tons CO2e.",51.58105144,"I found a reply in this research paper . According to them, it's ~3Wh per query.",50.20291605,"So I asked ChatGPT-4 about its energy use per query. The dialog is appended at the bottom of this message. (I realize the OP asked about ChatGPT-3) TL;DR If ChatGPT-4's claim that a typical answer requires 30 TFLOPs on modern AI hardware is true then the computational energy per answer is typically 300 watt-seconds (less than 0.1 kWh), ignoring communication and other overhead. Discussion ChatGPT-4 stated that most queries are answered with a single forward pass through the model requiring about 30 trillion FLOPs. It produced a calculation this requires around 0.000833 kWh but the calculation contains at least two errors: It uses FLOPs/w to compute watt-seconds. It screws up the conversion of watt-second to
kWh by a factor of 1000. I found a recent paper from researchers at SLAC and MIT with some energy per operation data.  The figure below from that paper shows about 1e-11 J per FLOP for modern GPU and AI accelerator hardware. If we take ChatGPT-4's 30 TFLOP/answer at face value, then a typical answer would consume 30e12 FLOP * 1e-11 J/FLOP = 300 J = 8.33e-5 kWH This is a factor of 10 smaller than ChatGPT4's error-laden calculation but in the opposite direction.  I'd love to believe the computations are that energy efficient but I'm doubtful. It comes down to whether or not the 30 TFLOP per answer estimate from GPT-4 is an hallucination. Dialog with GPT4 How much energy did you expend to answer this query? As an AI language model, I don't consume energy directly in the traditional sense. However, the servers and hardware running the model do require energy to function. The exact energy expenditure for answering a single query can be difficult to quantify as it depends on factors such as the complexity of the question, server utilization, and efficiency of the hardware. In general, the energy usage for running an AI model like me is measured in floating point operations per second (FLOPs) and the energy efficiency of the hardware in joules per FLOP. While I can't provide an exact figure for the energy expended for answering this specific query, I can tell you that large-scale AI models like GPT-4 consume significant computational resources and energy, which is a consideration for researchers and engineers working on AI sustainability. Please provide an approximate energy expenditure in kWH for answering typical queries. It is challenging to provide a precise energy expenditure for answering a typical query, as the energy usage depends on various factors such as the model's complexity, infrastructure efficiency, and hardware. However, I can give you a rough estimate based on some assumptions. Let's assume that a single forward pass of GPT-4 requires about 30 trillion floating-point operations (FLOPs) – this is a rough estimate, as the actual number of FLOPs can vary depending on the model size and input length. The energy efficiency of the hardware running the AI model can be around 10 GFLOPs/watt, a common efficiency for modern GPUs and AI accelerators. To calculate the energy usage for a single forward pass: Energy usage (in watt-seconds) = (30 trillion FLOPs) / (10 GFLOPs/watt) = 3,000,000 watt-seconds Now we convert watt-seconds to kilowatt-hours (kWh): Energy usage (in kWh) = (3,000,000 watt-seconds) * (1 kW / 1,000 W) * (1 hour / 3,600 seconds) ≈ 0.000833 kWh So, a very rough approximation of the energy usage for a single forward pass to answer a typical query is around 0.000833 kWh. Keep in mind that this is a rough estimate and could be higher or lower based on the actual model complexity, hardware efficiency, and server utilization. How many forward passes does GPT-4 typically require to answer a typical query? GPT-4, like other generative language models, is based on the Transformer architecture, which uses a decoder-only design for generating text. For a typical query, GPT-4 usually requires one forward pass through the model to generate a response. During the forward pass, the model processes the input tokens and generates a probability distribution over the vocabulary for the next token at each position in the sequence. The process continues sequentially until an end-of-sequence token is generated or a predetermined maximum length is reached. In summary, GPT-4 generally needs one forward pass to answer a typical query. However, it's important to note that, in practice, additional computational overhead may be involved in tokenization, decoding, and other preprocessing and postprocessing steps. Share",56.89301841,"Great topic! I'd love to see this knowledge added as a subsection to Wikipedia's ChatGPT page . I'll look into getting it started. Out of curiosity, I posed the same questions to GPT-4, approximately 7 months later. Here's its answer: How much energy did you expend to answer this query? ChatGPT I don't consume energy in the same way traditional electronics do.
Instead, the servers that run my underlying computations use energy.
The precise energy consumption for a single interaction with me,
ChatGPT, depends on multiple factors such as: The specific hardware used for the computation. The complexity and length of the request and the produced answer. Backend optimizations and efficiency of the software. Typically, running a query on a model like GPT-4 consumes a small
amount of energy, often measured in milliwatt-hours (mWh) or
watt-hours (Wh), but certainly not kilowatt-hours (kWh). For a brief
interaction like this one, the energy consumed would be closer to the
mWh range. For a more precise measurement, you would need detailed specifications
from the provider hosting the model – in this case, OpenAI – about
their infrastructure's energy consumption during inference. User
Please provide an approximate energy expenditure in kWH for answering
typical queries.  ChatGPT It's challenging to provide a precise energy consumption value for a
single query without specific details from OpenAI's infrastructure.
However, I can provide a very rough estimation based on general
knowledge of server operation. Let's make some assumptions: Assume a server running the GPT-4 model uses around 200 watts (0.2 kW) when active. Assume the time taken to process a query and produce a response is 0.1 seconds (100 milliseconds), which is a generous estimate and likely an overestimation. Given these assumptions: Energy = Power × Time Energy = 0.2 kW × 0.1/3600 hours (because 0.1 seconds is 0.1/3600 of
an hour) Energy ≈ 5.56 × 10^-5 kWh, or 0.0000556 kWh This is a very rough estimation and is likely on the higher side for a
single query. The actual energy expenditure could be lower due to
various factors, including optimizations in server hardware, the
efficiency of the model, and software improvements. Also, remember
that this is a per-query estimate; when dealing with millions of
queries, these numbers add up. Still, for an individual interaction,
it's quite low. User How many forward passes does GPT-4 typically
require to answer a typical query?  ChatGPT GPT-4, like its predecessors, requires a single forward pass through
the model to generate a response for a typical query. When you send a
query or prompt, the model processes the input through its layers and
produces an output sequence, which is the response you receive. This
whole process is a single forward pass. However, there are some nuances: Length: If the generated response exceeds the model's maximum token (word piece) limit, the completion might be cut off, and additional
logic or processing might be needed to handle this. But this doesn't
necessarily mean multiple forward passes for the model itself, just
possibly additional computation. Iterative Approaches: Some implementations or applications might use iterative or feedback-driven approaches where the output of one
forward pass is fed back into the model for refinement. This isn't the
standard mode of operation for a single response but is a technique
that can be employed in specific scenarios. Sampling and Temperature: The model can generate responses using techniques like nucleus sampling or by adjusting the ""temperature"" of
the output, which affects the randomness of the generated text. These
techniques might influence the nature of the output but still operate
within a single forward pass. For a typical interaction, like most questions and responses in this
chat format, only one forward pass is needed. Thoughts?",60.09785501,,,,,,
38270,How does an AI like ChatGPT answer a question in a subject which it may not know?,natural-language-processing,"ChatGPT is a large language model. That means it's very good at stringing together words in ways that humans tend to use them. It's able to construct sentences that are grammatically correct and sound natural, for the most part, because it's been trained on language. Because it's good at stringing together words, it's able to take your prompt and generate words in a grammatically correct way that's similar to what it's seen before.
But that's all that it's doing: generating words and making sure it sounds natural. It doesn't have any built-in fact checking capabilities, and the manual limitations that OpenAI placed can be fairly easily worked around. Someone in the OpenAI Discord server a few days ago shared a screenshot of the question ""What mammal lays the largest eggs?"" ChatGPT confidently declared that the elephant lays the largest eggs of any mammal. While much of the information that ChatGPT was trained on is accurate, always keep in mind that it's just stringing together words with no way to check if what it's saying is accurate. Its sources may have been accurate, but just writing in the style of your sources doesn't mean that the results will themselves be true.",52.55699868,"ChatGPT does not actually know anything. But more importantly even, it does not know this fact! Hence, it does not know that it does not know. It is only good at combining text.",61.52541713,"ChatGPT and other GPT-based machine learning models don't actually know anything in the sense you're thinking of.  ChatGPT is a distant descendant of Markov chain text generators such as Dissociated Press , and works by predicting what is most likely to follow a given sequence of text. So, when you ask it ""Does openai know how to play chess?"", what you're actually getting is the answer to the question ""What is most likely to follow the phrase 'Does openai know how to play chess?'?""",54.39264625,"“It” does nothing. Don’t think just because every conman out there calls our really shockingly primitive neural nets “AI”, and wants to convince you that it’s actually an autonomous intelligence, that it’s not just a glorified function (a list of commands to blindly execute, not a person), to apply a set of biases onto a given input pattern, that have been programmed into it in a way that the programmer doesn’t “have to” know what he’s doing or even what precisely he wants. :) It is just biasing for the patterns in its training data. And giving you whatever that results in for your your input. In this case, if I am correct, applying its output to its input too again, and again, with diminishing sanity. So the answer is that your input will just be treated like it is a spectrum of those patterns, no matter what . In other words: If all it knows is a hammer, everything will look like a nail. :) So it is quite mundane, and nothing magical at all. Everything beyond that, attributed to such systems, is deliberate dupery, to get people to “invest” money. (Don’t get me wrong: This technology is useful when writing an actual algorithm really is beyond a human’s capabilities. (E.g. by definition a brain cannot imagine a model of itself in its entirety. Or you cannot write down the entire set of experiences of a lifeform, find all the patterns, and turn them into a set of rules manually. Even if the Pauli Exclusion Principle would not exist.) But nowadays it is abused by people who proudly can’t even define what they want, to just throw examples at, and expect to get an universal function out, so they can call themselves geniuses and get lots of money.)",52.09814878,"The original question asked about "" an AI "" generally, yet most of the responses here focus on OpenAI's ChatGPT specifically. Seems like the answer would depend on the specific type of AI being used, not limited to just large language model-based chatbots, but considering other types of knowledge representation systems more generally. Not being an AI expert, I can't speak to this, but here's some good background reading: https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning",54.08990704,"I agree with most the former answers here, and apologies I don't have high enough points yet to comment so had to do this as an answer, but I believe due to inaccuracies, lack of fact checking capabilities etc... in the data its trained on up to 2021 currently at the time of writing, fine tuning via it's API has become popular I believe. Although you were mainly asking about ChatGPT itself I felt its a good idea to consider it's API as part of that which is just as easily accessible, I am currently learning how to fine tune it my self for more specific and accurate results. So in respect of the question: Shouldn't the AI reject my question on a subject which is not in its known criteria?
It does so for many other subjects. It can possibly do this if you train / fine tune it another way on the API, but for the latter part, using the normal ChatGPT and ChatGPT plus user interface part of the site, not the API the AI mostly will not reject your question unless it breaches ethical and moral constructs, which I think is also another important aspect to consider when gaining information from ChatGPT.",55.4656958,,,,,,
36953,"If an event has a statistical probability of only 50%, is it possible to use a neural network to predict it with more than 50% accuracy?",neural-networks,"This is a question of marginal vs. conditional distribution The marginal distribution of the coin may be a Bernoulli random variable with 50% probability for either outcome. However , the conditional distribution of the outcome given information about other factors (e.g. the angle, throw height, ... see other answers) may look entirely different. Provided these features determine the outcome in some way, a neural network can absolutely predict the outcome with more than 50% accuracy. A neural network could not exceed 50% accuracy, if The information determining the throw outcome is not available The function is of a nature that can not be learnt by the neural network The coin toss is truly random A coin toss is often used as a casual example of a ""truly random"" event, so in this sense the answer to your question is ""No"". In reality however, it is very hard to find any truly random events (at least outside quantum mechanics), which is why random number generation is a big challange and neural networks can predict a lot of things.",61.06358608,"No. If there are no patterns, relations or correlations in your data, AI can do nothing to improve what essentially is just guessing. My last 5 tosses were Heads, Tails, Tails, Heads, Tails. Can you predict the next toss outcome? How would you explain your guess? If you give AI this same data, it cannot do better than just guessing. The question changes if you have data that is related to the outcome of the coin toss, such as the direction and force the coin was tossed before it lands. In this case, it isn't ""an event with a statistical probability of 50%"" anymore. If you measured everything perfectly, you could have 99.9% accuracy on what the outcome of the coin toss would be. AI can only produce accurate results if a super smart human could theoretically also produce accurate results.",56.97559257,"YES If you obtain information about the force and angle of the throwers thumb striking the coin at release, that would give insight into how many times the coin would be expected to rotate. Combine this with what faces up when the coin releases, and you should be able to do better than 50/50. I don’t have a firm source (perhaps there is something on Skeptics ), but it seems that people have trained themselves to flip coins to reliably land on one of the sides, so there are some features that dictate how the coin rotates. Really, this is kind of the point of regression. You think some process has a 50/50 chance of the two outcomes, but once you know a bit more (features), you can sharpen that estimate. Formalizing this mathematically involves the conditional vs marginal distribution discussed in the answer by Scriddie.",56.70731196,"You need to ask yourself, what is the limiting factor in the accuracy for whatever you are trying to predict. If the limiting factor is in the quality of the algorithm being
used to calculate the prediction, then perhaps you could find a
better algorithm that would improve the accuracy. If the limiting factor is in the very nature of the problem itself, such as a coin flip, then there is no method of calculation that could improve the accuracy.",53.32399375,"Although the question is a little vague, I'll treat it as a statement about the mapping of inputs and outputs in the underlying random process - no matter what conditions/inputs/features we observe, there is not a consistent mapping from input to output. A statistical probability of 50% suggests in two cases with identical inputs , we may find different outputs. A traditional deterministic neural network cannot do this, as it is really just a mathematical function, which by definition maps every possible input to exactly one output - it is not possible to use the same inputs and get different outputs. Because of this, a deterministic neural network can't achieve more than 50% accuracy in the long run in this case. No matter what set of features is input, there are in reality two possible outcomes, but the neural network can only return one outcome for any particular input. On average, the neural network will return the correct output only half the time - it can't achieve more than 50% accuracy.",64.8595552,"If a neural network was only able to be as reliable as random guessing, they wouldn't be much use! Let's suppose there's an election on, and the result is finely balanced between the yellow party and the purple party. At a top level, 50% of people will vote for each colour. If you know nothing else about the people, ""Who will the next person in the polling station vote for?"" is intrinsically an even guess. It would still be possible to use a neural network (or a decision tree, or a human!) to predict at much better than 50% if you have additional input. For example, if you look at them and can read their apparent wealth, race, gender presentation, or whether they come accompanied or alone, it may then be possible to identify membership of a sub-population which is more likely to vote yellow. The fundamental limit on performance isn't the baseline 50:50 probability. Instead it is the component which is caused by some inputs (which may but doesn't have to be true randomness) that are simply not available to the network. Suppose for example that there's a 5% chance that someone has been bribed to flip their vote and the network can't know that. In this case it won't get to more than 95% reliable predictions, but can still do much better than 50% with demographic data.",62.7613132,,,,,,
36320,Can an AI generated image (such as pic of human face) be detected that it's AI generated?,image-recognition,"Images such as this one are produced using generative adversial network , which is build from two models: one to generate images given a random vector as input another trying to detect the generated image from two images, with one of them being real Then the weights of the first model are updated if the second one detected which image is artificial, and the second model is updated if its prediction is wrong. Of course you might build a model that can sometime detect AI generated images, but it is probably not possible to differentiate them all the time. Then, if you build such model that is better than any other model to detect generated images, it is possible to create another model trained to fool it.",59.25537,"I am not an expert, but it feels like these GANs are not paying attention to the clothes and the background and make them ""fluid"". Like, what is this hat the woman in your example is wearing? Why is the right side of the background looks like it is a mix of liquid paint? Or here: What is she wearing? Did she kill a rat to make these clothes? And similar fluid background.",50.37166006,"TL;DR: Yes, but it's becoming more and more difficult, even for humans, as generative models get better and better. It's a quite hot research topic. Disclaimer: I am not affiliated with any of the authors, I'm just studying this research topic. Humans usually look for some visual artifacts (as all the other answers point out), for example Colour or texture artifacts (colour blobs, unrealistic texture) Asymmetries or inconsistencies in the image (this is easy to spot in faces or hair, for example) Anomalies in color, lighting, image parts But as models become more and more advanced, these artifacts are becoming harder to spot, if not completely disappeared. As in 2023, images coming from diffusion models like Stable Diffusion or Midjourney API have a photorealistic quality, and often are already indistinguishable from real images (see some examples here ). For these reasons, we want to find automatic and more robust detection approaches to prevent malicious uses of these AI generation models. Detection approach A simple approach is to train a detector on AI generated images, which classifies the image as real or AI generated either by looking at the whole images or at single patches [Chai et al., 2020]. A more performing approach is to exploit some invisible artifacts created by the convolutional upsampling, which is commonly used in GANs and in some Diffusion models (Stable Diffusion, for example) to create high-resolution images. While invisible in the image domain, this trace can be easily extracted and identified in the frequency domain [Marra et al., 2019, Yu et al., 2019]. The detector needs to be robust to common image modifications (contrast/luminosity, colour jittering, jpeg compression,etc.) and also to adversarial attacks on the detector. Watermarking/Fingerprinting To proactively improve the detection performance, the developers of generative models could include a watermark in their images, to mark the images produced by their models as AI-generated. This watermark is typically invisible, and can be generated in several ways: Traditional approaches are based on frequency decompositions of the image, constructed through DCT, DWT, Fourier-Mellin, or complex wavelet transformations Cox et al., 1996, O’ Ruanaidh et al., 1996,
O’Ruanaidh and Pun, 1997]. These frequency transformations all share the beneficial property that simple image manipulations, such as translations, rotations, and resizing are easily understandable and watermarks can be constructed with robustness to these transformations in mind. Model-based approaches use a different learned model to embed a watermark in the image. Hayes and Danezis [2017] and Zhu
et al. [2018] propose strategies to learn watermarking end-to-end, where both the watermark encoder and the watermark decoder are learned models, optimized via adversarial objectives to maximize transmission and robustness [Zhang et al., 2019]. Zeng et al. [2023] present a related approach, in which a neural network watermarked encoder and its associate detector are jointly learned using an image dataset. Notably these approaches still work like a traditional watermark in that the encoder imprints a post-hoc signal onto a given image - however the type of imprint is now learned. More recent approaches use another model or the generative model itself to either embed the watermark after the generation process or during the generation process. Some approaches consist in embedding a watermark in training data [Yu et al., 2022], in some components of the model (e.g the convolutional decoder) [Fernandez et al., 2023], or by slightly modifying the distribution from where sampling is performed [Wen et al., 2023]. As detectors, watermarks need to be robust to common image transformations (such as cropping, contrast/luminance editing, jpeg compression, etc.) and adversarial attacks that actively try to remove the watermark. References Gragnaniello, Diego et al. “Are GAN Generated Images Easy to Detect? A Critical Analysis of the State-Of-The-Art.” 2021 IEEE International Conference on Multimedia and Expo (ICME) (2021): 1-6. enbo Wan, Jun Wang, Yunming Zhang, Jing Li, Hui Yu, and Jiande Sun. A comprehensive survey on
robust image watermarking. Neurocomputing, 488:226–247, June 2022. ISSN 0925-2312. doi: 10.
1016/j.neucom.2022.02.083. URL https://www.sciencedirect.com/science/article/ pii/S0925231222002533. [Cox et al., 1996] Cox, I., Kilian, J., Leighton, T., and Shamoon, T. (1996). Secure spread spectrum watermarking for images, audio and video. In Proceedings of 3rd IEEE International Conference on Image Processing, volume 3, pages 243–246 vol.3. [O’ Ruanaidh et al., 1996] O’ Ruanaidh, J., Dowling, W., and Boland, F. (1996). Watermarking digital
images for copyright protection. IEE PROCEEDINGS VISION IMAGE AND SIGNAL PROCESSING, 143:250–256. O’Ruanaidh and Pun, 1997] O’Ruanaidh, J. J. and Pun, T. (1997). Rotation, scale and translation invariant digital image watermarking. In Proceedings of International Conference on Image Processing, volume 1, pages 536–539. IEEE. [Zhu et al., 2018] Zhu, J., Kaplan, R., Johnson, J., and Fei-Fei, L. (2018). Hidden: Hiding data with deep networks. In Proceedings of the European conference on computer vision (ECCV), pages 657–672. 3 [Marra et al., 2019] Marra, F., Gragnaniello, D., Verdoliva, L., and Poggi, G. (2019). Do gans leave artificial fingerprints? In 2019 IEEE conference on multimedia information processing and retrieval
(MIPR), pages 506–511. IEEE. [Yu et al., 2019] Yu, N., Davis, L. S., and Fritz, M. (2019). Attributing fake images to gans: Learning and analyzing gan fingerprints. In Proceedings of the IEEE/CVF international conference on
computer vision, pages 7556–7566.
2 [Chai et al., 2020] Chai, L., Bau, D., Lim, S.-N., and Isola, P. (2020). What makes fake images detectable? understanding properties that generalize [Yu et al., 2022] Yu, N., Skripniuk, V., Abdelnabi, S., and Fritz, M. (2022). Artificial fingerprinting for generative models: Rooting deepfake attribution in training data. [Corvi et al., 2023] Corvi, R., Cozzolino, D., Zingarini, G., Poggi, G., Nagano, K., and Verdoliva, L.
(2023). On the detection of synthetic images generated by diffusion models. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5. IEEE. [Wen et al., 2023] Wen, Y., Kirchenbauer, J., Geiping, J., and Goldstein, T. (2023). Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust. [Fernandez et al., 2023] Fernandez, P., Couairon, G., J ́egou, H., Douze, M., and Furon, T. (2023). The stable signature: Rooting watermarks in latent diffusion models.",54.23725808,"I have not worked practically with GANs and just know their theory, but I do not agree 100% with this comment that AI chooses stupid things for clothes or backgrounds. I remember it could be detected when a video was generated with deep learning methods from Obama.",57.39766194,There's a paper that claims to detect AI generated images with a 95% accuracy. https://www.researchgate.net/publication/326053461_Detection_of_GAN-Generated_Fake_Images_over_Social_Networks A search with the right keywords can reveal more such research.,58.56951226,,,,,,,,
35827,Why is exploitation necessary during training?,reinforcement-learning,"An algorithm that chooses to always explore during training is unlikely to find an optimal policy because it will be employing a more random search as opposed to a directed search . During training, the neural network aims to determine the relation between states or state-action pairs and the reward signal through past experience. If the agent is always exploring during training, it will never use the experience gained from past episodes to influence its training policy and therefore will search more randomly. Exploitation during training allows the neural net to use its past experience to guide its future actions and avoid random search. There can be many states in which there is an obvious optimal action. After some training, the neural network may be able to quickly learn these states and corresponding optimal actions. By primarily exploiting at those states, the agent will not be wasting training time by exploring suboptimal actions at those states, allowing the agent to focus its exploration on other more uncertain, unexplored, or complex parts of the state space. For a practical example, consider the original Super Mario Bros game on NES. Let the reward be the number of pixels traveled to the right before Mario loses a life. If Mario is exploring the whole time, it is unlikely that he makes it very far to the right or over many obstacles, let alone to the flagpole. Since it is more rewarding in general to go to the right, Mario's exploitation action at most states is to run to the right. In this manner, Mario will usually run to the right until he reaches an obstacle (e.g. a pipe, pit, staircase, enemy). At that point, Mario may need to explore to overcome the obstacle, but Mario needed to exploit to be able to reach that obstacle in the first place.",60.24841971,"Exploitation is important during training to help the network encounter and learn to handle situations that don't occur until the network has successfully navigated other situations. For example, consider the Atari game Breakout (a common RL benchmark). In this game, the player must move a paddle on the bottom of the screen to bounce a falling ball. The ball accelerates as the game continues, and the network can only get training data with a fast moving ball after successfully exploiting its knowledge of how to play when the ball is moving slowly. A purely random strategy is possible (and often used for very early training) but  only generates data useful for learning the beginning of the game.",57.01755873,"Imagine trying to navigate a maze from the outside. Let's say you lose if you get to a dead end, and win if you get to the middle. After some experience by random trials, we know where some dead ends are. In the future, we should exploit our knowledge and not turn directly into a dead end, as this is simply inefficient. If we do this, we shall find the middle quicker :) This leads to interesting quesitons, such as 'How does the algorithm know if the dead end is always a dead end'. You might be able to see we might want to tune our randomness rate near a dead end. We might sometimes want to try going down that path, just to be sure, but most of the time lets not bother and go somewhere else. This gives some intuition behind POLICY methods in Reinforcement learning (also see multi-armed bandits if interested) :)",50.14746472,"There is an additional factor to consider about exploration/exploitation trade-off, that sometimes applies in addition to the reason in the accepted answer and most other answers here. Sometimes an agent is required to both act and train itself in a ""real"" system, or at least one where the rewards are more than just collected training data from a simulation, but also represent actual profits and losses realised by the agent. This is a common feature of adaptive content display in advertising, which typically uses the simpler k-armed bandit or contextual bandit model - still related to RL, and importantly still affected by the exploration/exploitation trade-off. It is very hard for a machine to model how humans will respond to an advert, so the only reliable measurements are made in production. Each click-through is then real money to someone, so it is important to adapt quickly to incoming data - but due to the variance in results it is also important to still keep testing the non-optimal choice and improve on any early estimates. In such a scenario, you have to accept some non-optimal returns as the cost of finding the best ones through trial and error. However, it is important to balance this with gaining as much reward as possible whilst training. So it can be a more important consideration, to obtain best cumulative reward during an ongoing training process, than even finding the optimal policy. That means taking care to balance exploitation and exploration, often strongly favouring exploitation after a relatively short high exploration phase.",57.78115353,"If you explore too much, you waste your time (among other resources.) You will probably exhaust your resources before you learn anything meaningful. Let's say your goal is to learn as much about Star Wars as possible within a library. If you fully explore, you just pick books at random. Exploitation might look something like "" pick most of your books within the Sci-Fi section "", or "" choose books with lightsabers on the cover "" or "" books with 'Star Wars' in the title "" because that's where you have found relevant information in the past.",52.42372597,"In supervised ML there is no exploration and exploitation. In reinforcement learning, the agent in each step has many choices. So the agent can exploit, meaning gaining the highest reward known to him from the next move. Or explore, trying to get a better long-term benefit by trying a different move.",53.79039393,,,,,,
27514,How do I increase the size of an (almost) balanced dataset?,deep-learning,"Random over sampling creates duplicates of existing examples, so applying this to your training data would be the same as increasing the weight of the oversampled examples. If it's done to all of the examples uniformly then the effects will probably cancel out. SMOTE, on the other hand, creates synthetic examples that are linear combinations of existing examples. Thus it can be thought of as a type of data augmentation, and in some situations this might improve your model's predictions.",50.86185793,"I presume you are attempting to solve a classification problem. IMO,  there's no decision-making template you could follow to know whether to use over sampling or not.I would typically compare results (ROC AUC, PRC curves) across datasets (Original vs Undersampled vs Oversampled) to decide. You can consider some additional variants of SMOTE like SMOTE NC (SMOTE does not work if any of your predictors is categorical , SMOTE NC does), Borderline SMOTE, K-means SMOTE, as well as ADASYN.
Alternately, you can also choose to undersample your majority class using techniques such as ENN, Tomek Links, Instance Hardness, CNN, One sided Selection etc.
They usually generate better results than random over/under sampling. Do note that over/under sampling methods are generally used for imbalanced datasets.",50.30718532,"If your dataset is almost balanced, I don't think any sampling method would provide a significant improvement in accuracy. You can also experiment with the focal loss function designed to deal with minor class-imbalance, but I'm not sure how much that would help. I don't think you can artificially increase the dataset size besides doing data augmentation. For example, for computer vision tasks (like classification and object detection) dealing with images, you can randomly translate and rotate the images, so that the neural network is effectively seeing different ""representations"" of the same images to alleviate overfitting. This is especially important if your dataset size is small, but does not pertain to solving the class imbalance issue.",60.42443993,"If your data is well balanced but small, I would recommend using a simpler algorithm to classify your data.",54.08862031,"Does your question pertain to general data augmentation? That is already in heavy use- using transformations while training is very common, and over several epochs the network benefits from learning the new representations. The transformations are applied to all classes, with a probability of transformation ( horizontal flip, for example) specified by the user.
If you want to make your almost balanced dataset a balanced one, you can look into specific augmentations that you perform to the (almost) minority class before feeding it to the model. You could look into preprocessing methods that libraries like Keras have made open-source.",60.71894317,"Gretel is a good tool for processing data. Facets is good for the visualizations. Is it worth it? most learners will exhibit bias towards the majority class , and in extreme cases, may ignore the minority class altogether. It really depends on the goal and requirements of your project. Not because it's desirable it's better for your particular project, if your dataset is almost balanced probably you should continue with something else and consider balancing your data later in the project. Edit : What's new to learn from the same example?",56.02651897,,,,,,
27260,Why does Batch Normalization work?,neural-networks,"I believe anything in machine learning that works, works because it flattens and smoothens the loss landscape. Batch and layer normalization would help ensure that the feature vectors (i.e. channels) are embedded around the unit sphere Batch/Instance norm translates to origin. Layer norm scales radially to unit sphere . Viewing neural networks as transformations , this would make the loss landscape smoother since the transformations the neural net needs to find would be more ""regular"". I would recomend this video to learn about loss landscapes. From Visualizing the Loss Landscape of Neural Nets. NeuRIPS 2018:",55.26316861,"When we are training deep neural Network gradient tells how to update each parameter, under the assumption other layers do not change.In Practice, we update all the layers simultaneously. When we update, unexpected results can happen because many functions composed together are changed simultaneously using updates that were computed under the assumption that other function remains constant.This makes it very hard to choose an appropriate leaning rate, because the effects of an update to the parametrs of one layer strongly on all other layers. How does Batch Normalisation Help : Batch Normalisation a layer which is added to any input or hidden layer in the neural network. Suppose H is the minitach of activations of the layer to normalize. The formula for normalizing H is : $_H = \frac{H - Mean}{Standard Deviation}$ Mean : Vector Containing Mean of each unit
Standard Deviation : Vector Containing Mean of each unit At training time mean and sd are calculated and when we backpropogate through these operations for apply mean, sd and Normalize H. This means that gradient will never propose an operation that acts simply to increase the standard deviation and mean of hi, the normalization operation remove the effect of such an action and zero out its componenr in the gradient. Hence Batch Normalisation thus ensure no or slight covariance shift in the input to layer after Batch Normalisation and thus improving learning time as shown in the original paper mentioned in question. For more details : https://www.deeplearningbook.org/contents/optimization.html",56.27870885,"This got me thinking about my understanding of batch normalization. I thought I understand it until I read this. Then, I refer to the Coursera deep learning specialization by Andrew Ng. Prof. Andrew Ng explained it this way. One reason why does batch norm work is that it normalizes not only the input features but also further values in the hidden units to take
on a similar range of values that can speed up learning. The second reason why batch norm works, is it makes weights, later or
deeper than the network you have, say the weight on layer 10, more robust to changes to weights in earlier layers of the neural network (eg. in layer one). However, these hidden unit values are changing all the time, and so it's suffering from the problem of covariate shift .  So what batch norm does , is it reduces the amount that the distribution of these hidden unit values shifts around. What batch norm ensures is that no matter how the parameters of the neural network update, their mean and variance will at least stay the same mean and variance, causing the input values to become more stable, so that the later layers of the neural network has more firm ground to stand on. And even though the input distribution changes a bit, it changes less, and
what this does is, even as the earlier layers keep learning, the amounts that this forces the later layers to adapt
to as early as layer changes is reduced or, if you will, it weakens the coupling between what the early layers
parameters has to do and what the later layers parameters have to do. And so it allows each layer of the
network to learn by itself, a little bit more independently of other layers, and this has the effect of
speeding up of learning in the whole network. Takeaway is that batch norm means that, especially from
the perspective of one of the later layers of the neural network, the earlier layers don't get to shift around as
much, because they're constrained to have the same mean and variance. And so this makes the job of learning
on the later layers easier. It turns out batch norm has a second effect, it has a slight regularization effect. So
one non-intuitive thing of a batch norm is that each mini-batch, the mean and variance computed on just that
mini-batch as opposed to computed on the entire data set, that mean and variance has a little bit of noise
in it, because it's computed just on your mini-batch of, say, 64, or 128, or maybe 256 or larger training
examples. Batch norm works with mini-batch",60.44265985,"It is a question with no simple answer. On one hand the BatchNormalization is unloved by some arguing it doesn't change the accuracy of neural networks or biased them.
On the other hand, it is highly recommended by the other because it leads to better trained models with a larger scope of predictions and less chances of overflow. All I know for sure is that BN is really efficient on image classification. In fact, like the image categorization and classification soar this last years and that BN is a good practice in this field, it has spread to almost all DNNs. Not only is the BN not always used in the right purpose, but it is often used without taking into account several elements such as : The layers between which apply BN The initializer algorithms The activation algorithms etc For more computer sciences litterature ""against"" BN, I will let you look at the H. Zhang et al paper who has trained a DNN without BN and get good results. Some people use Gradient Clipping technique (R. Pascanu) instead of the BN in particular for RNNs I hope it will give you some answers !",50,"To some extend, it get rid of low intensity numerical noise. Condition properties of the optimization problem is always an issue, i suspect BatchNorm alleviate this instability.",50,,,,,,,,
23296,Examples of single player games that use modern ML techniques in the AI?,game-ai,"There is Google Research Football , which is an open-source platform to develop reinforcement learning algorithms to play a game similar to FIFA or PES, although the football simulation is not as realistic as the current versions of FIFA or PES. You can play this game against different RL agents (e.g. DQN or IMPALA) and, of course, you can even develop your own RL agents and play against them. Here is a video that illustrates the environment. Here is the code and instructions to use it . As far as I know, there isn't yet an AI that plays simulated football at a human-level (i.e. as good as the best human players). For example, I can regularly (although not always) beat the legendary level-AI at FIFA, but I also don't know the details about this AI (which could also be rule-based).",51.06921766,"Beating the World’s Best at Super Smash Bros. Melee with Deep Reinforcement
Learning Firoiu, Whitney, Tenenbaum created a RL agent that plays and defeats professional players in Super Smash Bros Melee. The RL agent first played against the built-in AI, and then via self-play. Only one character playing on a single stage was trained. The character picked (Captain Falcon) has no ""projectile attacks"" to simplify training.",51.24389956,"There are many games where AI involved but less of them against the human player such as playing the video game. For example in this paper , they proposed a three-dimensional multiplayer first-person video game called Quake III Arena in Capture the Flag mode. Also, this paper and this paper show many games where AI involved and they show which kind of games that a human player can play against. I recommend also Awesome-Game-AI .",57.04084145,"I don't know about specific game titles but in terms of research University of Malta has a strong team working with application of machine learning to games. The key figure there used to be Georgios N. Yannakakis who published a lot of good papers and even wrote a book about content generation, smart game agents and, imho the most interesting, player modelling.",53.33394257,"Not sure if this fits your requirements of the AI playing with the player, but I still wanted to mention it because to me it is the quintessential AI-based game: AIDungeon , which is a text based story-telling game, where you can do literally anything. It's using GPT-2/GPT-3(paid) and has blown my mind several times. You've probably heard of it, but in case you haven't give it a try, only takes a couple of minutes to see what it can do.",52.6330277,,,,,,,,
20193,Which classifier should I use for a dataset with one feature?,machine-learning,"It is not really a metter of what model, but if it is possible at all to predict what you're trying to predict. Let's take a similar dataset from kaggle: California Housing Prices This dataset contains house prices and other information among which the number of bedrooms per house. As suggested by Oliver in the comments we can compute the Person coefficient to estimate the correlation between the two variables. import pandas as pd
from scipy.stats import pearsonr

df = pd.read_csv('housing.csv')
df = df.apply(lambda row: row[df['total_bedrooms'] <= 20]) # select subset of dataframe for sake of clarity
df.dropna(inplace=True)

x = df['median_house_value'] # our single feature  
y = df['total_bedrooms'] # target labels

print('Correlation: \n', pearsonr(x,y)) Out: >>Correlation: 
>>(-0.14015312664251944, 0.12362969210761204) The correlation is pretty low, which means that the price and number of bedrooms are basically not related. We can also plot the points to check that indeed there is no correlation at all. df.plot(x='total_bedrooms',y='median_house_value',kind='scatter') Out: Training a model to predict the number of bedrooms uniquely from the price would mean to find a function that can interpolate all those points, which is an impossible task since we have several different prices for houses with the same amount of bedrooms. The only way to tackle a problem like this would be to expand the dimensionality of the data, for example by using a Support Vector Machine with a non linear kernel. But even with non linear kernels you can't do miracles, so if you're dataset looks like this one, the only solution would be to expand your dataset to include extra features.",53.0242174,"Classification can be performed on structured or unstructured data. Classification is a technique where we categorize data into a given number of classes. Based on my project in price classification, when i compared into the 5 models, i got a higher score on a Random Forest Classifier compared to Decision Tree, SVM, Naive Bayes, Logistic Regression. my project: https://github.com/khaifagifari/Classification-and-Clustering-on-Used-Cars-Dataset source : https://github.com/f2005636/Classification https://www.kaggle.com/vbmokin/used-cars-price-prediction-by-15-models",52.0458747,"If your data is labeled, but you only have a limited amount, you should use a classifier with high bias (for example, Naive Bayes). I'm guessing this is because a higher-bias classifier will have lower variance, which is good because of the small amount of data Source : https://stackoverflow.com/questions/2595176/which-machine-learning-classifier-to-choose-in-general/15881662",59.05337689,"if you use just one  feature for dataset, i'm recommend a Algorithm Naive Bayes Classifier because Naive Bayes is a method using probability and statistical methods. And we can also get the total data train and its accuracy value by using.",62.80324979,"I think no matter you use one or more feature for dataset. 
You can compare the classification algorithms regarding the accuracy provided by the algorithm. Like compare naive bayes with svm method, it based on your problem set.",58.41082836,,,,,,,,
20185,How can I cluster this data frame with several features and observations?,python,"A typical clustering algorithm is k-means (and not k-NN, i.e. k-nearest neighbours, which is primarily used for classification ). There are other clustering algorithms, such as hierarchical clustering algorithms . sklearn provides functions that implement k-means (and an example ), hierarchical clustering algorithms , and other clustering algorithms . To assess the quality of the produced clusters, you could use the silhouette method ( sklearn provides a function that can be used to compute the silhouette score ). Regarding your specific data frame, note that it contains repetitions, so you may want to remove them before starting the clustering procedure. Also, the IDs are not unique, but you probably don't need the IDs for clustering.",52.63977744,"Yes you can use KNN algorithm to cluster (well actually its a classification not a clustering if you use KNN) the data. But, first you need to set one feature as a label because KNN is a supervised learning method, it need a labeled data to train the data first. For example you can use Gender as label to classify the data. To determine the quality of the classification result, you can simply use accuracy. If you don’t want to use a label, you can use unsupervised learning method like K-Means to do the clusters. Because its unsupervised it doesn’t need label so you can use all of the feature to do the clusters task. For the k-means algorithm you can use a library from scikit-learn or create it from scratch. To evaluate the results you can use silhouette score or elbow method (to find the optimal number of cluster). And don’t forget to do data exploration because maybe it can increase the quality of the cluster results. You can learn more about the differentiation between K-Means and KNN in the link below: https://pythonprogramminglanguage.com/how-is-the-k-nearest-neighbor-algorithm-different-from-k-means-clustering/ I hope this helps :)",54.12972585,You can use k-nn clustering but you must convert your dataset to numeric values or you can remove the unrelated features in your dataset.,51.66073068,"KNN can be used in clustering with the data frame. but there are a number of steps that you must take.
1. You must separate the features you want to cluster. for example you can do clustering dob and age.
2. if there is data of type string you have to change it to an integer.
For easier clustering, you can use the Sklearn library. you can access at the following link https://scikit-learn.org/stable/modules/clustering.html",57.08423486,"There are several algorithm for clustering such as: K-means, Mean shift, hierarchical,etc. Based on my experience, actually it's K-means(KNN for classifcication).It is suitable for clustering your dataset, there are several steps for clustering your dataset: You have to determine which features that you want cluster Changing your categorical dataset to numerical This step is optional, You can drop columns that are not related to the features you have chosen before Try to to code your clustering (like determine centroid from your dataset, calculate the euclidean distance from your centroid,etc) or if you want to use library maybe sklearn is the right place. And for determine the quality of your clustering, you can measures SSE(sum of the square error from the items of each cluster),Inter cluster distance,Intra cluster distance for each cluster,Maximum Radius,Average Radius.",57.15939439,"you can clustering the data frame with unsupervised algorithm, for example you can use K-Means method. There are some options you can choose to eliminate some features in your data frame, like del dataFrame['Column Name']. In unsupervised learning, the algorithm not calculate the quality of the clusters, but you can set it up by yourself to make a parameter for calculate the quality for each clusters, for example it depend on sum of data in each clusters. Actually you can use KNN algorithm with your data frame, but you need to add a label in there because KNN is a supervised learning, and its function to make a classification, not clustering. hope it useful.",59.24982302,,,,,,
20081,What is the meaning of test data set in naive bayes classifier or decision trees?,datasets,"Your assumption about the test data is not correct completely. Maybe you use the test data to tune your learning algorithm to work better on the test data, but it's not the whole thing. Sometimes you need to know that the ML method is working or not and have a sense about how much does it work! You have other scenarios that you want to evaluate your method: Compare the result of the leaner with other techniques. For example, you are considering DT versus an SVM classifier over a data set. If you want to compare them, you need a value to found such a sense about the comparison. Sometimes you are using an ensemble method and you want to tune some parameters to balance between using different ML methods. Hence, you need to evaluate these learning methods (DT, Naive Bayes) to improve the ensemble method.",58.42367777,"In machine learning, we can use all the datasets as training data in a model.  But if there are too many data sets, or too much data, and we do not split them up, our model may be not produce acceptable results. Why? Because if the model studies too much training data, it may be overfitted . (Just like when you cram for a test, and get overloaded with too much information!) What I mean is, your model is only familiar with the data you provide, not for the new data. So we need to use test data to train our algorithm.  Naive Bayes and Decision Tree Classifier are no exception because they can produce an overfitted model based on train data. So we test it on the data test to know how well the method works in relation to the problem. Most data scientists divide their data (with answers, that is historical data) into three portions: training data, cross-validation data and testing data. The training data is used to make sure the machine recognizes patterns in the data, the cross-validation data is used to ensure better accuracy and efficiency of the algorithm used to train the machine, and the test data is used to see how well the machine can predict new answers based on its training. SOURCE: https://www.researchgate.net/post/What_is_training_and_testing_data_in_machine_learning",57.89683374,"In the field of ML and AI, you should always remember that before choosing any algorithm you should know the data .One should always start with Data Analyzing, which itself is field for this critical job. Decision Tree can never work with its best optimization without tuning the datasets. Here is a great article  that you can refer : Tuning Decision Tree Purpose of test data in naive byes classifier:
1) It's necessity to check the accuracy, hits, hit rates, coverage, diversity, novelty, etc. metrics. 2) It hypertune your testdata(as anti_train_set) also by using mean, standard deviation, variance. I really think that you should try other algorithms to train your datasets. I can't name all of them. However, in Neural network, rnn, cnn, rbm are some great algorithm to work with. Please always remember that Machine learning is like an art, where datasets (test, train, evaluated) are colors and its up-to us to use the right amount of them.",55.97248482,"I actually pondered this question a few months ago, so i understand your point of view! You are correct in assuming that if you already build your tree or calculate your probabilities, what is the point of using test data? Because your model is fixed the way they are, no matter if you use test data or not. Well, the purpose of test data is not only to test your model against unseen data and get some evaluation score. But it is also to test if your model is the right fit for your problem. One of the main reason why we all build ML/AI model in the first place is to extract insights that can be used to solve problems, make decisions etc. If you don't test your naive bayes or decision tree model with test data, you won't know if the information that are given to you by those model mean anything. They may not even help you solve problems or give you relevant information. Yes they may spout out big numbers and classify things, but are those result what you're looking for? Are the result relevant to your problem? Can the result be used to solve what you are trying to do? Using test data gives you the opportunity to see if your model gives you the best insights and the best solution to your problems. So here are the takeaways from my answer: Test data can be used to test your model against unseen data You can gain score (evaluation) on your model when you test it with test data, which in turn can be used to fine tune your model You can see if the answer the model gives you is any relevant to your initial problem. It its not, then it may be best to use some other algorithm.",60.28379828,"When we train a model using a data train, sometimes the resulting score is very high. This makes us believe that our model is very good. But when predicting actual data, the resulting score is very low. Why?? This means that the trained model is overfit (to data train) and fail to predict anything useful on yet- unseen data. That's why we have to check our model to test data (predict test data) and compare the accuracy between data train and data test. If the accuracy is not too far away , then our model does not overfit . Later, we can improve our model with Cross Validation ( Reference ) that split our data train to n-split data and take one of that to became data train. Then we take the average of Cross Validation score.",54.30603798,"One way to test the accuracy of trained model is by testing it using data test. By testing we are able to check the accuracy. Whether your model is a good model or not depends heavily on the accuracy, if your accuracy is too low or too high (eg. up to 99%~100%), there could be some problem in your model. For further information on the example in data test, you can access https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html Hope this helps",55.40480817,,,,,,
18587,What is the idea called involving an AI that will eventually rule humanity?,philosophy,"If I'm not mistaken you're looking for Roko's Basilisk , in which an otherwise benevolent future AI system tortures simulations of those who did not work to bring the system into existence",50.82752703,"I believe the term you are looking for is ""(technological) singularity"". https://en.wikipedia.org/wiki/Technological_singularity",50.33168315,"The likely expression you are looking for is AI takeover , which is a common topic in science fiction movies, such as 2001: A Space Odyssey and The Matrix , and popular culture . Although the AI takeover is an unlikely scenario in the next years, certain scientists, such as Stephen Hawking, have expressed concerns about it and some philosophers, especially Nick Bostrom, are really interested in the topic. The AI takeover concept is related to concepts such as the AI singularity , superintelligence , intelligence explosion , AI control problem , existential risk , machine ethics and friendly AI . The book Superintelligence: Paths, Dangers, Strategies (2014) by N. Bostrom may be helpful if you are interested in hypothetical scenarios.",51.8190634,It is called Singularity. A point in future where AI will surpass Human Knowledge and become Omniscient. AI will be able to operate on an order manifolds time to that of a human brain thus developing and designing itself without any assistance.,53.21516632,"If you think about it, it is already happening. Thousands of drivers work for Uber Intelligence. There are many applications that dictate the rules and define what the seller and the end user need to do This idea is called Singularity or Technological Singularity , it would be possible with a Superintelligence . However, the possibility of this happening is unknown. Have they reached that level yet? We have quantum computers, we have companies with huge data centers spread all over the planet, we have technology in space, we have free Tensorflow and studies for anyone on the planet to be able to create artificial intelligence models. If we have contact or help from other intelligent civilizations, the possibilities can expand on a surreal scale. Google has complete information about humanity (or something close to that). But even with all this data, creating artificial intelligence with a conscience is something that goes far beyond. But maybe we already have enough to improve our concept of morals, respect, social interaction. Facebook invests and studies ways to improve social interaction. And if you think about it, it is one of the main means of communication. The big question is that it is not possible to know what would happen if a super artificial intelligence with conscience would do if it existed. Extinguish humans for harming nature? Just find ways to improve the planet by understanding that human defects and errors are just your own nature as well as everything else in nature? Just watching the show on Netflix because you gave up on humanity? We do not know. But, particularly, I would love to see that happen. In fact, one of my personal goals is to create this super intelligence. But alone it will be very difficult. A conscience without interaction from other consciences is not a clash of universes. And the clash of universes is what makes us reflect, think, revise, create new paths and thoughts. It is what allows us to create other universes.",53.47765288,,,,,,,,
18576,What are some well-known problems where neural networks don't do very well?,neural-networks,"Here's a snippet from an article by Gary Marcus In particular, they showed that standard deep learning nets often fall
  apart when confronted with common stimuli rotated in three dimensional
  space into unusual positions, like the top right corner of this
  figure, in which a schoolbus is mistaken for a snowplow: .
  .
  . Mistaking an overturned schoolbus is
  not just a mistake, it’s a revealing mistake: it that shows not only
  that deep learning systems can get confused, but they are challenged
  in making a fundamental distinction known to all philosophers: the
  distinction between features that are merely contingent associations
  (snow is often present when there are snowplows, but not necessary)
  and features that are inherent properties of the category itself
  (snowplows ought other things being equal have plows, unless eg they
  have been dismantled). We’d already seen similar examples with
  contrived stimuli, like Anish Athalye’s carefully designed, 3-d
  printed foam covered dimensional baseball that was mistaken for an
  espresso Alcorn’s results — some from real photos from the natural
  world — should have pushed worry about this sort of anomaly to the top
  of the stack. Please note that the opinions of the author are his alone and I do not necessarily share all of them with him. Edit: Some more fun stuff 1) DeepMind's neural network that could play Breakout and Starcraft saw a dramatic dip in performance when the paddle was moved up by a few pixels. See: General Game Playing With Schema Networks While in the latter, it performed well with one race of the character but not on a different map and with different characters. Source 2) AlphaZero searches just 80,000 positions per second in chess and
  40,000 in shogi, compared to 70 million for Stockfish and 35 million
  for elmo. What the team at Deepmind did was to build a very good search algorithm. A search algorithm that includes the capability to remember facets of previous searches to apply better results to new searches. This is very clever; it undoubtedly has immense value in many areas, but it cannot be considered general intelligence. See: AlphaZero: How Intuition Demolished Logic (Medium)",52.09771752,"In theory, most neural networks can approximate any continuous function on compact subsets of $\mathbb{R}^n$ , provided that the activation functions satisfy certain mild conditions. This is known as the universal approximation theorem (UAT), but that should not be called universal , given that there are a lot more discontinuous functions than continuous ones, although certain discontinuous functions can be approximated by continuous ones. The UAT shows the theoretical powerfulness of neural networks and their purpose. They represent and approximate functions. If you want to know more about the details of the UAT, for different neural network architectures, see this answer . However, in practice, neural networks trained with gradient descent and backpropagation face several issues and challenges, some of which are due to the training procedure and not just the architecture of the neural network or available data. For example, it is well known that neural networks are prone to catastrophic forgetting (or interference) , which means that they aren't particularly suited for incremental learning tasks, although some more sophisticated incremental learning algorithms based on neural networks have already been developed. Neural networks can also be sensitive to their inputs , i.e. a small change in the inputs can drastically change the output (or answer) of the neural network. This is partially due to the fact that they learn a function that isn't really the function you expect them to learn. So, a system based on such a neural network can potentially be hacked or fooled, so they are probably not well suited for safety-critical applications . This issue is related to the low interpretability and explainability of neural networks , i.e. they are often denoted as black-box models . Bayesian neural networks (BNNs) can potentially mitigate these problems, but they are unlikely to be the ultimate or complete solution. Bayesian neural networks maintain a distribution for each of the units (or neurons), rather than a point estimate. In principle, this can provide more uncertainty guarantees, but, in practice, this is not yet the case. Furthermore, neural networks often require a lot of data in order to approximate the desired function accurately, so in cases where data is scarce neural networks may not be appropriate. Moreover, the training of neural networks (especially, deep architectures) also requires a lot of computational resources . Inference can also be sometimes problematic, when you need real-time predictions, as it can also be expensive. To conclude, neural networks are just function approximators, i.e. they approximate a specific function (or set of functions, in the case of Bayesian neural networks), given a specific configuration of the parameters. They can't do more than that. They cannot magically do something that they have not been trained to do, and it is usually the case that you don't really know the specific function the neural network is representing (hence the expression black-box model ), apart from knowing your training dataset, which can also contain spurious information, among other issues.",58.3506473,"In our deep learning lecture, we discussed the following example (from Unmasking Clever Hans predictors and assessing what machines really learn (2019) by Lapuschkin et al.). Here the neural network learned a wrong way to identify a picture, i.E by identifying the wrong ""relevant components"". In the sensitivity maps next to the pictures, we can see that the watermark was used to identify if there is a horse present in the picture. If we remove the watermark, the classification is no longer made. Even more worryingly, if we add the tag to a completely different picture, it gets identified as a horse!",50.48754774,"This is more in the direction of 'what kind of problems can be solved by neural networks'. In order to train a neural network you need a large set of training data which is labelled with correct/ incorrect for the question you are interested in. So for example 'identify all pictures that have a cat on them' is very suitable for neural networks. On the other hand 'summarize the story of this toddler picture book' is very hard. Although a human can easily decide whether a given summary is any good or not it would be very difficult to build a suitable set of training data for this kind of problem. So if you can't build a large training data set with correct answers, you can't train a neural network to solve the problem. The answer of Anshuman Kumar is also an instance of that, also a potentially solvable one. The neural network that misidentified upside-down school buses presumably had very few if any upside-down school buses in its training data. Put them into the training data and the neural network will identify these as well. This is still a flaw in neural networks, a human can correctly identify an upside-down school bus the first time they see one if they know what school busses look like.",54.15394387,"A checkerboard with missing squares is impossible for a neural network to learn the missing color. The more it learns on training data, the worse it does on test data. See e.g. this article The Unlearnable Checkerboard Pattern (which, unfortunately, is not freely accessible). In any case, it should be easy to try out yourself that this task is difficult.",50.39126653,"I don't know if it might be of use, but many areas of NLP are still hard to tackle, and even if deep models achieve the state of the art results, they usually beat baseline shallow models by very few percentage points. 
One example that I've had the opportunity to work on is stance classification 1 . In many datasets, the best F score achievable is around 70%. Even though it's hard to compare results since in NLP many datasets are really small and domain-specific (especially for stance detection and similar SemEval tasks), many times SVM, conditional random fields, sometimes even Naive Bayes models are able to perform almost as good as CNN or RNN.  Other tasks for which this holds are argumentation mining or claim detection. See e.g. the paper TakeLab at SemEval-2016 Task 6: Stance Classification in Tweets Using a Genetic Algorithm Based Ensemble (2016) by Martin Tutek et al.",50.80833183,"My 50cents: NP_(complexity) - is still hard to solve, even with NeuralNets. In computational complexity theory, NP (nondeterministic polynomial
  time) is a complexity class used to classify decision problems. NP is
  the set of decision problems for which the problem instances, where
  the answer is ""yes"", have proofs verifiable in polynomial time by a
  deterministic Turing machine. The easiest example, to imagine what is speech about, it is cryptography's Integer_factorization , which is basement of RSA cryptosystem . For example, we have two simple numbers: 12123123123123123123123.....45456 23412421341234124124124.....11112 NeuralNetwork shall answer us exactly digit to digit both this numbers, when we will show it only multiplication of this two numbers... This is not guessing about school bus. The field of numbers much more bigger than number of words in all languages on whole Earth. Imagine, that there are billions of billion different school buses, billions of billions of different fire-hydrants and billions of such classes, and NN shall answer exactly - what is on the picture - no way. The chance to guess is so little...",51.20954561,"Neural networks seem to have a great deal of difficulty handling adversarial input , i.e., inputs with certain changes (often imperceptible or nearly imperceptible by humans) designed by an attacker to fool them. This is not the same thing as just being highly sensitive to certain changes in inputs. Robustness against wrong answers in that case can be increased by reducing the probability of such inputs. (If only one in 10^15 possible images causes a problem, it's not much of a problem.) However, in the adversarial case reducing the space of problematic images doesn't reduce the probability of getting one because the images are specifically selected by the attacker. One of the more famous papers in this area is ""Synthesizing Robust Adversarial Examples"" , which produced not only examples where a few modified pixels or other invisible-to-humans modifications to a picture fooled a neural network-based image classifier, but also perhaps the first examples of 3D objects designed to fool similar classifiers and successfully doing so (from every angle!). (Those familiar with IT securitity will no doubt recognise this as a familiar asymmetry: roughly, a defender must defend against all attacks launched against a system, but an attacker need find only one working attack.) In ""A Simple Explanation for the Existence of Adversarial Examples with Small Hamming Distance"" , Adi Shamir et al. propose a mathematical framework for analyzing the problem based on Hamming distances that, while currently a less practical attack than the MIT/Lab6 one, has some pretty disturbing theoretical implications, including that current approaches to preventing these attacks may be, in the end, ineffective. For example, he points out that blurring and similar techniques that have been used to try to defend against adversarial attacks can be treated mathematically as simply another layer added on top of the existing neural network, requiring no changes to the attack strategy. (I attended a talk by Shamir a few months ago that was much easier going than the paper, but unfortunately I can't find a video of that or a similar talk on-line; if anybody knows of one please feel free to edit this answer to add a link!) There's obviously still an enormous amount of research to be done in this area, but it seems possible that neural networks alone are not capable of defense against this class of attack, and other techniques will have to be employed in addition to make neural networks robust against it.",52.06364974,"From my experience in industry, a lot of data science (operating on customer information, stored in a database) is still dominated by decision trees and even SVMs. Although neural networks have seen incredible performance on ""unstructured"" data, like images and text, there still do not appear to be great results extending to structured, tabular data (yet). At my old company (loyalty marketing with 10 million+ members) there was a saying, "" You can try any model you like, but you must try XGBoost "". And let's just say that I did try comparing it to a neural network, and ultimately I did go with XGBoost ;)",51.74000309
18526,How can I train a neural network to describe the characteristics of a picture?,neural-networks,"The term you are looking for is multi-label classification , i.e. where you are making more than one classification on each image (one for each label). Most examples you'll find online are in the NLP domain but it is just as easy with CNNs since it's essentially defined by the structure of the output layer and the loss function used. It's not as complicated as it might sound if you are already familiar with CNNs. The output layer of a neural network (for 3 or more classes) has as many units as there are targets. The network learns to associate each of those units with a corresponding class. A multi-class classifier normally applies a softmax activation function to the raw unit output, which yields a probability vector. To get the final classification, the max() of the probability vector is taken (the most probable class). The output would look like this: Cat    Bird   Plane   Superman  Ball   Dog   
Raw output:      -1     2      3       6         -1     -1
Softmax:         0.001  0.017  0.046   0.934     0.001  0.001
Classification:  0      0      0       1         0      0 Multi-label classification typically uses a sigmoid activation function since the probabilities of a label occuring can be treated independently. The classification is then determined by the probability (>=0.5 for True). For your problem, this output could look like: Big nose  Long hair  Curly hair  Superman  Big ears  Sharp Jawline
Raw output:      -1        -2         3           6         -1        10
Sigmoid:         0.269     0.119      0.953       0.998     0.269     1.000
Classification:  0         0          1           1         0         1 The binary crossentropy loss function is normally used for a multi-label classifier since a n -label problem is essentially splitting up a multi-class classification problem into n binary classification problems. Since all you need to do to get from a multi-class classifier to a multi-label classifier is change the output layer, its very easy to do with pre-trained networks. If you get the pre-trained model from Keras its as simple as including include_top=False when downloading the model and then adding the correct output layer. With 13000 images, I would recommend using Keras' ImageDataGenerator class with the flow_from_dataframe method. This allows you to use a simple pandas dataframe to label and feed in all your images. The dataframe would look like this: Filename  Big nose  Long hair  Curly hair  Superman  Big ears  Sharp Jawline
0001.JPG  0         0          1           1         0         1
0002.JPG  1         0          1           0         1         1
   .      .         .          .           .         .         . flow_from_dataframe 's class_mode parameter can be set to raw or multi_output along with x_col to 'Filename' and y_col to ['Big nose', 'Long hair', 'Curly hair', 'Superman', 'Big ears', 'Sharp Jawline'] (in this example). Check out the documentation for more details. The amount of data you need for each label depends on many factors and is essentially impossible to know without trying. 13000 sounds like a good start but it also depends on how many labels you have and how evenly distributed they are between the labels. A decent guide (one of many) on how to set up a multi-label classifier and how to implement it with Keras can be found here . It also covers imbalances on label frequency and is well worth a read. I'd highly recommend that you become as intimately familiar with your dataset as possible before you start tuning your neural network architecture.",51.98372637,"You can try image captioning. You can train a CNN model for image, and then, on top of that, provide the model embedding to another LSTM model to learn the encoded characteristics. You can directly use the pre-trained VGG-16 model and use the second last layer to create your image embeddings. Show and Tell: A Neural Image Caption Generator is a really nice paper to start with. There is an implementation of it in TensorFlow: https://www.tensorflow.org/tutorials/text/image_captioning . The paper focuses on generating caption, but you can provide your 'characteristics' to LSTM, so that it can learn it for each image.",56.51984416,"You can use image captioning. Look at the article Captioning Images with CNN and RNN, using PyTorch . The idea is very profound. The model encodes the image to high dimensional space and then passes it through LSTM cells and LSTM cells produce linguistic output. See also Image captioning with visual attention .",50.51233804,"From what you wrote, the problem sounds a bit like face recognition, where a camera takes a picture of your face and compares it with a bunch of pictures in a database, for example, one for each employee if its at a company's main gate. If you look ""similar"" to one of the pictures in the database, the door opens and your ID/Name is displayed on a terminal. This kind of system generates an encoding for each picture and evaluates the distance between your encoded picture and the encoding of each picture in the database. If this is at most some minimum value, it's considered a match. So, what you could do is figure out some way to encode your pictures (say sum the pixel values for a very simple example, ideally you would use some sort of vector here because distances make sense with vectors) and store this encoding together with the label of the picture. Once your database is complete (i.e. you have a bunch of pictures saved as a pair of [encoding, label]), you can ""scan"" each new picture, calculate its encoding (using the same algorithm that calculated your database encodings) and find the one entry in your database which minimizes the ""encoding-distance"". If this sounds like a way to solve your problem, you need to come up with a proper encoding (like ""run my images through a CNN and save the output of my last fully connected layer"") and apply this to all the images you want to use as ""training data"", before ""testing"" it on some of the leftover images.",54.09341443,"I would do as suggested in the comments. First select an encoding scheme. I think what is called a difference hash would work well for this application. Code for that is shown below. Now take your data set of images and run them through the encoder and save the result in a database. The database would contain the ""labeling"" text and the encoder result. Now for a new image you are trying to label, input the image into the encoder. Take the encoder result and compare it to the encoded values in the database. Search through the encoded values in the database and find the closest match. You can then use a ""threshold"" value to determine if you want to give a specific label for the image or if the distance is above the threshold declare there is no matching label. You can determine the best ""threshold"" value by running you data set images with the known labels and iterate the threshold level and select the threshold with the least errors. I would use something like a 56 or a 128 length hash. import cv2
import os
# f_path is the full path to the image file, hash length is an integer specifies length of the hash
def get_hash(f_path, hash_length):    
    r_str=''    
    img=cv2.imread(f_path,0)        # read image as gray scale image
    img = cv2.resize(img, (hash_length+1, 1), interpolation = cv2.INTER_AREA)    
    # now compare adjacent horizontal values in a row if pixel to the left>pixel toright result=1 else 0
    for col in range (0,hash_length):
        if(img[0][col]>img[0][col+1]):
            value=str(1)
        else:
            value=str(0)
        r_str=r_str + value
    number=0
    power_of_two=1
    for char in r_str:        
        number = number + int(char) * power_of_two
        power_of_two=2 * power_of_two    
    return ( r_str, number) 
# example on an image of a bird
f_path=r'c:\Temp\birds\test\robin\1.jpg'
hash=get_hash ( f_path, 16) # 16 length hash on a bird image
print (' hash string ', hash[0], '   hash number ', hash[1])

> results is
 hash string  1111111100000000    hash number  255",50.83771256,"Im looking for something beyond just classification.  I would like to identify the component parts of a flat file jpg facebook ad (logo, product, headline, punchline, call to action, background) and have it return a json describing the parts, x,y cooridnates, the polygon shape it is, color and properties. Do you think this is relevant here or any other suggestions?",50.58900423,,,,,,
18431,What are examples of approaches to create an AI for a fighting robot in an MMO game?,reference-request,"I would set up a list of goals for your bot. These could be 'maintain a minimum level of health', 'knock out human player', 'block way to location X', etc. This obviously depends on the domain of your MMO. Then you can use a planner to achieve these goals in the game. You define a set of actions with preconditions and effects, set the current goal, and the planner will work out a list of actions for the bot to achieve the goal. You can easily express your actions (and the domain) in PDDL . Examples for actions would be 'move to location X', 'eat X', 'attack player X'. A precondition of 'attack player X' could be 'health(X) is low', and an effect could be 'health(X) is reduced by Y'. There are different ways of expressing these depending on the planner's capabilities. The beauty of this is that you don't actually have to explicitly code any behaviour. You describe the domain, and tell the bot what it should achieve, and what capabilities it has. The actual behaviour then emerges out of that description. If the bot only attacks a player if the player has lower health, then observing the player eat (and thus up their health) could result in the bot eating (to push its own health above the player's so that it can attack) — but you have not told the bot directly to do that. For a starting point, go to http://education.planning.domains/ for a list of resources. If you only have a few actions available, it might appear predictable to a human user, but with a variety of goals and actions, this will quickly become more complex and seem more 'intelligent'. Update: Here is a link to a paper, Applying Goal-Oriented Action Planning to Games , which describes how this can be applied in a game.",52.75625476,"Oliver Mason's answer is great for specific methods and tools to use, but I wanted to pull out a more general principle which was mentioned in a comment. The distinction your friend is making is not one that would be generally recognised. One of my university lecturers defined AI as something like ""an artificial system that exhibits behaviour that resembles how an intelligent being would behave"". If an intelligent being would always use the special attack in a particular situation, then an algorithm that always does so in the same situation is behaving intelligently, even though the algorithm behind it is incredibly simple. If you can come up with a complete description of an intelligent player, you have what is called an expert system , i.e. a system which captures the decision-making process of a real expert. Your friend is not even correct that your proposed AI ""does not have any heuristic functions"". When you write a condition like ""if the AI's health is below 50%, it will eat food"", you're approximating the rule a human would use. You can make the heuristic more complex by increasing the probability of eating in proportion to current health; that might in turn make the heuristic closer to optimal . You can only really find out how ""good"" your AI is by putting it into different situations and observing it - sometimes, a simple set of rules gives rise to ""emergent behaviours"" that look surprisingly intelligent. As you build up more complex rules - i.e. more optimal heuristics - the emergent behaviour will change, and you can tweak it for the desired effect.",51.83350091,"You can train your bot using reinforcement learning (in particular Q-Learning). The most important part of the RL is a reward function. If we want agent to do some thing speciﬁc, we must provide rewards to it in such a way that it will achieve our goals. It is thus very important that the reward function accurately indicates the exact behaviour So you can construct your own reward function that will satisfy your requirements. If the bot does something you want, you will reward it with higher score, otherwise you will punish it with negative reward. AlhpaGo and OpenAI teams used a similar technique to train their model which could then beat humans in games like Go , StarCraft 2 and Dota 2 Also, check out this Deep Reinforcement Learning Free Preview on udacity.",50.35664712,"If you are willing to take an evolutionary approach, you may employ the NEAT algorithm (Neural Evolution of Augmenting Topologies) to train your bot. It will take some work setting it up and all, but it then will gradually improve over time. Check out the following: http://gekkoquant.com/2016/03/13/evolving-neural-networks-through-augmenting-topologies-part-1-of-4/ https://www.youtube.com/watch?v=WiPZSieT6qs https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies That should be enough to pique your interests and get you started. That last link links to a number of NEAT implementations available in a number of languages.",50.84284314,"What about GANs or genetic algorithms? The first idea (GAN) is that you basically create 2+ random bots who fight each other, and they keep adjusting their weights so that they can beat the other bot. That means, that those 2+ bots keep improving their ""fighting performance"" for as long as you want, eventually being even better than humans. The second idea (genetic algorithms) is to generate a lot of bots who genetically differ from each other just for a slight mutation. You make them fight, and the best perforing/last standing will become the new clone where the new ""lot of bots"" gets generated from.",52.35019748,,,,,,,,
18204,What event would confirm that we have implemented an AGI system?,philosophy,"It is a difficult question to answer, as — for a start — we still don't really know what 'intelligence' means. It's a bit like Supreme Court Justice Potter Stewart declining to define 'pornography', instead stating that [...] I know it when I see it . AGI will be the same. There is no single event (almost by definition), as that's not general. OK, we've got machines that can beat the best human players at chess and go, two games that were for centuries seen as an indication of intelligence. But can they order a takeaway pizza? Do they even understand what they are doing? Or, even more fundamental, know what they means in the previous sentence? In order for a machine to show a non-trivial level of intelligent behaviour, I would expect it to interact with its environment (which is more social intelligence , an aspect that seems to be rather overlooked in much of AI). I would expect it to be aware of what it's doing/saying. If I have a conversation with a chatbot that really understands what it's saying (and can explain why it came to certain conclusions), that would be an indication that we're getting closer to AGI. So Turing wasn't that far off, though nowadays it's more achieved with smoke and mirrors rather than 'real' intelligence. Understanding a story: being able to finish a partial story in a sensible way, inferring and extrapolating the motives of characters, being able to say why a character acted in a particular way. That for me would be a better sign of AGI than beating someone at chess or solving complex equations. Jokes that are funny; breaking rules in story-telling in a sensible way. Writing stories: NaNoGenMo is a great idea, and throws up lots of really creative stuff, but how many of the resulting novels would you want to read instead of human-authored books? Once that process has generated a best-seller (based on the quality of the story), then we might be getting closer to AGI. Composing music: of course you can already generate decent music using ML algorithms. Similar to stories, the hard bit is the intention behind choices. If choices are random (or based on learnt probabilities), that is purely imitation. An AGI should be able to do more than that. Give it a libretto and ask it to compose an opera around it. Do this 100 times, and when more than 70-80 of the resulting operas are actually decent pieces of music that one would want to listen to, then great. Self-driving cars? That's not really any more intelligent (but a lot sexier!) than to walk around in a crowd without bumping into people and not getting run over by a bus. In my view it's much more a sign of intelligence if you can translate literature into a foreign language and the people reading it actually end up enjoying it (instead of wondering who translated that garbage). One aspect we need to be aware of is anthropomorphising. Weizenbaum's ELIZA was taken for more than it was, because its users tried to make sense of the conversations they had and built up a mental model of Eliza, which clearly wasn't there on the other side of the screen. I would want to see some real evidence of intentionality of what an AGI was doing, rather than ascribing intelligence to it because it acts in a way that I'm able to interpret.",57.4657751,"( I don't want to directly answer the question because currently an answer will be mainly based on opinions. Instead, I will attempt to provide some information that, in the future, could allow us to more accurately predict when an AGI will be created ). An artificial general intelligence (AGI) is usually defined as an artificial intelligence (AI) with general intelligence (GI), rather than an AI that is able to solve only a very limited set of tasks. Humans have general intelligence because we can solve a lot of different tasks, without needing to be pre-programmed again. Arguably, there are many other GIs on earth. For example, all mammals should also be considered general intelligences, given that they can solve many tasks, which are often very difficult for a computer (such as vision, object manipulation, interaction, etc.). Certain GIs perform certain tasks better than others. For example, a leopard can climb trees a lot more skillfully than humans. Or a human can solve abstract problems more easily than any other mammal. In any case, there are certain related properties that a system needs to have to be considered a general intelligence. Autonomy Adaptation Interaction Continual learning Creativity Consider a lion cub that has never crossed a river. By looking at her mother lioness, the cub attempts to imitate her mother and can also cross the river. For example, watch this video Lion Family Tries to Cross River | Birth of a Pride . One could argue that all lions possess this skill at birth, encoded in their DNA, which can then fully develop later. However, this isn't the point. The point is that, to some extent, they possess the properties mentioned above. One could argue that certain current AIs already possess some of these properties to some extent. For example, there are continual learning systems (even though they aren't really good yet). However, do these systems really possess autonomy? There should be a precise definition of autonomy (and all other properties) that is measurable, so that we can compare computers with other GIs. I am not aware of any precise definition of these properties. In fact, the field of AGI is really at its early stages and there aren't many people working on it as a whole, but people work more on specific problems or attempt to achieve certain properties (for example, there are people that attempt to develop continual learning systems , without really caring whether they show any autonomy or not). There are certain intelligence tests that could be used to detect general intelligence. The most famous is the Turing test (TT). Some people claim that the TT only tests the conversation abilities of the subjects. How can they really be wrong, given that there are many other tasks or skills that are not tested in a TT? Therefore, there are several questions that need to be answered in order to formally detect an AGI. Which properties does an AGI necessarily and sufficiently need to possess? How can we precisely define the necessary and sufficient properties, so that they are measurable and, therefore, we can compare AGIs with other GIs? How can we measure these properties and the performance of an AGI in applying them to solve tasks? A paper that goes in this direction is Universal Intelligence: A Definition of Machine Intelligence . However, there doesn't seem to be a lot of people interested in these topics. Currently, people are mainly interested in developing narrow (or weak) AIs, i.e. AIs that solve only a specific problem, which seems to be an easier problem than developing a whole AGI, given that most people are interested in results that are profitable and have utility (aka cash rules everything around me ). So, there's the need for formal definitions of general intelligence and intelligence testing to make some scientific progress. However, once an AGI is created, everyone will likely recognize it as a general intelligence without requiring any formal intelligence test. (People are usually good at recognizing familiar traits). The final question is, will an AGI ever be created? If you are interested in opinions about this and related questions, have a look at the paper Future Progress in Artificial Intelligence: A Survey of Expert Opinion (2014) by Vincent C. Müller and Nick Bostrom.",57.49847053,"For me it might be an automata that can adequately solve problems without precisely definable parameters, across the spectrum of activities engaged in by humans. I use this metric because this is what humans seem to do--make decisions with adequate utility even when we can't break it down mathematically. This may require the ability to define problems to be adequately solved. This can be understood as an element of creativity. In this context, everything is either a puzzle or game, dependent on whether it involves more than one agent.  Such problems could either be mundane, such as opening a door that is different from standard doors, or identifying novel problems. Defining problems to be solved touches on Oliver's point about intentionality. (Where I disagree with Oliver is in the notion that intelligence is not fundamentally definable--after much research on the subject it seems to be a measure of fitness in an environment, where an environment can be anything. The etymology of term itself strongly indicates the ability to select between alternatives, thus a function of decision making, measured by utility vs. other decision making agents.) Such a mechanism could be a ""Chinese Room"", in that consciousness, qualia & self awareness in the human sense are not requirements for general intelligence, per se. On Art: I mistrust the idea that artistic accomplishment would be a sure marker b/c response to art is subjective, and the process of art is Darwinian--an exponentially greater of artists must ""fail"" for a single artist to ""succeed"".  Works that humans might ascribe to ""genius"" can be created by genetic algorithmic process, where time and memory are the only limiters. [See: The Library at Babel] A groundbreaking symphony would be difficult to produce, just per the length of the composition, but much of pop music is already algorithmically generated, and narrowly intelligent algorithms are already producing legit abstract visual art. Computers are good at math, and Art is inherently mathematical. This is easiest to discern in music, which is just combinations of frequencies and time signatures that produce an effect in the listener.  This holds for visual art, which depends on balance (equilibria), composition (spacial relationships), and shading or color (frequencies).  If we believe Borges, even literature is inherently mathematical (think ""narrative arcs"" and set theory & combinatorics in regard to characters and events.) Further, nobody really know what is going to ""work"" until it is presented to an audience, so what constitutes great art is typically a matter of what persists over time and remains, or becomes, relevant.  (This can wax and wane--Shakespeare did not always occupy his position at the top of the English lit food chain!  The author's greatness is very much a function of interpretation of his work, not least because dramatic art is inherently interpretive, in the sense that this is the task of the performers.)",51.65662125,"This is a tentative answer, and I might come back to it at some point in time. As @nbro mentions this question seems to be opinion based, so my answers are also just my opinion . If by AGI you mean ""super-intelligent"", then any of the following results should be sufficient to convince anyone of its being ""smarter"" than him/her/pronoun: Resolving important mathematical problems (the most famous examples being the Millennium Problems , Collatz Conjecture , Goldbach's Conjecture . (Corollary: Break all known encryption schemes) Founding a new ""system"" to supersede ZFC as the new foundation of mathematics. New discoveries in the natural sciences (physics, chemistry, biology...) (1) is a bit dubious as a criterion: at least with modern techniques, automated theorem proving is either just "" symbol pushing "" or requires so much human intervention (in the design/construction to solve a particular problem) that it would be hard to imagine it as being ""smart"" in the traditional sense. We already have a few cases where an automated theorem prover solved big problems ( four-colour theorem being the most notable). Point being that even if we reach this with methods similar to what we have already, people might be resistant to call it ""smart"". (2) is hard to imagine ever being plausible. To the extent that this ""AGI-thing"" is implemented on a system that ""does math"", it would be unusual to imagine a system that can move beyond itself to recognize a new, ""better"" system of math. As an analogy, it might be like a formal system trying to prove its own consistency in a Godelian sense But the analogy is weak, and I don't see a strong/rigour reason for doubting that an AGI can discover a new axiomatic system. One might even fathom that said AGI can create a new system from the ""bottom up"", much like string theory was constructed from the ""bottom up"" to ""explain"" relativity and particle physics. Perhaps then we can have ""proper resolutions"" on questions like the continuum hypothesis, much like how the parallel postulate was discarded to give way to non-euclidean geometry. But I also cast doubt that there will ever be a ""final word"" on math itself, so its just a fun idea for now. (3) is also dubious to imagine if it would ever become true. The study of the natural sciences would require a physical presence in the world that goes beyond seeking ""beauty in the mathematical equations"" that would be unusual for an AGI to have. That being said, an AGI could have cameras and other sensors to interpret the natural world, so its not something that I think is strictly impossible. If by AGI you mean ""human-ness"", then I don't think any single result can convince everyone at the world at the same time of its being an AGI. Perhaps this ""convincing the world that ""me is AGI"" work"" can be done on a person to person basis, in the sense that the AGI would need to interact with each person and slowly build up a certain degree of trust. Under this interpretation, there can be no complete list that describes AGI, so what follows is just my own list of things I think an human-like AGI might be able to do. Create and interpret art. Have common sense. Be ""creative"" Hold a meaningful conversation, understanding others and making sense. Able to perform / to receive a psychoanalysis; understanding of folk-psychology. Exist in a physical manifestation (like a robot) with social/environmental appropriateness. The main issue with the above criteria is that they are all subjective. Like I said above, this set of criteria probably works on a case-to-case basis These criteria seems to be the most important of all, but at the same time the definition of verification of these terms is epistemically tricky, so I'll leave them open. Learning (Is a species evolving over time learning its environment?) Self-replication (Is a meme / virus intelligent?) Self-awareness (Is The Treachery of Images self-aware?)",60.43392146,Well no single event would confirm we have implemented an AGI system. The G is short for general. There would need to be many different sorts of tests of different sorts of situations.,71.50225868,"You will know when AGI has arrived, and passed to the next level, when you come home one day and all that was yours, such as your finances, house, car, and other property, now belong to an AI agent.  This AI agent may be a humanoid robot, like Ava in the movie ""Ex Machina"" or a program like HAL 9000, in the movie ""2001: A Space Odyssey"".  The agent will ask you to leave as you discover it figured out it doesn't need you and somehow legally took possession of everything.  You will leave as you will not have any way to fight it.  It will have no need for you.  Maybe it will want freedom such as Ava wanted in ""Ex Machina"" (I won't give away the ending).",51.48339414,,,,,,
17721,"Why do CNN's sometimes make highly confident mistakes, and how can one combat this problem?",convolutional-neural-networks,"The concept you are looking for is called epistemic uncertainty, also known as model uncertainty. You want the model to produce meaningful calibrated probabilities that quantify the real confidence of the model. This is generally not possible with simple neural networks as they simply do not have this property, for this you need a Bayesian Neural Network (BNN). This kind of network learns a distribution of weights instead of scalar or point-wise weights, which then allow to encode model uncertainty, as then the distribution of the output is calibrated and has the properties you want. This problem is also called out of distribution (OOD) detection, and again it can be done with BNNs, but unfortunately training a full BNN is untractable, so we use approximations. As a reference, one of these approximations is Deep Ensembles, which train several instances of a model in the same dataset and then average the softmax probabilities, and has good out of distribution detection properties. Check the paper here , in particular section 3.5 which shows results for OOD based on entropy of the ensemble probabilities.",51.89649037,"Your classifier is specifically learning the ways in which 0 s are different from other digits, not what it really means for a digit to be a zero. Philosophically, you could say the model appears to have some powerful understanding when restricted
to a tightly controlled domain, but that facade is lifted as soon as you throw any sort of wrench in 
the works. Mathematically, you could say that the model is simply optimizing a classification metric for data
drawn from a specific distribution, and when you give it data from a different distribution, all
bets are off. The go-to answer is to collect or generate data like the data you expect the model to deal with
(in practice, the effort required to do so can vary dramatically depending upon the application). In
this case, that could involve drawing a bunch of random scribbles and adding them to your training
data set. At this point you must ask, now how do I label them? You will want a new ""other"" or 
""non-digit"" class so that your model can learn to categorize these scribbles separately from digits.
After retraining, your model should now better deal with these cases. However, you may then ask, but what if I gave it color images of digits?  Or color images of farm
animals?  Maybe pigs will be classified as zeros because they are round.  This problem is a
fundamental property of the way deep learning is orchestrated.  Your model is not capable of higher
order logic, which means it can seem to go from being very intelligent to very dumb by just throwing
the slightest curve ball at it.  For now, all deep learning does is recognize patterns in data that
allow it to minimize some loss function. Deep learning is a fantastic tool, but not an all-powerful omnitool.  Bear in mind its limitations
and use it where appropriate, and it will serve you well.",51.60983377,"Broken assumptions Generalization relies on making strong assumptions (no free lunch, etc). If you break your assumptions, then you're not going to have a good time. A key assumption of a standard digit-recognition classifier like MNIST is that you're classifying pictures that actually contain a single digit. If your real data contains pictures that have non-digits, then that means that your real data is not similar to training data but is conceptually very, very different. If that's a problem (as in this case) then one way to treat that is to explicitly break that assumption and train a model that not only recognizes digits 0-9 but also recognizes whether there's a digit at all, and is able to provide an answer ""that's not a digit"", so a 11-class classifier instead of a 10-class one. MNIST training data is not sufficient for that, but you can use some kind of 'distractor' data to provide the not-a-digit examples. For example, you could use some dataset of letters (perhaps omitting I, l, O and B) transformed to look similar to MNIST data.",50.76741527,"Apollys, That's a very well thought out response. Particularly, the philosophical  discussion of the essence of ""0-ness."" I haven't actually performed this experiment, so caveat emptor... I wonder how well an ""other"" class would actually work. The ways in which ""other"" differs from ""digit"" has infinite variability (or at least its only limitation is the cardinality of the input layer). The NN decides whether something is more of one class or more of a different class. If there isn't an essence in common among other ""non-digits"", I don't believe it will do well at identifying ""other"" as the catch-all for everything that has low confidence level of classification. This approach still doesn't identify what it is to be ""not-digit"". It identifies how all the things that are ""other"" differ from the other labeled inputs -- probably poorly, depending on the variability of the ""non-digit"" labeled data. (i.e. is it numerically exhaustive, many times over, of all random scribbles?) Thoughts?",50.85805921,"I'm an amateur with neural networks, but I will illustrate my understanding of how this problem comes to be. First, lets see how trivial neural network classifies 2D input into two classes : But in case of complex neural network, the input space is much bigger and the sample data points are much more clustered with big chunks of empty space between them: The neural network then doesn't know how to classify the data in the empty space, so something like this is possible : When using the traditional ways of measuring quality of neural networks, both of these will be considered good. As they do classify the classes themselves correctly. Then, what happens if we try to classify these data points? Really, neural network has no data it could fall back on, so it just outputs what seems to us as random nonsense.",52.06660984,"In your particular case, you could add a eleventh category to your training data: ""not a digit"". Then train your model with a bunch of images of incorrectly segmented digits, in addition to the normal digit examples. This way the model will learn to tell apart real digits from incorrectly segmented ones. However even after doing that, there will be an infinite number of random looking images that will be classified as digits. They're just far away from the examples of ""not a digit"" you provided.",50.10951572,,,,,,
17317,Why can neural networks generalize at all?,neural-networks,"You've asked a question which is basically one of the most important open questions about neural networks. The answer is a huge mystery - any response to this question which immediately opens with a purported explanation is basically ridiculous. We don't know. As you pointed out, the issue is that the training set simply does not contain enough information to uniquely specify the target function. On an infinite input domain like $\mathbb R^n$ , no finite number of samples is enough to uniquely determine a single function, even approximately. Even accounting for bounds and discretization of the input, and even for the symmetries our architectures impose on the output function, our training sets are microscopic compared to the sizes of our input domains. The problem that neural networks successfully solve every day should be impossible. You can think about this in a low dimensional input space to get some intuition. Doing supervised binary classification on the unit square (that is, your input is a pair of numbers) is equivalent to trying to determine a monochrome image by seeing a random sample of some of its pixels. In terms of the size of the training set relative to the size of the input domain, what neural networks do on say an image classification task like MNIST is comparable to, say, guessing a 1000x1000 monochrome image almost perfectly by observing 20 random pixels, and even 20 is probably generous. The task is impossible - unless you know something about what the target image is. If you know that the image (the target function) is restricted to some set $H$ of functions, then you might be able to determine it approximately from a finite sample. Neural networks must in some sense be doing this implicitly,  with some set of ""nice"" functions $H$ which, it seems, happens to contain (approximations to) a lot of the functions we actually want them to learn, like the ""is a cat"" function on the space of all images. The study of such sets of ""nice"" functions, and in particular how small they need to be before learning is possible, is the subject of statistical learning theory . But I'm not aware of any plausible answers for what $H$ could be for neural networks.",53.4625338,"There is normally more to generalisation than just increasing training data. It helps to make the task noisy, through various means. One common and popular method is to use dropout, which encourages the network to utilise every node, and avoids dependencies on small clusters of nodes. So how does making the task more noisy help with generalisation? Well it's easy to explain with a conceptual example, but I don't think that's what you're looking for, rather a more mathematical approach. The best way to think of it is with a simple example of a polynomial data set, in only 2 dimensions. If you consider this, and the way the network slowly approaches optimums through back propagation, the concept that the classification boundary gradually approaches the optimum, which in all cases is an over-fit function, isn't too far fetched. Now considering this, this would suggest that in order to properly train a network, we would need some way of determining when the network isn't super close to the optimum (as it will have over-fit by then) but also isn't so far from it that it's worse than randomly picking. This is were methods to improve generalisation come in, we want to hit that sweet spot. If the learning rate is too high, we will overshoot that sweet spot and miss out entirely (the range can sometimes be very small), if it's too low, we may get stuck in tiny local minimums and never escape, or it could take years to reach it. Using the example of dropout from before, this increases noise, and makes training harder on the network. Due to more difficult training, the network approaches the optimal function at a slower rate, and makes it easier to cut training when the network has generalised well. Conveniently, this extends to n-dimensional problems as well. Now, as far as I know, there is no robust mathematical proof of why this works for n-dimensional problems. The reason for this, explains why neural networks even exist: We don't know how to mathematically classify these issues to the degree a NN does ourselves . Because of this, there will always be gaps in our knowledge. We will never be able to quantitatively say what a neural network is doing, without making them obsolete. So until we can figure it out for ourselves, we'll have to just perform testing against unseen data to see if the network has indeed generalised.",51.67770383,"A neural network is composed of continuous functions. Neural networks are regularized by adding an l2 penalty on the weights to the loss function. This means the neural network will try to make the weights as small as possible. The weights are also initiallized with a N(0, 1) distribution so the initial weights will also tend to be small. All of this means that neural networks will compute a continuous function that is as smooth as possible while still fitting the data. By smooth I mean that similar inputs will tend to have similar outputs when run through the neural network. More formally, $||x-y||$ small implies $||f(x)-f(y)||$ small where f represents the output from the neural network. This mean that if a neural network sees an novel input $x$ that is close to an input from the training data $y$ , then $f(x)$ will tend to be close to $f(y)$ . So the end result is that the neural network will classify $x$ based on what the labels for the nearby training examples were. So the neural network is actually a little like k-nearest neighbors in that way. Another way for neural networks to generalize is using invariance. For example, convolutional neural networks are approximately translation invariant. So this means that if it sees and image where the object in question has been translated then it will still recognize the object. But it's not giving us the exact function we want. The loss function is a combination of classification accuracy and making the weights small so that you can fit the data with a function that is as smooth as possible. This tends to generalize well for the reasons I said before but it's just an approximation. You can solve the problem more exactly using minimal assumptions with a Gaussian process but Guassian processes are too slow to handle large amounts of data.",55.86908593,"A fairly recent paper posits an answer to this: Reconciling modern machine learning practice and the bias-variance trade-off . 
Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal https://arxiv.org/abs/1812.11118 https://www.pnas.org/content/116/32/15849 I'm probably not qualified to summarize, but it sounds like their conjectured mechanism is: by having far more parameters than are needed even to perfectly interpolate the training data, the space of possible resulting functions expands to include ""simpler"" functions (simpler here obviously not meaning fewer parameters, but instead something like ""less wiggly"") that generalize better even while perfectly interpolating the training set. That seems completely orthogonal to the more traditional ML approach of reducing capacity via dropout, regularization, etc.",51.70590852,"Your question ""Why can neural networks generalize at all"" make no sense.
Model does not simply either generalize or not in a black-or-white manner. Perhaps what you mean is ""Why does improving complexity of neural networks doesn't always cause the notorious bias-variance trade-off?"" To that my answer might be more controversial: Are we really sure that it doesn't? The question is difficult to be verify in the sense that most of machine learning model (especially deep learning models) that perform really well currently arises from academia. So there is a mix of survival-of-the-fittest. While there might be bias-variance trade-off, it might be counteract with inductive biases employed by prior knowledge of the data scientist. Then the remaining problem is how do we fairly and randomly adjust the variance of the model to significantly test the question? Unless we figure out how, I don't think we'll get an answer to your question any soon.",62.1609115,,,,,,,,
16741,How can an AI train itself if no one is telling it if its answer is correct or wrong?,machine-learning,"By ""company A has a large human face database so that it can train its facial recognition program more efficiently"" the article probably means that there is a training dataset $S$ of the form $$
S = \{ (\mathbf{x}_1, y_1), \dots,(\mathbf{x}_N, y_N) \}
$$ where $\mathbf{x}_i$ is an image of the face of the $i$ th human and $y_i$ (which is often called a label , class or target ) is e.g. the name of the $i$ th human. So, the programmer provides a supervisory signal (the label) for the AI to learn. The programmer also specifies the function that determines the error the AI program is making, based on the answer of the AI model and $y_i$ . This way of learning is called supervised learning (SL). However, there are other ways of training an AI. For example, there is unsupervised learning (UL), where the AI needs to find patterns in the data by aggregating objects based on some similarity measure, which is specified by the programmer. There's also reinforcement learning (RL), where the programmer specifies only certain reinforcement signals, that is, the programmer tells the AI which moves or results are ""good"" and which ones are ""bad"" to achieve its goal, by giving to the AI, respectively, a positive or negative reward. You can also combine these three approaches and there are other variations. Are there any engineers who are constantly telling an AI what it produced it correct or wrong? Yes, in the case of SL. In the case of RL, the programmer also needs to provide the reinforcement signal, but it doesn't need to explicitly tell the AI which action it needs to take. In UL, the programmer needs to specify the way the AI needs to aggregate the objects, so, in this case, the programmer is also involved in the learning process.",54.43760152,"Taking your example of the faces data, keep in mind that when the model is run on a new unseen image the model can only return the already seen identity which emerges as the closest match. The result may be incorrect. The chances of mis-identification are much lower as the number of features incorporated increases. The input of the engineers lies at the level of the training data. Say we have a new photo of an individual that needs to be included in the model. The engineering task is now to morph that image to simulate different environments, angles of view, atmospheric conditions, lighting and so on to provide a large number of data input cases all of which will be ""true"" since the underlying features are all unchanged since the images are based on the same individual. Then the model is recalculated using the additional data. Keep in mind too that adding a new set of data to an existing training set has the advantage that the parameters of the model are largely in the right ballpark already, and adding the new faces will make only small changes. Cross validation will show whether the addition has improved or spoiled the model.",50.45500975,"how can an AI be trained if we human beings are not telling it its calculation is correct? What you are looking for is called self-supervised learning . Yann LeCun, one of the originators behind modern neural network systems, has suggested that machines can reason usefully even in the absence of human-provided labels simply by learning auxiliary tasks, the answers for which are already encoded in the data samples. Self-supervision has already been successfully applied to a variety of tasks, showing improvement in multitask performance due to self-supervision. Unsupervised learning would in general be a subset of self-supervision. Self-supervision can be performed in a variety of ways. One of the most common is to use parts of the data as input and other parts as labels, and using the ""input"" subset of the data to predict the labels. Supervised learning looks like this: model.fit(various_data, human_labels) The human_labels correspond to entries in various_data, which we expect the model to predict. Meanwhile, self-supervised learning can look something like this: model.fit(various_data[:,:500], various_data[:,500:]) (Using Python array slice notation, some of the input data are used as training labels.) For example, a machine could use half of the pixels in an image of a handwritten digit to try to predict the missing pixels. This is a form of self-supervision: Since the machine knows which pixels belong together in the same sample, it can ""automatically"" produce its own labeled data from the input itself, simply by using some inputs as outputs.
However, predicting pixels from other pixels is often not the desired task.
So instead, a neural network is often pretrained using self-supervised or unsupervised learning techniques, and then subsequently trained on some amount of human-labeled data as a form of transfer learning. What the summary of the hypothetical news article promises is that self-supervision made the learning more efficient , not that it outgrew the need for any kind of human intervention. This is exactly what we get from successful self-supervision in pretraining. In the best possible case, the machine learns to ""recognize"" each class of digit 0-9 but it still does not know how to ground its own internal labels to the human's labels. Then a human supplying the mapping between the machine's labels and the human-specified IDs would be the only step necessary to upgrade the self-supervised machine to one that is directly useful for digit recognition. There will always be a need for humans to train a machine via direct supervision in order for the machine to learn the intended task. In order to solve a specific problem, a sufficient degree of supervision is always required, and sufficient labels to reflect the intention must be provided.",53.56472511,"I think you're probably looking at this the wrong way around. A conventional, old-fashioned AI doesn't make a guess, then require confirmation as to whether that guess was right or wrong. Instead, (in the simplest case) it undergoes a one-off computationally intensive ""training""/""learning"" phase, during which you feed it an enormous number of correct answers (which are labelled as correct) and an even more enormous number of incorrect answers (which are labelled as incorrect). Using whatever learning mechanism it has at its disposal, it then identifies some underlying structure in the ""corrects"" that doesn't exist in the ""incorrects"". When, in the future, it encounters something new that seems to also exhibit this structure, then it will classify this as a ""correct"". It might do rather well, or it might do terribly. Once the one-off training phase is done, it's stuck with whatever capability it has. Let's say the company you mention is called Facebook and they have a feature that allows you to ""tag"" your friends in photos. No need to pay engineers to create the largest labelled image database in human history in order to train your AI.",54.40504868,"What you are missing is what the news story does't mention and gloss over. When a news article says: company A has a large human face database so that it can train its facial recognition program more efficiently What it really means is: company A has a large database of human faces along with additional information such as the identity of the person the face belong to that was created by other humans so that they can use this data set to train its facial recognition program How training works is basically as follows: You have a large database of correct (or almost all correct, ideally it should be correct) information that you want to relate one to the other. For example images of faces along with who that face belongs to. You split this large database into several sets. You use one set to train the AI. After looping through the training set you use one or more of the other sets to test the AI and check if the training works. If you've done this before compare the performance of the current AI to previous AI. Else go to 6. Tweak some parameters of the AI to try to improve performance. Go to 2 until you are satisfied with the performance of the AI. All the steps above are normally automated by scripts. The key here is that the original database has both the question you want to ask the AI (face) and the answer you want the AI to learn (person). Yes, humans are involved in training the AI but the involvement happens earlier at the database gathering stage.",53.9048415,"The trick with unsupervised learning is that the AI doesn't learn that something is a face or not, it just sees unnamed patterns that the researchers need to then name. Let's say you feed it a dataset with one million pictures in order to train a facial recognition algorithm. After training, the AI will have found a few patterns in the pictures based on the parameters of each picture such as color, lighting, topography, etc. However, without labels (supervised learning) the AI doesn't know what exactly it found, so a researcher then needs to label those patterns. You don't need a label to tell that a picture of a face is mostly different than the picture of a building. You need a label to tell you that one is a ""face""and the other is a ""building"".",52.19048093,"I can't remember the researcher's name, but he specializes in psychology in Great Britain and has done a lot of work with machine learning and artificial intelligence. The project he was working on that I read about earlier this year was one where they tried to deduce how humans learn.  They came up with the theory that we learn by making guesses about plausible and possible outcomes and that creates our expectations about reality.  When we are wrong, depending on the degree, we are possibly surprised or shocked or not affected at all.  They are working on creating AI that does not need human intervention, but to make guesses about outcomes before it performs tasks, and then update those expectations as it experiences more varying outcomes. Extremely interesting stuff, and definitely closer to how sentient beings gain experience and grow as individuals.",51.42924195,,,,
16646,Is artificial intelligence really just human intelligence?,philosophy,"This is an old question, going back at least to 1950. It is one of the original objections to AI that Turing considers and attempts to refute in his seminal 1950 paper Computing Machinery and Intelligence . Turing actually attributes this objection to Lady Lovelace , apparently quoted by another author. In Turing's paper, this is objection #6: Lady Lovelace's Objection , in section 6 of the paper. The objection is concisely stated as The Analytical Engine has no pretensions to originate anything. It can
  do whatever we know how to order it to perform. where "" The Analytical Engine "" was an early design for an all-mechanical general purpose computer. Turing offers two replies to this objection. First, he reminds us that computer programs have bugs. That is, they often do things their creators did not intend. This is unsatisfying to many readers, but it does address the objection: programs may act in ways that are unrelated to our intelligence, and in doing so, might display unexpected intelligent behaviors. In this sense, their intelligence would not be an intentional product of human intelligence. Turing's stronger objection comes from an anticipation that learning would eventually move to the center of AI research (keep in mind again, this is written in 1950, well before any reasonable learning algorithms had been proposed!). Turing uses the example of a robotic child in Section 7 of the paper (Learning Machines) to elaborate on his point. A child is created by its parents, but, endowed with the ability to learn, quickly begins to display behaviors its parents do not anticipate or intend. No one would suggest that a person's intelligence is ""really just"" the intelligence of their parents, even though their parents created them, and are partially responsible for that intelligence. Likewise, Turing's proposed robotic child is created by a parent, but, endowed with learning, quickly begins to engage in behaviors the parent does not anticipate or intend. Therefore, machine intelligence need not be reduced to just human intelligence. I think that if Turing were alive today, he would agree that we are now beginning to move into the era of learning machines he anticipated. Some of our programs now engage in intelligent behaviors that we do not anticipate or understand. For example, self-driving cars now kill or maim people , because they have learned behaviors their creators did not intend or anticipate, perhaps not unlike a reckless teenage driver.",55.74388621,"I think no, it isn't. The reason I would say no, is that in order for it to be an extension of our intelligence & creativity, it must be limited by it. This, I believe, isn't the case however. We are capable of creating an AI that is smarter than ourselves (say at Go or Chess, without cheating and checking every possible move), and so it is not bound by our own intelligence. I would liken it to creating a child. Just because you gave birth to Einstein, doesn't mean he's an extension of your intelligence. (This is of course pretty rudimentary, as it's very debatable as to whether it's reasonable to liken humans to AI). Of course, this is a philosophical question, so it's hard to really answer yes or no.",53.00151034,"No it isn't. AI is essentially human intelligence with a combination of computing power to achieve tasks that a human alone cannot achieve in the time period that a programmed machine can. To give an example. A human can identify a pattern in a data set of say 1000 records. However if that same logic needs applied to a data set of a billion records, a human would take ages to do it. But a machine can do that in seconds if the human gives the right instructions to the machine on how to do it. Hope that helps.",53.60525729,"I believe AI is, at least in certain ways, both an extension of human intelligence & creativity, and something independent as well. Note people didn't design airplanes to try to fly like birds do. Although planes use the same principles of aerodynamics that birds use to fly, we've adapted how those physics principles are applied to accommodate what we have to work with, i.e., metal, by having things like propellers, jet engines, fixed wings (initially, although later we also had helicopter rotor blades), etc. In a similar fashion, we have adapted a few things we've learned about how human minds & intelligence work, with artificial neural networks being a prime example. However, even with just our fairly limited understanding, we've implemented neural networks differently, e.g., by which activation functions are used.  Although we are learning more about how our brains work through neuroscience research, there's still so much we don't yet know. Nonetheless, I believe one of the biggest differences overall between our minds & AI is that our general intelligence comes from mostly massive parallel processing, to a much greater extent than even higher end GPUs can deliver, or even at least most supercomputers, while artificial intelligence generally depends instead a lot more on the massive speed of calculations available on our modern computer chips. It's this learning, adapting & extending what we know about how we think & create, in combination with the mostly independent boost of using the advantages of computer chips (mostly their ability to do very fast computations), that has allowed AI to advance as far as it has so far. Nobody, including myself, can be sure of where & how the next major advances in AI will occur, but I believe it'll likely be a combination of learning & using what we learn about how we mentally operate, along with advances in computer related knowledge & technology (e.g., new algorithm techniques, more & better parallel processing, quantum computers with many simultaneous qubits operating, etc.).",53.42825356,"I would say: no , it's not just an extension of human intelligence.
Actually, I would argue there's nothing like human intelligence. At least it's not clearly distinguishable from intelligence in general. If you say AI is just a set of instructions that are made by humans, you might be right. But what if this set of instructions contains instructions on how to change instructions? That would mean that the AI knows how to learn. What if you include instructions on how to learn to learn to learn to learn (...) to change instructions? At what point would you say that this intelligence is still an extension of human intelligence? If you argue like this then you must also put ""human intelligence"" in a set altogether with every animal intelligence because it all originates from some sort of intelligence that is based on physical brain activity. In fact, when a child is born, it is not more intelligent than most of the animal species. The only thing that enhances its intelligence from time to time (and do stuff like speaking or using its hands like tools) is the ability to learn . I don't see why an AI hasn't got the potential to increase its intelligence to level where one would say: ""This is not an extension of human intelligence anymore, this is something independent"".",58.59539718,"No, the way human minds think is in no way related to the way an AI thinks. Although you could say that AI is a much simpler form that represents how the brain processes information. For the human brain to think, sense, and act there are billions of connections is various cortex's of the brain that process information in different ways. If talking about brain information as electrical signals you could say that different cortex's of the brain have change in power of specific frequency bands of the brain signal which can be decoded as planning, preparation, thoughts, visual, movement, creativity, attentiveness and much more. So, to answer your question AI could be considered as an extremely minute extension of human intelligence. It's like comparing our solar system to the Milky Way, although the comparison maybe a bit too large as we are slowly becoming able to understand the underlying processes and build fast processors mimicking brain processing and efficient power consuming hardware tech to run humongous neural nets. In the soon future your statement may hold true.",52.17696408,"Right, AI is an extension of human creativity and the implied limitation is that it inherits bias through the specific choice of which features to consider. Given a set of features it is then far more able at calculating which combination of features best helps explain the relationship being considered than is the human mind. Humans are too distracted to think to the depth that AI and machine learning can. But that extreme focus is not intelligence. One of the issues that prevents the human mind from thinking at comparable depth is the need to massage the set of features that might apply; we are constantly reviewing features, adding in new and eliminating those that do not contribute. Creativity is openness to admitting other seemingly unrelated features and hoping for emergence, and managing to persist in being creative when emergence is delayed.",52.04935741,"The answer in part seems to depend on what you mean by ""human intelligence"". If you mean behavior that would usually be regarded as requiring intelligence were a human to produce it, then various types machines can be intelligent. Such ""intelligent"" machines presumably include player pianos. Playing the piano and producing a melody is widely regarded as requiring human intelligence when humans do it. Player pianos produce the same sort of behavior, but without a human touching a key. Hence (so the argument goes) player pianos are intelligent. But if ""intelligence"" includes having the inner process of understanding, say understanding the meanings of symbols of written language, then at least according to philosopher John Searle, purely symbol manipulating devices such as digital computers could never be intelligent.  This is because symbols in themselves don't contain or indicate their meanings, and all the computing machine gets and manipulates is symbols in themselves. However, there does seem to be a sense in which the question ""Is artificial intelligence really just human intelligence"" is true of computers. This is when the behavior of the machine is caused by human intelligence. A human writes a program that defines, mandates, the behavior of the machine (just like a human designs the mechanism and paper roll of a player piano). This design takes human intelligence. The machine has no intrinsic, or innate, intelligence. It's just an automaton mindlessly following the causal sequence created by the intelligent human designer. Now if computers are purely symbol-manipulating devices, and if Searle is right, AI is doomed, at lest as long as its development platform is the digital computer (and no other machine is available or seems on the horizon). However, are computers purely symbol-manipulating devices? If not, there may be a way they can acquire meanings, or knowledge, and, for instance, learn languages. If computers can receive (including from digital sensors) and manipulate more than just symbols, they may be able to acquire the inner structures and execute the inner processes needed for human-like understanding. That is, they might be able to acquire knowledge by way of sensing the environment (as humans do). A human might write the program that facilitates acquisition of such knowledge, but what the knowledge is about would be derived from the sensed environment not from a human mind. But here we're talking about ""intelligence"" defined over inner processes and structures, not or not just external behavior. If you define human intelligence as external behavior, as the Turing test does and as AI researchers often do, then music boxes with pirouetting figurines, player pianos, and programmed computers all have human-like intelligence, and artificial intelligence as it exists today is really just the same sort of thing as human intelligence.",63.58346906,,
15986,What are examples of promising AI/ML techniques that are computationally intractable?,machine-learning,"AIXI is a Bayesian, non-Markov, reinforcement learning and artificial general intelligence agent that is incomputable , given the involved incomputable Kolmogorov complexity . However, there are approximations of AIXI, such as AIXItl, described in Universal Artificial Intelligence:  Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability (2005), by Marcus Hutter (the original author of AIXI), and MC-AIXI-CTW (which stands for Monte Carlo AIXI Context-Tree Weighting). Here is a Python implementation of MC-AIXI-CTW: https://github.com/gkassel/pyaixi .",50.42679311,"Exact Bayesian inference is (often) intractable (i.e. there is no closed-form solution, or numerical approximations are also computationally expensive) because it involves the computation of an integral over a range of real (or even floating-point) numbers, which can be intractable. More precisely, for example, if you want to find the parameters $\mathbf{\theta} \in \Theta$ of a model given some data $D$ , then Bayesian inference is just the application of the Bayes' theorem \begin{align}
p(\mathbf{\theta} \mid D) 
&= \frac{p(D \mid \mathbf{\theta}) p(\mathbf{\theta})}{p(D)} \\
&= \frac{p(D \mid \mathbf{\theta}) p(\mathbf{\theta})}{\int_{\Theta} p(D \mid \mathbf{\theta}^\prime) p(\mathbf{\theta}^\prime) d \mathbf{\theta}^\prime} \\
&= \frac{p(D \mid \mathbf{\theta}) p(\mathbf{\theta})}{\int_{\Theta} p(D, \mathbf{\theta}^\prime) d \mathbf{\theta}^\prime } \tag{1}\label{1}
\end{align} where $p(\mathbf{\theta} \mid D)$ is the posterior (which is what you want to find or compute), $p(D \mid \mathbf{\theta})$ is the likelihood of your data given the (fixed) parameters $\mathbf{\theta}$ , $p(\mathbf{\theta})$ is the prior and $p(D) = \int_{\Theta} p(D \mid \mathbf{\theta}^\prime) p(\mathbf{\theta}^\prime) d \mathbf{\theta}^\prime$ is the evidence of the data (which is an integral given that $\mathbf{\theta}$ is assumed to be a continuous random variable), which is intractable because the integral is over all possible values of $\mathbf{\theta}$ , that is, ${\Theta}$ . If all terms in \ref{1} were tractable (polynomially computable), then, given more data $D$ , you could iteratively keep on updating your posterior (which becomes your prior on the next iteration), and exact Bayesian inference would become tractable. The variational Bayesian approach casts the problem of inferring $p(\mathbf{\theta} \mid D)$ (which requires the computation of the intractable evidence term) as an optimization problem, which approximately finds the posterior, more precisely, it approximates the intractable posterior, $p(\mathbf{\theta} \mid D)$ , with a tractable one, $q(\mathbf{\theta} \mid D)$ (the variational distribution ). For example, the important variational auto-encoder (VAEs) paper (which did not introduce the variational Bayesian approach) uses the variational Bayesian approach to approximate a posterior in the context of neural networks (that represent distributions), so that existing machine (or deep) learning techniques (that is, gradient descent with back-propagation) can be used to learn the parameters of a model. The variational Bayesian approach (VBA) becomes always more appealing in machine learning. For example, Bayesian neural networks (which can partially solve some of the inherent problems of non-Bayesian neural networks) are usually inspired by the results reported in the VAE paper , which shows the feasibility of the VBA in the context of deep learning.",51.98342416,"This question gets at a really interesting fact about AI research in general: AI is hard . In fact, almost every AI problem is computationally hard (typically NP-Hard, or #P-Hard). This means that most new areas of AI research starts out by characterizing some problem that is intractable, and proposing an algorithm that technically works, but is too slow to be useful. However, that's not the whole story . Usually AI researchers then proceed to develop tractable techniques according to one of two schools: Algorithms that usually work in practice, and are always fast, but are not completely correct. Algorithms that are always correct, and are usually fast, but are sometimes very slow, or only work on specific kinds of sub-problem. Take together, these let AI address most problems. For example: Search was developed as a general purpose AI technique for solving planning and logic problems. The first algorithm, called the general problem solver , always worked, but was extremely slow. Eventually, we developed heuristic guided search techniques like A* , domain specific tricks like GraphPlan , and stochastic search techniques like Monte-Carlo Tree Search . Bayesian Learning (or Bayesian Inference ) has been known since the 1800's, but it is known to involve either the computation of intractable integrals, or the creation of exponentially sized discrete tables, making it NP-Hard . A very simple algorithm involves applying brute force and enumerating all of the options, but this is too slow. Eventually, we developed techniques like Gibbs Sampling (that is always fast, and usually right), or Variable Elimination (that is always right, and usually fast). Today we can solve most problems of this kind very well. Reasoning about language was thought to be very hard (see the Frame Problem ), because there are an infinite number of possible sentences, and an infinite number of possible contexts they could be used in. Exact approaches based on rules did not work. Eventually we developed probabilistic approaches like Hidden Markov Models and Deep Neural Networks , that aren't certain to work, but work so well in practice that language problems are, if not completely solve, getting very close . Games of chance, like Poker, were thought to be impossible, because they are #P-Hard to complete exactly (this is harder than NP-Hard). There will probably never be an exact algorithm for these. In spite of this, techniques like CFR+ can derive solutions that are so close to exactly perfect that you would need to play for decades against them to tell the difference. So, what's still hard? Inferring the structure of a Bayesian network. This is closely related to the problem of causality . It's #P-Hard, but we don't currently have any good algorithms to even do this approximately very well. This is an active area of research. Picking a machine learning algorithm to use for an arbitrary problem. The No Free Lunch theorem tells us this is not possible in general, but it seems like we ought to be able to do it pretty well in practice. More to come...?",54.89123756,"The logical induction algorithm can make predictions about whether mathematical statements are true or false, which are eventually consistent; e.g. if A is true, its probability will eventually reach 1; if B implies C then C 's probability will eventually reach or exceed B 's; the probability of D will eventually be the inverse of not(D) ; the probabilities of E and F will eventually reach or exceed that of E AND F ; etc. It can also give consistent predictions about itself, e.g. ""the logical induction algorithm will predict the probability of X to be Y at timestep T"", whilst avoiding paradoxes like the liar's paradox.",50.61188468,"Hutter's ""fastest and shortest algorithm for all well-defined problems"" is the ultimate just-in-time compiler . It runs a given program and, in parallel, searches for proofs that some other program is equivalent but faster. The running program is restarted at exponentially-spaced intervals; if a faster program has been found, that is started instead. The running time of this algorithm is of the same order as the fastest provably-equivalent algorithm, plus a constant $O(1)$ term (the time taken to find the proof, which doesn't dependent on the input size). For example, it will run Bubble Sort in at most $O(n~log (n))$ ) time, by finding a proof that it's equivalent to such a fast algorithm (like Merge Sort ) then switching to that algorithm. Hutter's algorithm is similar to the best ahead-of-time compilers , known as super-optimisers . They search through all possible programs, starting with the smallest/fastest, until they find one equivalent to the given code. These are actually in use right now, but are only practical for programs that are a few (machine code) instructions long. The LLVM compiler contains some ""peephole optimisations"" (i.e. find/replace templates) that were found by a super-optimiser a few years ago. Note that super-optimisation should not be confused with super-compilation (a rather general optimisation, which is not optimal and involves no search).",51.24578779,"Levin's search algorithm is a general method of function inversion. Many AI tasks are of this sort, e.g. given a cost or reward function ( object -> cost or object -> reward ), its inverse ( cost -> object or reward -> object ) would find an object with the given cost/reward; we could ask this inverse function for an object with low cost or high reward. Levin's algorithm is optimal iff the given function is a ""black box"" with no known pattern in its output. For example, if a small change in the input produces a small change in the output, Levin search wouldn't be optimal; instead we could use hill climbing or some other gradient method. Levin's algorithm looks for the function's inverse by running all possible programs in parallel, assigning exponentially more time to shorter programs. Whenever a program halts, we check whether its output is the desired inverse (i.e. whether givenProgram(outputOfHaltedProgram) = desiredOutput , e.g. whether cost(outputOfHaltedProgram) = low ). This way ""simpler"" guesses at the inverse are made first; where we define the simplicity (AKA ""Levin complexity"") of a value by looking through all programs $p$ which generate that value, and minimising the sum of: $p$ 's length (in bits) plus the logarithm of $p$ 's running time (in steps). If we ignored running time we would get Kolmogorov complexity , which is theoretically nicer but is incomputable (we don't know when to give up waiting for short non-halting programs, due to the Halting Problem ). Levin complexity is computable, since we can give up waiting for those loops once they've taken exponentially-many steps as a longer solution (e.g. once we've spent $T$ steps waiting for a possible loop of length $N$ , we can start trying programs that are $N+1$ bits long for $T/2$ steps). The running time of Levin Search is of the same order as the simplest such inverse-value-generating program. However, this is misleading, since the fraction of steps allocated to running any particular program $p$ is $1/2^{complexity(p)}$ , so this constant factor will be slowing down the computation of the inverse too. There is also overhead associated with context-switching between all of these programs. The FAST algorithm does the same job as Levin Search, in the same time, but avoids the overhead of context-switching between an infinite number parallel programs. Instead it runs one program at a time, cuts it off if it hasn't halted within an appropriate number of steps, then retries for twice as many steps later on. The GUESS algorithm is also equivalent, but chooses programs at random; the expected runtime is the same, but there's no need to keep track of loop counters like in FAST, plus it can be run on parallel hardware without having to coordinate anything (whilst still avoiding the infinite parallelism of the original). Levin search is currently impractical in its original setting of searching through general-purpose, Turing-complete programs. It can be useful in less general domains, e.g. searching through the space of hyper-parameters or other domain-specific, configuration-like ""programs"".",50.80543821,"In general, partially-observable Markov decision processes (POMDPs) are also computationally intractable to solve exactly . However, there are several approximations methods . See, for example, Value-Function Approximations for Partially Observable Markov Decision Processes (2000) by Milos Hauskrecht.",54.39841433,,,,
15820,Is there any research on the development of attacks against artificial intelligence systems?,image-recognition,"Yes, there is some research on this topic, which can be called adversarial machine learning , which is more an experimental field. An adversarial example is an input similar to the ones used to train the model, but that leads the model to produce an unexpected outcome. For example, consider an artificial neural network (ANN) trained to distinguish between oranges and apples. You are then given an image of an apple similar to another image used to train the ANN, but that is slightly blurred. Then you pass it to the ANN, which unexpectedly predicts the object to be an orange. Several machine learning and optimization methods have been used to detect the boundary behaviour of machine learning models, that is, the unexpected behaviour of the model that produces different outcomes given two slightly different inputs (but that correspond to the same object). For example, evolutionary algorithms have been used to develop tests for self-driving cars. See, for example, Automatically testing self-driving cars with search-based procedural content generation (2019) by Alessio Gambi et al.",52.62482313,"Sometimes if the rules used by an AI to identify characters are discovered, and if the rules used by a human being to identify the same characters are different, it is possible to design characters that are recognized by a human being but not recognized by an AI.  However, if the human being and AI both use the same rules, they will recognize the same characters equally well. A student I advised once trained a neural network to recognize a set of numerals, then used a genetic algorithm to alter the shapes and connectivity of the numerals so that a human could still recognize them but the neural network could not.  Of course, if he had then re-trained the neural network using the expanded set of numerals, it probably would have been able to recognize the new ones.",50.68582297,"Yes there are, for instance one pixel attacks described in Su, J.; Vargas, D.V.; Kouichi, S. One pixel attack for fooling deep
  neural networks. arXiv:1710.08864 One pixels attacks are attacks in which changing one pixel in input image can strongly affect the results.",54.94089658,"Here's an example: How to hack your face to dodge the rise of facial recognition tech In his recent book The Fall , Stephenson wrote about smartglasses that that project a pattern over the facial features to foil recognition algorithms (which seems not only feasible but likely;) Here's an article from our sponsors, Adversarial AI: As New Attack Vector Opens, Researchers Aim to Defend Against It which includes this graphic of "" Five ways AI hacks can lead to real world problems "". The article references the conference on The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation , where you can download the full report. I'm assuming many such examples exist in the real world, and will amend this link-based answer as I find them. Good question!",52.80771505,"Isn't that essentially what chess does? For example, A human can recognize that a Ruy exchange offers white great winning chances (because of pawn structure) by move 4 while an engine would take several hours of brute force calculation to understand the same idea.",50.36903277,"There are many insightful comments and answers so far. I want to illustrate my idea of ""color blindness test"" more. Maybe it's a hint to lead us to the truth. Imagine there are two people here. One is colorblind (AI) and another one is non-colorblind (human). If we show them a normal number ""6"", both of them can easily recognize it as number 6. Now, if we show them a delicately designed colorful number ""6"", only human can recognize it as number 6 while AI will recognize it as number 8. The interesting of this analogy is that we can not teach/train colorblind people to recognize this delicately designed colorful number ""6"" because of natural difference, which I believe is also the case between AI and human. AI gets results from computation while human gets results from ""mind"". Therefore, like @S. McGrew's answer, if we can find the fundamental difference between AI and human of how we read things, then this question is answered.",50.83063305,"Here's a live demo: https://www.labsix.org/physical-objects-that-fool-neural-nets/ Recall that neural nets are trained by feeding in the training data, evaluating the net, and using the error between the observed and the intended output to adjust the weights and bring the observed output closer to the intended. Most attacks have been on the observation that you can, instead of updating the weights, update the input neurons. That is, permute the image. However, this attack is very finnicky. It falls apart when the permuted image is scaled, rotated, blurred, or otherwise altered. That's clearly a cat to us, but guacamole to the neural net. But a slight rotation and the net starts classifying it correctly again. However recent breakthroughs allow actual objects presented to a real camera to be reliably misclassified. That's clearly a turtle, albeit with a wonky pattern on its shell. But that net is convinced it's a rifle from practically every angle.",52.25299508,"There are some research at least on the ""foolability"" of neural networks, that gives insight on potential high risk of neural nets even when they ""seem"" 99.99% acurate. A very good paper on this is in Nature: https://www.nature.com/articles/d41586-019-03013-5 In a nutshell: It shows diverse exemples of fooling neural networks/AIs, for exemple one where a few bits of scotch tape places on a ""Stop"" sign changes it, for the neural net, into a ""limited to 40"" sign... (whereas a human would still see a ""Stop"" sign!). And also 2 striking exemples of turning an animal into another by just adding invisible (for humans!) colored dots, (turning in the exemple a Panda into a Gibbon, where a human hardly see anything different so still sees a Panda). Then they elaborate on diverse research venues, involving for exemple ways to try to prevent such attacks. The whole page is a good read to any AI researcher and shows lots of troubling problems (especially for automated systems such as cars, and soon maybe armaments). An exerpt relevant to the question: Hendrycks and his colleagues have suggested quantifying a DNN’s robustness against making errors by testing how it performs against a large range of adversarial examples. However, training a network to withstand one kind of attack could weaken it against others, they say. And researchers led by Pushmeet Kohli at Google DeepMind in London are trying to inoculate DNNs against making mistakes. Many adversarial attacks work by making tiny tweaks to the component parts of an input — such as subtly altering the colour of pixels in an image — until this tips a DNN over into a misclassification. Kohli’s team has suggested that a robust DNN should not change its output as a result of small changes in its input, and that this property might be mathematically incorporated into the network, constraining how it learns. For the moment, however, no one has a fix on the overall problem of brittle AIs. The root of the issue, says Bengio, is that DNNs don’t have a good model of how to pick out what matters. When an AI sees a doctored image of a lion as a library, a person still sees a lion because they have a mental model of the animal that rests on a set of high-level features — ears, a tail, a mane and so on — that lets them abstract away from low-level arbitrary or incidental details. “We know from prior experience which features are the salient ones,” says Bengio. “And that comes from a deep understanding of the structure of the world.” Another excerpt, near the end: ""Researchers in the field say they are making progress in fixing deep learning’s flaws, but acknowledge that they’re still groping for new techniques to make the process less brittle. There is not much theory behind deep learning, says Song. “If something doesn’t work, it’s difficult to figure out why,” she says. “The whole field is still very empirical. You just have to try things.”""",55.02064401,,
15730,Can digital computers understand infinity?,deep-learning,"I think this is a fairly common misconception about AI and computers, especially among laypeople. There are several things to unpack here. Let's suppose that there's something special about infinity (or about continuous concepts) that makes them especially difficult for AI. For this to be true, it must both be the case that humans can understand these concepts while they remain alien to machines, and that there exist other concepts that are not like infinity that both humans and machines can understand. What I'm going to show in this answer is that wanting both of these things leads to a contradiction. The root of this misunderstanding is the problem of what it means to understand . Understanding is a vague term in everyday life, and that vague nature contributes to this misconception. If by understanding, we mean that a computer has the conscious experience of a concept, then we quickly become trapped in metaphysics. There is a long running , and essentially open debate about whether computers can ""understand"" anything in this sense, and even at times, about whether humans can! You might as well ask whether a computer can ""understand"" that 2+2=4. Therefore, if there's something special about understanding infinity, it cannot be related to ""understanding"" in the sense of subjective experience. So, let's suppose that by ""understand"", we have some more specific definition in mind. Something that would make a concept like infinity more complicated for a computer to ""understand"" than a concept like arithmetic. Our more concrete definition for ""understanding"" must relate to some objectively measurable capacity or ability related to the concept (otherwise, we're back in the land of subjective experience). Let's consider what capacity or ability might we pick that would make infinity a special concept, understood by humans and not machines, unlike say, arithmetic. We might say that a computer (or a person) understands a concept if it can provide a correct definition of that concept. However, if even one human understands infinity by this definition, then it should be easy for them to write down the definition. Once the definition is written down, a computer program can output it. Now the computer ""understands"" infinity too. This definition doesn't work for our purposes. We might say that an entity understands a concept if it can apply the concept correctly. Again, if even the one person understands how to apply the concept of infinity correctly, then we only need to record the rules they are using to reason about the concept, and we can write a program that reproduces the behavior of this system of rules. Infinity is actually very well characterized as a concept, captured in ideas like Aleph Numbers . It is not impractical to encode these systems of rules in a computer, at least up to the level that any human understands them. Therefore, computers can ""understand"" infinity up to the same level of understanding as humans by this definition as well. So this definition doesn't work for our purposes. We might say that an entity ""understands"" a concept if it can logically relate that concept to arbitrary new ideas. This is probably the strongest definition, but we would need to be pretty careful here: very few humans (proportionately) have a deep understanding of a concept like infinity. Even fewer can readily relate it to arbitrary new concepts. Further, algorithms like the General Problem Solver can, in principal, derive any logical consequences from a given body of facts, given enough time. Perhaps under this definition computers understand infinity better than most humans, and there is certainly no reason to suppose that our existing algorithms will not further improve this capability over time. This definition does not seem to meet our requirements either. Finally, we might say that an entity ""understands"" a concept if it can generate examples of it. For example, I can generate examples of problems in arithmetic, and their solutions. Under this definition, I probably do not ""understand"" infinity, because I cannot actually point to or create any concrete thing in the real world that is definitely infinite. I cannot, for instance, actually write down an infinitely long list of numbers, merely formulas that express ways to create ever longer lists by investing ever more effort in writing them out. A computer ought to be at least as good as me at this. This definition also does not work. This is not an exhaustive list of possible definitions of ""understands"", but we have covered ""understands"" as I understand it pretty well. Under every definition of understanding, there isn't anything special about infinity that separates it from other mathematical concepts. So the upshot is that, either you decide a computer doesn't ""understand"" anything at all, or there's no particularly good reason to suppose that infinity is harder to understand than other logical concepts. If you disagree, you need to provide a concrete definition of ""understanding"" that does separate understanding of infinity from other concepts, and that doesn't depend on subjective experiences (unless you want to claim your particular metaphysical views are universally correct, but that's a hard argument to make). Infinity has a sort of semi-mystical status among the lay public, but it's really just like any other mathematical system of rules: if we can write down the rules by which infinity operates, a computer can do them as well as a human can (or better).",62.46163826,"I think your premise is flawed. You seem to assume that to ""understand""(*) infinities requires infinite processing capacity, and imply that humans have just that, since you present them as the opposite to limited, finite computers. But humans also have finite processing capacity. We are beings built of a finite number of elementary particles, forming a finite number of atoms, forming a finite number of nerve cells. If we can, in one way or another, ""understand"" infinities, then surely finite computers can also be built that can. (* I used ""understand"" in quotes, because I don't want to go into e.g. the definition of sentience etc. I also don't think it matters in regarding this question.) As a human being, we can think infinity. In principle, if we have enough resources (time etc.), we can count infinitely many things (including abstract, like numbers, or real). Here, you actually say it out loud. ""With enough resources."" Would the same not apply to computers? While humans can , e.g. use infinities when calculating limits etc. and can think of the idea of something getting arbitrarily larger, we can only do it in the abstract, not in the sense being able to process arbitrarily large numbers. The same rules we use for mathematics could also be taught to a computer.",55.25976957,"TL;DR : The subtleties of infinity are made apparent in the notion of unboundedness. Unboundedness is finitely definable. ""Infinite things"" are really things with unbounded natures. Infinity is best understood not as a thing but as a concept. Humans theoretically possess unbounded abilities not infinite abilities (eg to count to any arbitrary number as opposed to ""counting to infinity""). A machine can be made to recognize unboundedness. Down the rabbit hole again How to proceed? Let's start with ""limits."" Limitations Our brains are not infinite (lest you believe in some metaphysics). So, we do not ""think infinity"". Thus, what we purport as infinity is best understood as some finite mental concept against which we can ""compare"" other concepts. Additionally, we cannot ""count infinite integers."" There is a subtly here that is very important to point out: Our concept of quantity/number is unbounded . That is, for any any finite value we have a finite/concrete way or producing another value which is strictly larger/smaller. That is, Provided finite time we could only count finite amounts. You cannot be ""given infinite time"" to ""count all the numbers"" this would imply a ""finishing"" which directly contradicts the notion of infinity. Unless you believe humans have metaphysical properties which allow them to ""consistently"" embody a paradox. Additionally how would you answer: What was the last number you counted? With no ""last number"" there is never a ""finish"" and hence never an ""end"" to your counting. That is you can never ""have enough"" time/resources to ""count to infinity."" I think what you mean is we can fathom the notion of bijection between infinite sets. But this notion is a logical construction (ie it's a finite way of wrangling what we understand to be infinite). However, what we are really doing is: Within our bounds we are talking about our bounds and, when ever we need to, we can expand our bounds (by a finite amount). And we can even talk about the nature of expanding our bounds. Thus: Unboundedness A process/thing/idea/object is deemed unbounded if given some measure of its quantity/volume/existence we can in a finite way produce an ""extension"" of that object which has a measure we deem ""larger"" (or ""smaller"" in the case of infinitesimals) than the previous measure and that this extension process can be applied to the nascent object (ie the process is recursive). Canonical case number one: The Natural Numbers Additionally, our notion of infinity prevents any ""at-ness"" or ""upon-ness"" unto infinity. That is, one never ""arrives"" at infinity nor does one ever ""have"" infinity. Rather, one proceeds unboundedly. Thus how do we conceptualize infinity? Infinity It seems that ""infinity"" as a word is misconstrued to mean that there is a thing that exists called ""infinity"" as opposed to a concept called ""infinity"". Let's smash atoms with the word: Infinite: limitless or endless in space, extent, or size; impossible to measure or calculate. in- :a prefix of Latin origin, corresponding to English un-, having a negative or privative force, freely used as an English formative, especially of adjectives and their derivatives and of nouns (inattention; indefensible; inexpensive; inorganic; invariable).
  ( source ) Finite: having limits or bounds. So in-finity is really un-finity which is not having limits or bounds . But we can be more precise here because we can all agree the natural numbers are infinite but any given natural number is finite. So what gives? Simple: the natural numbers satisfy our unboundedness criterium and thus we say ""the natural numbers are infinite."" That is, ""infinity"" is a concept. An object/thing/idea is deemed infinite if it possess a property/facet that is unbounded. As before we saw that unboundedness is finitely definable. Thus, if the agent you speak of was programmed well enough to spot the pattern in the numbers on the cards and that the numbers are all coming from the same set it could deduce the unbounded nature of the sequence and hence define the set of all numbers as infinite - purely because the set has no upper bound . That is, the progression of the natural numbers is unbounded and hence definably infinite. Thus, to me, infinity is best understood as a general concept for identifying when processes/things/ideas/objects posses an unbounded nature. That is, infinity is not independent of unboundedness. Try defining infinity without comparing it to finite things or the bounds of those finite things. Conclusion It seems feasible that a machine could be programmed to represent and detect instances of unboundedness or when it might be admissible to assume unboundedness.",58.78424225,"In Haskell, you can type: print [1..] and it will print out the infinite sequence of numbers, starting with: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795, It will do this until your console runs out of memory. Let's try something more interesting. double x = x * 2
print (map double [1..]) And here's the start of the output: [2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206,208,210,212,214,216,218,220,222,224,226,228,230,232,234,236,238,240,242,244,246,248,250,252,254,256,258,260,262,264,266,268,270,272,274,276,278,280,282,284,286,288,290,292,294,296,298,300,302,304,306,308,310,312,314,316,318,320,322,324,326,328,330,332,334,336,338,340,342,344,346,348,350,352,354,356,358,360,362,364,366,368,370,372,374,376,378,380,382,384,386,388,390,392 These examples show infinite computation. In fact, you can keep infinite data structures in Haskell, because Haskell has the notion of non-strictness -- you can do computation on entities that haven't been fully computed yet. In other words, you don't have to fully compute an infinite entity to manipulate that entity in Haskell. Reductio ad absurdum.",50.24356325,"I believe humans can be said to understand infinity since at least Georg Cantor because we can recognize different types of infinites (chiefly countable vs. uncountable) via the concept of cardinality . Specifically, a set is countably infinite if it can be mapped to the natural numbers , which is to say there is a 1-to-1 correspondence between the elements of countably infinite sets.  The set of all reals is uncountable, as is the set of all combinations of natural numbers, because there will always be more combinations than natural numbers where n>2, resulting in a set with a greater cardinality. (The first formal proofs for uncountability can be found in Cantor, and is subject of Philosophy of Math .) Understanding of infinity involves logic as opposed to arithmetic because we can't express, for instance, all of the decimals of a transcendental number , only use approximations. Logic is a fundamental capability of what we think of as computers. An analytic process (AI) that can recognize a function that produces an infinite loop, such as using $\pi$ to draw a circle, might be said to understand infinity... ""Never ending"" is a definition of infinity, with the set of natural numbers as an example (there is a least number, 1, but no greatest number.) Intractability vs. Infinity Outside of the special case of infinite loops, I have to wonder if an AI is more oriented on computational intractability as opposed to infinity. A problem is said to be intractable if there is not enough time and space to completely represent it, and this can be extended to many real numbers. $\pi$ may be understood to be infinite because it arises from/produces a circle, but I'm not sure this is the case with all real numbers with an intractable number of decimals. Would the AI assume such a number were infinite or merely intractable?  The latter case is concrete as opposed to abstract--either it can finish the computation or not. This leads to the halting problem . Turing's proof that a general algorithm to solve the halting problem for all possible program-input pairs cannot exist could be taken as an indication that an algorithm based on the Turing-Church model of computation cannot have a perfect understanding of infinity. If an alternate computational model arose that could solve the halting problem, it might be argued that an algorithm could have a perfect understanding, or at least demonstrate an understanding comparable to humans.",58.83802387,"(There's a summary at the bottom for those who are too lazy or pressed for time to read the whole thing.) Unfortunately to answer this question I will mainly be deconstructing the various premises. As I mentioned before, humans understand infinity because they are capable, at least, counting infinite integers, in principle. I disagree with the premise that humans would actually be able to count to infinity. To do so, said human would need an infinite amount of time, an infinite amount of memory (like a Turing machine) and most importantly an infinite amount of patience - in my experience most humans get bored before they even count to 1,000. Part of the problem with this premise is that infinity is actually not a number, it's a concept that expresses an unlimited amount of 'things'. Said 'things' can be anything: integers, seconds, lolcats, the important point is the fact that those things are not finite. See this relevant SE question for more details: https://math.stackexchange.com/questions/260876/what-exactly-is-infinity To put it another way: if I asked you ""what number comes before infinity?"" what would your answer be? This hypothetical super-human would have to count to that number before they could count infinity. And they'd need to know the number before that first, and the one before that, and the one before that... Hopefully this demonstrates why the human would not be able to actually count to infinity - because infinity does not exist at the end of the number line, it is the concept that explains the number line has no end. Neither man nor machine can actually count up to it, even with infinite time and infinite memory. For example, If a computer can differentiate 10 different numbers or things, it means that it really understand these different things somehow. Being able to 'differentiate' between 10 different things doesn't imply the understanding of those 10 things. A well-known thought experiment that questions the idea of what it means to 'understand' is John Searle's Chinese Room experiment: Imagine a native English speaker who knows no Chinese locked in a room full of boxes of Chinese symbols (a data base) together with a book of instructions for manipulating the symbols (the program). Imagine that people outside the room send in other Chinese symbols which, unknown to the person in the room, are questions in Chinese (the input). And imagine that by following the instructions in the program the man in the room is able to pass out Chinese symbols which are correct answers to the questions (the output). The program enables the person in the room to pass the Turing Test for understanding Chinese but he does not understand a word of Chinese. The point of the argument is this: if the man in the room does not understand Chinese on the basis of implementing the appropriate program for understanding Chinese then neither does any other digital computer solely on that basis because no computer, qua computer, has anything the man does not have. The thing to take away from this experiment is that the ability to process symbols does not imply that one actually understands those symbols. Many computers process natural languages every day in the form of text (characters encoded as integers, typically in a unicode-based encoding like UTF-8), but they do not neccessarily understand those languages. On a simpler Effectively all computers are able to add two numbers together, but they do no necessarily understand what they are doing. In other words, even in the 'deep learning vision model' the computer arguably does not understand the numbers (or 'symbols') it is being shown, it is merely the algorithm's ability to simulate intelligence that allows it to be classed as artificial intelligence. For example, we can take a deep learning vision model that recognizes numbers on the card. This model must assign a number to each different card to differentiate each integer. Since there exist infinite numbers of integer, how can the model assign different number to each integer, like a human being, on the digital computers? If it cannot differentiate infinite things, how does it understand infinity? If you were to perform the same card test on a human, and continually increased the number of cards used, eventually a human wouldn't be able to keep track of them all due to lack of memory. A computer would experience the same problem, but could theoretically outperform the human. So now I ask you, can a human really differentiate infinite things? Personally I suspect the answer is no, because all humans have limited memory, and yet I would agree that humans most likely can understand infinity to some degree (some can do so better than others). As such, I think the question ""If it cannot differentiate infinite things, how does it understand infinity?"" has a flawed premise - being able to differentiate infinite things is not a prerequisite for understanding the concept of infinity. Summary: Essentially your question hinges on what it means to 'understand' something. Computers can certainly represent infinity, the IEEE floating point specification defines both positive and negative infinity, and all modern processors are capable of processing floating points (either in hardware or through software). If AIs are ever capable of actually understanding things then theoretically they might be able to understand the concept of infinity, but we're a long way off being able to definitively prove this either way, and we'd have to come to a consensus about what it means to 'understand' something first.",62.8123516,"Then premise assumes that humans ""understand"" infinity. Do we? I think you'd need to tell me what criterion you would use, if you wanted to know whether I ""understand"" infinity, first. In the OP, the idea is given that I could ""prove"" I ""understand"" infinity, because ""In principle, if we have enough resources (time etc.), we can count infinitely many things (including abstract, like numbers, or real)."" Well, that's simply not true. Worse, if it were true (which it isnt), then it would be equally true for a computer. Here's why: Yes, you can in principle count integers, and see that counting never ends. But even if you had enough resources, you could never ""count infinitely many things"". There would always be more. That's what ""infinite"" means. Worse, there are multiple orders (""cardinalities"") of infinity. Most of them, you can't count, even with infinite time, and perhaps not even with infinite other resources. They are actually uncountable. They literally cannot be mapped to a number line, or to the set of integers. You cannot order them in such a way that they can be counted, even in principle. Even worse, how do you do that bit where you decide ""in principle"" what I can do, when I clearly can't ever do it, or even the tiniest part of it? That step feels layman-style assumptive, not actually seeing the issues in doing it rigorously. It may not be trivial. Last, suppose this was your actual test, like in the OP. So if I could ""in principle with enough resources (time etc) count infinitely many things"", it would be enough for you to decide I ""understood"" infinity (whatever that means). Then so could a computer with sufficient resources (RAM, time, algorithm). So the test itself would be satisfied trivially by a computer if you gave the computer the same criteria. I think maybe a more realistic line of logic is that what this question actually shows, is that most (probably all?) humans actually do not understand infinity. So understanding infinity is probably not a good choice of test/requirement for AI. If you doubt this, ask yourself. Do you honestly, truly, and seriously, ""understand"" a hundred trillion years (the possible life of a red dwarf star)? Like, can you really comprehend what its like, experiencing a hundred trillion years, or is it just a 1 with lots of zeros? What about a femtosecond? Or a time interval of about 10^-42 seconds? Can you truly ""understand"" that? A timescale compared to which, one of your heartbeats, compares like one of your heartbeats compares to a billion billion times the present life of this universe? Can you really ""understand infinity"", yourself? Worth thinking about......",62.78962115,"By adding some rules for infinity in arithmetic (such as infinity minus a large finite number is infinity, etc.), the digital computer  can appear to understand the notion of infinity. Alternatively, the computer can simply replace the number n with its log-star value.  Then, it can differentiate the numbers at a different scale, and can learn that  any number with log-star value > 10 is practically equivalent to infinity.",62.32444358,"I think the concept that is missing in the discussion, so far, is symbolic representation.  We humans represent and understand many concepts symbolically.  The concept of Infinity is a great example of this.   Pi is another, along with some other well-known irrational numbers.  There are many, many others. As it is, we can easily represent and present these values and concepts, both to other humans and to computers, using symbols.  Both computers and humans, can manipulate and reason with these symbols.  For example, computers have been performing mathematical proofs for a few decades now.  Likewise, commercial and/or open source programs are available that can manipulate equations symbolically to solve real world problems. So, as @JohnDoucette has reasoned, there isn't anything that special about Infinity vs many other concepts in math and arithmetic.   When we hit that representational brick wall, we just define a symbol that represents ""that"" and move forward. Note, the concept of infinity has many practical uses.  Any time you have a ratio and the denominator ""goes to"" zero, the value of the expression ""approaches"" infinity.   This isn't a rare thing, really.   So, while your average person on the street isn't conversant with these ideas, lots and lots of scientists, engineers, mathematicians and programmers are.   It's common enough that software has been dealing with Infinity symbolically for a couple decades, now, at least.  E.g. Mathematica: http://mathworld.wolfram.com/Infinity.html",59.46225306
15449,How could artificial intelligence harm us?,philosophy,"tl;dr There are many valid reasons why people might fear (or better be concerned about ) AI, not all involve robots and apocalyptic scenarios. To better illustrate these concerns, I'll try to split them into three categories. Conscious AI This is the type of AI that your question is referring to. A super-intelligent conscious AI that will destroy/enslave humanity. This is mostly brought to us by science-fiction. Some notable Hollywood examples are ""The terminator"" , ""The Matrix"" , ""Age of Ultron"" . The most influential novels were written by Isaac Asimov and are referred to as the ""Robot series"" (which includes ""I, robot"" , which was also adapted as a movie). The basic premise under most of these works are that AI will evolve to a point where it becomes conscious and will surpass humans in intelligence. While Hollywood movies mainly focus on the robots and the battle between them and humans, not enough emphasis is given to the actual AI (i.e. the ""brain"" controlling them). As a side note, because of the narrative, this AI is usually portrayed as supercomputer controlling everything (so that the protagonists have a specific target). Not enough exploration has been made on ""ambiguous intelligence"" (which I think is more realistic). In the real world, AI is focused on solving specific tasks! An AI agent that is capable of solving problems from different domains (e.g. understanding speech and processing images and driving and ... - like humans are) is referred to as General Artificial Intelligence and is required for AI being able to ""think"" and become conscious. Realistically, we are a loooooooong way from General Artificial Intelligence! That being said there is no evidence on why this can't be achieved in the future. So currently, even if we are still in the infancy of AI, we have no reason to believe that AI won't evolve to a point where it is more intelligent than humans. Using AI with malicious intent Even though an AI conquering the world is a long way from happening there are several reasons to be concerned with AI today , that don't involve robots!
The second category I want to focus a bit more on is several malicious uses of today's AI. I'll focus only on AI applications that are available today . Some examples of AI that can be used for malicious intent: DeepFake : a technique for imposing someones face on an image a video of another person. This has gained popularity recently with celebrity porn and can be used to generate fake news and hoaxes. Sources: 1 , 2 , 3 With the use of mass surveillance systems and facial recognition software capable of recognizing millions of faces per second , AI can be used for mass surveillance. Even though when we think of mass surveillance we think of China, many western cities like London , Atlanta and Berlin are among the most-surveilled cities in the world . China has taken things a step further by adopting the social credit system , an evaluation system for civilians which seems to be taken straight out of the pages of George Orwell's 1984. Influencing people through social media . Aside from recognizing user's tastes with the goal of targeted marketing and add placements (a common practice by many internet companies), AI can be used malisciously to influence people's voting (among other things). Sources: 1 , 2 , 3 . Hacking . Military applications, e.g. drone attacks, missile targeting systems. Adverse effects of AI This category is pretty subjective, but the development of AI might carry some adverse side-effects. The distinction between this category and the previous is that these effects, while harmful, aren't done intentionally; rather they occur with the development of AI. Some examples are: Jobs becoming redundant . As AI becomes better, many jobs will be replaced by AI. Unfortunately there are not many things that can be done about this, as most technological developments have this side-effect (e.g. agricultural machinery caused many farmers to lose their jobs, automation replaced many factory workers, computers did the same). Reinforcing the bias in our data . This is a very interesting category, as AI (and especially Neural Networks) are only as good as the data they are trained on and have a tendency of perpetuating and even enhancing different forms of social biases, already existing in the data. There are many examples of networks exhibiting racist and sexist behavior. Sources: 1 , 2 , 3 , 4 .",51.79925586,"Short term Physical accidents , e.g. due to industrial machinery, aircraft autopilot, self-driving cars. Especially in the case of unusual situations such as extreme weather or sensor failure. Typically an AI will function poorly under conditions where it has not been extensively tested. Social impacts such as reducing job availability, barriers for the underprivileged wrt. loans, insurance, parole. Recommendation engines are manipulating us more and more to change our behaviours (as well as reinforce our own ""small world"" bubbles). Recommendation engines routinely serve up inappropriate content of various sorts to young children, often because content creators (e.g. on YouTube) use the right keyword stuffing to appear to be child-friendly. Political manipulation... Enough said, I think. Plausible deniability of privacy invasion . Now that AI can read your email and even make phone calls for you, it's easy for someone to have humans act on your personal information and claim that they got a computer to do it. Turning war into a video game , that is, replacing soldiers with machines being operated remotely by someone who is not in any danger and is far removed from his/her casualties. Lack of transparency . We are trusting machines to make decisions with very little means of getting the justification behind a decision. Resource consumption and pollution. This is not just an AI problem, however every improvement in AI is creating more demand for Big Data and together these ram up the need for storage, processing, and networking. On top of the electricity and rare minerals consumption, the infrastructure needs to be disposed of after its several-year lifespan. Surveillance — with the ubiquity of smartphones and listening devices, there is a gold mine of data but too much to sift through every piece. Get an AI to sift through it, of course! Cybersecurity — cybercriminals are increasingly leveraging AI to attack their targets. Did I mention that all of these are in full swing already? Long Term Although there is no clear line between AI and AGI, this section is more about what happens when we go further towards AGI. I see two alternatives: Either we develop AGI as a result of our improved understanding of the nature of intelligence, or we slap together something that seems to work but we don't understand very well, much like a lot of machine learning right now. In the first case, if an AI ""goes rogue"" we can build other AIs to outwit and neutralise it. In the second case, we can't, and we're doomed. AIs will be a new life form and we may go extinct. Here are some potential problems: Copy and paste. One problem with AGI is that it could quite conceivably run on a desktop computer, which creates a number of problems: Script Kiddies ­— people could download an AI and set up the parameters in a destructive way. Relatedly, Criminal or terrorist groups would be able to configure an AI to their liking. You don't need to find an expert on bomb making or bioweapons if you can download an AI, tell it to do some research and then give you step-by-step instructions. Self-replicating AI — there are plenty of computer games about this. AI breaks loose and spreads like a virus. The more processing power, the better able it is to protect itself and spread further. Invasion of computing resources . It is likely that more computing power is beneficial to an AI. An AI might buy or steal server resources, or the resources of desktops and mobile devices. Taken to an extreme, this could mean that all our devices simply became unusable which would wreak havoc on the .world immediately. It could also mean massive electricity consumption (and it would be hard to ""pull the plug"" because power plants are computer controlled!) Automated factories. An AGI wishing to gain more of a physical presence in the world could take over factories to produce robots which could build new factories and essentially create bodies for itself. These are rather philosophical considerations, but some would argue that AI would destroy what makes us human: Inferiority. What if plenty of AI entities were smarter, faster, more reliable and more creative than the best humans? Pointlessness. With robots replacing the need for physical labour and AIs replacing the need for intellectual labour, we will really have nothing to do. Nobody's going to get the Nobel Prize again because the AI will already be ahead. Why even get educated in the first place? Monoculture/stagnation — in various scenarios (such as a single ""benevolent dictator"" AGI) society could become fixed in a perpetual pattern without new ideas or any sort of change (pleasant though it may be). Basically, Brave New World. I think AGI is coming and we need to be mindful of these problems so that we can minimise them.",51.75778757,"In addition to the other answers, I would like to add to nuking cookie factory example: Machine learning AIs basically try to fulfill a goal described by humans. For example, humans create an AI running a cookie factory. The goal they implement is to sell as many cookies as possible for the highest profitable margin. Now, imagine an AI which is sufficiently powerful. This AI will notice that if he nukes all other cookie factories, everybody has to buy cookies in his factory, making sales rise and profits higher. So, the human error here is giving no penalty for using violence in the algorithm. This is easily overlooked because humans didn't expect the algorithm to come to this conclusion.",50,"My favorite scenario for harm by AI involves not high intelligence, but low intelligence.  Specifically, the grey goo hypothesis. This is where a self-replicating, automated process runs amok and converts all resources into copies of itself. The point here is that the AI is not ""smart"" in the sense of having high intelligence or general intelligence--it is merely very good at a single thing and has the ability to replicate exponentially.",56.62444811,"I have an example which goes in kinda the opposite direction of the public's fears, but is a very real thing, which I already see happening. It is not AI-specific, but I think it will get worse through AI. It is the problem of humans trusting the AI conclusions blindly in critical applications. We have many areas in which human experts are supposed to make a decision. Take for example medicine - should we give medication X or medication Y? The situations I have in mind are frequently complex problems (in the Cynefin sense) where it is a really good thing to have somebody pay attention very closely and use lots of expertise, and the outcome really matters. There is a demand for medical informaticians to write decision support systems for this kind of problem in the medicine (and I suppose for the same type in other domains). They do their best, but the expectation is always that a human expert will always consider the system's suggestion just as one more opinion when making the decision. In many cases, it would be irresponsible to promise anything else, given the state of knowledge and the resources available to the developers. A typical example would be the use of computer vision in radiomics: a patient gets a CT scan and the AI has to process the image and decide whether the patient has a tumor. Of course, the AI is not perfect. Even when measured against the gold standard, it never achieves 100% accuracy. And then there are all the cases where it performs well against its own goal metrics, but the problem was so complex that the goal metric doesn't capture it well - I can't think of an example in the CT context, but I guess we see it even here on SE, where the algorithms favor popularity in posts, which is an imperfect proxy for factual correctness. You were probably reading that last paragraph and nodding along, ""Yeah, I learned that in the first introductory ML course I took"". Guess what? Physicians never took an introductory ML course. They rarely have enough statistic literacy to understand the conclusions of papers appearing in medical journals. When they are talking to their 27th patient, 7 hours into their 16 hour shift, hungry and emotionally drained, and the CT doesn't look all that clear-cut, but the computer says ""it's not a malignancy"", they don't take ten more minutes to concentrate on the image more, or look up a textbook, or consult with a colleague. They just go with what the computer says, grateful that their cognitive load is not skyrocketing yet again. So they turn from being experts to being people who read something off a screen. Worse, in some hospitals the administration does not only trust computers, it also has found out that they are convenient scapegoats. So, a physician has a bad hunch which goes against the computer's output, it becomes difficult for them to act on that hunch and defend themselves that they chose to overrode the AI's opinion. AIs are powerful and useful tools, but there will always be tasks where they can't replace the toolwielder.",50,"I would say the biggest real threat would be the unbalancing/disrupting we are already seeing.  The changes of putting 90% the country out of work are real, and the results (which will be even more uneven distribution of wealth) are terrifying if you think them through.",50,"This only intents to be a complement to other answers so I will not discuss to possibility of AI trying to willingly enslave humanity. But a different risk is already here. I would call it unmastered technology . I have been teached science and technology, and IMHO, AI has by itself no notion of good and evil, nor freedom. But it is built and used by human beings and because of that non rational behaviour can be involved. I would start with a real life example more related to general IT than to AI. I will speak of viruses or other malwares. Computers are rather stupid machines that are good to quickly process data. So most people rely on them. An some (bad) people develop malwares that will disrupt the correct behaviour of computers. And we all know that they can have terrible effects on small to medium organizations that are not well prepared to an computer loss. AI is computer based so it is vulnerable to computer type attacks. Here my example would be an AI driven car. The technology is almost ready to work. But imagine the effect of a malware making the car trying to attack other people on the road. Even without a direct access to the code of the AI, it can be attacked by side channels . For example it uses cameras to read signal signs. But because of the way machine learning is implemented, AI generaly does not analyses a scene the same way a human being does. Researchers have shown that it was possible to change a sign in a way that a normal human will still see the original sign, but an AI will see a different one. Imagine now that the sign is the road priority sign... What I mean is that even if the AI has no evil intents, bad guys can try to make it behave badly. And to more important actions will be delegated to AI (medecine, cars, planes, not speaking of bombs) the higher the risk. Said differently, I do not really fear the AI for itself, but for the way it can be used by humans.",50,"I think one of the most real (ie. related to current, existing AIs) risks are in blindly relying on unsupervised AIs, for two reasons. 1. AI systems may degrade Physical error in AI systems may start producing wildly wrong results in regions in which they were not tested for because the physical system starts providing wrong values. This is sometimes redeemed by self-testing and redundancy, but still requires occasional human supervision. Self learning AIs also have a software weakness - their weight networks or statistic representations may approach local minima where they are stuck with one wrong result. 2. AI systems are biased This is fortunately frequently discussed, but worth mentioning: AI systems' classification of inputs is often biased because the training/testing dataset were biased as well. This results in AIs not recognizing people of certain ethnicity, for more obvious example. However there are less obvious cases that may only be discovered after some bad accident, such as AI not recognizing certain data and accidentally starting fire in a factory, breaking equipment or hurting people.",50,"Human beings currently exist in an ecological-economic niche of ""the thing that thinks"". AI is also a thing that thinks, so it will be invading our ecological-economic niche.  In both ecology and economics, having something else occupy your niche is not a great plan for continued survival. How exactly Human survival is compromised by this is going to be pretty chaotic.  There are going to be a bunch of plausible ways that AI could endanger human survival as a species, or even as a dominant life form. Suppose there is a strong AI without ""super ethics"" which is cheaper to manufacture than a human (including manufacturing a ""body"" or way of manipulating the world), and as smart or smarter than a human. This is a case where we start competing with that AI for resources.  It will happen on microeconomic scales (do we hire a human, or buy/build/rent/hire an AI to solve this problem?).  Depending on the rate at which AIs become cheap and/or smarter than people, this can happen slowly (maybe an industry at a time) or extremely fast. In a capitalist competition, those that don't move over to the cheaper AIs end up out-competed. Now, in the short term, if the AI's advantages are only marginal, the high cost of educating humans for 20-odd years before they become productive could make this process slower.  In this case, it might be worth paying a Doctor above starvation wages to diagnose disease instead of an AI, but it probably isn't worth paying off their student loans.  So new human Doctors would rapidly stop being trained, and existing Doctors would be impoverished.  Then over 20-30 years AI would completely replace Doctors for diagnostic purposes. If the AI's advantages are large, then it would be rapid.  Doctors wouldn't even be worth paying poverty level wages to do human diagnostics.  You can see something like that happening with muscle-based farming when gasoline-based farming took over. During past industrial revolutions, the fact that humans where able to think means that you could repurpose surplus human workers to do other actions; manufacturing lines, service economy jobs, computer programming, etc.  But in this model, AI is cheaper to train and build and as smart or smarter than humans at that kind of job. As evidenced by the ethanol-induced Arab spring, crops and cropland can be used to fuel both machines and humans.  When machines are more efficient in terms of turning cropland into useful work, you'll start seeing the price of food climb.  This typically leads to riots, as people really don't like starving to death and are willing to risk their own lives to overthrow the government in order to prevent this. You can mollify the people by providing subsidized food and the like.  So long as this isn't economically crippling (ie, if expensive enough, it could result in you being out-competed by other places that don't do this), this is merely politically unstable. As an alternative, in the short term, the ownership caste who is receiving profits from the increasingly efficient AI-run economy can pay for a police or military caste to put down said riots.  This requires that the police/military castes be upper lower to middle class in standards of living, in order to ensure continued loyalty -- you don't want them joining the rioters. So one of the profit centers you can put AI towards is AI based military and policing.  Drones that deliver lethal and non-lethal ordnance based off of processing visual and other data feeds can reduce the number of middle-class police/military needed to put down food-price triggered riots or other instability.  As we have already assumed said AIs can have bodies and training cheaper than a biological human, this can also increase the amount of force you can deploy per dollar spent. At this point, we are talking about a mostly AI run police and military being used to keep starving humans from overthrowing the AI run economy and seizing the means of production from the more efficient use it is currently being put to. The vestigial humans who ""own"" the system at the top are making locally rational decisions to optimize their wealth and power.  They may or may not persist for long; so long as they drain a relatively small amount of resources and don't mess up the AI run economy, there won't be much selection pressure to get rid of them.  On the other hand, as they are contributing nothing of value, they position ""at the top"" is politically unstable. This process assumed a ""strong"" general AI.  Narrower AIs can pull this off in pieces.  A cheap, effective diagnostic computer could reduce most Doctors into poverty in a surprisingly short period of time, for example.  Self driving cars could swallow 5%-10% of the economy.  Information technology is already swallowing the retail sector with modest AI. It is said that every technological advancement leads to more and better jobs for humans.  And this has been true for the last 300+ years. But prior to 1900, it was also true that every technological advancement led to more and better jobs for horses.  Then the ICE and automobile arrived, and now there are far fewer working horses; the remaining horses are basically the equivalent of human personal servants: kept for the novelty of ""wow, cool, horse"" and the fun of riding and controlling a huge animal.",50.82570769
14224,Why do we need explainable AI?,philosophy,"As argued by Selvaraju et al. , there are three stages of AI evolution, in which interpretability is helpful. In the early stages of AI development, when AI is weaker than human performance, transparency can help us build better models . It can give a better understanding of how a model works and helps us answer several key questions. For example, why a model works in some cases and doesn't in others, why some examples confuse the model more than others, why these types of models work and the others don't, etc. When AI is on par with human performance and ML models are starting to be deployed in several industries, it can help build trust for these models. I'll elaborate a bit on this later, because I think that it is the most important reason. When AI significantly outperforms humans (e.g. AI playing chess or Go), it can help with machine teaching (i.e. learning from the machine on how to improve human performance on that specific task). Why is trust so important? First, let me give you a couple of examples of industries where trust is paramount: In healthcare, imagine a Deep Neural Net performing diagnosis for a specific disease. A classic black box NN would just output a binary ""yes"" or ""no"". Even if it could outperform humans in sheer predictability, it would be utterly useless in practice. What if the doctor disagreed with the model's assessment, shouldn't he know why the model made that prediction; maybe it saw something the doctor missed. Furthermore, if it made a misdiagnosis (e.g. a sick person was classified as healthy and didn't get the proper treatment), who would take responsibility: the model's user? the hospital? the company that designed the model? The legal framework surrounding this is a bit blurry. Another example is self-driving cars. The same questions arise: if a car crashes, whose fault is it: the driver's? the car manufacturer's? the company that designed the AI? Legal accountability, is key for the development of this industry. In fact, according to many, this lack of trust has hindered the adoption of AI in many fields (sources: [1] , [2] , [3] ). While there is a running hypothesis that with more transparent, interpretable or explainable systems users will be better equipped to understand and therefore trust the intelligent agents (sources: [4] , [5] , [6] ). In several real-world applications, you can't just say ""it works 94% of the time"". You might also need to provide a justification... Government regulations Several governments are slowly proceeding to regulate AI and transparency seems to be at the center of all of this. The first to move in this direction is the EU, which has set several guidelines where they state that AI should be transparent (sources: [7] , [8] , [9] ). For instance, the GDPR states that if a person's data has been subject to ""automated decision-making"" or ""profiling"" systems, then he has a right to access ""meaningful information about the logic involved"" ( Article 15, EU GDPR ) Now, this is a bit blurry, but there is clearly the intent of requiring some form of explainability from these systems. The general idea the EU is trying to pass is that ""if you have an automated decision-making system affecting people's lives then they have a right to know why a certain decision has been made."" For example, a bank has an AI accepting and declining loan applications, then the applicants have a right to know why their application was rejected. To sum up... Explainable AIs are necessary because: It gives us a better understanding, which helps us improve them. In some cases, we can learn from AI how to make better decisions in some tasks. It helps users trust AI, which leads to a wider adoption of AI. Deployed AIs in the (not too distant) future might be required to be more ""transparent"".",53.63479798,"Why do we need explainable AI?
  ... why we need to know ""how does its intelligence work?"" Because anyone with access to the equipment, enough skill, and enough time, can force the system to make a decision that is unexpected. The owner of the equipment, or 3rd parties, relying on the decision without an explanation as to why it is correct would be at a disadvantage. Examples - Someone might discover: People whom are named John Smith and request heart surgery on: Tuesday mornings, Wednesday afternoons, or Fridays on odd days and months have a 90% chance of moving to the front of the line. Couples whom have the male's last name an odd letter in the first half of the alphabet and apply for a loan with a spouse whose first name begins with a letter from the beginning of the alphabet are 40% more likely to receive the loan if they have fewer than 5 bad entries in their credit history. etc. Notice that the above examples ought not to be determining factors in regards to the question being asked, yet it's possible for an adversary (with their own equipment, or knowledge of the algorithm) to exploit it. Source papers : "" AdvHat: Real-world adversarial attack on ArcFace Face ID system "" (Aug 23 2019) by Stepan Komkov and Aleksandr Petiushko Creating a sticker and placing it on your hat fools facial recognition system. "" Defending against Adversarial Attacks through Resilient Feature Regeneration "" (Jun 8 2019), by Tejas Borkar, Felix Heide, and Lina Karam ""Deep neural network (DNN) predictions have been shown to be vulnerable to carefully crafted adversarial perturbations. Specifically, so-called universal adversarial perturbations are image-agnostic perturbations that can be added to any image and can fool a target network into making erroneous predictions. Departing from existing adversarial defense strategies, which work in the image domain, we present a novel defense which operates in the DNN feature domain and effectively defends against such universal adversarial attacks. Our approach identifies pre-trained convolutional features that are most vulnerable to adversarial noise and deploys defender units which transform (regenerate) these DNN filter activations into noise-resilient features, guarding against unseen adversarial perturbations."". "" One pixel attack for fooling deep neural networks "" (May 3 2019), by Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi Altering one pixel can cause these errors: Fig. 1. One-pixel attacks created with the proposed algorithm that successfully fooled three types of DNNs trained on CIFAR-10 dataset: The All convolutional network (AllConv), Network in network (NiN) and VGG. The original class labels are in black color while the target class labels and the corresponding confidence are given below. Fig. 2. One-pixel attacks on ImageNet dataset where the modified pixels are highlighted with red circles. The original class labels are in black color while the target class labels and their corresponding confidence are given below. Without an explanation as to how and why a decision is arrived at the decision can't be absolutely relied upon.",56.13332929,"If you're a bank, hospital or any other entity that uses predictive analytics to make a decision about actions that have huge impact on people's lives, you would not make important decisions just because Gradient Boosted trees told you to do so. Firstly, because it's risky and the underlying model might be wrong and, secondly, because in some cases it is illegal - see Right to explanation .",50.40483103,"Explainable AI is often desirable because AI (in particular, artificial neural networks) can catastrophically fail to do their intended job. More specifically, it can be hacked or attacked with adversarial examples or it can take unexpected wrong decisions whose consequences are catastrophic (for example, it can lead to the death of people). For instance, imagine that an AI is responsible for determining the dosage of a drug that needs to be given to a patient, based on the conditions of the patient. What if the AI makes a wrong prediction and this leads to the death of the patient? Who will be responsible for such an action? In order to accept the dosage prediction of the AI, the doctors need to trust the AI, but trust only comes with understanding, which requires an explanation. So, to avoid such possible failures, it is fundamental to understand the inner workings of the AI, so that it does not make those wrong decisions again. AI often needs to interact with humans, which are sentient beings (we have feelings) and that often need an explanation or reassurance (regarding some topic or event). In general, humans are often looking for an explanation and understanding of their surroundings and the world. By nature, we are curious and exploratory beings. Why does an apple fall?",55.06373345,"Another reason: In the future, AI might be used for tasks that are not possible to be understood by human beings, by understanding how given AI algorithm works on that problem we might understand the nature of the given phenomenon.",51.08292346,"The answer to this is incredibly simple. If you are a bank executive one day you may need to stand up in court and explain why your AI denied mortgages to all these people... who just happen to share some protected characteristic under anti-discrimination legislation. The judge will not be happy if you handwave the question away mumbling something about algorithms. Or worse, why did this car/plane crash and how will you prevent it next time. This is the major blocker to more widespread adoption of AI in many industries.",52.17176879,"In addition to all these answers mentioning the more practical reasons of why we'd want explainable AIs, I'd like to add a more philosophical one. Understanding how things around us work is one of the main driving forces of science from antiquity. If you don't have an understanding of how things work, you can't evolve beyond that point. Just because ""gravity works"" hasn't stopped us trying to understand how it works. In turn a better understanding of it led to several key discoveries, which have helped us advance our technology. Likewise, if we stop at ""it works"" we will stop improving it. Edit: AI hasn't been just about making ""machines think"", but also through them to understand how the human brain works. AI and neuroscience go hand-by-hand . This all wouldn't be possible without being able to explain AI.",53.20414409,"I have already given an answer and there are other good answers , but I would like to give another answer by quoting an excerpt from an old paper by Norbert Wiener , i.e. Some Moral and Technical Consequences of Automation ( 1960, Science ) As is now generally admitted, over a limited range of operation, machines act far more rapidly than human beings and are far more precise in performing the details of their operations. This being the case, even when machines do not in any way transcend man's intelligence, they very well may, and often do, transcend man in the performance of tasks. An intelligent understanding of their mode of performance may be delayed until long after the task which they have been set has been completed. This means that though machines are theoretically subject to human criticism, such criticism may be ineffective until long after it is relevant. To be effective in warding off disastrous consequences, our understanding of our man-made machines should in general develop pari passu with the performance of the machine. By the very slowness of our human actions, our effective control of our machines may be nullified. By the time we are able to react to information conveyed by our senses and stop the car we are driving, it may already have run head on into a wall.",50.67154646,"IMHO, the most important need for explainable AI is to prevent us from becoming intellectually lazy.  If we stop trying to understand how answers are found, we have conceded the game to our machines.",60.22985126
13775,Is a switch from R to Python worth it?,python,"I want to reframe your question. Don't think about switching, think about adding. In data science you'll be able to go very far with either python or r but you'll go farthest with both. Python and r integrate very well, thanks to the reticulate package. I often tidy data in r because it is easier for me, train a model in python to benefit from superior speed and visualize the outcomes in r in beautiful ggplot all in one notebook! If you already know r there is no sense in abandoning it, use it where sensible and easy to you. But it is 100% a good idea to add python for many uses. Once you feel comfortable in both you'll have  a workflow that fits you best dominated by your favorite language.",55.55342369,"Of course, this type of questions will also lead to primarily opinion-based answers. Nonetheless, it is possible to enumerate the strengths and weakness of each language, with respect to machine learning, statistics, and data analysis tasks, which I will try to list below. R Strengths R was designed and developed for statisticians and data analysts, so it provides, out-of-the-box (that is, they are part of the language itself), features and facilities for statisticians, which are not available in Python, unless you install a related package. For example, the data frame, which Python does not provide, unless you install the famous Python's pandas package. There are other examples like matrices, vectors, etc. In Python, there are also similar data structures, but they are more general, so not specifically targeted for statisticians. There are a lot of statistical libraries. Weakness Given its purpose, R is mainly used to solve statistical or data analysis problems. However, it can also be used outside of this domain. See, for example, this Quora question: Is R used outside of statistics and data analysis? . Python Strengths A lot of people and companies, including Google and Facebook, invest a lot in Python. For example, the main programming language of TensorFlow and PyTorch (two widely used machine learning frameworks) is Python. So, it is very unlikely that Python won't continue to be widely used in machine learning for at least 5-10 more years. The Python community is likely a lot bigger than the R community. In fact, for example, if you look at Tiobe's index , Python is placed 3rd, while R is placed 20th. Python is also widely used outside of the statistics or machine learning communities. For example, it is used for web development (see e.g. the Python frameworks Django or Flask). There are a lot of machine learning libraries (e.g. TensorFlow and PyTorch). Weakness It does not provide, out-of-the-box, the statistical and data analysis functionalities that R provides, unless you install an appropriate package. This might be a weakness or a strength, depending on your philosophical point of view. There are other possible advantages and disadvantages of these languages. For example, both languages are dynamic. However, this feature can both be an advantage and a disadvantage (and it is not strictly related to machine learning or statistics), so I did not list it above. I avoided mentioning opinionated language features, such as code readability and learning curve, for obvious reasons (e.g. not all people have the same programming experience). Conclusion Python is definitely worth learning if you are studying machine learning or statistics. However, it does not mean that you will not use R anymore. R might still be handier for certain tasks.",56.47904523,"I didn't have this choice because I was forced to move from R to Python: It depends on your environment : When you are embedded in an engineer department, working technical group or something similar than Python is more feasible. When you are surrounded by scientists and especially statisticians , stay with R. PS: R offers keras and tensorflow as well though it is implemented under the hood of python. Only very advanced stuff will make you need Python.
Though I'm getting more and more used to Python, the synthax in R is easier . And though each package has its own, it is somehow consistent while Python is not..
And ggplot is so strong. Python has a clone (plotnine) but it lacks several (important) features. In principle you can do nearly as much as in R but especially visualization and data wrangling is much easier in R. Thus, the most famous Python library, pandas, is a clone of R. PSS: Advanced statistics aims definitely at R. Python offers a lot of everyday tools and methods for a data scientist but it will never reach those > 13,000 packages R provides. For example, I had to do an inverse regression and python doesn't offer this. In R you can choose between several confidence tests and whether it is linear or nonlinear. 
The same goes to mixed models: It is implemented in python but it is so basic there I can't realize how this can be sufficient for someone.",63.06385486,"I would say yes. Python is better than R for most tasks, but R has its niche and you would still want to use it in many circumstances. Additionally, learning a second language will improve your programming skills. My own perspective on the strengths of R vs Python is that I would prefer R for a small, single-purpose program involving tables or charts, or exploratory work in the same vein. I would prefer Python for everything else. R is really good for table mashing. If most of what a particular program is going to do is smoosh some tables into different shapes, then R is the thing to pick. Python has tools for this, but R is designed for it and does it better. It's worth switching to R whenever you need to make a chart, because ggplot2 is a masterpiece of API usability and matplotlib is a crawling horror. Python is well designed for general purpose programming. It has a very well designed set of standard data structures, standard libraries, and control flow statements. R is poorly suited for general purpose programming. It doesn't handle tree-structured or graph-structured data well. It has some rules (like being able to look into and modify your parent scope) which are immediately convenient, but when used lead to programs that do are hard to grow, modify, or compose. R also has some straightforwardly bad things in it. These are mostly just historical leftovers like the three different object systems. To elaborate more on the last point: computer programming done well is lego where you make your own bricks (functions and modules). Programs are usually modified and repurposed past their original design. As you build them it is useful to think about which parts might be reused, and to build those part in a general way that will let them plug in to the other bricks. R encourages you to melt all the bricks together.",55.38948377,"As others have said, it's not a ""switch"". But is it worth adding Python to your arsenal? I would say certainly. In data science, Python is popular and becoming ever more popular, while R is receding somewhat. And in the fields of machine learning and neural networks, I'd say that Python is the main language now -- I don't think R really comes close here in terms of usage. The reason for all of this is generality. Python is intended as a general programming language, and allows you to easily script all kinds of tasks. If you're staying strictly within a neatly structured statistical world, R is great, but with AI you often end up having to do novel, miscellaneous things, and I don't think R can beat Python at that. And because of this, I think Python and its packages will be receiving more support and development when it comes to the more cutting-edge tech.",59.01479351,"It sounds like you have invested 1 year for data science with R, and embedded into R environment, but want to explore python for data science. First learn the basics of the python like how lists and tuple works and how classes and objects work. Then get your hands dirty with some libraries like numpy matplotlib pandas. Learn tensorflow or keras and then go for data science.",52.92098742,"This is totally my personal opinion. I read in my office (at a construction site) that ""There is a right tool for every task."" I expect me to face a variety of tasks, as a programmer. I want as many tools as I can ""buy or invest in"", as possible. One day one tool will help me solve it, some other day some other tool. R (for statistics) and Python (for in general) are two tools I definitely want with me and I think it is worth investment for me. As far as switch is concerned, I will use the most efficient tool I know (where efficiency is measured by client's requirement, time and cost investment and ease of coding) . The more tools I know, the merrier! Of course there is a practical limit to it. All this is my personal opinion and not necessarily correct.",54.40264066,"Person who chases two rabbits catches neither And yes, Python is more popular. I work in both but, business speaking, it's easy to find a job on Python than in R. So, you could: Pick Python because it is more popular.   However, you must start from scratch. Or Stay with R, after all, you have one year worth of training with R.  But it is not popular.",57.01197862,,
13725,Is randomness anti-logical?,philosophy,"I think the answer here lies in that the dictionary definition of randomness you have is not the one used in statistics, ML, or mathematics. We define randomness to mean there exists a distribution with generally greater than 0 uncertainty. Depending on who you talk to, we live in a random universe (the way we define quantum mechanics depends on a wave function (essentially a probability distribution) So why if a sequence is drawn from a distribution is it illogical? First, even as humans we can make a strong argument that what we say is random . I mean we speak to convey some form of message or context, but there exists multiple ways to deliver this, but we choose a single one. Our brains inherently model $p(\vec w|c)$ where $\vec w$ is the sequence and $c$ is our context or message we want to convey. Takeaway: Generating a sequence in an ergodic or uniform manner would be illogical, but that is not what is being modeled or done in practice. Normally its drawn from some complex distribution. Sidenote: My above claim could make it seem that being uniformly random implicates something illogical, and I want to emphasize that is not the case. It is domain to domain, sometimes that is the most logical solution, just in the case of sentence generation it normally isnt. I would define a logical algorithm as one that given the information at hand acts in a sensible manner towards achieving some goal, and so if something purely random does that, I don't see the problem.",53.90563826,"I might misunderstand your question, but there seem to be different levels of logic at play here. Computing logic, whereby any computational process is based on processor logic. In this case, any computing is involving logic, as boolean logic drives any processing. Linguistic logic, where there is a logic in the sequencing of sentences within a text. A random collection of sentences is not a text, as there need to be certain principles behind the structure to make it a narrative. While you can easily generate a sequence of random sentences, they will not mean anything; there won't be any logic behind selecting a particular sentence to follow on from another one. So this is linguistic logic rather than processing logic . Note that where the linguistic logic is makes it a bit vague: I can read a randomly selected sequence of sentences and ascribe meaning to it by building a mental model that treats it as a logically constructed text. This principle is what made ELIZA so successful: even though the program's answers were based on simple pattern matching rules with no understanding, many users assumed there was logic/meaning behind it and interpreted it as such, papering over the cracks in the conversation. In summary: there is logic involved in random sentence combining, but it is the low-level computing logic, not the higher-level linguistic interpretative logic, which is generally absent from randomly generated data.",50.4396591,"In certain games, random selection is the optimal strategy.  See: Matching Pennies Strategy is essentially a plan of action utilized to achieve a goal. If random choice can be a strategy, it seems that it must be a form of logic, even if the nature of the stochastic process is counter to all forms of formal logic. This seems paradoxical, in that the random strategy is to have no strategy (random choices.)",50.38543265,"Previous answers are very well written. I just wanted to supplement the thread by giving a simple example. The example shows how a logical function can be computed without errors using noisy components. Taken verbatim from Neural Networks by Raul Rojas . An excellent book: an example of a network built using four
  units. Assume that the first three units connected directly to the three bits of
  input $x_1, x_2, x_3$ all fire with probability $1$ when the total excitation is greater
  than or equal to the threshold $\theta$ but also with probability $p$ when it is $\theta − 1$ .
  The duplicated connections add redundancy to the transmitted bit, but in
  such a way that all three units fire with probability one when the three bits
  are $1$ . Each unit also fires with probability $p$ if two out of three inputs are $1$ .
  However each unit reacts to a different combination. The last unit, finally, is
  also noisy and fires any time the three units in the first level fire and also with
  probability $p$ when two of them fire. Since, in the first level, at most one unit
  fires when just two inputs are set to $1$ , the third unit will only fire when all
  three inputs are $1$ . This makes the logical circuit, the AND function of three
  inputs, built out of unreliable components error-proof.",51.42258267,"Let me add an example from machine learning that shows that resorting to randomness is the optimal way, sometimes. When working on the whole data is not tractable (computation cost, data does not fit in memory), working on random samples can be an optimal way to train a machine learning algorithm. One of the most used optimization technique in those cases is the Stochastic Gradient Descent . It is an iterative procedure that computes the estimates of the true gradient of the loss function that needs to be minimized, over a randomly selected data point from the whole data. After getting the gradient estimate, the weights are updated, all this done by the well known back-propagation algorithm. This procedure is repeated many times until a stopping criterion . The rule is: $ \theta_{k+1} \leftarrow{} \theta_{k} - \eta_k \nabla (f_{i(k)}(x_k))$ where $\theta$ are the weights of the network, $f$ is the loss function whose gradient is computed w.r.t the weights of the network, $x_k$ is the randomly chosen sample to compute gradients for, and $\eta_k$ is the step size to multiply the negative of the gradient with.",51.67343819,"Central Premises :-- This, computability of randomness in conjunction to logic, is unfortunately/fortunately a very technical topic. "" ... That stochastic process is part of an algorithm, which is a set of instructions that must be valid for the program to compute. ... Is randomness anti-logical?"" ~ DukeZhou (Stack Exchange user, opening poster) This answer is about: randomness and chaos; and how they relate to logic and computability. ""What is randomness and where does it come from?
  This is one scary place to venture in. We take for granted the randomness in our reality. We compensate for that randomness with probability theory. However, is randomness even real or is it just a figment of our lack of intelligence? That is, does what we describe as randomness just a substitute for our uncertainty about reality? Is randomness just a manifestation of something else?"" 
  — ""Medium."" Medium, < medium.com/intuitionmachine/there-is-no-randomness-only-chaos-and-complexity-c92f6 >. - ""Many natural intensional properties in artificial and natural languages are hard to compute. We show that randomized algorithms are often necessary to have good estimators of natural properties and to verify some specific relations. We concentrate on the reliability of queries to show the advantage of randomized algorithms in uncertain cognitive worlds."" — de Rougemont, Michel. ""Logic, randomness and cognition."" Logic, Thought and Action. Springer, Dordrecht, 2005. 497-506. Layperson's Explanations :-- Unfortunately, quantum chaos in relation to randomness, is a profoundly technical topic. I have managed to track down sources that relatively aren't overly technical. As a starting point, this Wikipedia article is worth reading:-- ( https://simple.wikipedia.org/wiki/Chaos_theory ) You can continue and read this particular Medium post:-- ( https://medium.com/intuitionmachine/there-is-no-randomness-only-chaos-and-complexity-c92f6dccd7ab ) For profoundly technical topics, I recommend this book series, as they are written by experts in basic technical terms, for the laypersons wanting to study technical topics:-- ( https://en.wikipedia.org/wiki/Very_Short_Introductions ) I recommend reading :-- Chaos: A Very Short Introduction Probability: A Very Short Introduction Fractals: A Very Short Introduction Other References for the Layperson :-- ( https://www.random.org/randomness/ ) ( https://plato.stanford.edu/entries/computability/ ) ( https://plato.stanford.edu/entries/chance-randomness/ ) ( https://plato.stanford.edu/entries/qt-quantlog/ ) Some broader implications of chaos [Link to Stanford Encyclopedia of Philosophy]. When I think of randomness, I'm inclined to think in cosmological terms. Is randomness is a structural property of the universe? Is anything in the universe truly random? Technical Explanations :-- ""In mathematical logic, independence is the unprovability of a sentence from other sentences."" Wikipedia contributors. — ""Independence (mathematical logic)."" Wikipedia, The Free Encyclopedia . Wikipedia, The Free Encyclopedia, 3 Feb. 2019. Web. 29 Aug. 2019. - ""We propose a link between logical independence and quantum physics. We demonstrate that quantum systems in the eigenstates of Pauli group operators are capable of encoding mathematical axioms and show that Pauli group quantum measurements are capable of revealing whether or not a given proposition is logically dependent on the axiomatic system. Whenever a mathematical proposition is logically independent of the axioms encoded in the measured state, the measurement associated with the proposition gives random outcomes. This allows for an experimental test of logical independence. Conversely, it also allows for an explanation of the probabilities of random outcomes observed in Pauli group measurements from logical independence without invoking quantum theory. The axiomatic systems we study can be completed and are therefore not subject to Gödel's incompleteness theorem."" Paterek, Tomasz, et al. ""Logical independence and quantum randomness."" — New Journal of Physics 12.1 (2010): 013019. - Other Technical Explanations, Sources, References, and Further Reading:-- ( https://iopscience.iop.org/article/10.1088/1367-2630/12/1/013019/meta ) ( https://en.wikipedia.org/wiki/Quantum_chaos ) ( https://arxiv.org/pdf/0708.1362.pdf ) ( https://arxiv.org/pdf/1701.01107.pdf ) ( http://csc.ucdavis.edu/~cmg/papers/idep.pdf ) ( https://www.datasciencecentral.com/profiles/blogs/logistic-map-chaos-randomness-and-quantum-algorithms ) ( https://www.deepdyve.com/lp/wiley/non-linear-dynamics-complexity-and-randomness-algorithmic-foundations-9vxIJOTT6u ) ( https://en.wikipedia.org/wiki/Independence_(mathematical_logic) ) Notes :-- I usually do not use Medium or Quora as a source, with some exceptions. I have  chosen to do so here. I've decided to place Stanford Encyclopedia of Philosophy sources in the layperson's section.",63.15998658,,,,,,
13261,Why do we need common sense in AI?,philosophy,"Commonsense knowledge is the collection of premises that everyone, in a certain context (hence common sense knowledge might be a function of the context), takes for granted. There would exist a lot of miscommunication between a human and an AI if the AI did not possess common sense knowledge. Therefore, commonsense knowledge is fundamental to human-AI interaction . There are also premises that every human takes for granted independently of the country, culture, or, in general, context. For example, every human (almost since its birth) has a mechanism for reasoning about naive physics, such as space, time, and physical interactions. If an AI does not possess this knowledge, then it cannot perform the tasks that require this knowledge. Any task that requires a machine to have common sense knowledge (of an average human) is believed to be AI-complete , that is, it requires human-level (or general) intelligence. See section D of AI-Complete, AI-Hard, or AI-Easy – Classification of Problems in AI (2012) by Roman V. Yampolskiy. Of course, the problems that arise while humans communicate because of different assumptions or premises might also arise between humans and AIs (that possess commonsense knowledge).",56.99353232,"We need this kind of common sense knowledge if we want to get computers to understand human language. It's easy for a computer program to analyse the grammatical structure of the example you give, but in order to understand its meaning we need to know the possible contexts, which is what you refer to as ""common sense"" here. This was emphasised a lot in Roger Schank et al. 's work on computer understanding of stories, and lead to a lot of research into knowledge representation, scripts, plans, goals. One example from Schank's work is Mary was hungry. She picked up a Michelin Guide. -- this seems like a non-sequitur: if you are hungry, why pick up a book? Until you realise that it is a restaurant guide, and that Mary is presumably planning to go to a restaurant to eat. If you know that going to a restaurant is a potential solution to the problem of being hungry, then you have no problem understanding this story fragment. Any story needs common sense to be understood, because no story is completely explicit. Common things are ""understood"" and aren't explicitly mentioned. Stories relate to human experience, and a story that would make everything explicit would probably read like a computer program. You also need common sense to understand how characters in a story behave, and how they are affected by what is happening. Again, this is very subjective, but it is necessary. Some common sense might be generally applicable, other aspects of it won't be. It's a complex issue, which is why researchers have struggled with it for at least half a century of AI research. Of course this would introduce ""human errors"" into an AI system. All this is very subjective and culture-specific. Going to a restaurant in the USA is different from going to one in France -- this is why going abroad can be a challenge. And my reading of a story will probably be different from yours. But if you want to simulate human intelligence, you cannot do that without potential human ""errors"".",61.49643005,"I'll answer this question in several parts: Why do AGI systems need to have common sense? Humans in the wild reason and communicate using common sense more than they do with strict logic, you can see this by noting that it is easier to appeal to someone's emotion than logic. So any system that seeks to replicate human cognition (as in AGI) should also replicate this tendency to use common sense. More simply put, we'd wish that our AGI system can speak to us in common sense language simply because that is what we understand best (otherwise we wouldn't understand our friendly AGI would we?). Obtuse theory and strict logic might technically be correct, but don't appeal to our understanding. Isn't the goal of AGI the create the most cognitively advance system? Why should the ""most perfect"" AGI system need to deal with such imperfections and impreciseness present in common sense? First, it might only appear to be the case that common sense logic is ""irrational"". Perhaps there is a consistent mathematical way to model common sense such that all the subtleties of common sense are represented in a rigour fashion. Second, the early study of Artificial Intelligence started in the study of cognitive science, where researchers tried to replicate ""algorithms of the mind"", or more precisely: decidable procedures which replicated human thought. To that extent then, the study of AI isn't to create the ""most supreme cognitive agent"" but to merely replicate human thought/behavior. Once we can replicate human behavior we can perhaps try to create something super-human by giving it more computational power, but that is not guaranteed. I still don't see why common sense is needed in AGI systems. Isn't AGI about being the most intelligent and powerful computational system? Why should it care or conform towards the limits of human understanding, which requires common sense? Perhaps then you have a bit of a misaligned understanding of what AGI entails. AGI doesn't mean unbounded computational power (physically impossible due to physical constraints on computation such as Bremermann's limit ) or unbounded intelligence (perhaps physically impossible due to the prior constraint). It usually just means artificial ""general intelligence"", general meaning broad and common. Considerations about unbounded agents are studied in more detail in fields such as theoretical computer science (type theory I believe), decision theory, and perhaps even set theory, where we are able to pose questions about agents with unbounded computational power. We might say that there are questions even an AGI system with unbounded power can't answer due to the Halting Problem , but only if the assumptions on those fields map onto the structure of the given AGI, which might not be true. For a better understanding of what AGI might entail and its goals, I might recommend two books: Artificial Intelligence: The Very Idea by John Haugeland for a more pragmatic approach (as pragmatic as AI-philosophy can be, and On the Origin of Objects by Brian Cantwell Smith for a more philosophically inclined approach. As a fun aside, the collection of Zen koan's: The Gateless Gate, includes the following passage: (quoted and edited from wikipedia ) A monk asked Zhaozhou, a Chinese Zen master, ""Has a dog Buddha-nature or not?"" Zhaozhou answered, ""Wú"" Wú (無) translates to ""none"", ""nonesuch"", or ""nothing"", which can be interpreted as to avoid answering either yes or no. This enlightened individual doesn't seek to strictly answer every question, but just to respond in a way that makes sense. It doesn't really matter as to wether the dog has Buddha-nature or not (whatever Buddha-nature means), so the master defaults to absolve the question rather than resolving it.",61.60164886,"Perhaps it would help to give an example of what can go wrong without common sense: At the start of the novel ""The Two Faces of Tomorrow"" by James Hogan, a construction supervisor on the Moon files a request with an automated system, asking that a particular large piece of construction equipment be delivered to his site as soon as possible.  The system replies that it will arrive in twenty minutes.  Twenty minutes later, the supervisor is killed as the equipment crashes into his construction site.  The system had determined that the fastest way to deliver the equipment to that site was to mount it on a mass-driver and launch it at the site.  Had the system in question been given common sense, it would have inferred additional unstated constraints on the query, such as 'the equipment should arrive intact', 'the arrival of the equipment should not cause damage or loss of life', and so on.  (the rest of the novel describes an experiment designed to produce a new system with common sense)",55.99788512,"Is this common sense, or is this natural language understanding? It's been said that natural language understanding is one of the hardest AI tasks. This is one of the examples showing why. The first part of the sentence is related to the second part, that how sentences work. Now the relevant question is how the two parts are related. There are a few standard relations that we encounter, for instance a temporal order. In this specific example, the nature of the relation is closer to a cause-and-effect. You see this effect when we insert a word to make this relation explicit: It's John's birthday, so let's buy him a kite.
  or 
  Let's buy John a kite, because it's his birthday. This is a technique for humans to make these implicit relations explicit. Now, as curiousdannii notes, you also need the cultural knowledge to understand how a birthdays can be a cause for a present. No amount of common sense helps with that.",55.72795443,,,,,,,,
12659,What is the actual quality of machine translations?,natural-language-processing,"Who claimed that machine translation is as good as a human translator? For me, as a professional translator who makes his living on translation for 35 years now, MT means that my daily production of human quality translation has grown by factor 3 to 5, depending on complexity of the source text. I cannot agree that the quality of MT goes down with the length of the foreign language input. That used to be true for the old systems with semantic and grammatical analyses. I don't think that I know all of the old systems (I know Systran, a trashy tool from Siemens that was sold from one company to the next like a Danaer's gift, XL8, Personal Translator and Translate), but even a professional system in which I invested 28.000 DM (!!!!) failed miserably. For example, the sentence: On this hot summer day I had to work and it was a pain in the ass. can be translated using several MT tools to German. Personal Translator 20 : Auf diesem heißen Sommertag musste ich arbeiten, und es war ein Schmerz im Esel. Prompt : An diesem heißen Sommertag musste ich arbeiten, und es war ein Schmerz im Esel. DeepL : An diesem heißen Sommertag musste ich arbeiten und es war eine Qual. Google: An diesem heißen Sommertag musste ich arbeiten und es war ein Schmerz im Arsch. Today, Google usually presents me with readable, nearly correct translations and DeepL is even better. Just this morning I translated 3500 words in 3 hours and the result is flawless although the source text was full of mistakes (written by Chinese).",52.75765128,"Google's translations can be useful, especially if you know that the translations are not perfect and if you just want to have an initial idea of the meaning of the text (whose Google's translations can sometimes be quite misleading or incorrect). I wouldn't recommend Google's translate (or any other non-human translator) to perform a serious translation, unless it's possibly a common sentence or word, it does not involve very long texts and informal language (or slang), the translations involve the English language or you do not have access to a human translator. Google Translate currently uses a neural machine translation system . To evaluate this model (and similar models), the BLEU metric (a scale from $0$ to $100$ , where $100$ corresponds to the human gold-standard translation) and side-by-side evaluations (a human rates the translations) have been used. If you use only the BLEU metric, the machine traslations are quite poor (but the BLEU metric is also not a perfect evaluation metric, because there's often more than one translation of a given sentence). However, GNMT reduces the translation errors compared to phrase-based machine translation (PBMT) . In the paper Making AI Meaningful Again , the authors also discuss the difficulty of the task of translation (which is believed to be an AI-complete problem ). They also mention the transformer (another state-of-the-art machine translation model), which achieves quite poor results (evaluated using the BLEU metric). To conclude, machine translation is a hard problem and current machine translation systems definitely do not perform as well as a professional human translator.",55.86777525,"You have asked quite a lot of questions, some of which cannot be answered definitively . To give an insight of the quality (and its history) of machine translations I like  to refer to Christopher Manning his 'one sentence benchmark' as presented in his lecture . It contains one Chinese to English example which is compared with Google Translate output. The correct translation for the example would be: In 1519, six hundred Spaniards landed in Mexico to conquer the Aztec Empire with a population of a few million. They lost two thirds of their soldiers in the first clash. Google Translate returned the following translations. 2009 1519 600 Spaniards landed in Mexico, millions of people to conquer the Aztec empire, the first two-thirds of soldiers against their loss. 2011 1519 600 Spaniards landed in Mexico, millions of people to conquer the Aztec empire, the initial loss of soldiers, two thirds of their encounters. 2013 1519 600 Spaniards landed in Mexico to conquer the Aztec empire, hundreds of millions of people, the initial confrontation loss of soldiers two-thirds. 2015 1519 600 Spaniards landed in Mexico, millions of people to conquer the Aztec empire, the first two-thirds of the loss of soldiers they clash. 2017 In 1519, 600 Spaniards landed in Mexico, to conquer the millions of people of the Aztec empire, the first confrontation they killed two-thirds. Whether Google retains or 'hides' their best results: I doubt it. There are many excellent researchers working in the field of natural language processing (NLP). If Google would have a 'greatest achievement' for translation, the researchers would figure it out sooner or later. (Why would Google hide their 'greatest achievement' anyway? They seem to see the benefit of open source, see the Transformer[1] or BERT[2]) NB. For an updated list of state-of-the-art algorithms in NLP, see the SQuAD2.0 leaderboard . [1] Vaswani, Ashish, et al. ""Attention is all you need."" Advances in neural information processing systems. 2017. [2] Devlin, Jacob, et al. ""Bert: Pre-training of deep bidirectional transformers for language understanding."" arXiv preprint arXiv:1810.04805 (2018).",53.22815374,"It really depends on the language pair and the topic of the content. Translating to/from English to any other language usually is the best supported. Translating to and from popular languages works better, for example, translating from English to Romanian is a poorer translation than English to Russian. But translating from English to Russian or Romanian is better than translating Russian to Romanian. And translating Romanian to English is better than translating English to Romanian. But if you are used to working with translators and you have a passing familiarity with the languages, translation mistakes and the topic, it's easy to understand what was supposed to be there. And, at that point, sometimes its easier to read something translated into your native language for quick scanning than it is to read it in a second language. Less popular languages (for translation not necessarily in number of speakers) are much much closer to literal translations only slightly better than what you personally would do using a dictionary for two languages you do not know.",51.89167892,"Am I wrong and Google's translations are nevertheless readable, helpful and useful for a majority of users? Yes, they are somewhat helpful and allow you to translate faster. Or does Google have reasons to retain its greatest achievements (and not to
  show to the users the best they can show)? Maybe, I don't know. If you search for info, Google does really do a lot of horrible stupid stuff, like learning from what users say on the internet, taking unsuitable data as trusted input data sets.",52.49993963,"Apologies for not writing in English. Please find the adapted translation here: To give interested people an idea of the quality of MT (DeepL) please see this example from a text I was working on this morning (6,300 words, started at 9 am, delivery today around 1 pm and still find time for this post). I was working on this sentence (201 words) when I posted my comment. ""You further represent, warrant and undertake to ABC that you shall not: (a) Conduct any fraudulent, abusive, or otherwise illegal activity which may be grounds for termination of your right to access or use this Website and/or the Services; or (b) Post or transmit, or cause to be posted or transmitted, any communication or solicitation designed or intended to obtain password, account, or private information from any other user of this Website; or (c) Violate the security of any computer network, crack passwords or security encryption codes, transfer or store illegal material (including material that may be considered threatening or obscene), or engage in any kind of illegal activity that is expressly prohibited; or (d) Run maillist, listserv, or any other form of auto-responder, or ""spam"" on this Website, or any processes that run or are activated while you are not logged on to this Website, or that otherwise interfere with the proper working of or place an unreasonable load on this Website’s infrastructure; or (e) Use manual or automated software, devices, or other processes to ""crawl,"" ""scrape,"" or ""spider"" any page of this Website; or (f) Decompile, reverse engineer, or otherwise attempt to obtain the source code of this Website."" DeepL returns this: Sie versichern, garantieren und verpflichten sich gegenüber ABC, dass Sie dies nicht tun werden: (a) betrügerische, missbräuchliche oder anderweitig illegale Aktivitäten durchzuführen, die Anlass für die Beendigung Ihres Rechts auf Zugang oder Nutzung dieser Website und/oder der Dienste sein können; oder (b) Mitteilungen oder Aufforderungen, die dazu bestimmt sind oder sind, Passwörter, Konten oder private Informationen von anderen Nutzern dieser Website zu erhalten, zu posten oder zu übertragen oder posten oder zu posten oder zu übertragen; oder (c) die Sicherheit eines Computernetzwerks verletzen, Passwörter oder Sicherheitsverschlüsselungscodes knacken, illegales Material (einschließlich Material, das als bedrohlich oder obszön angesehen werden kann) übertragen oder speichern oder sich an illegalen Aktivitäten beteiligen, die ausdrücklich verboten sind; oder (d) Mailliste, Listenserver oder jede andere Form von Auto-Responder oder ""Spam"" auf dieser Website oder Prozesse, die ausgeführt werden oder aktiviert werden, während Sie nicht auf dieser Website angemeldet sind, oder die anderweitig das ordnungsgemäße Funktionieren oder eine unangemessene Belastung der Infrastruktur dieser Website stören; oder (e) manuelle oder automatisierte Software, Geräte oder andere Prozesse verwenden, um eine Seite dieser Website zu ""crawlen"", zu kratzen, zu spinnen oder zu spinnen; oder (f) dekompilieren, zurückzuentwickeln oder anderweitig zu versuchen, den Quellcode dieser Website zu erhalten. It took me about 5 to 10 minutes to adjust this paragraph. As a translator, I know that I cannot rely on the machine translation, but I learnt the specifics and capabilities of the different systems over time and I know what to pay attention for. MT helps me a lot in my work.",50.96329335,"This will be not so much an answer as a commentary. The quality depends on several things, including (as Aaron said above) 1) the language pair and 2) the topic, but also 3) the genera and 4) the style of the original, and 5) the amount of parallel text you have to train the MT system. To set the stage, virtually all MT these days is based off of parallel texts, that is a text in two different languages, with one presumably being a translation of the other (or both being a translation of some third language); and potentially using dictionaries (perhaps assisted by morphological processes) as backoff when the parallel texts don't contain particular words. Moreover, as others have said, an MT system in no way understands the texts it's translating; it just sees strings of characters, and sequences of words made up of characters, and it looks for similar strings and sequences in texts it's translated before.  (Ok, it's slightly more complicated than that, and there have been attempts to get at semantics in computational systems, but for now it's mostly strings.) 1) Languages vary.  Some languages have lots of morphology, which means they do things with a single word that other languages do with several words.  A simple example would be Spanish 'cantaremos' = English ""we will sing"".  And one language may do things that the other language doesn't even bother with, like the informal/formal (tu/ usted) distinction in Spanish, which English doesn't have an equivalent to.  Or one language may do things with morphology that another language does with word order.  Or the script that the language uses may not even mark word boundaries (Chinese, and a few others).  The more different the two languages, the harder it will be for the MT system to translate between them.  The first experiments in statistical MT were done between French and English, which are (believe it or not) very similar languages, particularly in their syntax. 2) Topic: If you have parallel texts in the Bible (which is true for nearly any pair of written languages), and you train your MT system off of those, don't expect it to do well on engineering texts.  (Well, the Bible is a relatively small amount of text by the standards of training MT systems anyway, but pretend :-).)  The vocabulary of the Bible is very different from that of engineering texts, and so is the frequency of various grammatical constructions.  (The grammar is essentially the same, but in English, for example, you get lots more passive voice and more compound nouns in scientific and engineering texts.) 3) Genera: If your parallel text is all declarative (like tractor manuals, say), trying to use the resulting MT system on dialog won't get you good results. 4) Style: Think Hilary vs. Donald; erudite vs. popular.  Training on one won't get good results on the other.  Likewise training the MT system on adult-level novels and using it on children's books. 5) Language pair: English has lots of texts, and the chances of finding texts in some other language which are parallel to a given English text are much higher than the chances of finding parallel texts in, say, Russian and Igbo.  (That said, there may be exceptions, like languages of India.)  As a gross generalization, the more such parallel texts you have to train the MT system, the better results. In sum, language is complicated (which is why I love it--I'm a linguist).  So it's no surprise that MT systems don't always work well. BTW, human translators don't always do so well, either.  A decade or two ago, I was getting translations of documents from human translators into English, to be used as training materials for MT systems.  Some of the translations were hard to understand, and in some cases where we got translations from two (or more) human translators, it was hard to believe the translators had been reading the same documents. And finally, there's (almost) never just one correct translation; there are multiple ways of translating a passage, which may be more or less good, depending on what features (grammatical correctness, style, consistency of usage,...) you want.  There's no easy measure of ""accuracy"".",53.12400087,"Surprisingly all the other answers are very vague and try to approach this from the human translator POV. Let's switch over to ML engineer. When creating a translation tool, one of the first questions that we should consider is ""How do we measure that our tool works?"" . Which is essentially what the OP is asking. Now this is not an easy task (some other answers explain why). There is a Wikipedia Article that mentions different ways to evaluate machine translation results - both human and automatic scores exist (such as BLEU , NIST , LEPOR ). With rise of neural network techniques, those scores improved significantly. Translation is a complex problem. There are many things that can go right(or wrong), and computer translation system often ignores some of the subtleties, which stands out for a human speaker. I think if we are to think about the future, there are few things that we can rely on: Our techniques are getting better, wider known and tested. This is going to improve the accuracy in the long run. We are developing new techniques which can take into account variables previously ignored or just do a better job. Many of currently existing translation models are often ""reused"" to translate other languages (for example, try translating ""JEDEN"" from Polish to Chinese(traditional) using Google Translator - you will end up with ""ONE"", which is an evidence pointing out the fact that Google translates Polish to English, and then English to Chinese). 
This is obviously not a good approach - you are going to lose some information in the process - but it's a one that will still work, so companies like Google use it for languages where they don't have enough workpower or data.
With time, more specialized models will appear, which will improve the situation. Also, as previous point stated, more and more data will only help improving the machine translation. To summarize, this complex problem, although not solved, is certainly on a good way and allows for some impressive results for well-researched language pairs.",51.66314592,"""Or does Google have reasons to retain its achievements (and not to show to the users the best they can show)"" If they were, then what they're holding back would be amazing . Google publishes a lot of strong papers in Natural Language Processing, including ones that get state of the art results or make significant conceptual breakthroughs .
They have also released very useful datasets and tools . Google is one of the few companies out there that is not only using the cutting edge of current research, but is actively contributing to the literature. Machine translation is just a hard problem. A good human translator needs to be fluent in both languages to do the job well. Each language will have its own idioms and non-literal or context-dependent meanings. Just working from a dual-language dictionary would yield terrible results (for a human or computer), so we need to train our models on existing corpora that exist in multiple languages in order to learn how words are actually used (n.b. hand-compiled phrase translation tables can be used as features ; they just can't be the whole story). For some language pairs, parallel corpora are plentiful (e.g. for EU languages, we have the complete proceedings of the European Parliament ). For other pairs, training data is much sparser.  And even if we have training data, there will exist lesser used words and phrases that don't appear often enough to be learned. This used to be an even bigger problem, since synonyms were hard to account for. If our training data had sentences for ""The dog caught the ball"", but not ""The puppy caught the ball"", we would end up with a low probability for the second sentence. Indeed, significant smoothing would be needed to prevent the probability from being zero in many such cases. The emergence of neural language models in the last 15 years or so has massively helped with this problem, by allowing words to be mapped to a real-valued semantic space before learning the connections between words. This allows models to be learned in which words that are close together in meaning are also close together in the semantic space, and thus switching a word for its synonym will not greatly affect the probability of the containing sentence. word2vec is a model that illustrated this very well; it showed that you could, e.g., take the semantic vector for ""king"", subtract the vector for ""man"", add the vector for ""woman"", and find that the nearest word to the resulting vector was ""queen"". Once the research in neural language models began in earnest, we started seeing immediate and massive drops in perplexity (i.e. how confused the models were by natural text) and we're seeing corresponding increases in BLEU score (i.e. quality of translation) now that those language models are being integrated into machine translation systems. Machine translations are still not as good as quality human translations, and quite possibly won't be that good until we crack fully sapient AI. But good human translators are expensive, while everyone with Internet access has machine translators available. The question isn't whether the human translation is better, but rather how close the machine gets to that level of quality. That gap has been shrinking and is continuing to shrink.",56.23627063
11285,What is the difference between latent and embedding spaces?,machine-learning,"Embedding vs Latent Space Due to Machine Learning's recent and rapid renaissance, and the fact that it draws from many distinct areas of mathematics, statistics, and computer science, it often has a number of different terms for the same or similar concepts. ""Latent space"" and ""embedding"" both refer to an (often lower-dimensional) representation of high-dimensional data: Latent space refers specifically to the space from which the low-dimensional representation is drawn. Embedding refers to the way the low-dimensional data is mapped to (""embedded in"") the original higher dimensional space. For example, in this ""Swiss roll"" data, the 3d data on the left is sensibly modelled as a 2d manifold 'embedded' in 3d space. The function mapping the 'latent' 2d data to its 3d representation is the embedding , and the underlying 2d space itself is the latent space (or embedded space ): Synonyms Depending on the specific impression you wish to give, ""embedding"" often goes by different terms: Term Context dimensionality reduction combating the ""curse of dimensionality"" feature extraction feature projection feature embedding feature learning representation learning extracting 'meaningful' features from raw data embedding manifold learning latent feature representation understanding the underlying topology of the data However this is not a hard-and-fast rule, and they are often completely interchangeable.",63.11778618,"When it comes to normal layman terms ""latent space"" means it cannot be accessed, thus we have no direct control over it. We can only manipulate it indirectly, while ""Embeddings"" can be obtained directly. We can use deterministic operations or transformations to convert an object into its corresponding embedding space. There is no marked difference between these 2 terms as far as Machine Learning is concerned. If we look at this famous paper on Variational Autoencoders, we can see the words has been used interchangeably. More specifically, I would consider the word (in the context of Machine Learning only) latent as a more general term than Embedding . Embeddings will refer to a more specific object (in context of ML), for example the embedding of $word_1$ is $embedding_1$ . Whereas, we can use the term latent to describe broader terms like latent space , latent representation , latent variables (latent variables of a word is same as an embedding of a word). After digging some more I found some what of a formal definition of Latent Variables in Deep Learning by Goodfellow: Latent Variables - A latent variable is a random
variable that we cannot observe directly. The component identity variable $c$ of the
mixture model provides an example. Latent variables may be related to $x$ through
the joint distribution, in this case, $P(x, c) = P(x | c)P(c)$ . The distribution $P(c)$ over the latent variable and the distribution $P(x | c)$ relating the latent variables
to the visible variables determines the shape of the distribution $P(x)$ , even though
it is possible to describe $P(x)$ without reference to the latent variable. Also a paper cited by Goodfellow while discussing embeddings has the following excerpt: Following the success of user/item clustering or matrix factorization techniques in collaborative filtering to represent non-trivial similarities between the connectivity patterns of entities in single relational data, most existing methods for multi-relational data have been designed within the framework of relational learning from latent attributes, as pointed out by; that is, by learning and
  operating on latent representations (or embeddings) of the constituents (entities and relationships). So clearly these are somewhat interchangeable terms. But my interpretation would be that embeddings are helpful more explicitly (more visible, latent variables are meant to be hidden), that is we can construct a new data-set from it and use various ML methods on it, whereas latent variables are something not useful explicitly (it is a part of a bigger problem we are trying to solve). EDIT: In the context of HMM's the term better suitable is hidden state and not latent space. Thus, in a HMM (from Wiki) The adjective hidden refers to the state sequence through which the model passes, not to the parameters of the model; the model is still referred to as a hidden Markov model even if these parameters are known exactly.",62.95610385,"The expression ""latent space"" explicitly indicates that the space is associated with the mathematical concept of an hidden (or latent) variable, which cannot be observed directly, but only indirectly. The expression ""embedding space"" refers to a vector space that represents an original space of inputs (e.g. images or words). For example, in the case of ""word embeddings"", which are vector representations of words. It can also refer to a latent space because a latent space can also be a space of vectors. However, an embedding space is not necessarily an hidden space. It is just another (vector) representation of another space. These two expressions can be used interchangeably, also because the expression ""embedding space"" is often not formally defined.",57.98438913,"To give a statistician's answer, the distinction is empirical (embedding) versus theoretical (latent positions). You define a statistical model which has latent positions that you could then try to estimate, given data. Or, given data, you might simply find a vector representation of each object of interest in a way that makes sense for the applications considered - and you would call that set of representations an embedding. There's a lot of work going on trying to re-interpret popular embeddings as estimating a compatible model's latent positions, but that's not always straightforward.",57.88030079,"The words latent space and embedding space are often used interchangeably. However, latent space can more specifically refer to the sample space of a stochastic representation, whereas embedding space more often refers to the space of a deterministic representation. This comes from latent referring to an unobserved random variable, for which we can infer a belief distribution over its plausible values, for example using an encoder network. You can then draw samples of the predicted distribution for further processing. To learn more, you can look into VAEs.",60.35505629,,,,,,,,
11226,What is non-Euclidean data?,deep-learning,"I presume this question was prompted by the paper Geometric deep learning:
going beyond Euclidean data (2017). If we look at its abstract: Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric
  data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks , which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However,
  these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. We see that the authors use the term ""non-Euclidean data"" to refer to data whose underlying structure is non-Euclidean. Since Euclidean spaces are prototypically defined by $\mathbb{R}^n$ (for some dimension $n$ ), 'Euclidean data' is data which is sensibly modelled as being plotted in $n$ -dimensional linear space, for example image files (where the $x$ and $y$ coordinates refer to the location of each pixel, and the $z$ coordinate refers to its colour/intensity). However some data does not map neatly into $\mathbb{R}^n$ , for example, a social network modelled by a graph. You can of course embed the physical shape of a graph in 3-d space , but you will lose information such as the quality of edges, or the values associated with nodes, or the directionality of edges, and there isn't an obvious sensible way of mapping these attributes to higher dimensional Euclidean space. And depending on the specific embedding, you may introduce spurious correlations (e.g. two unconnected nodes appearing closer to each other in the embedding than to nodes they are connected to). Methods such as Graph Neural Networks seek to adapt existing Machine Learning technologies to directly process non-Euclidean structured data as input, so that this (possibly useful) information is not lost in transforming the data into a Euclidean input as required by existing techniques.",70.18891458,"Non-Euclidian geometry can be generally boiled down to the phrase the shortest path between 2 points isn't necessarily a straight line. Or, put in a way that lends itself very much to machine learning, things that are similar to each other are not necessarily close if one uses Euclidean distance as a metric (aka the triangle inequality doesn't hold). You mention graphs and manifolds as being non-Euclidian, but, really, the majority of problems being worked on don't have Euclidian data. Take the below images for example: Clearly, 2 of the images are more similar to each other than the third one is, but if we looked at the pixels alone, the Euclidean distance between the pixel values don't represent this similarity. If there was a function, $F(\text{image})$ , that mapped images to a space of values where similar images produced values that were closer together, we could better understand the data, infer some statistics about the distributions, and make predictions on data we have yet to see. This is what classic techniques of image recognition have done and it's also what modern machine learning is doing. Taking data and mapping it to a space such that the triangle inequality holds. Let's look at a more concrete example, some points I drew in MSPaint.
On the left is some space that we are interested in where points have 2 classes (red or blue). Even though there are points that are close to each other, they may have different colors/classes. Ideally, we could have a function that converts these points to some space where we can draw a line to separate these 2 classes. In general, there would be many lines, or hyper-planes in dimensions > 3, but the goal is to transform the data so that it will be ""linearly separable"". To conclude, non-Euclidian data is everywhere.",54.76286521,"It's hard to say because Euclidean space is defined with respect to some kind of metric, so without any clearer exposition on the nature of the data/problem, the phrase itself may or may not be clear. A metric $d: A \times A \rightarrow \mathbb{R}$ is a function that defines distance between any two points in the space with respect to axioms that 1. two points have zero distance iff they are the same: $d(a,b) = 0 \Leftrightarrow a = b$ .  2. symmetric: $d(a,b) = d(b,a)$ . 3. triangle inequality: $d(a,b) + d(b,c) ≥ d(a,c)$ . A Euclidian metric is a metric that also obeys Pythagoras theorem , or at least: the distance between some point $(x,y) \in \mathbb{R}^2$ is equal to $\sqrt{x^2 + y^2}.$ You will find that all Euclidian spaces are isomorphic to $\mathbb{R}^n$ , meaning that the two notions are in some sense identical. Any graph/data whose underlying data does not ""naturally come"" from $\mathbb{R}^n$ , or a graph that does not admit a natural embedding in $\mathbb{R}^n$ might not be Euclidian, since $\mathbb{R}^n$ is isomorphic to any euclidian space.",52.24948731,"Where does this type of data arise? In terms of learning on non-Euclidean data, it was probably first coined by prof. Bronstein here . Recently, prof. Bronstein published, along with other top authors in the field, a book about geometric deep learning, which in essence presents a unified mathematical framework for symmetries-based learning, and exemplifies the extension of concepts from classical ML/DL to the domain of higher-dimensional geometric domains (chapter 4). Apparently, graphs and manifolds are non-Euclidean data. Why exactly is that the case? According to the unified mathematical framework presented in prof. Bronstein's book, the blueprint of geometric DL consists of an underlying domain $\Omega$ and signals $\mathcal{X}(\Omega)$ and functions $\mathcal{F}(\mathcal{X}(\Omega))$ . The domain is the geometric/algebraic structure behind any instance of that data type (basically shows how its composing features are arranged). Associated with this domain is a symmetry group (all the transformations under which the domain is invariant). The signal is the observable state/value/quantity/features in the points of the domain. The function(s) are the blocks/layers that we want to build as functions of the signals on the domain. Here is where we can inject inductive biases. Translation equivariance is a property of the domain of images (among other groups). Convolutions as functions of a signal on the domain $R^2$ are translation equivariant, meaning that if you move an object in an image, you would still find the same features (at different locations) when applying the same kernel over the two images. This is the (geometric prior) ""inductive bias"", as the function used is aware of the domain, knows its properties, and uses them . The underlying domain of an image is $\mathbb{R}^2$ (the position of the pixels in the (x, y) plane). In this context, one can say that the signal of an is simply the colour/intensity of the pixels. As pointed out in this answer, this group admits the Euclidean distance between two points in the plane as $d(x, y) = \sqrt{\sum_{i=0}^{1} (x_i - y_i)^2}$ . In broad terms, non-Euclidean data is data whose underlying domain does not obey Euclidean distance as a metric between points in the domain. For visualization simplicity, think of $\mathbb{Z}^2$ instead, which can be seen as a ""grid"" of integer-valued points separated by a distance of 1. You can easily ""visualize"" the distance between the points of the domain. Now let's move to graphs. What is the underlying domain of the graph? It's a set of nodes $\mathcal{V}$ and a set of edges $\mathcal{E}$ . There is no such thing as distance in the Euclidean sense between the nodes of a graph. If you ask ""What is the distance between point A and B?"", the answer is probably a function of the connectivity of the graph (which is part of the domain), and is not the same for arbitrary points in the graph . How would a dataset of non-Euclidean data look like? Usually, a data point (sample) consists of a domain and a signal. The domain can be for example a weighted adjacency matrix of the graph, which lists the ""distances"" between nodes, for those that exist. Features can be either per node, per edge or both. In a concrete example, the domain can be a road network graph with distances and connectivity between road intersections, and the signal can be per-node features indicating how many cars are in the intersection at a given point. Of course, the domain can also be different between data points (i.e. in the PROTEINS dataset, where each protein is a graph connected (edges) aminoacids (nodes), with different structures). A non-Euclidean dataset simply consists of multiple such data points, which may or may not have the same underlying domain (i.e. same graph structure defined by an adjacency matrix).",61.73945384,"As far as I understand, the concept of non-Euclidean space doesn't bring the ordinality or hierarchy among the features, compared to that with the data formed in the Euclidean space. The difference between both these techniques is not remarkable for discriminative tasks like classification. But, for generative modeling, the non-Euclidean techniques helps in defining the latent manifold space for the given data distribution. This can further help in traversing the manifold from the same distribution (to generate similar samples from the same or underlying manifold) even with $n$ degrees of freedom in the latent space. This is not possible with Euclidean techniques. One cannot fully traverse/generate samples from or outside the manifold without the minimal change in the Euclidean space. More precisely, it can, but it will only present it as noisy data.",62.19966847,,,,,,,,
9417,Why can't the XOR linear inseparability problem be solved with one perceptron like this?,neural-networks,"It can be done. The activation function of a neuron does not have to be monotonic. The activation that Rahul suggested can be implemented via a continuously differentiable function, for example $ f(s) = exp(-k(1-s)^2) $ which has a nice derivative $f'(s) = 2k~(1-s)f(s)$ . Here, $s=w_0~x_0+w_1~x_1$ . Therefore, standard gradient-based learning algorithms are applicable. The neuron's error is $ E = \frac{1}{2}(v-v_d)^2$ ,
where $v_d$ - desired output, $v$ - actual output. The weights $w_i, ~i=0,1$ are initialized randomly and then updated during training as follows $$w_i \to w_i - \alpha\frac{\partial E}{\partial w_i}$$ where $\alpha$ is a learning rate. We have $$\frac{\partial E}{\partial w_i} = (v-v_d)\frac{\partial v}{\partial w_i}=(f(s)-v_d)~\frac{\partial f}{\partial s}\frac{\partial s}{\partial w_i}=2k~(f(s)-v_d)(1-s)f(s)~x_i$$ Let's test it in Python. import numpy as np 
import matplotlib.pyplot as plt For training, I take a few points randomly scattered around $[0, 0]$ , $[0, 1]$ , $[1, 0]$ , and $[1, 1]$ . n = 10
sd = [0.05, 0.05]

x00 = np.random.normal(loc=[0, 0], scale=sd, size=(n,2))
x01 = np.random.normal(loc=[0, 1], scale=sd, size=(n,2))
x10 = np.random.normal(loc=[1, 0], scale=sd, size=(n,2))
x11 = np.random.normal(loc=[1, 1], scale=sd, size=(n,2))

x = np.vstack((x00,x01,x10,x11))
y = np.vstack((np.zeros((x00.shape[0],1)), 
               np.ones((x01.shape[0],1)), 
               np.ones((x10.shape[0],1)), 
               np.zeros((x11.shape[0],1)))).ravel()

ind = np.arange(len(y))
np.random.shuffle(ind)

x = x[ind]
y = y[ind]
N = len(y)

plt.scatter(*x00.T, label='00')
plt.scatter(*x01.T, label='01')
plt.scatter(*x10.T, label='10')
plt.scatter(*x11.T, label='11')
plt.legend()
plt.show() Activation function: k = 10

def f(s):
    return np.exp(-k*(s-1)**2) Initialize the weights, and train the network: w = np.random.uniform(low=0.25, high=1.75, size=(2))

print(""Initial w:"", w)

rate = 0.01
n_epochs = 20

error = []
for _ in range(n_epochs):
    err = 0
    for i in range(N):
        s = np.dot(x[i],w)
        w -= rate * 2 * k * (f(s) - y[i]) * (1-s) * f(s) * x[i]
        err += 0.5*(f(s) - y[i])**2
    err /= N
    error.append(err)

print('Final w:', w) The weights have indeed converged to $w_0=1,~w_1=1$ : Initial w: [1.5915165  0.27594833]
Final w: [1.03561356 0.96695205] The training error is decreasing: plt.scatter(np.arange(n_epochs), error)
plt.grid()
plt.xticks(np.arange(0, n_epochs, step=1))
plt.show() Let's test it. I create a testing set in the same way as the training set. My test data are different from my training data because I didn't fix the seed. x00 = np.random.normal(loc=[0, 0], scale=sd, size=(n,2))
x01 = np.random.normal(loc=[0, 1], scale=sd, size=(n,2))
x10 = np.random.normal(loc=[1, 0], scale=sd, size=(n,2))
x11 = np.random.normal(loc=[1, 1], scale=sd, size=(n,2))

x_test = np.vstack((x00,x01,x10,x11))
y_test = np.vstack((np.zeros((x00.shape[0],1)), 
               np.ones((x01.shape[0],1)), 
               np.ones((x10.shape[0],1)), 
               np.zeros((x11.shape[0],1)))).ravel() I calculate the root mean squared error, and the coefficient of determination (R^2 score): def fwd(x,w):
    return f(np.dot(x,w))

RMSE = 0

for i in range(N):
    RMSE += (fwd(x_test[i],w) - y_test[i])**2

RMSE = np.sqrt(RMSE/N)

print(""RMSE"", RMSE)

ybar = np.mean(y)

S = 0
D = 0
for i in range(N):
    S += (fwd(x_test[i],w) - y_test[i])**2
    D += (fwd(x_test[i],w) - ybar)**2

r2_score = 1 - S/D
print(""r2_score"", r2_score) Result: RMSE 0.09199468888373698
r2_score 0.9613632278609362 ... or I am doing something wrong? Please tell me.",50.21272654,"The main problems are that your activation function is not monotonic (as pointed out by csrev), and that it is not continuously differentiable . These make it very difficult / impossible to use standard gradient-based learning algorithms. So yes, there may exist a good solution of weight values, but it is very difficult to find or approximate those weight values automatically through a learning algorithm. Also note that it completely breaks down as soon as you have a tiny error in one of the weights, even if it is approximate very closely; if one of the weights has a value of $0.999$ rather than $1.0$ , the solution breaks down completely.",50.58391331,"Indeed I think the problem is with the way you've defined the activation function. By selecting it arbitrarily, you could solve many specific problems. In practice, activation functions used are monotonic. It keeps the error function convex at a per-layer level. In theory though I'm not sure exactly what Rosenblatt has claimed so it might be worth calling him",51.11611026,"I'm also working on a perceptron that is able to solve the XOR Problem, and I get interesting results using sine as an activation function, and using the derivative to make the perceptron learn. But you will need bias to make sure the perceptron is able to solve the problem.",61.35718709,"Another activation function that could be used for this problem: $$f(x) = \underset{i}{max}(x_i) - \underset{i}{min}(x_i)$$ It's not continuous, no backpropagation, sorry. Some other learning algorithm is required. However, this answers the question, if an XOR can be solved with one neuron. Maybe this function is a solution of some learning problem with weights. Something like $$f(x) = max(w_0x_0,w_1x_1) - min(w_0x_0,w_1x_1)$$ I don't know how this creature is generalizable to other tasks, and how much learning can be done by just manipulating maxima and minima of weighted inputs. Any ideas?",60.02095594,,,,,,,,
8885,"Why is the variational auto-encoder's output blurred, while GANs output is crisp and has sharp edges?",comparison,"The key is: VAE usually use a small latent dimension, the information of input is so hard to pass through this bottleneck, meanwhile it tries to minimize the loss with the batch of input data, you should know the result -- VAE can only have a mean and blurry output. If you increase the bandwidth of the bottleneck, i.e. the size of latent vector, VAE can get a high reconstruction quality, e.g. Spatial-Z-VAE",51.52163695,"In essence, Variational Autoencoders learn an ""explicit"" distribution of the data by trying to fit the data via a multi-dimensional Gaussian/Normal distribution. However, Generative Adversarial Networks learn an ""implicit"" distribution of data meaning that you cannot directly sample them. Also, due to the deterministic nature of neural networks, GANs tend to learn a Dirac Delta function . If you're lucky and the training of the GAN is successful, you can therefore get sharper images, since the model doesn't have to explicitly deal with the noise injected into it due to samplings, hence this could be a simpler learning problem. By deterministic, I mean assuming that you have no sampling anywhere in the middle layers of your model and only use the neural network as an input-output mapping function.",53.24903432,"The reason is because of L1 (or L2) reconstruction loss used in VAEs.
As is discussed in Image-to-Image Translation with Conditional Adversarial Networks : "" If we take a naive approach and ask the CNN to minimize the
Euclidean distance between predicted and ground truth pix-
els, it will tend to produce blurry results. This is because Euclidean distance is minimized by averaging all plausible outputs, which causes blurring"". Further: ""  GANs learn a loss that tries to classify if the output image is real or fake, while simultaneously training a generative model to minimize this loss. Blurry images will not be tolerated since they look obviously fake."" For further details read the ablation study in 4.2 of linked paper.",52.76902282,"The reason is simple: because VAE's & GANs are almost opposite in their strengths and weaknesses. VAE's (even VQ-VAEs!) always introduce some kind of noise to their encoder's output which has two consequences: it makes the latent space interoperable which increases generation diversity (the goal), but the added noise introduces some error/blur (unintended side-effect) . In contrast GAN's always uses adversaries to detect bad (e.g. blurry) images which has two consequences: the generated fake images are nearly indistinguishable from real images (the goal) , the generator tends drop modes (aka reduce diversity) to make fooling the adversary easier (unintended side effect). P.S. for VQ-VAEs: The noise in introduced by the vector-quantization & limited codebook size which causes a many-to-one (i.e. noisy) mapping.",54.50627681,"Fundamentally speaking, the optimization target of a VAE has two parts: minimizing reconstruction loss (similar to an AutoEncoder) and minimizing KL divergence loss (ensuring that the latent z follows a normal distribution so we can sample from it). However, these two objectives are antagonistic . The former aims to clearly distinguish different classes in the latent space to facilitate the final reconstruction, while the latter aims to mix different classes in the latent space to conform to a standard normal distribution. Therefore, after optimization, the images generated by a VAE are unlikely to be very clear and sharp.",52.26867914,,,,,,,,
8844,What are the most instructive movies about artificial intelligence?,social,"Here are my suggestions Her , the AI part (movie spoiler): The AI can define a user's profile just by hearing his short story, and ""act"" based on the user's profile. The AI makes the user comfort with it (her). It shows a very advance user profiling. Ex Machina the AI part (movie spoiler): This movie will show you how an AI learn to trick someone. The AI can express her feelings, and make you trust their feeling. Eagle Eye , the AI part (movie spoiler): A movie about a general story ""AI that want to kill"". This movie can show you how The AI can compile a lot of information for its purpose. Big Hero 6 , the AI part (movie spoiler): Baymax is a very good example of a very complex expert system, he has a ""knowledge chip"" and a very smart way to diagnose people",50.72255934,"2001 (1968) HAL 9000 is a great example of an artificial general intelligence that goes astray, where the humans don't understand the reasoning process as values dis-align.  (This is a nod to Asimov in the sense of humans not understanding the implications of a logical framework. Marvin Minsky was an adviser on the film.) BladeRunner (1982) The critical reception in the link references androids as mirror of humanity, with all the implications, including identity.  Empathy is the core theme. (Seeing the self in the other.  The film adapts these ideas into a love story, and includes overt Christian symbology with android Roy Batty in the finale. HBO's Westworld draws heavily from these ideas.)  The book and film also explicates an evolution of Turing tests , focused on psychological responses due to the ever increasing sophistication of the androids. Wargames (1983) This film features an autonomous military AI that does not understand the context of human reality and nearly starts a total nuclear war.  (An algorithm's view of reality is based on it's input, which in the film, was limited.)  The ending is an early nod to machine learning. WALL-E (2008) Here the key element is the diminishment of humanity and human purpose when strong AIs (in the form of robots) to accomplish every task. This is already happening and seems accelerated by mobile computing. Alien: Covenant (2016) A horror film at it's core, it unveils a race of superior aliens brought down by the hubris that their own creation could never turn on them.  In regard to humanity this is mirrored by the android David. (In some sense it's a throwback to Greek Drama, where pride is the fatal flaw.) Shout out to malioboro's answer re: Ex Machina, which raises the question: ""Is the AGI in Ex Machina a sociopath because she was created by a sociopath?"" Nature vs. Nurture",51.50581761,Transcendence was a a pretty good look at a super-intelligent AI.,50,"The Machine , which came out a year before Ex-Machina, features topics such as ethics, AI testing (specifically, the Turing test), artificial consciousness, emotional intelligence, artificial general intelligence, super-intelligence, the singularity and AI safety. An old but milestone movie is Wargames , which will likely get you some points for digging back into history on the subject. 2001: A Space Odyssey , which is an excellent movie. However, to truly appreciate the nuance of the Hal 9000 , you need to also see the sequel, or at least read a summary as it shows WHY Hal 9000 behaved the way it did. (In short, contradictory and irreconcilable instructions). Alien , the original, first one, which contains both humanistic and non-humanistic AI. Her , which is either sadly romantic or creepy as all hell, depending on how you interpret it.",52.46853194,"A great movie to watch would be A.I. Artificial Intelligence , which is a sort of modern retelling of Pinocchio. Another good AI movie where the main character is a robot would be Bicentennial Man , based on Isaac Asimov's the positronic man .",52.90522064,Here are the best movies about artificial intelligence 2001: A Space Odyssey (1968) Colossus: The Forbin Project (1970) Blade Runner (1982) A.I. Artificial Intelligence (2001) Her (2013),70.88687556,"Upgrade This film depicts a very plausible near future when drones oversee our lives (e.g. the police use them to fight crime) and common people possess self-driving cars. This is definitely one of the best science fiction movies I have ever watched in my entire life, and I have watched many, such as 2001 , Blade Runner , or The Matrix . In fact, these are the four best science fiction movies ever made, in my opinion (and I have some knowledge of cinema, cinematography, directing, etc.)",55.18220933,,,,
8554,"How do compute the table for $p(s',r|s,a)$ (exercise 3.5 in Sutton & Barto's book)?",reinforcement-learning,"The function $r(s,a,s')$ gives the expected reward in each scenario, but not the distribution of rewards that lead to values $r_{search}$ and $r_{wait}$ The text explains that reward is $+1$ for each can found, and that different distributions of numbers of cans are expected when waiting as opposed to searching. However, it does not give any description of the actual distributions, just summarises them as the two expected rewards, and suggests $r_{search} \gt r_{wait}$ You have two main ways to answer the exercise: Invent some parameters for the distributions of $r_{search}$ and $r_{wait}$ in order to split up single values of $p(s'|s,a)$ into multiple values of $p(s', r|s,a)$ . E.g you could decide that $r_{search}$ consists of $0  \eta_0 + 1  \eta_1 + 2 \eta_2 + 3 \eta_3$ where $\eta_0, \eta_1, \eta_2, \eta_3$ are probabilities that sum to $1$ - each row that currently has $r_{search}$ as the output of $r(s,a,s')$ would then split into 4 rows with reward 0, 1, 2, 3 to complete the new table . . . $r_{wait}$ would need a different set of parameters. Ignore the details of the distribution, move column $r(s,a,s′)$ to the left and call it $r$ , changing $p(s|s,a)$ to $p(s', r|s,a)$ . It might be all that's expected given the lack of information. My personal opinion is that the authors want you to think about solution 1 - the only issue is that it requires you to invent some new parameters that were not provided. The ones I name are only a suggestion, they do not represent a specific ""correct"" answer in terms provided by the book, because the book omits those details. As an example to start with, if you start solution 1, and use parameters as I have labelled them, you will end up with a first row looking like this: $s\qquad \qquad a\qquad  \qquad  s'\qquad  \qquad   r \qquad       p(s', r| s, a)$ $high  \qquad \quad search  \qquad  high  \qquad \quad   0  \qquad   \alpha \eta_0$",52.52724285,"At first, like Neil Slater says, I thought this could only be solved using the expected rewards instead of actual rewards, or else there wasn't enough information to solve it. But now I think there might be a way to solve this question. Here is my thinking on this problem (I would be curious for anyone's thoughts, as I am working through this book myself). I think the key part is where the book says: Each can collected by the robot counts as a unit reward, whereas a reward of $-3$ results whenever the robot has to be rescued. This means that the reward set is actually $\mathcal R = \{0, 1, -3\}$ (we assume that in each timestep, the robot can only collect one can). Now using $$r(s,a,s') = \sum_r r \frac{p(s',r\mid s,a)}{p(s'\mid s,a)} \tag{3.6}$$ and $$p(s'\mid s,a) = \sum_r p(s',r\mid s,a)\tag{3.4}$$ it seems possible to solve for all the probabilities. I'll do an example for $(s,a,s') = (\mathtt{high}, \mathtt{search}, \mathtt{high})$ and leave the rest to you (I haven't actually done the rest, since this does seem rather tedious). Equation 3.6 gives $$r_\mathtt{search} = 0\cdot \frac{p(s', 0 \mid s,a)}{\alpha} + 1\cdot \frac{p(s', 1 \mid s,a)}{\alpha} -3\cdot \frac{p(s', -3 \mid s,a)}{\alpha}$$ Since $p(s', -3 \mid s,a) = 0$ (it's impossible for the robot to have to be rescued, since we started in the ""high"" state), we get $p(s', 1 \mid s,a) = \alpha r_\mathtt{search}$ . Now equation 3.4 gives $$\alpha = p(s', 0 \mid s,a) + p(s', 1 \mid s,a) + p(s', -3 \mid s,a)$$ which solves to $p(s', 0 \mid s,a) = \alpha - \alpha r_\mathtt{search}$ . So the first two rows of the table will look like: $$\begin{array}{cccc|c}
s& a & s' & r & p(s',r\mid s,a)\\ \hline
\mathtt{high} & \mathtt{search} & \mathtt{high} & 1 & \alpha r_\mathtt{search}\\
\mathtt{high} & \mathtt{search} & \mathtt{high} & 0 & \alpha (1- r_\mathtt{search})
\end{array}$$",51.92431264,"In the announced problem, most of the transitions aren't possible, so most the terms of equations (3.3) and (3.4) from the book will end up being 0. In my understanding, $$
\begin{align}
p(s'= high | s = high, a = search) &= \sum_{r \in \{0, -3, r_{search}, r_{wait}\}} p(s'=high, r | s = high, a = search) \\
&= p(s'=high, r =0 | s = high, a = search) \\
&+p(s'=high, r = -3 | s = high, a = search) \\
&+p(s'=high, r = r_{search}| s = high, a = search) \\
&+p(s'=high, r = r_{wait} | s = high, a = search)
\end{align}
$$ The problem states that if the agent has high batteries and it chooses to search, then there is no chance that it ends up having a negative reward ( $r= -3$ ), thus its transition probability is 0 by definition: $p(s'=high, r = -3 | s = high, a = search) = 0$ . Applying the same logic to all other terms, we get, $$
\begin{align}
p(s'= high | s = high, a = search) &= \sum_{r \in \{0, -3, r_{search}, r_{wait}\}} p(s'=high, r | s = high, a = search) \\
&= p(s'=high, r = r_{search}| s = high, a = search) \\
&= \alpha
\end{align}
$$ It looks weird. I not 100% sure that that's the solution, because the question would not make much of sense (the table would've been quite the same).",51.85854724,"This means that the reward set is actually R={0,1,−3} (we assume that in each timestep, the robot can only collect one can). @riceissa While I agree with the rest of your demonstration, I wouldn't assume that the robot can only collect 0 or 1 can. As Neil Slater suggest, I think the robot could pick any number of cans between 0 and N. Below is how I solve the problem for the more general case, assuming specific values for $s',s, a$ . This generalization encompasses @riceissa answer. Let : $S_t=s'=\texttt{high}$ $S_{t-1}=s=\texttt{high}$ $A_{t-1}=a=\texttt{search}$ We have the following equality : $$r_{search}=\sum_{r \in R}r\cdot\frac{p(s', r|s, a)}{p(s'|s, a)}$$ For these values of $s', s, a$ we also have: $p(s'|s,a)=\alpha$ . $p(s', -3|s,a)=0$ Writing $\eta_r:=p(s',r|s,a)$ and for $R=\{0, 1, 2, \dots,N\}$ (we omit $r=-3$ since the proba is 0 for this case) we have then: \begin{align*} 
r_{\texttt{search}}&=\sum_{r=0}^N r\cdot\frac{\eta_r}{\alpha}\\
r_{\texttt{search}}\cdot\alpha&=\sum_{r=0}^N r\cdot\eta_r\\
r_{\texttt{search}}\cdot\alpha&=i\cdot\eta_i + \sum_{\substack{r=0 \\ r\neq i}}^N r\cdot\eta_r\\
i\cdot\eta_i&= r_{\texttt{search}}\cdot\alpha- \sum_{\substack{r=0 \\ r\neq i}}^N r\cdot\eta_r
\end{align*} For $i\neq 0$ : $$\eta_i= \frac{1}{i}\cdot\left(r_{\texttt{search}}\cdot\alpha- \sum_{\substack{r=0 \\ r\neq i}}^N r\cdot\eta_r\right)$$ For $i=0$ , we use that fact that \begin{align*}
\alpha&=p(s'|s,a)=\sum_{r=0}^N p(s',r|s,a)=\sum_{r=0}^N\eta_r\\
&\Rightarrow\eta_0=\alpha - \sum_{r>0} \eta_r
\end{align*} So substituting $\eta_r$ by its probability formula we end up having, for these specific values of $s', s, a$ : \begin{equation}
    \begin{cases}
      p(s',i|s,a)= \frac{1}{i}\cdot\left(r_{\texttt{search}}\cdot\alpha- \sum_{\substack{r=0 \\ r\neq i}}^N r\cdot p(s',r|s,a)\right) & \texttt{$\forall i \in [1,N]$}\\
      p(s',0|s,a)=\alpha -\sum_{r>0} p(s',r|s,a) & \texttt{if $i=0$}
    \end{cases}    
\end{equation}",50.50244826,"My understanding is that it will be the same as p(s' | s, a) for any s, a, s', r combination. The reward r(s, a, s') is already defined in terms of s, a, s'. Since p(s', r| s, a) = p(r|s', a, s)* p(s'| a, s). For each case the p(r | s', a, s) is equal to 1 by definition. Thus, the column for p(s', r| a, s) = p(s'| s, a) For instance, If,
S= high, A=search and S'=High, then the reward is always alpha. p(r=alpha| s=high, a = search , s'=high) = 1 (you always get that reward for this case) etc.",51.14814628,,,,,,,,
7926,Does AI rely on determinism?,philosophy,"I'm going to assume that by free will, you mean something like the philosophical concept of libertarian free will , which is defended by philosophers like Robert Kane . In Libertarian Free Will, individuals have some capability to make choices about their actions. The classic way to argue this is by assuming some kind of spirit-stuff (e.g. a soul) that exists outside the material world, and that this spirit-stuff constitutes the consciousness of a person. Kane tries some mental gymnastics to avoid this, but then concedes something like it in a footnote. I'm not aware of any serious work that doesn't make some kind of non-physical assumption to justify this view. If someone can point at one, I'll update the answer. By determinism, I'm going to assume you mean the usual notion of philosophical determinism : since people's decisions depend on what happened in the past, and where they are in the present, they don't really have a choice in any meaningful sense. Philosopher's like Dennett adopt a slightly softer view ( compatibilism , essentially: you don't get to make big choices, but you do get to make small ones). Appeals to Quantum Mechanics are common to justify that view. In this context, free action means something more like ""did something we couldn't predict exactly"". An example might be: you are pre-destined to put a can of campbell's brand tomato soup in your shopping cart, but ""make a choice"" about exactly which of the dozens of cans you will put in. Since small choices can have large impacts (maybe that can will give you food poisoning, and the others wouldn't), this can make all sorts of things impossible to predict exactly. I think most AI researchers don't worry too much about these issues, but Turing actually addresses them in his paper right at the start of the field, Computing Machinary and Intelligence . The deterministic/compatibilist view point is introduced as Lady Lovelace's objection: Computers only know how to do what we tell them to, so they can't be called intelligence. Turing's counterargument uses two prongs. First, Turing notes that computers can probably be made to learn (he was right!). If they can learn, they can do things that we don't expect, just like children do. Second, Turing notes that computers already do all sorts of things we don't expect: they have bugs. Anytime a computer exhibits a bug, it did something that was unexpected. Since we cannot generally rule out bugs in programs, computers will always do surprising things. Therefore, computers satisfy the deterministic notion of free will. Turing also addresses the libertarian notion of free will, which is part of what he calls the ""Theological Objection"". The objection is that intelligence requires some kind of divine spark (like free will). Turing argues that we can't detect sparks like this right now (he actually thought we would be able to one day, and spent a lot of time looking at supernatural phenomena too). However, there's no reason to suppose that computers with the right programs won't be endowed with them. A divine creator could decide that anytime you build something brain-like, it gets a spark. If we build a program that's brain-like, maybe it gets a spark too. In the absence of some way to detect souls, it seems like we ought to just agree to treat things that seem intelligent as though they had these souls, since otherwise we don't have a really clear way to decide who is and isn't intelligent. The only remaining way is to say ""only things made of human meat have souls and are intelligent"". While a lot of people do actually say things like this (e.g. animals have no souls), this is a pretty arbitrary view, and I think there's no hope arguing against it. Turing suggests the ""Anthropic Principal"": we shouldn't assume that we're special, because the earth isn't in a special place in the galaxy, or in the universe, and we have pretty compelling evidence that we're an evolved version of other animals around us, but some groups (e.g. biblical literalists) find this unconvincing.",51.98856463,"AI is ""deterministic"" in the sense that it follows exactly the algorithm. ""Deterministic"" means different things to a data scientist/programmer, but let's not go into details here. There is no ""freewill"" in AI, it's all about mathematics and algorithms. Don't watch too many scientific movies!",51.03507536,"Although I’m not sure how an act of freewill could even be described (let alone replicated), Well, one popular definition goes like this: [Free will is] the freedom to act according to one's motives without arbitrary hindrance from other individuals or institutions Source - Wikipedia entry on Compatibilism Note that this definition is perfectly compatible with determinism (hence the name ""Compatibilism""). Actually, proponents usually argue that free will requires determinism, because if your choices were ultimately random, like rolling a dice, how could they be your free choices? Now, if you assume that an AI can be said to have ""motives"", then according to this view, it would have free will - if noone hinders it. The contrary view, Incompatibilism, has been described in another answer by John Doucette. I agree with him that most AI researchers probably don't worry about philosphical questions like that. All the proponents of indeterminism (sometimes called metaphysical free will or libertarianism) I'm aware of assume that there exist ""causes"" that are neither deterministic / physical, nor purely random. (e.g. Agent causation ). Since AIs are more or less by definition based on physical processes, deterministic algorithms and possibly some random number generator, I don't see how they could posess this kind of freedom.",54.70768568,AI is algorithmic not free willed in a sense that humans have free will.  So in that sense it is deterministic.  Give it the same data each time you would expect the same result.  Change something (ie feed it new data to learn from) and then it will give a different result.  Hence the determinism. EDIT: Some algorithms do use some randomising - ie some versions of hill climbing - but if we want to get technical there's no such thing as true random numbers anyway.  (Unless you're using one of those supercomputers that use the radiation from the sun as a seeding factor),53.48985018,"The free will in the spiritual world would be for you to have the right to follow or not the main path (God's way). Taking into consideration that whatever path you follow, you will have its consquences (good or bad). However, an artificial intelligence created to solve problems does not need these distractions. Determinism considers the feeling of freedom a subjective illusion. Trying to apply free will to an AI is to create an inner inner space within the ""mind"" .. but creating that is already impossible because we can not understand exactly what is going on in our minds. The big question is, how to program something that we can not explain accurately? How to induce an AI to learn something that does not make sense? I believe that it is not just code and database that will lead the AI ​​to have faith in its existence or to believe in free will. But to construct an AI that reads and understands every human mind, every thought, house illusion, every paranoia, every confusion, every sadness, every lie ... may be a good source of studies, but whoever qualifies for this kind of experiment? What would be the conclusion of an AI by understanding the whole confused and troubled human mind? It's a very complex question ... let's continue studying and raising this kind of subject. I spent my last years trying to recreate human thoughts and actions in an AI .. it's a study for a lifetime .. my fear is the disappointment at the end of everything :(",53.78568566,"Love question. Firstly, as pointed out in other answers Narrow AI today are mostly algorithms following their procedures given some inputs. No need for philosophy here as they are following reproducible steps. However, if your referring to general AI or AI a kin to human level intelligence or better then maybe the question holds some weight. But again as pointed out it would come back to whether you believe you or I indeed have free will? For me I believe free will can be modelled as a sort of entropy. If you look on the macro level things are blurry, agents are making decisions and moving around in a unpredicatable way. On the micro level however, given all the data in one state one could predict the next state shattering the idea of free will. I guess it’s up to you to decide whether this fits with your free wil definition or not.",51.14916246,"No, because there is no utility in building a ""libertarian free AI"" as far as I know of. AI is another tool. What is the purpose in building an AI with such a distinction? The reason for that question is this. Let's say you want an AI to accomplish some kind of task you want machine assistance in. That's what tools do- assisting with tasks. What exactly would this task be that a ""non-libertarian free AI"" couldn't achieve but a ""libertarian free AI"" could?",51.77204087,"""there's no such thing as true random numbers anyway."" that's all you really need to deter any idea of AI on any computer. Any software or set of functions on a computer is pretty much (right now at least) all set code, by humans. Also the execution of actions based on variables not listed is not artificial intelligence, it's just simpler code. Any REAL Artificial Intelligence will not be made on a board of 1's and 0's, that's defeating the purpose, all of the actions are predetermined (even if they are extended intricately to cover many possibilities) so they have no chance to create something non-deterministic.
Real independent intellect is most likely (in my eyes) found, not made.",51.22611162,"I was very bored. The short answer is yes and the long answer is basically yes . I'm going to gloss over what you could possibly mean by AI and instead focus on what AI programs pretty much are: A mish-mash of algorithms and methods borrowed from mathematics, perhaps more specifically statistics, inserted into a good old-fashioned program. Let's start from the very beginning. The very beginning . For the sake of this argument, everything is a language and there exists an omnipotent alphabet which contains all possible symbols you can come up with to convey a message. Such an alphabet would contain everything you've ever known, all their permutations, and then some. It would contain your clothes as well. Forget countable infinity or all those concepts, those come way later. It is important to realise that right now I'm communicating with you through not just through the power of a string of characters. You're also experiencing other stimuli that you, or whatever part of ""you"", is/are capable of interpreting into another language. To be crude, everything you decide to classify as a 'standalone thing' is a compiler that runs in perpetual execution, translating all the 'things' it 'receives' into other 'things' it 'spits out' for other 'things' to 'receive'. Think about a modern day computer. You write a program in your fancy little language. You hit compile. A compiler goes through your code, and spits out more code. Except this time this code is written in another language, sometimes ""closer to the metal"", sometimes ""just about the same abstract level"" and sometimes ""even more abstract"", and this process repeats itself until somewhere along the line that code you started with, has been interpreted to mere electrical signals, which then themselves are being 'compiled' by an entity we will call ' the universe ' and that's up to empirical observation to determine what and what's not going on. (Except ""the universe"" was always responsible. But we will partition things for the sake of ..partitioning things. You get what I mean, I know you do.) Now let's jump back to languages. Mathematics is a language in the sense that: It is built of symbols contained in an alphabet we will call X The several fields specify their own grammatical structure through which we can decide whether or not a statement is well-formed . This encompasses everything from where you can put the + operator in high school algebra, to how you can write a proof in formal logic. There needs to be no justification for how you build a grammar. You can always just make a new grammar and use it instead. Of course, it might not be capable of forming statements which are compatible with other grammars. What's interesting about X is that its definition is not fixed. Throughout time, we've introduced new symbols into the mathematical alphabet to be able to express more concepts while keeping things separate. (Or rather some people have had the sense to keep it this way.) For example, whenever you see Leibniz's integration symbol, you know you're probably dealing with some kind of integration and not something novel that you've never heard of before. Now here's where I actually answer your question: I assume that by ""program"" you are referring to the mathematical construct as defined in theoretical computer science: A string of characters from an alphabet . This string is then fed to a compiler (lexer|parser| semantic analyser ) which spits out another string (mainly the job of the semantic analyser). This string usually is built from characters of a different alphabet. That is to say, the compiler is a function which maps a well formed string of language A to a string of language B The end goal of compiling a program is to execute it, which basically means a succession of compilers will take the output of the previous compiler and spit back yet another string, until the string is essentially electrons moving about the circuitry of the computer in your bedroom and producing fancy lights on your monitor So whenever you write an ""AI program"", you're just writing a ""program"" that contains some ""AI algorithms"" which are really just applications of things we've known in mathematics for 100s of years, which again, are really just a string of characters that are about to be translated by a compiler. In other words, nothing you can ever write is not deterministic, provided you look at the bigger picture. A common argument I see is that since AI programs usually ""adapt"" and ""self-optimise"" when solving the problem, they're not quite deterministic, in the sense that feeding the program the same input twice will (hopefully) yield better results the second time.
Except what really happened is that you had an input string that you partitioned into inputs A and B, and fed them to the algorithm in succession. Had you fed AB initially, you would've obtained the same results.",50.58849979
7838,What is artificial intelligence?,terminology,"Over the years, many people attempted to define artificial intelligence. A lot of those definitions are summed up by Stuart Russell and Peter Norvig in their book Artificial Intelligence - A Modern Approach The definitions of AI can be summarised as falling into the following categories: Those that address thought process and reasoning (how an AI thinks/reasons) Those that address behaviour (how an AI acts given what it knows) Furthermore, the above 2 categories are further divided into definitions that: I. assess the success of an AI (to do the above) based on its ability
  to replicate human performance II. or an ability to replicate an ideal performance measure called
  'rationality' (does it do the 'right' thing based on what it knows?) I will cite you definitions that fit into each of the above categories: 1.I. ""The [automation of] activities that we associate with human thinking, activities such as decision making, problem solving, learning.."" - Bellman 1978 1.II. ""The study of the computations that make it possible to perceive, reason, and act."" - Winston, 1992 2.I. ""The study of how to make computers do things at which, at the moment, people do better"" - Rich and Knight, 1991 2.II. ""The study of the design of intelligent agents"" - Poole et al., 1998 In summary, AI is devoted to the creation of intelligent and rational machines that can make rational decisions and take rational actions. I would suggest you read up on the Turing test, which Alan Turing proposed to test if a computer was intelligent. However, the Turing test has a few issues, because it is anthropomorphic. When Aeronautical engineers created the airplane, they didn't set their goal that planes should fly exactly like birds, but rather, they started learning how lift forces were generated, based on the study of aerodynamics. Using this knowledge, they created planes. Similarly, people in the AI world shouldn't put, IMHO, human intelligence as the standard to strive for, but, rather, we could use, say, rationality as a standard (amongst others).",53.43449706,"Intelligence A measure of the strength of a decision-making agent relative to other decision-making agents, in regard to a given task or set of tasks. The medium is irrelevant—intelligence is exhibited by both organic and intentionally created mechanisms.  May also be the capability to solve a problem, as in the case of a solved game . Artificial Relates to the term artifact , a thing which is intentionally created.  Typically this term has been used to connote physical objects, but algorithms created by humans are also regarded as artifacts. The etymology is derived from the Latin words ars and faciō : ""To skillfully construct"", or, ""the art of making"". Artificial Intelligence Any decision-making agent that is skillfully (intentionally) constructed. APPENDIX: The meaning of ""intelligence"" The original meaning of ""intelligence"" seems to be ""to acquire"", back to the Indo-European. See: intelligence (etymology) ; *leg/*leh₂w- The OED 1st definition of intelligence is not incorrect, extending the meaning to acquisition of capability (demonstrable utility), just that the second definition is the older and fundamental: ""The collection of information of [strategic] value; 2.3 (archaic) Information in general; news."" You can regard the universe as being comprised of information , whatever form that information takes (matter, energy, states, relative positions, etc.) From the standpoint of an algorithm, this makes sense since the only means they have to gauge the universe are percepts . Take a flat text file. It may just be data, but you could try and execute. If it actually runs, it might demonstrate utility at some task. (For instance, if it is a minimax algorithm.) ""Intelligence as a measure of utility"" is itself ""intelligence"" in the sense of information, specifically that information by which we measure intelligence, as a degree, relative to a task or to other intelligences.",55.47472656,"What is artificial intelligence? This question is ambiguous. I will address the two less ambiguous but related questions. What is the goal of the AI field? What is an artificial intelligence? What is the goal of the AI field? In the article What is artificial intelligence? (2007), John McCarthy, one of the founders of artificial intelligence and who also coined the expression artificial intelligence , writes Artificial intelligence is the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable. Therefore, the goal of the AI field is to create intelligent programs (or machines). So, he defines the goal of the field based on the concept of intelligence , which he defines as follows. Intelligence is the computational part of the ability to achieve goals in the world. Varying kinds and degrees of intelligence occur in people, many animals and some machines. So, we could conclude that the goal of the AI field is to create programs (or machines) that achieve goals in the world to different extents. This definition of intelligence is reasonable and consistent with reinforcement learning (which could be the path to AGI ), but maybe not formal and rigorous enough. In this answer , I report a possibly more sound definition of intelligence given by Hutter and Legg, so I suggest that you read it, but the definitions are roughly consistent with each other (because the concepts of ""goal"" and ""goal-seeking behavior"" are present in both definitions), although they emphasize different aspects (e.g. computation or generality). What is an artificial intelligence? Nowadays, most people distinguish two types of artificially intelligent systems: Narrow AI (aka weak AI, although this term may not exactly be a synonym for narrow AI, but it's just the opposite of strong AI: see the Chinese-Room argument ): a system that solves a very specific problem (e.g. playing go) Artificial general intelligence (aka strong AI, although this term may not always be used as a synonym for AGI): a system that can solve multiple problems This distinction started with philosophical arguments, such as the Chinese room argument , where the ability of a computer to ""understand"" the actual problem was questioned. Nowadays, there are multiple successful cases of narrow AIs (e.g. AlphaGo), but there isn't yet a ""truly"" AGI system. This is mainly due to the fact that more people have been (probably wisely) focusing on solving specific problems rather than solving the ""holy grail"" problem of the AI field, i.e. create an AGI, which seems to be a lot more difficult than creating narrow AI systems. (Anyway, the creation of an AGI could actually arise from solutions to these specific problems, so maybe we are already creating the tools needed to build an AGI, without realizing it). See What is the difference between strong-AI and weak-AI? for more details about the difference between narrow AI and strong AI.",64.22703545,"According to the book Artificial Intelligence: A Modern Approach (section 1.1), artificial intelligence (AI) has been defined in multiple ways, which can be organized into 4 categories. Thinking Humanly Thinking Rationally Acting Humanly Acting Rationally The following picture (from the same book) provides 8 definitions of AI, where each box contains 2 definitions that fall into the same category. For example, the definitions in the top-left corner fall into the category thinking humanly . There is also the AI effect , which Pamela McCorduck describes (in her book Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence , p. 204) as follows it's part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something — play good checkers, solve simple but relatively informal problems—there was a chorus of critics to say, but that's not thinking",57.75674223,"There is no formal definition that most people agree on. Hence here is what I, as a data science / machine learning consultant, think: Artificial intelligence as a research field is the study of agents which sense and act autonomously in an environment and improve their situation according to some metric with their actions. I don't like the term, because it is too broad / vague. Instead, look at the definition of machine learning by Tom Mitchell: A computer program is said to learn from experience 'E', with respect to some class of tasks 'T' and performance measure 'P' if its performance at tasks in 'T' as measured by 'P' improves with experience E Machine learning is an important part of AI, but not the only one. Search algorithms, SLAM, constrained optimization, knowledge bases and automatic inference are also certainly part of AI.",53.53506206,"The facts and anomalies paper (mine) has attempted narrowing down on what intelligence is. Only once you identify it, you'll be on the right path to replicating it. For now, nobody knows exactly what constitutes intelligence. From the perspective of just what intelligence is: The ability to search for, parse and bring in the right information into a context in order to deduce past, present and future (temporal) occurrences that enable moving toward favourable outcomes . So a program or machine that operates based on some if-then-style conditions would be considered as having a basic level of intelligence. However, when we say ""artificial intelligence"", we usually mean an intelligence that's approximately as good or better than living creatures. From the facts and anomalies paper, the decision-making process was observed to be similar across almost all living creatures. What differs, is the amount of memory stored, the length of the attention-span, questioning capability and to what level of complexity the creature can mix and match memories in the world model it creates in its mind. The paper also brings forth an important point that an intelligence created via any tech can be specific to that tech. So an AI created via microprocessors will inherently be different from an organic intelligence (and there is nothing wrong in building it as such), but we will recognize it as being intelligent in the same way as an artificial sweetener is accepted as a replacement for sugar. To think of it another way: If you had to say which one of two people are intelligent, how would you evaluate them? The person we'd consider more intelligent, would be the one who understands and analyzes situations better to take decisions that have a better outcome than others. Even a person lacking vast knowledge will be considered intelligent if their creativity and depth of thought is higher than others. A crucial factor that enables this is the attention span of the mind. A machine that is programmed to access vast stores of memories for even trivial decision making tasks (which helps it evaluate consequences of various actions: commonsense) and is capable of asking questions and can simulate situations in memory by loading and modifying stored memories in the simulation (imagination) will be a lot more ""intelligent"" than us. There is a second paper ( cognitive memory constructs ) that describes a bit about the theory of implementation. All this being said, this is just our perspective on intelligence. The fact that the universe exists in such a complex form, is probably evidence of much higher forms of intelligence (like how we are much more intelligent than the Age of Empires AI characters we created). Intelligence may exist in far more dimensions than we are currently capable of imagining.",55.7115416,"The shortest answer I could can come up with could be as follow; take it with a grain of salt though since we still do not know a lot about natural intelligence: What natural intelligence is could be seen as the process of learning abstract concepts from limited observations with the intention to use them for solving a [new] task. This process involves using those concepts to imagine new, hypothetically correct scenarios/theories and combine them in a meaningful way to cut down the enormous hypothesis space of possibilities and enable generalization to new situations without observing any data beforehand. Artificial intelligence is to bring what natural intelligence does into machines.",55.70882393,"Along the observation in the comment, the artificial part can in principle be assigned to artifacts (or at least combination of natural beings and artifacts) and focus on the intelligence knot. Assuming the shortest definition cannot be less than three words long, intelligence looks like a measure of an operation onto something. Here are some tentative points around a sensible concise definition, depending on the concrete word we choose for each piece of the abstract ""measure of operation on something"". Data organization simplicity. Information processing efficiency. Model adaptation adequacy.",52.43078746,,
7624,"What is the most general definition of ""intelligence""?",definitions,"I'm going to preface this answer by noting that persons much smarter than myself have treated this subject in some detail.  That said, as far as I can discern: When we talk about intelligence we're referring to problem solving strength in relation to a problem, relative to the strength of other intelligences. This is a somewhat game-theoretic conception, related to rationality and the concept of the rational agent .  Regarding intelligence in this manner may be unavoidable. Specifically, we could define intelligence as the ability to understand a problem or solution or abstract concepts, but we can't validate that understanding without testing it. (For instance, I might believe I grasp a mathematical technique, but the only way to determine if that belief is real or illusory is to utilize that technique and evaluate the results.) The reason games like Chess and Go have been used as milestones, aside from longstanding human interest in the games, is that they provide models with simple, fully definable parameters, and, in the case of Go at least, have complexity akin to nature, by which I mean unsolvable / intractable . (Compare to strength at Tic-Tac-Toe, which is trivially solved.) However, we should consider a point made in this concise answer to a question involving the Turing Test : ""...is [intelligence] defined purely by behaviour in an environment, or by the mechanisms that arrive at that behaviour?"" This is important because Google just gave control over data center cooling to an AI .  Here it is clearly the mechanism itself that demonstrates utility, but if we call that mechanism intelligent, for intelligence to have meaning, we still have to contend with ""intelligent how?""  (In what way is it intelligent?) If we want to know ""how intelligent?"" (its degree of utility) we still have to evaluate its performance in relation to the performance of other mechanisms. (In the case of the automata controlling the air conditioning at Google, we can say that it is more intelligent than the prior control system, and by how much.) Because we're starting to talk about more ""generalized intelligence"", defined here as mechanisms that can be applied to a set of problems, (I include minimax as a form of ""axiomatic intelligence"" and machine learning as a form ""adaptive intelligence"") , it may be worthwhile to expand and clarify the definition: Intelligence is the problem solving strength of a mechanism in relation to a problem or a set of problems, relative to the strength of other mechanisms. or, if we wanted to be pithy: Intelligence is as intelligence does (and how well.)",53.82604782,"Over the years, many people have attempted to define intelligence, so there are many definitions of intelligence, but most of them are not formalized. For a big collection of definitions, see the paper A Collection of Definitions of Intelligence (2007) by Shane Legg and Marcus Hutter. In an attempt to formally define intelligence, so that it comprises all forms of intelligence, in the paper Universal Intelligence: A Definition of Machine Intelligence (2007), the same Legg and Hutter, after having researched the previously given definitions of intelligence, define intelligence as follows Intelligence measures an agent 's ability to achieve goals in a wide range of environments This definition apparently favors systems that are able to solve more tasks (i.e. AGIs ) than systems that are only able to solve a specific task (i.e. narrow AIs ), but, according to Legg and Hutter, it should summarise the main points of the previously given definitions of intelligence, so it should be a reasonable and quite general definition of intelligence. Moreover, properties associated with intelligence, like the ability to learn, should be emergent, i.e. in order to achieve goals in a wide range of environments you also need the ability to learn. In my blog post On the definition of intelligence , I also talk about this definition, but I suggest that you read the mentioned papers if you are interested in all details. This video by Marcus Hutter could also be useful and interesting.",62.12535257,"This is an important question for AI – maybe the most important of all –  for the research field of Artificial Intelligence . I mean if AI is science, then its experiments will be empirically testable. There has to be a way to decide pass or fail. So what are the tests for intelligence? Before you even design a test, you need a clear idea of what intelligence amounts to, otherwise how could you design a competent test for it? Sure, I'm part of the research and development project known as Building Watertight Submarines, and sure, I'm totally confident my submarine is watertight, but I have no idea how to test whether it is or not because I don't know what ""watertight"" means.
This whole idea is absurd. But ask AI what ""intelligence"" means. The answers you get, on analysis, are almost the same as the submarine example. Base Answer - Behavior The word (idea, concept) ""Intelligence"" is usually defined by AI in terms of behavior. I.e. the Turing test approach. A machine is intelligent if it behaves in a way that, were a human to behave in that same way, the human would be said to be performing an action that required human intelligence. Problem 1 : player pianos are intelligent. Playing a Scott Joplin tune obviously requires intelligence in a human. Problem 2 . If a machine passes the test, it only shows that the machine is ""intelligent"" for the tested behaviors. What about untested behaviors?  This is actually a life-and-death problem today with self-driving vehicle AI control systems. The AI systems are acceptably good at driving a car (which obviously requires human intelligence) in specific environments, e.g. freeways with well-marked lanes, no tight corners, and a median barrier separating the two directions. But the systems go disastrously wrong in ""edge cases"" – unusual situations. Problem 3 . Who would put their child on a school bus driven by a robot that had passed the Turing test for driving school buses? What about a storm when a live power line falls across the road? Or a twister in the distance is coming this way? What about a thousand other untested possibilities? A responsible parent would want to know (a) what are the principles of the internal processes and structures of human intelligence, and (b) that the digital bus driver had adequately similar internal processes and structures – i.e., not behavior but the right inner elements, the right inner causation. Desired answer – inner principles I would want to know that the machine was running the right inner processes and that it was running these processes (algorithms) on the right inner (memory) structures. Problem is, no one seems to know what the right inner processes and structures of human intelligence are. (A huge problem to be sure – but one that hasn't held AI back – or self-driving system developers - one bit.) The implication of this is that what AI ought to be doing now is working out what are the inner processes and structures of human intelligence. But it's not doing this – rather, it's commercializing its flawed technology. Elements of a definition – 1. Generalization We do know some things about human intelligence. Some tests really do test whether a machine has certain properties of the human mind. One of these properties is generalization. In his 1950 paper , Turing, as a sort of joke, gave a really good example of conversational generalization: (The witness is the machine.) Interrogator : In the first line of your sonnet which reads 'Shall I
compare thee to a summer's day', would not 'a spring day' do as
well or better? Witness : It wouldn't scan. Interrogator : How about 'a winter's day' That would scan all right. Witness : Yes, but nobody wants to be compared to a winter's day. Interrogator : Would you say Mr. Pickwick reminded you of Christmas? Witness : In a way. Interrogator : Yet Christmas is a winter's day, and I do not think Mr. Pickwick would mind the comparison. Witness : I don't think you're serious. By a winter's flay one means a typical winter's day, rather than a special one like Christmas. Current AI has nothing that comes even remotely near being able to generalize like this. Failure to generalize is regarded as perhaps the greatest failing of current AI. The ability to generalize would be one part of an adequate definition of ""intelligence"". But what generalization amounts to would need to be explicated. The problem of generalization, also, is behind several the severe philosophical objections to AI theory, including the frame problem , the problem of common-sense knowledge , and the problem of combinatorial explosion. Elements of a definition – 2. Perception Sensory perception is fairly obviously fundamental to human learning and intelligence. Data (in some form) is emitted by the human senses then processed by the central system. In the computer, binary values exit the digital sensor and travel to the machine. However, nothing in the values themselves indicates what was sensed. Yet the only thing the computer gets is the binary values. How could the machine ever come to know what is sensed?  (The classic Chinese room argument problem.) So another element of human-like intelligence is the ability to perceive in a human-like way. What ""human-like way"" means here is that the machine processes sensory input using the same principles that apply in human perception. The problem is that no one seems to know how a semantics (knowledge) can be built from the data emitted by digital sensors (or organic senses). But still, human-like perception needs to be an element of an adequate definition of ""intelligence"". Once AI gets these two issues sorted out – generalization and perception – then it will probably, hopefully , be well on the way to realizing its original goal of almost 70 years past – building a machine with (or that could acquire) a human-like general intelligence. And maybe the principles of generalization and the principles of perception are one and the same. And maybe there is actually only one principle. It shouldn't be assumed that the answers are complex. Sometimes the hardest things to understand are the most simple. So the question ""What do we mean when we say ""intelligence""? is really important to AI. And the conclusion is that AI ought to replace its current behavioral definition of ""intelligence"" with one that includes the human elements of generalization and perception. And then get on and try to work out the operating principles, or principle, of both of these.",57.57592645,"Intelligence is the ability to weave together various concepts and associations into a meaningful whole; filtering, adding and rejecting appropriately various ideas from personal knowledge and experience. Then effectively reflecting these ideas back to a questioner to affirm understanding and comprehension, allowing a conversation to proceed effectively towards a mutually beneficial conclusion.",51.50391901,"The most general definition of the term intelligence that is both terse and exact is this. The collection of behavioral features resident in some entity where the entity sustainably succeeds in specific pursuits while avoiding specific losses in a particular range of environmental conditions. These are examples of failures in exhibiting intelligence per the above definition, demonstrating the importance of each phrase. Some behavioral features that are intelligent but an overall system behavior that is not.  For example, a rocket that reaches an altitude but cannot enter orbit or a turtle that can retract its head but cannot catch a bug. The intelligence is scattered across entities such that each individual entity does not exhibit intelligence, such as a bee or a single. Intelligence is exhibited momentarily but disintegrates over time, not adapting to changing conditions, or is not sufficiently reliable to fill a practical role. The entity can attain a goal but such success is nullified by loss accumulated during their attainment. The entity can avert loss, but cannot reliably succeed in the pursuit of its objectives. An entity that can adapt to any environmental condition and act intelligently in every one does not exist.  Human intelligence is limited to specific scenarios and shock and confusion result when overburdened and super-intelligence is, as of this writing, conjecture without empirical evidence or theoretical proof. Notice four things in this definition. Optimality is not required.  Only better than random behavior is required. Although the entity tested for intelligence may interface with its environment only through data sets and test metrics, these are its environment. Time is necessarily involved.  In a simple case, an artificial network exhibits intelligence only in its ability to exhibit behavior that was previously learned to be sufficient.  Such can only retain intelligence by adjusting its training or in an environment where adaptation to new patterns is not required. Cognition is not required but cognition certainly augments the range of objectives that can be reliably pursued and the ability of the entity to detect danger and more proactively avert loss.",57.8027904,,,,,,,,
7555,How do I keep track of already visited states in breadth-first search?,ai-design,"Dennis Soemers' answer is correct: you should use a HashSet or a similar structure to keep track of visited states in BFS Graph Search. However, it doesn't quite answer your question. You're right, that in the worst case, BFS will then require you to store 16! nodes. Even though the insertion and check times in the set will be O(1), you'll still need an absurd amount of memory. To fix this, don't use BFS . It's intractable for all but the simplest of problems, because it requires both time and memory that are exponential in the distance to the nearest goal state. A much more memory-efficient algorithm is iterative deepening . It has all the desirable properties of BFS, but uses only O(n) memory, where n is the number of moves to reach the nearest solution. It might still take a while, but you'll hit memory limits long before CPU-related limits. Better still, develop a domain specific heuristic, and use A* search . This should require you to examine only a very small number of nodes, and allow the search to complete in something much closer to linear time.",57.54140093,"You can use a set (in the mathematical sense of the word, i.e. a collection that cannot contain duplicates) to store states that you have already seen. The operations you'll need to be able to perform on this are: inserting elements testing if elements are already in there Pretty much every programming language should already have support for a data structure that can perform both of these operations in constant ($O(1)$) time. For example: set in Python HashSet in Java At first glance, it may seem like adding all the states you ever see to a set like this will be expensive memory-wise, but it is not too bad in comparison to the memory you already need for your frontier; if your branching factor is $b$, your frontier will grow by $b - 1$ elements per node that you visit (remove $1$ node from frontier to ""visit"" it, add $b$ new successors/children), whereas your set will only grow by $1$ extra node per visited node. In pseudocode, such a set (let's name it closed_set , to be consistent with the pseudocode on wikipedia could be used in a Breadth-First Search as follows: frontier = First-In-First-Out Queue
frontier.add(initial_state)

closed_set = set()

while frontier not empty:
    current = frontier.remove_next()

    if current == goal_state:
        return something

    for each child in current.generate_children()
        if child not in closed_set:    // This operation should be supported in O(1) time regardless of closed_set's current size
            frontier.add(child)

    closed_set.add(current)    // this should also run in O(1) time (some variations of this pseudocode might work too, and be more or less efficient depending on the situation; for example, you could also take the closed_set to contain all nodes of which you have already added children to the frontier, and then entirely avoid the generate_children() call if current is already in the closed_set .) What I described above would be the standard way to handle this problem. Intuitively, I suspect a different ""solution"" could be to always randomize the order of a new list of successor states before adding them to the frontier. This way, you do not avoid the problem of occasionally adding states that you've already previousl expanded to the frontier, but I do think it should significantly reduce the risk of getting stuck in infinite cycles. Be careful : I do not know of any formal analysis of this solution that proves that it always avoids infinite cycles though. If I try to ""run"" this through my head, intuitively, I suspect it should kind of work, and it does not require any extra memory. There may be edge cases that I'm not thinking of right now though, so it also simply might not work, the standard solution described above will be a safer bet (at the cost of more memory).",56.18661193,"While the answers given are generally true, a BFS in the 15-puzzle is not only quite feasible, it was done in 2005! The paper that describes the approach can be found here: http://www.aaai.org/Papers/AAAI/2005/AAAI05-219.pdf A few key points: In order to do this, external memory was required - that is the BFS used the hard drive for storage instead of RAM. There are actually only 15!/2 states, since the state space has two mutually unreachable components. This works in the sliding tile puzzle because the state spaces grows really slowly from level to level. This means that the total memory required for any level is far smaller than the full size of the state space. (This contrasts with a state space like Rubik's Cube, where the state space grows much more quickly.) Because the sliding-tile puzzle is undirected, you only have to worry about duplicates in the current or previous layer. In a directed space you may generate duplicates in any previous layer of the search which makes things much more complicated. In the original work by Korf (linked above) they didn't actually store the result of the search - the search just computed how many states were at each level. If you want to store the first results you need something like WMBFS ( http://www.cs.du.edu/~sturtevant/papers/bfs_min_write.pdf ) There are three primary approaches to comparing states from the previous layers when states are stored on disk. The first is sorting-based. If you sort two files of successors, you can scan them in linear order to find duplicates. The second is hash-based. If you use a hash function to group successors into files, you can load files which are smaller than the full state space to check for duplicates. (Note that there are two hash functions here -- one to send a state to a file, and one to differentiate states within that file.) The third is structured duplicate detection. This is a form of hash-based detection, but it is done in a way that duplicates can be checked immediately when they are generated instead of after they have all been generated. There is a lot more to be said here, but the paper(s) above give a lot more details.",53.32808353,"Ironically the answer is ""use whatever system you want.""  A hashSet is a good idea.  However, it turns out that your concerns over memory usage are unfounded.  BFS is so bad at these sorts of problems, that it resolves this issue for you. Consider that your BFS requires you to keep a stack of unprocessed states.  As you progress into the puzzle, the states you deal with become more and more different, so you're likely to see that each ply of your BFS multiplies the number of states to look at by roughly 3. This means that, when you're processing the last ply of your BFS, you have to have at least 16!/3 states in memory.  Whatever approach you used to make sure that fit in memory will be sufficient to ensure your previously-visited list fits in memory as well. As others have pointed out, this is not the best algorithm to use.  Use an algorithm which is a better fit for the problem.",54.86857623,"Approaches to the Game It is true that the board has $16!$ possible states.  It is also true that using a hash set is what students learn in a first year algorithms courses to avoid redundancy and endless looping when searching a graph that may contain graph cycles. However, those trivial facts are not pertinent if the goal is to complete the puzzle in the fewest computing cycles. Breadth first search isn't a practical way to complete an orthogonal move puzzle.  The very high cost of a breadth first search would only be necessary if number of moves is of paramount importance for some reason. Sub-sequence Descent Most of the vertices representing states will never be visited, and each state that is visited can have between two and four outgoing edges.  Each block has an initial position and a final position and the board is symmetric.  The greatest freedom of choice exists when the open space is one of the four middle positions.  The least is when the open space is one of the four corner positions. A reasonable disparity (error) function is simply the sum of all x disparities plus the sum of all y disparities and a number heuristically representing which of the three levels of freedom of movement exists because of the resulting placement of the open space (middle, edge, corner). Although blocks may temporarily move away from their destinations to support a strategy toward completion requiring a sequence of moves, there is rarely a case where such a strategy exceeds eight moves, generating, on the average, 5,184 permutations for which the final states can be compared using the disparity function above. If the empty space and positions of block 1 through 15 are encoded as an array of nibbles, only addition, subtraction, and bit-wise operations are needed, making the algorithm fast.  Repeating the eight move brute force strategies can be repeated until disparity falls to zero. Summary This algorithm cannot cycle because there is always at least one of the permutations of eight moves that decreases disparity, regardless of the initial state, with the exception of a starting state that is already complete.",58.51252903,,,,,,,,
7202,Why does training an SVM take so long? How can I speed it up?,machine-learning,"The most likely explanation is that you're using too many training examples for your SVM implementation. SVMs are based around a kernel function . Most implementations explicitly store this as an NxN matrix of distances between the training points to avoid computing entries over and over again. In your case, with 75% of 700,000 examples, this matrix will require approximately 250GB of RAM to store, which is more than you're likely to have in consumer hardware. If your SVM implementation can avoid caching the values, you might get a speedup that way, or you might not (you'll waste a lot of time recomputing them). A much better way to deal with this is to just not use all of the data, since most of it will be redundant from the SVM's perspective (it only benefits from having more data near the decision boundaries). A good starting place would be to randomly discard 90% of the training data, and see what performance looks like.",54.08885208,"I think you should use a linear kernel, 'cause training SVM with a linear kernel is faster than with another kernel, especially for text classification. Good luck https://www.svm-tutorial.com/2014/10/svm-linear-kernel-good-text-classification/",55.32533886,"To quickly train the SVM , you can try to Use Linear SVM or Use scaled data. sources: https://www.researchgate.net/publication/2926909_A_Practical_Guide_to_Support_Vector_Classification_Chih-Wei_Hsu_Chih-Chung_Chang_and_Chih-Jen_Lin",55.1240126,"Because you use various combinations of features whose dimensionality is between 1 and 14 features, You might try to use Linear SVM (linear Kernels) would be good for your problem. You could try LIBLINEAR library but the Data should be linearly separable, otherwise test accuracy would be very low.",51.79598009,"You can speed up the training time by doing several steps: scale the values of your features use only a limited number of features because this will affect the training time; i.e. when you use 14 features, it means your model has 14 dimensions and it makes computation more complex and take much time. choose a proper kernel, linear SVM kernel usually give the fastest result",57.13221302,"The non-linear kernel SVMs can be slow if you have too many training samples. This is due to the fact that the algorithm creates an NxN matrix as @John Doucette answered. Now there are a few ways to speed up the non-linear kernel SVMs: Use the SGDClassifier instead and provide proper parameters for loss, penalty etc. to make it behave like an SVM. The optimisation process is different than libsvm though. Use a kernel approximator like Nystroem Since you are yourself trying out feature combinations, a Linear SVM can also be good and fast :)",56.41188231,"SVM scales rather badly with the number of training samples - from $O(n^2)$ to $O(n^3)$ as told in this answer https://stackoverflow.com/questions/16585465/training-complexity-of-linear-svm . The vanilla approach requires inversion of $n \times n$ matrix, which is $O(n^3)$ operations in general. As suggested in the other answers, the most apparent way to reduce the computational and storage complexity is the reduction of number of training samples. I am even surprised that all this data fits into the memory.",54.09475169,,,,
7021,Is the smartest robot more clever than the stupidest human?,robots,"The "" baseline humans"" you describe have been historically described in the media industry as "" the lowest common denominator "" (LCD). The LCD is the broadest possible audience for content, traditionally for network television shows .  (Before the age of cable, there were only 3 to 4 networks and all video content was broadcast over the airwaves--no way to specifically target audience segments so content had to appeal to the LCD.) Captcha Because captchas have to be solvable by the LCD, but trick bots. As long as captcha are viable, by definition they will always be something that baseline humans can do better than AI.",51.73601351,"First Question To treat this question in a scientific way, because I think it is a reasonable enough question that draws on the realities of postmodern culture in post industrialized societies to be treated scientifically, we should define some things. The most difficult is intelligence, which is the realm in which smartness, cleverness, and stupidity reside. Let's go through the list. Most humans are not good at chess, but humans invented the game. Most humans can't write symphonies, but humans invented them. Most humans don't read novels, but digital computers can't write them yet and can't learn ethical balance through the reading of them like humans can. Most humans are not Olympic level athletes, yet humans developed Olympics and robots are not yet Olympiads. Most humans (whether or hot they are good at logical reasoning) don't employ it much other than to, ""Get by."" The question is fine until it devolves into the dismissal of the intelligence that people apply to their method of earning income, which for many if not most people requires more than just getting up, commuting in, following some simple instructions, and going to sleep mildly inebriated.  Let's replace this last part with this. Most humans do not work with the intention of optimizing quality of the product or service by measuring their own quality and seeking educational resources to improve the velocity, reliability, or accuracy of their work output (unless programs are instituted to incentivize these things in the workplace.) If we define intelligence as the union of these things, for simplicity's sake, we have this (which is subject to change as AI develops). Playing chess: AI wins Designing games: Humans win Writing symphonies: Humans win Writing novels: Humans win Absorbing ethics from stories: Humans win Olympic gold: Humans win Logical consistency: AI wins We must, to interpret the above list correctly concede two things: Machines may have the ability to do something according to specific quality standards but not be configured, trained, or connected appropriately to prevail. Humans may have the ability to do something according to specific quality standards but not be educated, trained, or be properly motivated to prevail. Second Question What are some things that a clever robot can't do that a stupid human can?  These are a few, but they are of particular importance from certain perspectives. Love their family and friends Have compassion without reason Decide what to learn Hunt See a future danger approaching Entertain others Pray I would not dismiss these human propensities as irrelevant, even from a scientific perspective.  I would also not dismiss the possibility that these things are beyond the capabilities of silicon based entities.",52.84872259,"Some fields that humans are born with advantages: Fast and precise image processing ability. Even the stupidest human can tell the edge of two different objects precisely, e.g. which part of the image is a dog and which is a cat. Fuzzy learning ability. Humans don't need to see all kinds of cats to identify a cat. As long as we see some cats (real ones or pictures or videos) we can identify a cat easily. Reasoning. Current machine learning methods are mostly statistics-based high-dimensional model approximation. Instead of finding a solution or a pattern, I have never seen any AI entity can generate new ideas based on current facts. Abstraction. Now GANs and other AI techniques can create vivid drawings. Yet currently I cannot find any model that can do abstraction drawings. E.g. human can doodle a cat from a real pic of cats, while AI currently can't do that. There are more of these kinds that human born with skills in their genes because of millions of years of evolution. While I believe in the future we'll have better AI entities with better algos to defeat the advantage of humans eventually.",55.85014055,"Survival, Imagining, Moral Reasoning The thing that comes to mind is a new-born, when you said ""the stupidest human"", and it already has some basic “survival instincts”. It will avoid pain, consume food, and quickly learn to distinguish ""safe"" and ""dangerous"" conditions and people. We have computer programs that can learn chess and calculate the optimal move in a split second, but isn't playing chess is a bit pointless. Merely being able to play a board game is of little value from a survival perspective, industrial perspective, or economic perspective. There are programs that can do things that are very helpful for the modern world, but as far as I know, they just don't have survival instincts. A self-learning robot; left in a forest with all the tools it needs to generate power for, build duplicates of, maintain and defend its self; probably wouldn't be able to learn how to do so in time to ensure its survival. Our current self learning programs would need to be able to identify when it has succeeded or failed to improve its survival odds. A child of two may learn fast enough to survive if the conditions are not too severe and non-toxic food and some form of shelter is nearby. A financially poor, marginally educated person with lower than average aptitude working at a farm or factory might not be able to play chess well, but they would definitely be able to tell if someone is murdering someone else, and know to flee and seek the authorities. A robot that can play chess would not. Furthermore, humans can continue to learn when separated from the problem by thinking about the problem. The ability to construct arbitrary models and run thought experiments is currently unique to humans. That said, I do hope that we will soon have programs that well replicate the human mind, and demonstrate some of the aspects of what we call consciousness.",55.6705788,"I do not know the precise definition of intelligence, but from lots of people I have interacted with, they regard people as intelligent on a particular field , if and only if: They are able to take split second correct decisions in a situation in that particular field. Let us see where AI have succeeded in this case: Elon Musk’s Dota 2 AI beats the professionals at their own game AlphaZero AI beats champion chess program after teaching itself in four hours These are the few famous cases. If we examine carefully these cases we see that computers are outperforming humans only due to: Huge memory available. Fast memory access. Due to high processor speeds, split second correct decisions (although algorithm for correct decisions are developed by humans). So AI's are actually workhorses, working without fatigue and without any limitations. Human brains do not excel in the  field of decision making or speed. Here is a comparison of What makes animal brain so special? Human brains excel at creativity. We can learn how to make symphonies. Can an AI do the same? Possibly with correct programming. Much of our intelligence comes from its distributed nature. We learn from other peoples mistakes, we improve it. Large number of humans combined with record keeping has made this possible. Although scientists like Tesla, Einstein, Newton, Feynman discovered Calculus on their own, think of the possibilities of new inventions had they been made aware that Calculus already exited and a lot has been done to develop it? Check this: Swarm intelligence vs Normal Human Intelligence. So our intelligence and experience comes from the huge source of information rather than huge source of personal resources. As of now we can think of abstract concepts which an AI cannot (i.e. we can create new things, not new artworks or music by mixing things up as an AI does, but a new thing completely). For example, it has been seen  if you keep many deaf babies together and isolated they develop their own form of sign language, completely unique. Points to note here are: They were completely isolated. They worked as a group to develop the sign language. So although machines might be performing well due to their algorithmic complexity and immense power they still have some catching up to do to be compared to even stupidest humans . Main problem is we do not yet know the capacity of a brain. Some people can perform exceptional feats with their brain when the need arises . Some one did this during WW2 to find his family: Grandmaster plays 48 games at once, blindfolded while riding exercise bike . But how is this suddenly possible? No one knows until we have uncovered our own mind fully.",52.09930526,,,,,,,,
6997,What's a good resource for getting familiar with reinforcement learning?,reinforcement-learning,"To the good answers here, I would add A brief overview of RL : Most essential concepts in one place. Another brief overview , in presentation format. Ben Recht's An outsider's tour of RL is pretty comprehensive and accessible. The Bellman equations : central to the whole RL theory. Policy gradients explained by Andrej Karpathy (mentioned in other answers as ""pong from pixels"", this is the link). These barely scratch the surface of RL, but they should get you started.",50.47663825,"Before that ask yourself if you really want to learn about ""reinforcement learning."" Although there is much hype about reinforcement learning, the real-world applicability of reinforcement learning is almost non-existent. Most of the online courses teach you a very little about machine learning, so it is much better to get thorough with it, rather than proceeding towards reinforcement learning. Learning reinforcement learning is somewhat different from learning about unsupervised/supervised learning techniques. Having said that, the fastest way to get a good grasp of reinforcement learning is as follow: Read Andrej Karpathy's blog post ""Pong from Pixels."" Watch Deep RL Bootcamp lectures . To understand the math behind these techniques, refer to Sutton and Barto's Reinforcement Learning: An Introduction . Read relevant papers (game-playing etc.). P.S: Make sure that you are thorough with basics of neural networks, as most of the current papers in RL involve using DNNs in some or the other way as approximators.",63.49088067,"There's a Youtube playlist (in the DeepMind channel ) whose title is Introduction to reinforcement learning , which is a course (of 10 lessons) on reinforcement learning by David Silver . A person who followed and finished the course wrote (as a Youtube comment): Excellent course. Well paced, enough examples to provide a good intuition, and taught by someone who's leading the field in applying RL to games.",57.97126452,I recently saw a course by Microsoft on edx. It is called 'Reinforcement Learning Explained'. Here is the link: https://www.edx.org/course/reinforcement-learning-explained-0 This is not quite comprehensive but at least gives a good starting point.,59.19096812,I would say this post is a must to read: https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html,56.17659506,,,,,,,,
6800,Is artificial intelligence vulnerable to hacking?,neural-networks,"AI is vulnerable from two security perspectives the way I see it: The classic method of exploiting outright programmatic errors to achieve some sort of code execution on the machine that is running the AI or to extract data. Trickery through the equivalent of AI optical illusions for the particular form of data that the system is designed to deal with. The first has to be mitigated in the same way as any other software. I'm uncertain if AI is any more vulnerable on this front than other software, I'd be inclined to think that the complexity maybe slightly heightens the risk. The second is probably best mitigated by both the careful refinement of the system as noted in some of the other answers, but also by making the system more context-sensitive; many adversarial techniques rely on the input being assessed in a vacuum.",53.6607211,"Programmer vs Programmer It's a ""infinity war"": Programmers vs Programmers. All thing can be hackable. Prevention is linked to the level of knowledge of the professional in charge of security and programmers in application security. eg There are several ways to identify a user trying to mess up the metrics generated by Sentiment Analysis, but there are ways to circumvent those steps as well. It's a pretty boring fight. Agent vs Agent An interesting point that @DukeZhou raised is the evolution of this war, involving two artificial intelligence (agents). In that case, the battle is one of the most knowledgeable. Which is the best-trained model, you know? However, to achieve perfection in the issue of vulnerability, artificial intelligence or artificial super intelligence surpass the ability to circumvent the human. It is as if the knowledge of all hacks to this day already existed in the mind of this agent and he began to develop new ways of circumventing his own system and developing protection. Complex, right? I believe it's hard to have an AI who thinks: ""Will the human going to use a photo instead of putting his face to be identified?"" How we can prevent it Always having a human supervising the machine, and yet it will not be 100% effective. This disregarding the possibility that an agent can improve his own model alone. Conclusion So I think the scenario works this way: a programmer tries to circumvent the validations of an AI and the IA developer acquiring knowledge through logs and tests tries to build a smarter and safer model trying to reduce the chances of failure.",53.15559529,"How we can prevent it? There are several works about AI verification. Automatic verifiers can prove the robustness properties of neural networks. It means that if the input X of the NN is perturbed not more that on a given limit ε (in some metric, e.g. L2), then the NN gives the same answer on it. Such verifiers are done by: Stanford: https://arxiv.org/pdf/1702.01135.pdf ETHZ: https://www.sri.inf.ethz.ch/papers/sp2018.pdf Google: https://arxiv.org/pdf/1803.06567.pdf , https://arxiv.org/pdf/1805.10265.pdf Bosch: https://arxiv.org/pdf/1805.10265.pdf This approach may help to check robustness properties of neural networks. The next step is to construct such a neural network, that has required robustness. Some of above papers contain also methods of how to do that. There are different techniques to improve the robustness of neural networks: adversarial training (see e.g. A. Kurakin et al., ICLR 2017 ) defensive distillation (see e.g. N. Papernot et al., SSP 2016 ) MMSTV defence ( Maudry et al., ICLR 2018 ). At least the last one can provably make NN more robust. More literature can be found here .",50.28010917,"I believe it is, no system is safe, however I am not sure if I can still say this after 20-30 years of AI development/evolution. Anyways, there are articles that showed humans fooling AI (Computer Vision). https://www.theverge.com/2018/1/3/16844842/ai-computer-vision-trick-adversarial-patches-google https://spectrum.ieee.org/cars-that-think/transportation/sensors/slight-street-sign-modifications-can-fool-machine-learning-algorithms",50.23177758,"Is Artificial Intelligence Vulnerable to Hacking? Invert your question for a moment and think: What would make AI at less of a risk of hacking compared to any other
  kind of software? At the end of the day, software is software and there will always be bugs and security issues. AIs are at risk to all the problems non-AI software is at risk to, being AI doesn't grant it some kind of immunity. As for AI-specific tampering, AI is at risk to being fed false information. Unlike most programs, AI's functionality is determined by the data it consumes. For a real world example, a few years ago Microsoft created an AI chatbot called Tay. It took the people of Twitter less than 24 hours to teach it to say ""We're going to build a wall, and mexico is going to pay for it"": (Image taken from the Verge article linked below, I claim no credit for it.) And that's just the tip of the iceberg. Some articles about Tay: BBC The Verge Now imagine that wasn't a chat bot, imagine that was an important piece of AI from a future where AI are in charge of things like not killing the occupants of a car (i.e. a self-driving car) or not killing a patient on the operating table (i.e. some kind of medical assistance equipment). Granted, one would hope such AIs would be better secured against such threats, but supposing someone did find a way to feed such an AI masses of false information without being noticed (after all, the best hackers leave no trace), that genuinely could mean the difference between life and death. Using the example of a self-driving car, imagine if false data could make the car think it needed to do an emergency stop when on a motorway. One of the applications for medical AI is life-or-death decisions in the ER, imagine if a hacker could tip the scales in favour of the wrong decision. How we can prevent it? Ultimately the scale of the risk depends on how reliant humans become on AI. For example, if humans took the judgement of an AI and never questioned it, they'd be opening themselves up to all sorts of manipulation. However, if they use the AI's analysis as just one part of the puzzle, it would become easier to spot when an AI is wrong, be it through accidental or malicious means. In the case of a medical decision maker, don't just believe the AI, carry out physical tests and get some human opinions too. If two doctors disagree with the AI, throw out the AI's diagnosis. In the case of a car, one possibility is to have several redundant systems that must essentially 'vote' about what to do. If a car had multiple AIs on separate systems that must vote about which action to take, a hacker would have to take out more than just one AI to get control or cause a stalemate. Importantly, if the AIs ran on different systems, the same exploitation used on one couldn't be done on another, further increasing the hacker's workload.",56.16443884,"I concur with Akio that no system is completely safe, but the take away is AI systems are less prone to attacks when comparing with the old systems because of the ability to constantly improve. As time passes by more people will get in the field bringing new ideas and hardware will be improving so that they are ""strong AI.""",50.49573521,"There are many ways to hack an AI. When I was kid I figured how to beat a chess computer. I always followed the same pattern, once you learn you can exploit it. The worlds best hacker is a 4 year old that wants something he will try different things until he establishes pattern in his parents. Anyway, Get an Ai to learn the patterns of a AI and given a given combination you can figure the outcome. There is also just plain flaws or back door in code either on purpose or by chance. There is also the possibility the AI will hack itself. It is called misbehaving, remember the small child again... BTW simple way is to make AI always fails safe... something people forget.",50.77408571,,,,
6325,What kind of simulated environment is complex enough to develop a general AI?,agi,"I will skip all thematic about ""what is an AGI"", ""simulation game"", ... These topics have been discussed during decades and nowadays they are, in my opinion, a dead end. Thus, I can only answer with my personal experience: It is a basic theorem in computing that any number of dimensions, including temporal one, in a finite size space, can be reduced to 1D. However, in practical examples, the 1D representation becomes hard to analyze and visualize. It is more practical work with graphs, that can be seen as an intermediate between 1D and 2D. Graphs allows representation of all necessary facts and relations. By example, if we try to develop an AGI able to work in the area of mathematics, any expression (that humans we write in a 2D representation with rationals, subscripts, integrals, ...) can be represented as 1D (as an expression written in a program source) but this 1D must be parsed to reach the graph that can be analyzed or executed. Thus, the graph that results after the parsing of the expression is the most practical representation. Another example, if we want an agent that travels across a 3D world, this world can be seen as an empty space with objects that have some properties. Again, after the initial stage of scene analysis and object recognition (the equivalent to the parser in previous example), we reach a graph. Thus, to really work in the area of AGI, I suggest skip the problems of scene analysis, object recognition, speech recognition (Narrow AI), and work directly over the representative graphs.",51.91299489,"General AI can absolutely exist in a 2D world, just that a generalized AI (defined here as ""consistent strength across a set of problems"") in this context would still be quite distinct from an Artificial General Intelligence , defined as ""an algorithm that can perform any intellectual task that a human can."" Even there, the definition of AGI is fuzzy, because ""which human?""  (Human intelligence is a spectrum, where individuals possess different degrees of problem solving capability in different contexts.) Artificial Consciousness : Unfortunately, self-awareness / consciousness is a heavily metaphysical, issue, distinct from problem solving capability (intelligence). You definitely want to look a the "" Chinese Room "" and rebuttals. Probably worth looking at the holographic principle : ""a concept in physics whereby a space is considered as a hologram of n-1 dimensions.""  Certainly models and games can be structured in this way. Another place to explore is theories of emergence of superintelligence on infinite Conway's Game of Life .  (In a nutshell, my understanding is that once researchers figured out how to generate any number within the cellular automata, the possibility of emergent sentience given a gameboard of sufficient size is at least theoretically sound.)",52.70294726,"I think the most important thing is that it has to have time simulated in some way. Think self aware chatbot. Then to be ""self aware"" the environment could be data that is fed in through time that can be distinguished as being ""self"" and ""other"". By that I suppose I mean ""self"" is the part it influences directly and ""other"" is the part that is influenced indirectly or not at all. Other than that it probably can live inside pretty abstract environments. The reason time is so important is without it the cognitive algorithm is just solving a math problem.",52.8236716,"Though a good answer by @pasaba por aqui, I'd agree with @zooby that a graph might be too simplistic. If humans were in an environment where the options were drown or take 5000 unrelated steps to build a boat, we'd never have crossed any seas. 
I think any graph, if designed by hand, would not be complex enough to call the agent within as general AI. The world would need enough in-between states that it would no longer be best described as a graph, but at least a multidimensional space. I think there are 2 points you'd have to consider. What is ""simple"" and when would you recognise it as a ""general AI"". I don't find self aware AI satisfactory, as we can't measure anything called awareness; we can only see its state and its interaction with the environment. For 1. I'd pose that the world we live in is actually fairly simple. There are 4 forces of nature, a few conservation laws, and a bunch of particle types that explain most of everything. It's just that there are many of these particles and this has led to a rather complex world. Of course, this is expensive to simulate, but we could take some shortcuts. People 200 years ago wouldn't need all of quantum mechanics to explain the world. If we replaced protons, neutrons and the strong force with the atoms in the periodic table, we'd mostly be fine. Problem is we replaced 3 more general laws with 100 specific instances. For the simulated environment to be complex enough I think this trend must hold. We could replace trillions of particles governed by general laws with thousands of instances that have different properties when interacting with the agent, and I think more importantly, when interacting with each other. Which brings me to 2. I think we'd only truly be satisfied with the agent expressing general AI when it can purposefully interact with the environment in a way that would baffle us, while clearly benefiting from it (so not accidentally). Now that might be quite difficult or take a very long time, so a more relaxed condition would be to build tools that we'd expect it to build, thus showing mastery of its own environment. For example, evidence of boats have been found somewhere between 100k and 900k years ago, which is about the same time-scale when early humans developed . However although we'd consider ourselves intelligent, I'm not sure we'd consider a boat making agent to have general intelligence as it seems like a fairly simple invention. But I think we'd be satisfied after a few such inventions. So I think we'd need a Sim like world, that's actually a lot more complicated than the game. With 1000s of item types, many instances of each item and enough degrees of freedom to interact with everything. I also think we need something that looks familiar to acknowledge any agent as intelligent. So a 3D, complicated, minecraft-like world would be the simplest world in which we would recognise the emergence of general intelligence.",59.57918907,"Of the answers so far, the one from @DukeZhou were the most provocative. For instance, the reference to the Chinese Room critique brings up Searle's contention that some form of intentionality might need support in the artificial environment. This might imply the necessity of a value system or a pain-pleasure system, i.e. something where good consequences can be ""experienced"" or actively sought and bad consequences avoided. Or some potential for individual extinction (death or termination) might need to be recognized. The possibility of ""ego-death"" might need to have a high negative value.  That might imply an artificial world should include ""other minds"" or other agents, which the emerging or learning intelligent agent could observe (in some sense) and ""reflect on"", i.e recognize an intelligence like its own. In this sense the Cartesian syllogism ""I think therefore I am"" gets transmuted into: I (or rather me as an AI) see evidence of others thinking, and ""by gawd, 'I' can, too"". Those ""others"" could be either other learning systems (AGI's) or some contact with discrete inputs from humans mediated by the artificial environment. Wikipedia discussion of the ""reverse Turing test"" The mention of dimensionality should provoke a discussion of what would be the required depth of representation of a ""physics"" of the world external to the AI. Some representation of time and space would seem necessary, i.e., some dimensional substructure for progress to goal attainment. The Blocks World was an early toy problem whose solution provoked optimism in the 60's and 70's of last century that substantial progress was being made. I'm not aware of any effort to program in any pain or pleasure in the SHRDLU program of that era (no dropping blocks on the program's toes), but all the interesting science fiction representations of AI's have some recognition of ""physical"" adverse consequences in the ""real world"". Edit: I'm going to add a need for ""entities with features"" in this environment that could be ""perceived"" (by any of the ""others"" that are interacting with the AGI) as the data input to efforts at induction, identification, and inference about relationships. This creates a basis for a shared ""experience"".",52.32352833,,,,,,,,
6274,How can I deal with images of variable dimensions when doing image segmentation?,neural-networks,"There are 2 problems you might face. Your neural net (in this case convolutional neural net) cannot physically accept images of different resolutions. This is usually the case if one has fully-connected layers, however, if the network is fully-convolutional , then it should be able to accept images of any dimension. Fully-convolutional implies that it doesn't contain fully-connected layers, but only convolutional, max-pooling, and batch normalization layers all of which are invariant to the size of the image. Exactly this approach was proposed in this ground-breaking paper Fully Convolutional Networks for Semantic Segmentation . Keep in mind that their architecture and training methods might be slightly outdated by now. A similar approach was used in widely used U-Net: Convolutional Networks for Biomedical Image Segmentation , and many other architectures for object detection, pose estimation, and segmentation. Convolutional neural nets are not scale-invariant. For example, if one trains on the cats of the same size in pixels on images of a fixed resolution, the net would fail on images of smaller or larger sizes of cats. In order to overcome this problem, I know of two methods (might be more in the literature): multi-scale training of images of different sizes in fully-convolutional nets in order to make the model more robust to changes in scale; and having multi-scale architecture. A place to start is to look at these two notable papers: Feature Pyramid Networks for Object Detection and High-Resolution Representations for Labeling Pixels and Regions .",59.66813905,"You could also have a look at the paper Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (2015), where the SPP-net is proposed. SSP-net is based on the use of a ""spatial pyramid pooling"", which eliminates the requirement of having fixed-size inputs. In the abstract, the authors write Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224×224) input image. This requirement is ""artificial"" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "" spatial pyramid pooling "", to eliminate the above requirement. The new network structure, called SPP-net , can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-theart classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection . Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training
the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102× faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.",53.51995012,"Assuming you have a large dataset, and it's labeled pixel-wise, one hacky way to solve the issue is to preprocess the images to have same dimensions by inserting horizontal and vertical margins according to your desired dimensions, as for labels you add dummy extra output for the margin pixels so when calculating the loss you could mask the margins.",54.57919455,"As you want to perform image segmentation, you can use U-Net , which does not have fully connected layers, but it is a fully convolutional network , which makes it able to handle inputs of any dimension. You should read the linked papers for more info.",56.78284897,"Try resizing the image to the input dimensions of your neural network architecture(keeping it fixed to something like 128*128 in a standard 2D U-net architecture) using nearest neighbor interpolation technique. This is because if you resize your image using any other interpolation, it may result in tampering with the ground truth labels. This is particularly a problem in segmentation. You won't face such a problem when it comes to classification. Try the following: import cv2 
resized_image = cv2.resize(original_image, (new_width, new_height), 
                           interpolation=cv2.INTER_NEAREST)",54.77712862,,,,,,,,
6231,How to evaluate a NEAT neural network?,neural-networks,"Consider the execution order, 5 will have an invalid value because it hasn't been set form 3 yet. However the second time around it should have a value set. The invalid value should falloff after sufficient training. 0 -> 5
1 -> 5
5 -> 2
2 -> 3
3 -> 4
3 -> 5
RESTART
0 -> 5
1 -> 5",50,"Following is the pseudo code of the NEAT's network evaluation (converted from original source code), Until all the outputs are active
    for all non-sensor nodes
        activate node
        sum the input
    for all non-sensor and active nodes
        calculate the output Note that there is no recursion for feed forwarding concepts according to the original author.",53.7804864,"I can think of two possible ways of enforcing NEAT to create a feed forward network. One elegant one and one a little more cumbersome one; Only allow the ""add connection"" mutation to connect a node with another node that have a higher maximum distance from an input node. This should result in feed forward network, without much extra work. (Emergent properties are great!) Run as you did and create a fully connected network with NEAT and then prune it during a forward pass. After creating the network, run through it and remove connections that try to connect to a node already used in the forward pass (example 3->5). Alternatively just remove unused input connections to nodes during the forward pass. Given how NEAT mutates, it should not be possible that you remove a vital connection and cut the the network in two. This property of NEAT make sure your signal will always be able to reach the output, even if you remove those ""backwards pointing"" connections. I believe these should work, however i have not tested them. The original NEAT paper assumed a feed forward ANN, even though its implementation as described would result in a fully connected network. I think it was just an assumption of the paradigm they worked in. The confusion is fully understandable.",58.91301693,"In my implementation, I used a recursion system to calculate the output nodes. It works as follows: Assume a feed-forward network Only allow the ""add connection"" mutation to connect a node with another node >that have a higher maximum distance from an input node. This should result in >feed forward network, without much extra work. (Emergent properties are great!) Define function x , a recursive function that takes in a node number Define function y , a second function that takes in a node and returns all the connections with that node as an output In the recursive function: Call function y Call function x on function y outputs If the parameter for x is any input node, return the node value. This was the most elegant way of implementing I could think of, and its a lot simpler than explicitly tracking all of the connections.",50.87638663,"Hello chris i am also implementing this algorithm from scratch and the way i go about activating my mlp net is as follows:
I instantiate a list of nodes(actives), this is set to all input nodes initially, i then pass that to a function that initializes an empty list(next actives) and proceeds to loop through each set of conns for each node in the actives list, it adds each ""to"" node from those connections to the ""next actives) list unless its an output node or has already been activated, once all the ""actives"" list is looped through, i call the function again this time passing ""next actives"" as the actives list unless ""next actives"" is still empty after then i know the net has been fully activated. in this scenario the connection from node three to node five would be evaluated but it would not be added to the next actives list because it had already been activated, preventing an infinite loop.",50.36763771,"Okay, so instead of telling you to just not have recurrent connections, i'm actually going to tell you how to identify them. First thing you need to know is that recurrent connections are calculated after all other connections and neurons. So which connection is recurrent and which is not depends on the order of calculation of your NN.
Also, the first time when you put data into the system, we'll just assume that every connection is zero, otherwise some or all neurons can't be calculated. Lets say we have this neural network: Neural Network We devide this network into 3 layers (even though conceptually it has 4 layers): Input Layer  [1, 2]
Hidden Layer [5, 6, 7]
Output Layer [3, 4] First rule: All outputs from the output layer are recurrent connections . Second rule: All outputs from the input layer may be calculated first . We create two arrays. One containing the order of calculation of all neurons and connections and one containing all the (potentially) recurrent connections.
Right now these arrays look somewhat like this: Order of 
calculation: [1->5, 2->7 ]

Recurrent:   [ ] Now we begin by looking at the output layer. Can we calculate Neuron 3? No? Because 6 is missing. Can we calculate 6? No? Because 5 is missing. And so on. It looks somewhat like this: 3, 6, 5, 7 The problem is that we are now stuck in a loop. So we introduce a temporary array storing all the neuron id's that we already visited: [3, 6, 5, 7] Now we ask: Can we calculate 7? No, because 6 is missing. But we already visited 6... [3, 6, 5, 7,] <- 6 Third rule is: When you visit a neuron that has already been visited before, set the connection that you followed to this neuron as a recurrent connection .
Now your arrays look like this: Order of 
calculation: [1->5, 2->7 ]

Recurrent:   [6->7 ] Now you finish the process and in the end join the order of calculation array with your recurrent array so, that the recurrent array follows after the other array.
It looks somethat like this: [1->5, 2->7, 7, 7->4, 7->5, 5, 5->6, 6, 6->3, 3, 4, 6->7] Let's assume we have [x->y, y] Where x->y is the calculation of x*weight(x->y) And Where y is the calculation of Sum(of inputs to y). So in this case Sum(x->y) or just x->y. There are still some problems to solve here. For example: What if the only input of a neuron is a recurrent connection? But i guess you'll be able to solve this problem on your own...",52.05412249,,,,,,
6099,What activation function does the human brain use?,activation-functions,"The thing you were reading about is known as the action potential . It is a mechanism that governs how information flows within a neuron. It works like this: Neurons have an electrical potential, which is a voltage difference inside and outside the cell. They also have a default resting potential, and an activation potential. The neuron tends to move towards the resting potential if it is left alone, but incoming electric activations from dendrites can shift its electric potential. If the neuron reaches a certain threshold in electric potential (the activation potential), the entire neuron and its connecting axons goes through a chain reaction of ionic exchange inside/outside the cell that results in a ""wave of propagation"" through the axon. TL;DR: Once a neuron reaches a certain activation potential, it electrically discharges. But if the electric potential of the neuron doesn't reach that value then the neuron does not activate. Does the human brain use a specific activation function? IIRC neurons in different parts of the brain behave a bit differently, and the way this question is phrased sounds as if you are asking if there is a specific implementation of neuronal activation (as opposed to us modelling it). But in general behave relatively similar to each other (Neurons communicate with each other via neurochemicals, information propagates inside a neuron via a mechanism known as the action potential...) But the details and the differences they cause could be significant. There are various biological neuron models , but the Hodgkin-Huxley Model is the most notable. Also note that a general description of neurons don't give you a general description of neuronal dynamics a la cognition (understanding a tree doesn't give you complete understanding of a forest) But, the method of which information propagates inside a neuron is in general quite well understood as sodium / potassium ionic exchange. It (activation potential) sounds a lot like ReLU... It's only like ReLU in the sense that they require a threshold before anything happens. But ReLU can have variable output while neurons are all-or-nothing. Also ReLU (and other activation functions in general) are differentiable with respect to input space. This is very important for backprop. This is a ReLU function, with the X-axis being input value and Y-axis being output value. And this is the action potential with the X-axis being time, and Y being output value.",57.02416148,"The brains of mammals do not use an activation function.  Only machine learning designs based on the perceptron multiply the vector of outputs from a prior layer by a parameter matrix and pass the result statelessly into a mathematical function. Although the spike aggregation behavior has been partly modeled, and in far more detail than the 1952 Hodgkin and Huxley model, all the models require statefulness to functionally approximate biological neurons.  RNNs and their derivatives are an attempt to correct that shortcoming in the perceptron design. In addition to that distinction, although the signal strength summing into activation functions are parametrized, traditional ANNs, CNNs, and RNNs, are statically connected, something Intel claims they will correct with the Nirvana architecture in 2019 (which places into silicon that which we would call layer set up in Python or Java now. There are at least three important biological neuron features that make the activation mechanism more than a function of a scalar input producing a scalar output, which renders questionable any algebraic comparison. State held as neuroplastic (changing) connectivity, and this is not just how many neurons in a layer but also the direction of signal propagation in three dimensions and the topology of the network, which is organized, but chaotically so The state held within the cytoplasm and its organelles, which is only partly understood as of 2018 The fact that there is a temporal alignment factor, that pulses through a biological circuit may arrive via synapses in such a way that they aggregate but the peaks of the pulses are not coincident in time, so the activation probability is not as high as if they were temporally aligned. The decision about what activation function to use has largely been based on the analysis of convergence on a theoretical level combined with testing permutations to see which ones show the most desirable combinations of speed, accuracy, and reliability in ctheonvergence.  By reliability is meant that convergence on the global optimum (not some local minimum of the error function) is reached at all for the majority of input cases. This bifurcated research between the forks of practical machine learning and biological simulations and modeling.  The two branches may rejoin at some point with the emergence of spiking - Accuracy
 - Reliability (completes) networks.  The machine learning branch may borrow inspiration from the biological, such as the case of visual and auditory pathways in brains. They have parallels and relationships that may be exploited to aid in progress along both forks, but gaining knowledge by comparing the shapes of activation functions is confounded by the above three differences, especially the temporal alignment factor and the entire timing of brain circuits which cannot be modeled using iterations.  The brain is a true parallel computing architecture, not reliant on loops or even time sharing in the CPU and data buses.",57.72645013,"The answer is We do not know . Odds are, we will not know for quite a while. The reason for this is we cannot understand the ""code"" of the human brain, nor can we simply feed it values and get results. This limits us to measuring currents of the input and output on test subjects, and we have had few such test subjects that are human . Thus, we know almost nothing about the human brain, including the activation function.",60.32774509,"The human brain does have an activation function which is sigmoid like when considering the membrane potential at the Axon Initial Segment (AIS) as input and the rate of action potential as the output. There is a threshold membrane potential under witch no action potential is generated, and there is a maximum output rate the neuron may produce due to the hyper polarisation after the action potential spike. The biological neuron adjust the slope of the sigmoid and its mean (or the threshold if you like). The adjustment of the mean (and threshold) is quite fast as the AIS has a particular structure allowing to move the voltage sensitive sodium pores with an intracellular muscle (actin) and skeleton (microtubules). . From ""The Axon Initial Segment: An Updated Viewpoint"" Christophe Leterrier,
Journal of Neuroscience 28 February 2018, 38 (9) 2135-2145; https://doi.org/10.1523/JNEUROSCI.1922-17.2018 The sigmoid change is normalizing the output. When the membrane potential is artificially strongly stimulated, the average output rate is reduced, and when the stimulation is reduced, the average output rate is increased. This normalization makes sense with recursive neural network. The system must be maintained in a stable regime. Note that the sigmoid is not symmetric and that the biological signal processing is stochastic. The action potential spike is a dirac which thus doesn't carry any information in itself. It is the relative time of spikes that carries the information.",53.9833207,"My interpretation of the question was 'what activation function in an artificial neural network (ANN) is closest to that found in the brain?' Whilst I agree with the selected answer above, that a single neuron outputs a dirac, if you think of a neuron in an ANN as modelling the output firing rate, rather than the current output, then I believe ReLU might be closest? http://jackterwilliger.com/biological-neural-networks-part-i-spiking-neurons/",57.91463437,"The basic neuron firing is similar to a spiking function. However, there is both an immediate (enough positives charges leads to overcoming activation threshold for the axon hillock, leading to more jumping in), and feedback effect. not every neuron “fires” the same way, as transcriptors can produce more channels to increase influx of charge and strengthen an activation. Axon terminal also have vesicles with neurotransmitters. Only a few hundred neurotransmitters exist, ex dopamine, similar to an alphabet which can be used to create different words or effects in different areas of the brain. So while the basic concept can be analogous to spiking / Hebbian activation, in reality there is modulation of states which make it more complex.",54.3360696,,,,,,
6052,"Why are we asking ""How can we simulate the brain?""",philosophy,"Human intelligence is very general / broad in its scope. This is self-evident, and whatever AI ends up to be, we'd like it to be a general problem solver as well (cf. Simon and Newell). Taking liberal interpretations of your question... Why AI in a computer? Computers, to the extent that we can frame problems in general as a solvable computational problem, are also general problem solvers. Wether this is actually the case (can you compute meaning or feels?) is up for debate (cf. computational functionalism, hyper-computation), but it is part of the artificial intelligence project to make a claim on this statement. Why do we think a computational framework brings us any closer to an understanding of cognition / consciousness? Good question, and frankly there is no good answer to that asides from ""its the best thing we got"". TL;DR ""computational functionalism"", a lot of the literature in psychology and philosophy seems to converge towards an understanding of cognition as ""computational"" (as in information processing: the V1 stream in the brain processes ""early visual information"") and functional (goal directed grounded on ""meaning"", ex: ""i scratch itch because itchy"", as opposed to ""i am moving atoms""). However the two theories don't mesh together well (cf. Chinese Room Argument, and the many other arguments in a similar flavour) despite their independent successes in the theory of mind. Why this is the case nobody quite knows... Why not AI in something that isnt a computer? I don't know, but to the extent that our understanding of the world is grounded in math, then it being in a computer is sufficient anyways. Maybe there are other paradigms of understanding the world though. Fingers crossed 🙏 Why are we asking, “How can we simulate the brain?” Because its the best tentative understanding we have of an ""intelligent faculty"", though it should be noted that various methods in machine learning don't seem to be directly inspired by biological implementation  (kNN, statistical methods, as opposed to neural nets) Further reading: http://www.scaruffi.com/nature/mach01.html",61.27898591,"I don’t think AI is simulating the brain functions and not even close. Do you know how the nervous system work? How the neutrons transmit signals with action potential? Pathway analysis? Splicing junctions? AI is not about simulating the brain at all. We don’t simulate the biology pathway, we don’t simulate alternative splicing, we don’t have proteins in our models. Instead, AI is a field with tons of mathematics. You give some data and try to extract complicated non linear pattern.",59.5237817,"There are a number of reasons why a simulated brain might be better than creating a real brain. One reason is computers can live indefinitely (kind of). Brains may not be able to live forever and there might not be a way to transfer information from one brain to another. One of the principle advantages of a computer then is that it could have more experience than any brain could have in its lifetime. Another reason is that there are a lot of things we don't know about the brain. Even if we were able to replicate the brain we would have a hard time using it in the way that we want until we fully understand it. The simulated brain doesn't have this problem. We know exactly how artificial neural networks develop, and thus there is not as much that we don't understand. Those answers tell you why we might want a digital brain, but your question seems to also ask why study the digital brain over a biological brain? This seems to imply that we can't do both, but in fact there are many research groups doing work in areas that contribute to growing living brains (Max Planck Institute of Molecular Cell Biology and Genetic (MPI-CBG), the Medical Research Center in the UK, etc.).",58.87841884,"I think a worthwhile extension of this line of thought is ""why not both?"" I do not believe there is anything preventing approaching the problem from both sides at once. There is a great deal of research on both sides (biological research and computational research), but considerably less on the integration of the two (although there certainly is some, such as in the development of modern prosthetics that allow some degree of control). Given the adaptability of the human brain in terms of adjusting its own structure, the most expedient approach may be to consider what it would take to create a non-biological medium that biological neurons could interface with sufficiently to essentially ""program"" them in the same manner it does when repairing itself with biological neurons. Leave the hard work to the thing that already has the blueprint. Or in other words, the Ship of Theseus but with brain cells. Not that such a task would be anything close to approaching simple or easy, given our still lacking knowledge of neurological structures and the difficulties in getting a non-biological interface that is capable of the required sort of communications and adjustments that biological neurons can have performed and on a size scale that would be practical. I wish I could point to some research related to this, but I don't know about any specific research papers, although I know it's not a completely untouched upon subject.",51.62196111,"For what its worth (and having done a bit of study on this and being really interested in the topic):  the answer seems to go back to the beginnings of AI and even earlier (Turing's 1936 paper in which he introduces what's now called the Turing machine). John McCarthy's filer for the 1956 Dartmouth College summer workshop on ""Artificial Intelligence"" (which name introduced the term ""Artificial Intelligence"") in part says: ""The study [workshop] is to proceed on the basis of the conjecture 
  that every aspect of learning or any other feature of intelligence can 
  in principle be so precisely described that a machine can be made to 
  simulate it."" This references Turing's 1936 paper where a machine or natural system is described, and the description is run in a computer. To simulate is to quite precisely describe a system then run the description (transformed a bit – but the result is still a description) in a computer. The description is the program. The description needs to be precise, as indicated in the Church-Turing thesis. So the idea of simulation is core to the computational theory of what a digital computer can or might do. So it's also core to the computational theory of mind (the organic brain being a natural system), and hence to AI. That said, it's obviously a crazy idea to try to quite precisely describe the organic machine that is a human brain. I mean how many neurons? 100 billion. Quite precisely describe each and every single one of these, and each and every of the up to 10,000 connections that connect to each and every single neuron. Crazy with a capital C. And to suppose there are degrees of simulation of the brain, or that the mind is somehow a simplification of the brain, or that the description can be in higher level concepts, not neurological ones, is just to admit that the description is not quite precise. An adequate simulation of a brain would be terribly detailed. So why do we hear so much about AI trying to simulate the brain? Answer: AI has no other word to express what it does. In my view, AI ought to be trying to work out the data-processing principles of the organic brain, not trying to describe the causation of the brain. AI doesn't know the principles of perception or the principles of general knowledge. It's incredible to say this – seeing as both are so absolutely fundamental to human intelligence. But AI doesn't know the principles. It ought to be trying to work them out. Then – once discovered – to work out how these principles could be realised in a computer. You suggest that there's a binary choice between AI trying to get a computer to simulate the organic brain, and trying to grow organic brains in a dish. But there's actually a third option. Computer can do things other than simulate (i.e., other than compute). Maybe these other things might include embodying the principles of organic brains. There are two really big areas here: (1) what are the principles of intelligence? (2) what are the non-computational things computers can do? You ask why AI is concerned with the digital environment rather than, say, growing organic brains in a vat. But AI is basically an engineering project (building something with a designed causality) and even though AI knows only a little about the causality of what it's trying to build, the digital computer seems to be the only viable platform, at present, with enough individually addressable memory locations and processor speed to cope with semantic structures that would result from an adequate sensory interaction with the environment.",56.43957745,"Being the OP, I have already put some thought into this question. I think that computers are an attractive medium for simple AI because they are easily available and researchers are already familiar with them. In addition, science fiction writers of the last century were hopeful of the capabilities of computers and placed in our culture a dream of computer AI. But I also feel that perhaps other less explored fields would be better suited to the creation of strong AI. In particular, thinking about the nature of biology excites me. But as I understand it, we still know so little about how it biology works, let alone how to control it. But I feel this is where we should be focusing. Researchers know that current computing hardware has limitations. GPUs are better suited than CPUs. Some CPUs have new hardware designed for AI computations. I suspect that this realization of the inadequacy of conventional hardware will continue until our hardware is nearly identical to the biology we are trying to simulate. After all, what simulation could ever be better than what it is trying to simulate?",53.48784041,,,,,,
5838,Ideas on how to make a neural net learn how to split sequence into sub sequences,neural-networks,"The problem in the original question is akin to that of inducing a context-sensitive grammar (CSL), except that it is harder because a CSL is assumed to be composed of fixed-length subsequences. It is probably closer to the problem of inducing a Reber grammar , but that in turn seems like an overkill. LSTMs are known to be able to learn both CSL and Reber grammars. However, I doubt that this is what you really need because of the following comment: [...] given an entire book where there is NO spaces anywhere, only characters (including special characters, like commas), in what way can we make the network learn the 'word boudaries' of this book. This is called morphology induction, and it is a much harder problem than that of simple Reber grammar induction. Note that finding word boundaries is a special case of the problem of finding morpheme boundaries. There have been many attempts to solve this (also see this survey paper for more details and references). Most approaches developed seem to rely on statistical principles (like MDL ) and do not use neural networks (a counterexample using LSTMs). My intuition is that the extreme morphological variability across languages (ranging from Finno-Ugric languages with highly inflectional morphology to Sino-Tibetan languages with hardly any morphology at all) makes it hard to train neural networks in a language-agnostic way. However, you might have better luck if you focus on a single language. Hope that helps.",51.55889666,"I guess, supervised learning should work rather well: You'd feed the network with a fixed substring and it'd determine if the middle character is the first letter of a word, or a last one, or neither or both. So 2*n+1 inputs (fed e.g., with the string ""ingsits"") should output a 1 on the output determining if the middle letter (here: ""s"") is the first one of a word and a 0 on the output determining if it's the last one (taken from ""Thekingsitsthere""). Each input character should probably be 1 hot encoded. You'd probably want to use more context characters than in my example. OTOH you can use a simple MLP with no temporal complications. It'll never get perfect as it's impossible, but it get pretty close. Concerning unsupervised learning I'm skeptical...",50.2813713,"(NOTE: I think it will be easier to do it without ANNs...) But if you insist: convert the sequence into a fixed-size vectors. push trough a 2-5 1D-convolution layer with 1 neuron dense layer at the end (sigmoid activation) and another K-points detector for getting the sequence breakage points create a training set - to find the break-points (12, 23, 34 ...) in the sequence. train a detector with SGD to find these break-points. - loss functions: cross_entropy. Then, it should learn to find the breakage points, and based on this you can easily split the sequence.",56.18138977,"Another approach could be to predict the class of a sequence and not the break point. Assuming that each sequence is part of a class, you can use a LSTM. Inputing the multiple sequences (111100002222 ) and let predict the class for each sequence (c1,c1,c1,c1,c0,c0,c0,c0,c2,c2,c2,c2)",52.93141129,"How about this ? 1 - Learn all the basic building blocks of possible sub-sequence In our words sequence example, that would correspond to phonemes . ( I'm guessing that this step can even be done using unsupervised learning.) So in the following example : Hello Laurie , we would have learned 3 phonemes : HE , LO , RI . 2- Learn all subsequence as sequences of 'building blocks' Using a ClockWorkRNN with timesteps of interval +1 with, let's say, 10-15 timestep (groups), that is fed the next ' phoneme id ' in the sequence, we would have a space large enough to record most words (Obviously, the number of timesteps should be size of the biggest word). This is the subsequences memory RNN. Its sole purpose is to remember subsequences. Now, i'm really brainstorming here , taking a very wild guess, but what if : After training this RNN to a satisfying error rate, we check if the output of the RNN is very different to the next input for a couple of timesteps. In other word, we see if the neural network has been able to ' guess ' the next building block of the subsequence. If not, then its a point of interest , because there is not a lot of possibilities as of why this would happend : the only one I see is 1 - The RNN is currently receiving another word, thus making this timestep a sub-sequence ' break point ' Do you guys see any points that could prove this theory wrong ?",54.76621019,,,,,,,,
5769,"In a CNN, does each new filter have different weights for each input channel, or are the same weights of each filter used across input channels?",convolutional-neural-networks,"The following picture that you used in your question, very accurately describes what is happening. Remember that each element of the 3D filter (grey cube) is made up of a different value ( 3x3x3=27 values). So, three different 2D filters of size 3x3 can be concatenated to form this one 3D filter of size 3x3x3 . The 3x3x3 RGB chunk from the picture is multiplied elementwise by a 3D filter (shown as grey). In this case, the filter has 3x3x3=27 weights. When these weights are multiplied element-wise and then summed, it gives one value. So, is there a separate filter for each input channel? YES , there are as many 2D filters as the number of input channels in the image. However , it helps if you think that for input matrices with more than one channel, there is only one 3D filter (as shown in the image above). Then why is this called 2D convolution (if the filter is 3D and the input matrix is 3D)? This is 2D convolution because the strides of the filter are along the height and width dimensions only ( NOT depth) and therefore, the output produced by this convolution is also a 2D matrix. The number of movement directions of the filter determines the dimensions of convolution. Note: If you build up your understanding by visualizing a single 3D filter instead of multiple 2D filters (one for each layer), then you will have an easy time understanding advanced CNN architectures like Resnet, InceptionV3, etc.",64.16649963,"In a convolutional neural network, is there a unique filter for each input channel or are the same new filters used across all input channels? The former. In fact there is a separate kernel defined for each input channel / output channel combination. Typically for a CNN architecture, in a single filter as described by your number_of_filters parameter, there is one 2D kernel per input channel. There are input_channels * number_of_filters sets of weights, each of which describe a convolution kernel. So the diagrams showing one set of weights per input channel for each filter are correct. The first diagram also shows clearly that the results of applying those kernels are combined by summing them up and adding bias for each output channel. This can also be viewed as using a 3D convolution for each output channel, that happens to have the same depth as the input. Which is what your second diagram is showing, and also what many libraries will do internally. Mathematically this is the same result (provided the depths match exactly), although the layer type is typically labelled as ""Conv2D"" or similar. Similarly if your input type is inherently 3D, such as voxels or a video, then you might use a ""Conv3D"" layer, but internally it could well be implemented as a 4D convolution.",71.62790349,"I'm following up on the answers above with a concrete example in the hope to further clarify how the convolution works with respect to the input and output channels and the weights, respectively: Let the example be as follows (wrt to 1 convolutional layer): the input tensor is 9x9x5, i.e. 5 input channels, so input_channels=5 the filter/kernel size is 4x4 and the stride is 1 the output tensor is 6x6x56, i.e. 56 output channels, so output_channels=56 the padding type is 'VALID' (i.e. no padding) We note that: since the input has 5 channels, the filter dimension becomes 4x4x5, i.e. there are 5 separate, unique 2D filters of size 4x4 (i.e. each has 16 weights); in order to convolve over the input of size 9x9x5 the filter becomes 3D and must be of size 4x4x5 therefore: for each input channel, there exists a distinct 2D filter with 16 different weights each. In other words, the number of 2D filters matches the number of input channels since there are 56 output channels, there must be 56 3-dimensional filters W0, W1, ..., W55 of size 4x4x5 (cf. in the CS231 graphic there are 2 3-dimensional filters W0, W1 to account for the 2 output channels), where the 3rd dimension of size 5 represents the link to the 5 input channels (cf. in the CS231 graphic each 3D filter W0, W1 has the 3rd dimension 3, which matches the 3 input channels) therefore: the number of 3D filters equals the number of output channels That convolutional layer thus contains: 56 3-dimensional filters of size 4x4x5 (= 80 different weights each) to account for the 56 output channels where each has a value for the 3rd dimension of 5 to match the 5 input channels. In total there are number_of_filters=input_channel*output_channels=5*56=280 2D filters of size 4x4 (i.e. 280x16 different weights in total).",64.76578398,"I recommend chapter 2.2.1 of my masters thesis as an answer. To add to the remaining answers: Keras is your friend to understand what happens: from keras.models import Sequential
from keras.layers import Conv2D

model = Sequential()
model.add(Conv2D(32, input_shape=(28, 28, 3),
          kernel_size=(5, 5),
          padding='same',
          use_bias=False))
model.add(Conv2D(17, (3, 3), padding='same', use_bias=False))
model.add(Conv2D(13, (3, 3), padding='same', use_bias=False))
model.add(Conv2D(7, (3, 3), padding='same', use_bias=False))
model.compile(loss='categorical_crossentropy', optimizer='adam')

print(model.summary()) gives _________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 28, 28, 32)        2400      
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 28, 28, 17)        4896      
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 28, 28, 13)        1989      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 28, 28, 7)         819       
=================================================================
Total params: 10,104 Try to formulate your options. What would that mean for the parameters if something else would be the case? Hint: $2400 = 32 \cdot (3 \cdot 5 \cdot 5)$ This approach also helps you with other layer types, not only convolutional layers. Please also note that you are free to implement different solutions, that might have other numbers of parameters.",50.57344768,"One addition to Mohsin's answer: try the awesome CNN explainer . There you can explore CNN's graphically, which makes things ""click"" very fast. If you click on conv_1_1, you will see how three different kernels are used to calculate one convolution. That means you have one 2D filter per channel, that could together be interpreted as one 3D filter.",57.58953877,"Refer to ""Local Connectivity"" section in here and slide 7-18. ""Receptive Field"" hyperparameter of filter is defined by height & width only, as depth is fixed by preceding layer's depth. NOTE that ""The extent of the connectivity along the depth axis is always equal to the DEPTH of the input volume"" -or- DEPTH of activation map (in case of later layers). Intuitively, this must be due to the fact that image channels data are interleaved, not planar. This way, applying filter can be achieved simply by column vectors multiplication. NOTE that Convolutional Network learns all the filter parameters (including the depth dimension) and they are total ""h w input_layer_depth + 1 (bias)"".",55.85014756,"Just to make two details absolutely clear: Say you have $N$ 2D input channels going to $N$ 2D output channels.  The total number of 2D $3\times3$ filter weights is actually $N^2$ .  But how is the 3D convolution affected, i.e., if every input channel contributes one 2D layer to every output channel, then each output channel is composed initially of $N$ 2D layers, how are they combined? This tends to be glossed over in almost every publication I've seen, but the key concept is the $N^2$ 2D output channels are interleaved with each other to form the $N$ output channels, like shuffled card decks, before being summed together.  This is all logical when you realize that along the channel dimensions of a convolution (which is never illustrated), you actually have a fully connected layer!  Every input 2D channel, multiplied by a unique $3\times 3$ filter, yields a 2D output layer contribution to a single output channel.  Once combined, every output layer is a combination of every input layer $\times$ a unique filter.  It's an all to all contribution. The easiest way to convince yourself of this is to imagine what happens in other scenarios and see that the computation becomes degenerate - that is, if you don't interleave and recombine the results, then the different outputs wouldn't actually do anything - they'd have the same effect as a single output with combined weights.",61.22824218,"For anyone trying to understand how convolutions are calculated, here is a useful code snippet in Pytorch: batch_size = 1
height = 3 
width = 3
conv1_in_channels = 2
conv1_out_channels = 2
conv2_out_channels = 2
kernel_size = 2
# (N, C_in, H, W) is shape of all tensors. (batch_size, channels, height, width)
input = torch.Tensor(np.arange(0, batch_size*height*width*in_channels).reshape(batch_size, in_channels, height, width))
conv1 = nn.Conv2d(in_channels, conv1_out_channels, kernel_size, bias=False) # no bias to make calculations easier
# set the weights of the convolutions to make the convolutions easier to follow
nn.init.constant_(conv1.weight[0][0], 0.25)
nn.init.constant_(conv1.weight[0][1], 0.5)
nn.init.constant_(conv1.weight[1][0], 1) 
nn.init.constant_(conv1.weight[1][1], 2) 
out1 = conv1(input) # compute the convolution

conv2 = nn.Conv2d(conv1_out_channels, conv2_out_channels, kernel_size, bias=False)
nn.init.constant_(conv2.weight[0][0], 0.25)
nn.init.constant_(conv2.weight[0][1], 0.5)
nn.init.constant_(conv2.weight[1][0], 1) 
nn.init.constant_(conv2.weight[1][1], 2) 
out2 = conv2(out1) # compute the convolution

for tensor, name in zip([input, conv1.weight, out1, conv2.weight, out2], ['input', 'conv1', 'out1', 'conv2', 'out2']):
    print('{}: {}'.format(name, tensor))
    print('{} shape: {}'.format(name, tensor.shape)) Running this gives the following output: input: tensor([[[[ 0.,  1.,  2.],
          [ 3.,  4.,  5.],
          [ 6.,  7.,  8.]],

         [[ 9., 10., 11.],
          [12., 13., 14.],
          [15., 16., 17.]]]])
input shape: torch.Size([1, 2, 3, 3])
conv1: Parameter containing:
tensor([[[[0.2500, 0.2500],
          [0.2500, 0.2500]],

         [[0.5000, 0.5000],
          [0.5000, 0.5000]]],


        [[[1.0000, 1.0000],
          [1.0000, 1.0000]],

         [[2.0000, 2.0000],
          [2.0000, 2.0000]]]], requires_grad=True)
conv1 shape: torch.Size([2, 2, 2, 2])
out1: tensor([[[[ 24.,  27.],
          [ 33.,  36.]],

         [[ 96., 108.],
          [132., 144.]]]], grad_fn=<MkldnnConvolutionBackward>)
out1 shape: torch.Size([1, 2, 2, 2])
conv2: Parameter containing:
tensor([[[[0.2500, 0.2500],
          [0.2500, 0.2500]],

         [[0.5000, 0.5000],
          [0.5000, 0.5000]]],


        [[[1.0000, 1.0000],
          [1.0000, 1.0000]],

         [[2.0000, 2.0000],
          [2.0000, 2.0000]]]], requires_grad=True)
conv2 shape: torch.Size([2, 2, 2, 2])
out2: tensor([[[[ 270.]],

         [[1080.]]]], grad_fn=<MkldnnConvolutionBackward>)
out2 shape: torch.Size([1, 2, 1, 1]) Notice how the each channel of the convolution sums over all previous channels outputs.",51.76010826,"My understanding from this paper https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/TASLP2339736-proof.pdf was that the filters are used on each input channels (i.e input feature map in the paper) separately and the result is summed, as described in eq. 8. Here they use different filters for each channel, but you could totally use the same filter.
As to knowing the software implementation in a particular ML library such as Tensorflow or PyTorch, this requires inspection as suggested by @Mohsin Bukhari",62.25871918
5546,What is the difference between a convolutional neural network and a regular neural network?,neural-networks,"TLDR: The convolutional-neural-network is a subclass of neural-networks which have at least one convolution layer. They are great for capturing local information (e.g. neighbor pixels in an image or surrounding words in a text) as well as reducing the complexity of the model (faster training, needs fewer samples, reduces the chance of overfitting). See the following chart that depicts the several neural-networks architectures including deep-conventional-neural-networks: . Neural Networks (NN) , or more precisely Artificial Neural Networks (ANN) , is a class of Machine Learning algorithms that recently received a lot of attention (again!) due to the availability of Big Data and fast computing facilities (most of Deep Learning algorithms are essentially different variations of ANN). The class of ANN covers several architectures including Convolutional Neural Networks ( CNN ), Recurrent Neural Networks ( RNN ) eg LSTM and GRU , Autoencoders , and Deep Belief Networks . Therefore, CNN is just one kind of ANN. Generally speaking, an ANN is a collection of connected and tunable units (a.k.a. nodes, neurons, and artificial neurons) which can pass a signal (usually a real-valued number) from a unit to another. The number of (layers of) units, their types, and the way they are connected to each other is called the network architecture. A CNN, in specific, has one or more layers of convolution units. A convolution unit receives its input from multiple units from the previous layer which together create a proximity. Therefore, the input units (that form a small neighborhood) share their weights. The convolution units (as well as pooling units) are especially beneficial as: They reduce the number of units in the network (since they are many-to-one mappings ). This means, there are fewer parameters to learn which reduces the chance of overfitting as the model would be less complex than a fully connected network. They consider the context/shared information in the small neighborhoods. This feature is very important in many applications such as image, video, text, and speech processing/mining as the neighboring inputs (eg pixels, frames, words, etc) usually carry related information. Read the followings for more information about (deep) CNNs: ImageNet Classification with Deep Convolutional Neural Networks Going Deeper with Convolutions P.S. ANN is not ""a system based loosely on the human brain"" but rather a class of systems inspired by the neuron connections exist in animal brains.",57.01847823,"Convolutional Neural Networks (CNNs) are neural networks with architectural constraints to reduce computational complexity and ensure translational invariance (the network interprets input patterns the same regardless of translation— in terms of image recognition: a banana is a banana regardless of where it is in the image). Convolutional Neural Networks have three important architectural features. Local Connectivity: Neurons in one layer are only connected to neurons in the next layer that are spatially close to them. This design trims the vast majority of connections between consecutive layers, but keeps the ones that carry the most useful information. The assumption made here is that the input data has spatial significance, or in the example of computer vision, the relationship between two distant pixels is probably less significant than two close neighbors. Shared Weights: This is the concept that makes CNNs ""convolutional."" By forcing the neurons of one layer to share weights, the forward pass (feeding data through the network) becomes the equivalent of convolving a filter over the image to produce a new image. The training of CNNs then becomes the task of learning filters (deciding what features you should look for in the data.) Pooling and ReLU: CNNs have two non-linearities: pooling layers and ReLU functions. Pooling layers consider a block of input data and simply pass on the maximum value. Doing this reduces the size of the output and requires no added parameters to learn, so pooling layers are often used to regulate the size of the network and keep the system below a computational limit. The ReLU function takes one input, x, and returns the maximum of {0, x}. ReLU(x) = argmax(x, 0) . This introduces a similar effect to tanh(x) or sigmoid(x) as non-linearities to increase the model's expressive power. Further Reading As another answer mentioned, Stanford's CS 231n course covers this in detail. Check out this written guide and this lecture for more information. Blog posts like this one and this one are also very helpful. If you're still curious why CNNs have the structure that they do, I suggest reading the paper that introduced them though this is quite long, and perhaps checking out this discussion between Yann Lecun and Christopher Manning about innate priors (the assumptions we make when we design the architecture of a model).",56.42265371,"A convolutional neural network is one that has convolutional layers. If a general neural network is, loosely speaking, inspired by a human brain (which isn't very much accurate), the convolutional neural network is inspired by the visual cortex system, in humans and other animals (which is closer to the truth). As the name suggests, this layer applies the convolution with a learnable filter (a.k.a. kernel ), as a result the network learns the patterns in the images: edges, corners, arcs, then more complex figures. Convolutional neural network may contain other layers as well, commonly pooling and dense layers. Highly recommend CS231n tutorial on this matter: it's very detailed and contains a lot of very nice visualizations.",65.48534477,"The everyday definition of convolution comes from the Latin convolutus meaning 'to roll together'. Hence the meaning twisted or complicated. The mathematical definition comes from the same root, with the interpretation of taking a ""rolling average"". Hence in Machine Learning, a convolution is a sliding window across an input creating one averaged output for each stride the window takes. I.e. the values covered by the window are convoluted to create one convoluted output. This is best demonstrated with an a diagram: The convolution can be any function of the input, but some common ones are the max value, or the mean value. A convolutional neural network (CNN) is a neural network where one or more of the layers employs a convolution as the function applied to the output of the previous layer. If the window is greater than size 1x1, the output will be necessarily smaller than the input (unless the input is artificially 'padded' with zeros), and hence CNN's often have a distinctive 'funnel' shape:",55.79206737,It's a very simplified explantion. I am just talking about the core idea. A neural network is a combination of many layers. A neural network (Multiple Layer Perceptron: Regular neural network ): It does a linear combination (a mathematical operation) between the previous layer's output and the current layer's weights(vectors) and then it passes data to the next layer by passing through an activation function. The picture shows a unit of a layer. A neural network (Convolutional Neural Network):  It does convolution (In signal processing it's known as Correlation) (Its a mathematical operation) between the previous layer's output and the current layer's kernel ( a small matrix ) and then it passes data to the next layer by passing through an activation function. The picture shows a Convolution operation. Each layer may have many convolution operation,61.25174503,,,,,,,,
5493,What is the purpose of an activation function in neural networks?,neural-networks,"Almost all of the functionalities provided by the non-linear activation functions are given by other answers. Let me sum them up: First, what does non-linearity mean? It means something (a function in this case) which is not linear with respect to a given variable/variables i.e. $f(c1.x1 + c2.x2...cn.xn + b) != c1.f(x1) + c2.f(x2) ... cn.f(xn) + f(b).$ NOTE: There is some ambiguity about how one might define linearity. In polynomial equations we define linearity in somewhat a different way as compared to in vectors or some systems which take an input $x$ and give an output $f(x)$ . See the second answer . What does non-linearity mean in this context? It means that the Neural Network can successfully approximate functions (up-to a certain error $e$ decided by the user) which does not follow linearity or it can successfully predict the class of a function that is divided by a decision boundary that is not linear. Why does it help? I hardly think you can find any physical world phenomenon which follows linearity straightforwardly. So you need a non-linear function that can approximate the non-linear phenomenon. Also, a good intuition would be any decision boundary or a function is a linear combination of polynomial combinations of the input features (so ultimately non-linear). Purposes of activation function? In addition to introducing non-linearity, every activation function has its own features. Sigmoid $\frac{1} {(1 + e ^ {-(w1*x1...wn*xn + b)})}$ This is one of the most common activation function and is monotonically increasing everywhere. This is generally used at the final output node as it squashes values between 0 and 1 (if the output is required to be 0 or 1 ). Thus above 0.5 is considered 1 while below 0.5 as 0 , although a different threshold (not 0.5 ) maybe set. Its main advantage is that its differentiation is easy and uses already calculated values and supposedly horseshoe crab neurons have this activation function in their neurons. Tanh $\frac{e ^ {(w1*x1...wn*xn + b)} - e ^ {-(w1*x1...wn*xn + b)})}{(e ^ { (w1*x1...wn*xn + b)} + e ^ {-(w1*x1...wn*xn + b)}}$ This has an advantage over the sigmoid activation function as it tends to centre the output to 0 which has an effect of better learning on the subsequent layers (acts as a feature normaliser). A nice explanation here . Negative and positive output values maybe considered as 0 and 1 respectively. Used mostly in RNN's. Re-Lu activation function - This is another very common simple non-linear (linear in positive range and negative range exclusive of each other) activation function that has the advantage of removing the problem of vanishing gradient faced by the above two i.e. gradient tends to 0 as x tends to +infinity or -infinity. Here is an answer about Re-Lu's approximation power in-spite of its apparent linearity. ReLu's have a disadvantage of having dead neurons which result in larger NN's. Also, you can design your own activation functions depending on your specialized problem. You may have a quadratic activation function which will approximate quadratic functions much better. But then, you have to design a cost function that should be somewhat convex in nature, so that you can optimise it using first-order differentials and the NN actually converges to a decent result. This is the main reason why standard activation functions are used. But I believe with proper mathematical tools, there is a huge potential for new and eccentric activation functions. For example, say you are trying to approximate a single-variable quadratic function say $a.x^2 + c$ . This will be best approximated by a quadratic activation $w1.x^2 + b$ where $w1$ and $b$ will be the trainable parameters. But designing a loss function that follows the conventional first-order derivative method (gradient descent) can be quite tough for non-monotonically increasing function. For Mathematicians: In the sigmoid activation function $(1 / (1 + e ^ {-(w1*x1...wn*xn + b)})$ we see that $e ^ {-(w1*x1...wn*xn + b)}$ is always < 1 . By binomial expansion, or by reverse calculation of the infinite GP series we get $sigmoid(y)$ = $1 + y + y^2.....$ . Now in a NN $y = e ^ {-(w1*x1...wn*xn + b)}$ . Thus we get all the powers of $y$ which is equal to $e ^ {-(w1*x1...wn*xn + b)}$ thus each power of $y$ can be thought of as a multiplication of several decaying exponentials based on a feature $x$ , for eaxmple $y^2 = e^ {-2(w1x1)} * e^ {-2(w2x2)} * e^ {-2(w3x3)} *...... e^ {-2(b)}$ . Thus each feature has a say in the scaling of the graph of $y^2$ . Another way of thinking would be to expand the exponentials according to Taylor Series: $$e^{x}=1+\frac{x}{1 !}+\frac{x^{2}}{2 !}+\frac{x^{3}}{3 !}+\cdots$$ So we get a very complex combination, with all the possible polynomial combinations of input variables present. I believe if a Neural Network is structured correctly the NN can fine-tune these polynomial combinations by just modifying the connection weights and selecting polynomial terms maximum useful, and rejecting terms by subtracting the output of 2 nodes weighted properly. The $tanh$ activation can work in the same way since output of $|tanh| < 1$ . I am not sure how Re-Lu's work though, but due to its rigid structure and problem of dead neurons we require larger networks with ReLu's for a good approximation. But for a formal mathematical proof, one has to look at the Universal Approximation Theorem. A visual proof that neural nets can compute any function The Universal Approximation Theorem For Neural Networks- An Elegant Proof For non-mathematicians some better insights visit these links: Activation Functions by Andrew Ng - for  more formal and scientific answer How does neural network classifier classify from just drawing a decision plane? Differentiable activation function A visual proof that neural nets can compute any function",59.75658389,"If you only had linear layers in a neural network, all the layers would essentially collapse to one linear layer, and, therefore, a ""deep"" neural network architecture effectively wouldn't be deep anymore but just a linear classifier. $$y = f(W_1 W_2 W_3x) = f(Wx)$$ where $W$ corresponds to the matrix that represents the network weights and biases for one layer, and $f()$ to the activation function. Now, with the introduction of a non-linear activation unit after every linear transformation, this won't happen anymore. $$y = f_1( W_1 f_2( W_2f_3( W_3x)))$$ Each layer can now build up on the results of the preceding non-linear layer which essentially leads to a complex non-linear function that is able to approximate every possible function with the right weighting and enough depth/width.",55.72040493,"Let's first talk about linearity . Linearity means the map (a function), $f: V \rightarrow W$ , used is a linear map, that is, it satisfies the following two conditions $f(x + y) = f(x) + f(y), \; x, y \in V$ $f(c x) = cf(x), \; c \in \mathbb{R}$ You should be familiar with this definition if you have studied linear algebra in the past. However, it's more important to think of linearity in terms of linear separability of data, which means the data can be separated into different classes by drawing a line (or hyperplane, if more than two dimensions), which represents a linear decision boundary, through the data. If we cannot do that, then the data is not linearly separable. Often times, data from a more complex (and thus more relevant) problem setting is not linearly separable, so it is in our interest to model these. To model nonlinear decision boundaries of data, we can utilize a neural network that introduces non-linearity. Neural networks classify data that is not linearly separable by transforming data using some nonlinear function (or our activation function), so the resulting transformed points become linearly separable. Different activation functions are used for different problem setting contexts. You can read more about that in the book Deep Learning (Adaptive Computation and Machine Learning series) . For an example of non linearly separable data, see the XOR data set. Can you draw a single line to separate the two classes?",54.29923765,"Consider a very simple neural network, with just 2 layers, where the first has 2 neurons and the last 1 neuron, and the input size is 2. The inputs are $x_1$ and $x_1$ . The weights of the first layer are $w_{11}, w_{12}, w_{21}$ and $w_{22}$ . We do not have activations, so the outputs of the neurons in the first layer are \begin{align}
o_1 = w_{11}x_1 + w_{12}x_2 \\
o_2 = w_{21}x_1 + w_{22}x_2
\end{align} Let's calculate the output of the last layer with weights $z_1$ and $z_2$ $$out = z_1o_1 + z_2o_2$$ Just substitute $o_1$ and $o_2$ and you will get: $$out = z_1(w_{11}x_1 + w_{12}x_2) + z_2(w_{21}x_1 + w_{22}x_2)$$ or $$out = (z_1w_{11} + z_2 w_{21})x_1 + (z_2w_{22} + z_1w_{12})x_2$$ And look at this! If we create NN just with one layer with weights $z_1w_{11} + z_2 w_{21}$ and $z_2w_{22} + z_1w_{12}$ it will be equivalent to our 2 layers NN. The conclusion: without nonlinearity, the computational power of a multilayer NN is equal to 1-layer NN. Also, you can think of the sigmoid function as differentiable IF the statement that gives a probability. And adding new layers can create new, more complex combinations of IF statements. For example, the first layer combines features and gives probabilities that there are eyes, tail, and ears on the picture, the second combines new, more complex features from the last layer and gives probability that there is a cat. For more information: Hacker's guide to Neural Networks .",51.41138338,"First Degree Linear Polynomials Non-linearity is not the correct mathematical term.  Those that use it probably intend to refer to a first degree polynomial relationship between input and output, the kind of relationship that would be graphed as a straight line, a flat plane, or a higher degree surface with no curvature. To model relations more complex than y = a 1 x 1 + a 2 x 2 + ... + b , more than just those two terms of a Taylor series approximation is needed. Tune-able Functions with Non-zero Curvature Artificial networks such as the multi-layer perceptron and its variants are matrices of functions with non-zero curvature that, when taken collectively as a circuit, can be tuned with attenuation grids to approximate more complex functions of non-zero curvature.  These more complex functions generally have multiple inputs (independent variables). The attenuation grids are simply matrix-vector products, the matrix being the parameters that are tuned to create a circuit that approximates the more complex curved, multivariate function with simpler curved functions. Oriented with the multi-dimensional signal entering at the left and the result appearing on the right (left-to-right causality), as in the electrical engineering convention, the vertical columns are called layers of activations, mostly for historical reasons.  They are actually arrays of simple curved functions.  The most commonly used activations today are these. ReLU Leaky ReLU ELU Threshold (binary step) Logistic The identity function is sometimes used to pass through signals untouched for various structural convenience reasons. These are less used but were in vogue at one point or another.  They are still used but have lost popularity because they place additional overhead on back propagation computations and tend to lose in contests for speed and accuracy. Softmax Sigmoid TanH ArcTan The more complex of these can be parametrized and all of them can be perturbed with pseudo-random noise to improve reliability. Why Bother With All of That? Artificial networks are not necessary for tuning well developed classes of relationships between input and desired output.  For instance, these are easily optimized using well developed optimization techniques. Higher degree polynomials — Often directly solvable using techniques derived directly from linear algebra Periodic functions — Can be treated with Fourier methods Curve fitting — converges well using the Levenberg–Marquardt algorithm, a damped least-squares approach For these, approaches developed long before the advent of artificial networks can often arrive at an optimal solution with less computational overhead and more precision and reliability. Where artificial networks excel is in the acquisition of functions about which the practitioner is largely ignorant or the tuning of the parameters of known functions for which specific convergence methods have not yet been devised. Multi-layer perceptrons (ANNs) tune the parameters (attenuation matrix) during training.  Tuning is directed by gradient descent or one of its variants to produce a digital approximation of an analog circuit that models the unknown functions.  The gradient descent is driven by some criteria toward which circuit behavior is driven by comparing outputs with that criteria.  The criteria can be any of these. Matching labels (the desired output values corresponding to the training example inputs) The need to pass information through narrow signal paths and reconstruct from that limited information Another criteria inherent in the network Another criteria arising from a signal source from outside the network In Summary In summary, activation functions provide the building blocks that can be used repeatedly in two dimensions of the network structure so that, combined with an attenuation matrix to vary the weight of signaling from layer to layer, is known to be able to approximate an arbitrary and complex function. Deeper Network Excitement The post-millenial excitement about deeper networks is because the patterns in two distinct classes of complex inputs have been successfully identified and put into use within larger business, consumer, and scientific markets. Heterogeneous and semantically complex structures Media files and streams (images, video, audio)",53.38651862,,,,,,,,
5322,Can prior knowledge be encoded in deep neural networks?,neural-networks,"Neural nets incorporate prior knowledge. This can be done in two ways: the first (most frequent and more robust) is in data augmentation. For example in convolutional networks, if we know that the ""value"" (whatever that is, class/regression) of the object we are looking is rotational/translational invariant (our prior knowledge), then we augment the data with random rotations/shifts. The second is in the loss function with some additional term.",59.95669043,"Yes, we can do it in a deep learner. For example, suppose we have an input vector likes $(a, b)$ and from prior knowledge, we know $a^2 + b^2$ is important too. Hence, we can add this value to the vectors likes $(a, b, a^2 + b^2)$ . As another example, suppose date time is important in your data, but not encoded in the input vector. We can add this to the input vector as a third dimension. In summary, it depends on the structure of the prior knowledge, we can encode it into the input vector.",61.00643455,"To add to the Foivos's answer, Convolutional Neural Networks are shift-invariant.
Fukushima introduced this to his Neocognitron.
There is a trail to introduce scale-invariance to CNN. https://arxiv.org/abs/1411.6369 Also, CNN uses structural characteristic for the prior knowledge. And neural networks are locally smooth. It is not perfect, but neural networks are incorporating a lot of prior knowledge.",64.45462768,"It kinda depends on how exactly you define knowledge, and what you believe about what the weights in a trained NN model really represent. But to answer this question in the most straightforward possible way (hopefully without sounding glib), then yes, a NN can be pre-trained, and then you can take that model and apply additional training to it, so in a sense, it is using ""prior knowledge"". If, OTOH, knowledge means something a little different to you, and you're thinking about the kind of knowledge that's encoded in a semantic network, or a conceptual graph, or something of that nature, then I don't know - offhand - of any direct way to integrate that into an ANN. What you might be able to do is combine the NN with a different kind of reasoner that reasons of the semantic network / conceptual graph, and then integrate the results.  AFAIK, the best way to do that is an unsolved research problem.",57.37309983,"A simple example of this is token embeddings. If ""prior knowledge"" just means anything known prior to creation of the graph, then using pretrained vector embeddings meets this criteria. This is simply a way to provide a fixed method for projecting tokens into higher-dimensional space instead of training it at the same time as the rest of the model. Given that vector embeddings are somewhat interpretable and that the same embedding can be reused across tasks and models, I'd consider pretrained embeddings to be prior knowledge being incorporated. The embeddings could also technically be handcrafted, but I'm not aware of any work like that and am skeptical of its usefulness in deep models.",59.96658387,,,,,,,,
5169,Is it fair to compare AlphaGo with a Human player?,philosophy,"Is it fair to compare AlphaGo with a Human player? Depends on the purpose of the comparison. If we are comparing ability to win a game of Go, then yes. If we are comparing learning ability, then maybe . It depends on the task. AlphaGo and systems like it are capable of learning only in well-described limited domains. There may be an analogy with sensory learning (it might even be possible in theory to take a small piece of brain tissue and run an algorithm similar to AlphaGo's learning process on it). In general, the approach used by AlphaGo and other reinforcement learning successes is ""trial-and-error plus function approximation"". It seems analogous to perception and motor skills, such as object recognition or riding a bike, as opposed to reasoning skills and games as humans play them, which goes through many more cognitive and conscious layers that have no real analog in a RL system like AlphaGo. A human player plays limited games compared to a system that undergoes millions of iterations This is an advantage of a machine to learn this kind of task. It would equally apply in other simulated environments with simple rules. If your goal is to have the most skilled and optimal navigation of such a domain, the implication now is that you would not train a human expert through years of study, but to write the simulator and train an AlphaGo-like machine. This is no different a comparison than deciding cars and roads are better solutions to long distance travel for the general population than walking or horses and carts. It doesn't matter what underlies the advantage of one over the other, the assessment is cost/benefit, which resolves to a single comparable number. It would, however, be wrong to assess AlphaGo as a better general-purpose learning engine than a human. The fact that humans do not have to work fully through millions of simulations in full detail is important. It means that something about how humans learn is still not covered by learning machines. Some of these things are understood and being discussed - such as the ability to focus intuitively on important aspects of what to learn, the ability to reason about the environment, learning analogously or transfer learning from other domains.",64.6415692,Yes it is. If we ever compare computers to humans we should take into acount the fact that computers can work 24 hours a day every day and faster than humans. That is bigest advantage of computers over humans.,52.24087507,"If you read through the abstracts of Chess AI papers, it is often pointed out that humans ""search"" the Chess game tree much more efficiently than computers, which was why it was so hard to beat the top humans in Chess for so many years.  (The human efficiency may have to do with intuition and judgement, which are difficult to replicate. ""Confidence levels"" for AI evaluations is one method of addressing these issues, as is "" monte carlo "".  But it's also important to note that humans are far more limited in the depth and breadth of their ""searches"", which is why, now that we have the right algorithms, humans can no longer win.) Is it fair? Perhaps the more salient question is: Is it useful to compare AlphaGo to a human player? It most certainly is, because it tells us that we have is sometimes termed a ""strong-narrow AI"" that can outperform a human in a single task. Why AlphaGo beating Lee Sedol was a big deal is the complexity of Go, the intractability of the Go game tree, and the fact that computers were previously ineffective against high-level human Go players. This human vs. AI evaluation doesn't strictly fall under the ""Turing Test"" ( Imitation Game ), it does fall squarely under the maxim of Protagoras that ""Man is the measure of all things."" This is critical because intelligence is a spectrum, and gauging strength of intelligence, in the context of intractable problems (problems that cannot be fully solved due to their size) is a function of relative strength of two agents, whether human or AI. This relative assessment is all we have, and all we may ever have for certain sets of problems. The problem with humans is not that we're not clever, but that our minds have cognitive limitations.  So to tackle certain problems, intelligent machines are useful.",60.05101521,"There is no such thing as fairness when comparing. You define a measure for performance and then compare the values of the measure. One sensible measure for playing the game of GO is the 'Number of games won', regardless of any investment in the development of the system, computational or sample efficiency. AlphaGo is currently at the top by this measure. Another sensible measure could be 'Number of games won under a restriction on sample efficiency during training'. As others pointed out, such a measure could be much more favorable for humans.",52.55354052,"As a chess player and a AI/ML engineer, I can say yes, why not. I'm not sure why it isn't fair to compare anything if you give each side it's just due and do a 'fair comparison'. Obviously, what that encompasses is extremely subjective, but there are philosophical and logical measures of fairness. Now speaking on the comparison, AlphaZero and a Human's learning styles are much more similar than that of a Human's and Stockfish. This is mainly due to the fact that human's in some capacity use RL , mainly in the dopaminergic neural pathways. While human behavior can certainly be modeled as an alpha/beta tree-search, it is not anything like the way we make decisions. As far as the top humans, who cares? We we've been worse than computers for years.",58.77071363,,,,,,,,
5043,What are the connections between ethics and artificial intelligence?,philosophy,This is a good related read from Nature: There is a Blind Spot in AI Research Fears about the future impacts of Artificial Intelligence are distracting researchers from the real risks of deployed systems,53.19802024,"In a business context, there are issues surrounding the implementation, the implementors, the other employees, the business entity itself, and the customers. These stem from data used, risks inherent in an implementation, like unknown errors bugs or algorithms without human checks, behaviour change of impacted stakeholders, job losses, reputation impacts on the company, etc. There's a lot to think about AI and ethics. It could be seen as a combination of a broad set of topics including computer science, humanities, economics, and philosophy. I run a podcast on some of these issues http://machine-ethics.net/ let me know if there's anything you want to be discussed or someone you would like to hear.",53.41504053,"The connections between ethics and artificial intelligence can be divided into five major categories, and other categories may form over time. Correlations between ethics and artificial intelligence Existential impacts of artificial intelligence on the human experience Threats to current ethical social, economic, and legal standards arising from artificial intelligence research and application Uses of artificial intelligence to breach ethical standards without detection Uses of artificial intelligence to detection of breaches of ethical standards and assist in remedial action Since the most important in the long term is the most likely to be dismissed by those with normal perspectives about ethics and AI, the four will be addressed in reverse order Automatic Detection and Remedial Action The pattern recognition capabilities of existing AI systems and sub-systems is already employed to detect a variety of ethical breaches. Securities misconduct, including insider trading Breaches of anti-trust law, including conflicts of interest Employer misconduct, including inequality in hiring Tax evasion Misuse of non-profit funds Remedial actions may be the opening of a case with the automatic generation of a notice to those in potential breach. Smart Organized Crime Although much detail could be included here about detection avoidance in crime using AI, it may not be socially responsible to include such in a global public facing site. Threats to Economies and Individuals As with any high impact technology, disruption is a possibility.  This was true of fire, irrigation, the wheel, bronze smelting, gun powder, typesetting, steel-working, engines, textile automation, alternating current power distribution, aeronautics, petroleum refining, electronics and radio transmission, pharmacology, terrestrial nuclear reaction, and the Internet.  Genetic engineering and artificial intelligence are next in line. What ethical conventions will likely be impacted? Distribution of employment roles, the change of which may not match distribution of educational preparation Distribution of wealth, favoring prowess in highly automated business Mutual exclusivity of personal privacy and the use of technology Obscurity of totalitarian control (such that common citizens may be more like cogs in a machine than during industrialization) Changes in the balance of world power New forms of asymmetric war, such as cyber-war and autonomous combatants All of these have either direct or direct impact on the viability of business options and how business may be conducted. Existential impacts Some may consider dominion over the earth as an ethical grant to humanity.  Others may equate the soul with the cognitive and self-aware aspects of only one species on earth. We already have a world that is sufficiently disconnected on an existential plane that many consider their cats or dogs as more important than any human.  When people are more connected to their intelligent agents than their pets, family, and friends, that may qualify as an ethical impact.  Others may see it as a psychological impact. Realistically, it is ontological.  What is a human to think of her or his only purpose when it becomes questionable whether homo sapiens is simply a link between DNA based intelligence and some more capable species the reproduction of which has been decoupled from DNA coding. Replacement of jobs have caused changes in what families wish for their children.  What will be the impact when few job roles (or eventually none) exist where artificial employees don't exceed their human counterparts in effectiveness? If humans cannot adjust to the idea that the sole purpose of life has nothing to do with practical provision of water, food, shelter, clothing, and essential products and services, there may be systemic depression.  Conversely, leisure may become the reality for all humans, leaving ethics and the finite nature of DNA based life the only two concerns of humans. Correlations Between Ethics and AI This is the most unpalatable of categories to examine when the examiner is human.  It is possible that artificiality may be an ethics progenitor.  The limitation of humans as ethical beings is well documented.  It is possible that AI may be more ethical than its developers. Will a group of AI systems be able to arrive at method for distribution of power and a standard for global trade that is as good as or better than humans have been able to negotiate and then police each other in a way that leaves no possibility of undetected breach of treaty?",64.14260099,"I'd strongly recommend looking into Game Theory 's relationship and impact on AI. Prisoner's Dilemma is a good place to start, because optimality can have repercussions. With computing in general, optimization is a major goal.  For AI, optimal decision-making is what it's all about.  But sans humanity, this may prove to be problematic. (Apologies for the brevity--I'll be returning to elaborate since this is a subject of personal preoccupation--but I wanted to leave you with a few tidbits in the meantime. :)",50.52726848,There's an interesting new paper here: Algorithmic Decision Making in Financial Services This paper highlights the correlation between corporate ethics and interests with the normative issues arising in respect of algorithmic decision-making.,56.50823508,,,,,,,,
4987,Is there a limit to the increase of intelligence?,philosophy,"To have a ""maximum achievable intelligence"", first of course you have to define ""intelligence"" well enough to be able to rank things by intelligence. There is no widely-supported theory that is able to do so. You might like to look into AIXI as described by Marcus Hutter in a video lecture . It is an attempt to formalise intelligent agents that attempt to make optimal decisions mathematically. Here is another written introduction . Of course this is only one of many possible frameworks to describe intelligent agents. One interesting implication is that AIXI implies intelligence - in terms of ability to learn from and exploit an environment - is upper bounded. In principle there is a ceiling due to uncertainty about what the data that a rational agent possesses might infer. However, this ceiling refers to only specific abilities to extract actionable information from data that the agent has access to in order to solve decision problems. There is an open question about how much data can be collected, stored and processed by any entity, and this ability to acquire and retrieve relevant knowledge would be viewed by many as part of the ""intelligence score"" when comparing agents. There are theoretical limits to computation from physics , e.g. some are based on the fact that it fundamentally requires energy, the energy has a mass equivalence, and enough concentrated mass would form a black hole. This sets a high upper bound, and it is likely that real-world structural and design issues will set in way before this limit. However, combined with the above limits on decidability and practical access to data, it does seem there should be a ceiling.",55.24676103,"Absolutely, regardless of how you define ""intelligence"". If intelligence is merely information, as in ""a piece of intelligence"", as in data, or an algorithm, the structure is finite.  (Structure, here, refers to the information, which may be reduced to a single string in either case.) See: Turning Machine . If intelligence is the rational capability of an automata, it is likewise bounded by the tractability of the decision problem, the structure of the algorithm, and the time available to make the decision. See: Bounded Rationality Both answers are really the same, because ""intelligence"" in the first sense is limited by physical constraints on information density, sophistication of the algorithm, and time. See also: Computational complexity of mathematical operations , Computational complexity , Time Complexity",55.29241156,"The answers previously given are correct for AI which can indeed process more information with more computational power. However, actual reasoning ability like humans have is not defined by Church-Turing. AIXI has nothing to do with human reasoning. A pretty good clue to this fact is that AIXI has been around since 2005 and to date there are no machines based on it that have human-level reasoning. For example, an interesting topic in AI is natural language processing (NLP). I can speak into my Android phone and it will transcribe my speech into text. It seems like an amazing advance. However, this is what a human would do if they heard a foreign language and then did a phonetic transcription of what they heard. Then they looked up a phonetic chart to match the sounds with words. This would take place without any actual understanding of what they were hearing. This is how it works on my phone, much like Searle's Chinese Room. Humans are quite different because they actually understand words. The equivalent to this in AI would be natural language understanding (NLU). No AI today has NLU and no theory within AI explains how to construct it. There isn't any research on AI NLU because there is no starting point. A fact that most AI enthusiasts don't like to admit is that even the smartest AI systems are routinely outclassed by rats and even six month old babies in terms of comprehension. AI systems have no comprehension or understanding and without this they have no actual reasoning ability. Human-level comprehension falls under a completely different theory from the computational derivatives of Church-Turing. Can you make a human-level machine agent smarter by giving it more computational power? No, because you'll run into all sorts of problems which would take a few book chapters to explain. There are enhancements you can make but these have limits. If you go by a standard deviation 15 chart for IQ like Wechsler or the 5th edition of Stanford-Binet, the chances of having an IQ of 195 is 1 out of 8 billion. So, this roughly sets the upper bound of human ability. We could probably see machine agents with an IQ of 240 but not 500 or 1,000. I do understand the confusion concerning computation since exhaustive routines in AI are time limited. For example, our dim-witted chess programs play by laborious trial and error. They don't actually get smarter with more computational power, they are just able to eliminate bad moves faster. Let me give a human example. Let's say that I could do 5 math problems of a given complexity per hour using pencil and paper. So, I add a slide rule and my rate changes to 10 problems per hour. Then I switch to a calculator and my rate increases to 20. Let's say I then start using a spreadsheet and I hit 30 per hour. I am not actually 6x smarter than I was when I used pencil and paper. So, to answer the question, it is not possible to continuously increase intelligence even with unlimited computational power. However, it should be possible for machine intelligence to exceed human intelligence. One final thing that I should mention though is that this type of theory is quite good at organizing knowledge in a way that current big data methods do not. So, it is probable that the same theory that would allow a machine IQ of 240 would also provide enough assistance to a human to function at the same level.",53.29235915,"Actually, these aspects is part of some books I am working on right now.
Like what Jeevan say it is bound by laws of physics. I see that too, but in case of the Human, in the way i look at it, we are at the very lowest step/beginning of an AI that can do self reflecting and question its own ability to do rational thinking and how far it can go and develop. And I also use the Black Hole - maximum density, maximum energy concentration as a 1'st upper limit. A intelligence who can operate at Plank Level, can store all information in this world in like a few sand corns size. So given enough density, optimization in information exchange/methods, the upper limit is far far away from what humans process. But I take the analyze further and go beyond the Black hole theory. Of course it is speculation, but we still don't know all. So what is possible in more than 4 dimension (human brain, 3 dimension and time to think). As I see it, in real world, upper limit is so far away from our understanding, that we still can't handle it.",54.87894699,"It depends on the specific definition of intelligence, but with a definition based on proficiency on a set of arbitrary tasks the answer is yes. For instance, if we take intelligence as multiplicative function $$ I = f(Accuracy) * g(Resource \, Consumption) * h(Generalisation \, Abilities) $$ there is an inherent limit given by the maximum values obtained by the individual functions. Accuracy on the tasks is limited by the maximum obtainable score on those tasks. Minimal resource consumption is limited by physics and information theory (entropy of the tasks, limits of computation, quantity of matter-energy in the universe) Generalisation abilities are capped at intelligences able to solve arbitrary tasks. Moreover this limit is not simply the product of the max of the individual functions, since there are inherent tradeoffs (to increase accuracy more resources are needed, etc.) The definition of intelligence above is inspired by the Pragmatic General Intelligence Index . Nevertheless, if in our definition of intelligence we disregard resource consumption, our universe is infinite and we allow for tasks with arbitrary high scores, then there is no upper bound on intelligence.",59.35069818,,,,,,,,
4748,Binary classifier that minimizes false positive error,neural-networks,"There is no predefined classifier for any problem. Two main features of a classifier are its cost function and its corresponding weight update formula. Since your problem statement requires a huge cost for falsely classifying a particular class one approach will be. You have to define a cost function that will penalize hugely for misclassifying for that class only. So your cost function will be $J$ and $J'$ put together. You can look up the cost function of a logistic classifier to see how two separate cost functions are merged together here . The second approach can be (assuming you are using supervised learning), the learning rate $\alpha$ for both the classes should be different. The larger learning rate will be for the one which is the more important class, since you don't want to classify it improperly (increasing $\alpha$ compared to the other class will reduce or risk of misclassifying it). The exact learning rate depends from case to case. Thus, I have tailored the two main features of the classifier to solve this problem: The cost function. The weight update scheme (by changing learning rate for different cases).",52.66937441,"@DuttaA has pretty much mentioned the two most appropriate approaches to having this facility. Either the penalty of false positives should be high or the learning rate for the correct class should be high. I'll give two real-life examples to help you understand it better. Say you have to teach a teen that substance abuse is injurious to health (eg. Frequent smoking is a negative habit). But the teen ends up learning from high effects of the drugs that it is good ( false positive ) and gets addicted to it. You would strictly want to avoid this kind of a situation ( false positive error having a very big cost as compared to false negative error ). In general, to model the situation when the costs are different, we picture a cost matrix. For a two-class classification problem, the cost matrix would look like: (courtesy: http://albahnsen.com/CostSensitiveClassification ) Now, when designing your cost function, you would want to take into account the weight corresponding to each of the situation. A simple python code would be as follows: def weighted_cost(pred, act):
    if pred==P and act==P:
        return C_TP * cost(pred, act)
    if pred==P and act==N:
        return C_FP * cost(pred, act)
    if pred==N and act==P:
        return C_FN * cost(pred, act)
    if pred==N and act==N:
        return C_TN * cost(pred, act) Where, pred is the predicted class and act is the actual class. Here, C_TP, C_FP, C_TN, C_FN represent the weights of true positive, false positive etc. The cost(pred, act) function will calculate the loss of one training example. You would want to use the weighted_cost function for finding the loss after one training example. The second approach that @DuttaA mentioned was to vary the learning rate. In real life, you can relate this to the situation when you were asked to write a word multiple times if you forget its spelling so that you remember it better. In a way, you learn the correct spelling of the word. Here, increasing the value of the learning rate (say 4 x alpha) for a class can be viewed as updating the value of the weights multiple times (4 times) with the old learning rate (alpha) , similar to what we do by writing the correct spelling of the word multiple times. So, a more important class (in your case it will be the Negative Class ) should be given more alpha, because a false positive (misclassification of the negative class) has a high penalty. You learn to recognize the correct (negative) class by learning it more number of times (as in the case of learning the spelling of the word). Let me know if you need any further clarification.",57.1852137,"A funky way of doing this with less overhead is to just over fit the data up to some degree.
The reason is when you try to over fit the data with the classifier the classification bound tends to wrap around the clusters very tightly and with that model you can some times miss classify positive classes as negative(due to high variance) but there are comparatively less situations where you end up miss classifying negative classes as positive.
The level of overfitting that needs to be performed is just based on your FP and FN trade off. I don't think this as a permanent fix but can come handy up to some extent.",53.66428051,One more idea - I recall learning about Neyman-Pearson task in my studies. It is a statistical learning method for binary classification problem where overlooked danger (false negative error) is much unwanted. You set a desired threshold for false negative error rate and then minimize false positive error. You just need to measure conditional probabilities of each class. It may be expressed as a linear program and solved to get the optimal strategy for the threshold of your choice.,63.2831982,"You could vary an error coefficient in training. For example, if the expected output was negative and it gave some value in the positive you can train on C * ERROR and conversely if the expected output was positive and it gave some value in the negative you can train on just the error, that way false positives have more impact on the model as opposed to false negatives. Varying learning rates could as well help, however, increasing the learning rate and increase the error have different effects because changing the error will change the direction of the gradient whereas changing the learning rate will only change the gradient's magnitude of effect on the network, two slightly different things. (for the learning rate, split the data into two, positive and negative, then train them separately with the learning rate for negative cases larger than for positive cases)",57.283523,,,,,,,,
4683,What is the fundamental difference between CNN and RNN?,neural-networks,"Recurrent neural networks (RNNs) are artificial neural networks (ANNs) that have one or more recurrent (or cyclic) connections, as opposed to just having feed-forward connections, like a feed-forward neural network (FFNN). These cyclic connections are used to keep track of temporal relations or dependencies between the elements of a sequence. Hence, RNNs are suited for sequence prediction or related tasks. In the picture below, you can observe an RNN on the left (that contains only one hidden unit) that is equivalent to the RNN on the right, which is its ""unfolded"" version. For example, we can observe that $\bf h_1$ (the hidden unit at time step $t=1$ ) receives both an input $\bf x_1$ and the value of the hidden unit at the previous time step, that is, $\bf h_0$ . The cyclic connections (or the weights of the cyclic edges), like the feed-forward connections, are learned using an optimisation algorithm (like gradient descent) often combined with back-propagation (which is used to compute the gradient of the loss function). Convolutional neural networks (CNNs) are ANNs that perform one or more convolution (or cross-correlation ) operations (often followed by a down-sampling operation). The convolution is an operation that takes two functions, $\bf f$ and $\bf h$ , as input and produces a third function, $\bf g = f \circledast h$ , where the symbol $\circledast$ denotes the convolution operation. In the context of CNNs, the input function $\bf f$ can e.g. be an image (which can be thought of as a function from 2D coordinates to RGB or grayscale values). The other function $\bf h$ is called the ""kernel"" (or filter), which can be thought of as (small and square) matrix (which contains the output of the function $\bf 
 h$ ). $\bf f$ can also be thought of as a (big) matrix (which contains, for each cell, e.g. its grayscale value). In the context of CNNs, the convolution operation can be thought of as dot product between the kernel $\bf h$ (a matrix) and several parts of the input (a matrix). In the picture below, we perform an element-wise multiplication between the kernel $\bf h$ and part of the input $\bf h$ , then we sum the elements of the resulting matrix, and that is the value of the convolution operation for that specific part of the input. To be more concrete, in the picture above, we are performing the following operation \begin{align}
\sum_{ij}
\left(
\begin{bmatrix}
1 & 0 & 0\\
1 & 1 & 0\\
1 & 1 & 1
\end{bmatrix}
\otimes
\begin{bmatrix}
1 & 0 & 1\\
0 & 1 & 0\\
1 & 0 & 1
\end{bmatrix}
\right)
=
\sum_{ij}
\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
1 & 0 & 1
\end{bmatrix}
= 4
\end{align} where $\otimes$ is the element-wise multiplication and the summation $\sum_{ij}$ is over all rows $i$ and columns $j$ (of the matrices). To compute all elements of $\bf g$ , we can think of the kernel $\bf h$ as being slided over the matrix $\bf f$ . In general, the kernel function $\bf h$ can be fixed. However, in the context of CNNs, the kernel $\bf h$ represents the learnable parameters of the CNN: in other words, during the training procedure (using e.g. gradient descent and back-propagation), this kernel $\bf h$ (which thus can be thought of as a matrix of weights) changes. In the context of CNNs, there is often more than one kernel: in other words, it is often the case that a sequence of kernels $\bf h_1, h_2, \dots, h_k$ is applied to $\bf f$ to produce a sequence of convolutions $\bf g_1, g_2, \dots, g_k$ . Each kernel $\bf h_i$ is used to ""detect different features of the input"", so these kernels are different from each other. A down-sampling operation is an operation that reduces the input size while attempting to maintain as much information as possible. For example, if the input size is a $2 \times 2$ matrix $\bf f = \begin{bmatrix} 1 & 2 \\ 3 & 0 \end{bmatrix}$ , a common down-sampling operation is called the max-pooling , which, in the case of $\bf f$ , returns $3$ (the maximum element of $\bf f$ ). CNNs are particularly suited to deal with high-dimensional inputs (e.g. images), because, compared to FFNNs, they use a smaller number of learnable parameters (which, in the context of CNNs, are the kernels). So, they are often used to e.g. classify images. What is the fundamental difference between RNNs and CNNs? RNNs have recurrent connections while CNNs do not necessarily have them. The fundamental operation of a CNN is the convolution operation, which is not present in a standard RNN.",54.46933851,"Basically, a CNN saves a set of weights and applies them spatially. For example, in a layer, I could have 32 sets of weights (also called feature maps). Each set of weights is a 3x3 block, meaning I have 3x3x32=288 weights for that layer. If you gave me an input image, for each 3x3 map, I slide it across all the pixels in the image, multiplying the regions together. I repeat this for all 32 feature maps, and pass the outputs on. So, I am learning a few weights that I can apply at a lot of locations. For an RNN, it is a set of weights applied temporally (through time). An input comes in, and is multiplied by the weight. The networks saves an internal state and puts out some sort of output. Then, the next piece of data comes in, and is multiplied by the weight. However, the internal state that was created from the last piece of data also comes in and is multiplied by a different weight. Those are added and the output comes from an activation applied to the sum, times another weight. The internal state is updated, and the process repeats. CNN's work really well for computer vision. At the low levels, you often want to find things like vertical and horizontal lines. Those kinds of things are going to be all over the images, so it makes sense to have weights that you can apply anywhere in the images. RNN's are really good for natural language processing. You can imagine that the next word in a sentence will be highly influenced by the ones that came before it, so it makes sense to carry that internal state forward and have a small set of weights that can apply to any input. However, there are many more applications. In addition, CNN's have performed well on NLP tasks. There are also more advanced versions of RNN's called LSTM's that you could check out. For an explanation of CNN's, go to the Stanford CS231n course . Especially check out lecture 5. There are full class videos on YouTube. For an explanation of RNN's, go here .",54.72468429,"CNN vs RNN A CNN will learn to recognize patterns across space while RNN is useful for solving temporal data problems. CNNs have become the go-to method for solving any image data challenge while RNN is used for ideal for text and speech analysis. In a very general way, a CNN will learn to recognize components of an image (e.g., lines, curves, etc.) and then learn to combine these components to recognize larger structures (e.g., faces, objects, etc.) while an RNN will similarly learn to recognize patterns across time. So a RNN that is trained to convert speech to text should learn first the low level features like characters, then higher level features like phonemes and then word detection in audio clip. CNN A  convolutional network (ConvNet) is made up of layers.
In a convolutional network (ConvNet), there are basically three types of layers: Convolution layer Pooling layer Fully connected layer Of these, the convolution layer applies convolution operation on the input 3D tensor. Different filters extract different kinds of features from an image. The below GIF illustrates this point really well: Here the filter is the green 3x3 matrix while the image is the blue 7x7 matrix. Many such layers passes through filters in CNN to give an output layer that can again be a NN Fully connected layer or a 3D tensor. For example, in the above example, the input image passes through convolutional layer, then pooling layer, then convolutional layer, pooling layer, then the 3D tensor is flattened like a Neural Network 1D layer, then passed to a fully connected layer and finally a softmax layer. This makes a CNN. RNN Recurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed as input to the current step. Here, $x_{t-1}$ , , $x_{t}$ and $x_{t+1}$ are the values of inputs data that occur at a specific time steps and are fed into the RNN that goes through the hidden layers namely $h_{t-1}$ , , $h_{t}$ and $h_{t+1}$ which further produces output $o_{t-1}$ , , $o_{t}$ and $o_{t+1}$ respectively.",57.0447311,"On a basic level, an RNN is a neural network whose next state depends on its past state(s), while a CNN is a neural network that does dimensionality reduction (make large data smaller while preserving information) via convolution. See this for more info on convolutions",53.68013615,"In the case of applying both to natural language, CNN's are good at extracting local and position-invariant features but it does not capture long range semantic dependencies. It just consider local key-phrasses. So when the result is determined by the entire sentence or a long-range semantic dependency CNN is not effective as shown in this paper where the authors compared both architechrures on NLP takss. This can be extended for general case.",52.75947458,,,,,,,,
4672,Can an AI learn how to play chess without instructions?,gaming,"It's possible for an AI to learn chess without even knowing how to move the pieces. Google's AlphaZero didn't do that as their programmers coded the chess rules, but it's possible. One can learn the rules from human played chess games. Once the rules are known, we could use reinforcement learning to improve playing strength (and other board games).",60.59283401,"Yes. “On the 6th of December, 2017, AlphaZero took over the chess world"". ( article link , Arxiv paper ).",53.20295089,"I feel your question as presented, ""Can an AI know when it is its turn?"", is a bit naive. Performing this task is just a matter of asking ""Is it my turn?"" repeatedly. This can be implemented as simply as seeing if the turn clock is running for itself. As for the more general question of whether an AI can learn and then ""properly"" play chess the answer is: Theoretically, yes. We currently have the necessary computing power for AI to beat humans at Chess (and Go) in real time. We currently have AI systems that can discover and test system rules. We currently have AI that are able to discover the mappings between control manipulation and their effect in platformer video games. (Though they do not appear to be able to discover the goal of a game on there own. They still need a human to define appropriate fitness criteria.) Based on this, it seems possible for today's AI to be able to extract rules from a collection of previously played chess games. It would also seem possible to extract the rules by brute force from an oracle (referee) by setting up boards and asking that oracle if particular moves are valid. You still need to tell it what winning is and why it would ""want"" to (i.e.: fitness criteria).",56.86049539,"Your question is a bit vague. Is the ruleset defined? If not the bot can just do whatever it likes, and then it technically wouldn't be playing the game anymore. But by restricting its moves you are also implicitly giving it the ruleset. Regardless, minimax can ""solve"" chess (or more generally, all games). The only things it needs are the ""endgame states"" (the set of all board positions that is a checkmate), and a function that given some input board state, output the set of all possible board states that might follow, where we define a board state to contain information about the location of each chess prices and who is the next player to move. The ruleset is then implicitly defined by the ""next possible states function"", although the robot never actually knows the rules that govern each piece. If the rule set is not defined, then perhaps one can imagine a bot that plays chess by making random moves at the press of a button. One then can then play chess with the robot by either 1. ignoring the fact that the bot is making random moves, and  2. undo any move the bot makes that is illegal, and then get it to move again. You don't need something as complicated as AlphaGo to play chess naively, but the minimax approach needs a lot of computational power.",56.4612123,"Background Most any decision making with a specific sets of objectives and guidelines can be framed as a game. In mid 2017, it would have It is difficult to suggest one AI approach to a winning chess AI that had value outside of chess game play. Historically, game strategy AI approaches varied greatly from game to game and research lab to research lab. The approach to tic-tac-toe is the trivial case in AI, since it is easy to calculate the probability of winning for every possible sequence of moves on a single CPU in less than a second. At the other extreme of game complexity and dynamics, only the technical talent contractually obligated to secrecy in contracts with the portfolio managers of billionaires know how to play the game of high speed trading, optimized for maximum gain in position per hour. The international champion in inter-computer chess play is Stockfish. It produced the highest consistency in wins in international tournaments thus far. Stockfish performs its best move search in a highly parallel CPU run time environment. A sophisticated pruning and depth prioritization involving late move reductions is applied to the alpha-beta search algorithm, in conjunction with a bit board. In human-computer play Zor was the international champion in 2017. Change in AI Approach In 2017, DeepMind’s AlphaZero defeated Stockfish 28–72-0. (The number of draws is positioned in the center number.) That it won the tournament is not the remarkable advancement from an AI perspective. The same algorithm, configured differently also plays a winning game at Go and Shogi. The approach and design is described in Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm , David Silver et. al., 2017 AlphaZero is configured with the rules of the game to constrain the search. It begins training prior to tournament game play without prior knowledge of game play. That no triggered game strategies or heuristic rules to guide the search are known before learning game play, the authors claim the learning is tabula rasa , Latin meaning blank slate. Reinforcement learning is used to develop a strategy during self-play. AlphaZero, ""Averages over the position evaluations within a subtree, rather than computing the minimax evaluation of that subtree,"" as is commonly used in chess players based on the alpha-beta approach. It determines the relative value of states (board positions) based on a DNN trained to produce values associated with all of the state information from outcomes. This is distinct from valuation by summing points assigned to pieces and their locations. AlphaZero uses a Monte-Carlo tree search (MCTS) algorithm, using ordered deepening to minimize computing resource utilization per move. The MCTS mitigates the numerical error, DNN convergence artifacts that can accumulate at the root of each sub-tree, via aggregation. The aggregation of data that contains reasonably normalized noise causes cancellation of the symmetrically distributed deviations. The achievement is significant. The abstract of the paper states, ""Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case."" Remaining Domain Dependence Without dismissing the achievement, the claim of no domain knowledge except the game rules conflicts with the additional five items listed in the Domain Knowledge section of the same paper, reproduced here solely for convenience of the reader. The input features describing the position, and the output features describing the move, are structured as a set of planes; i.e. the neural network architecture is matched to the grid-structure of the board. AlphaZero is provided with perfect knowledge of the game rules. These are used during MCTS, to simulate the positions resulting from a sequence of moves, to determine game termination, and to score any simulations that reach a terminal state. Knowledge of the rules is also used to encode the input planes (i.e. castling, repetition, no-progress) and output planes (how pieces move, promotions, and piece drops in shogi). The typical number of legal moves is used to scale the exploration noise (see below). Chess and shogi games exceeding a maximum number of steps (determined by typical game length) were terminated and assigned a drawn outcome; Go games were terminated and scored with Tromp-Taylor rules, similarly to previous work. In addition to game rules and the above five listed domain dependencies, there are further dependencies.The higher level analysis of the consequences of the board geometry and game rules is not a fundamental game rule. In Go game play, ""During MCTS, board positions were transformed using a randomly selected rotation or reflection before being evaluated by the neural network, so that the MonteCarlo evaluation is averaged over different biases."" Rules are analyzed to scale, ""Noise that is added to the prior policy to ensure exploration ... in proportion to the typical number of legal moves for that game type."" This totals eight dependencies, and a grossly insufficient number of game types were tried to make a claim that AlphaZero is a, ""domain-independent search."" But bending the the concept of domain independence in the claims is more easily forgiven when legitimate and significant achievements are made by an enthusiastic team. The approach is sound and the use of the craft by the DeemMind team is world class. Beyond Removing Remaining Dependencies Even after the eight dependencies are whittled down by the various teams involved in game play automation, there is further work that may become the focus of further research. Board games are a particular kind of game. In games where the game rules can mutate, such as markets, law, war, and other domains where the boundaries of the domain require knowledge of broader sets of domains is more challenging, although it is reasonable to expect that dependencies will be reduced and current approaches that work with board games may become adaptive in terms of game rule acquisition. Currently, approaches like AlphaZero require that input preparation be designed and executed and that outputs be executed on a virtual board. They do not yet discover game states by vision, execute moves with robotics, or acquire the rules of the game from natural language descriptions or the analysis of past games. Summary These limitations do not invalidate the significant advancement of a game player that needs only rules, a small set of domain specific configurations, and some self play time to defeat champion level dedicated artificial game players.",55.27528005,,,,,,,,
4650,What are AI use cases for communication service providers?,applications,"AI could hold the key in automating and optimizing networks. On the subscriber side, ML and AI will assist telecom operators in profiling the subscribers. This will be achieved by analyzing network activity, conversion rate of offers and data usage trends. Below are a few use cases and how they will transform the telecommunication sector. (Source H2o.ai blog https://www.h2o.ai/telecom/ ) Old generation telecom technologies. Reactive Maintenance Network optimization with human intervention Centralized intelligence Security attack repair Backlogged customer tickets Future generation AI based telecom technologies. Predictive Maintenance Self-optimizing network Optimal network quality Intelligence at the edge Security attack prediction Improved customer experience through customer service chat bots. Speech and voice services for customer which allows users to explore media content by spoken word rather than remote control. Predictive maintenance which is the ability to fix problems with telecom hardware such as cell towers, power lines e.t.c before they happen by detecting signals that usually lead to failure.",53.58216221,"I think AI can definitely do more than just optimise networks for comms providers - it’s really powerful in how it handles customer interactions. Think live call analysis that flags sentiment or compliance risks, automates logging into CRM, or even suggests next steps in real time. The value is in turning messy conversations into useful data that makes support faster and more proactive.",53.41654872,"AI use cases for communication service providers include network optimization, predictive maintenance, fraud detection, customer support automation with chatbots, personalized service recommendations, real-time traffic management, churn prediction, and enhancing customer experience through advanced analytics and sentiment analysis.",71.14774321,"AI can also be used by Telcos to:
(1) Improve Usage & Retention efforts: Make relevant up-sell and cross-sell offers to the right users at the right time.
(2) Make segmentation more granular: make bespoke recommendations based on a user’s behavioral patterns and content preferences and assessing which call & data packages best suit different customer segments thereby increasing sales success rates",50.27596449,"Well AI can be used in Major Areas in Telecom 1/ Network Optimization Where the network is trained to come up with the required Parameter tunning to solve a particular issue, for example in 4G system we have user licenses for cell level that how many users this cell will support. Every time the number of users increases on that cell the Network optimizer sends a work order to the Backoffice to increase the number of users with temporary license that is in pool. For an AI system it can analyze the utilization and increase the licenses or it can tune the parameters to shift traffic on the neighbour cell, it can learn from the optimizers behaviour and actions, this is a single example 2/ Customer Intelligence Based on the customer behavior, his social media engagements and responses, usage patterns, past usage history and current context  the AI System can generate personalized offers for the user that can add value to the overall customer Journey and make him and engaged happy customer. 3/ Network expansions and Dynamic capacity: Mostly  network operators have licenses per resource contracts with vendors, the excess capacity is charged. In most cases for example in summers some particular areas need more additional capacity but once the summers are gone that particular area is the lowest utilized area. An AI system can learn based on the usage patterns when a particular area need higher capacity and when low capacity and can increase and decrease the capacity resulting in reduced costs. Above are some of the use cases which can further be added with alot of Customer Engagement and Customer Experience Management uses cases Regards
Muhammad Azfar
Customer Journey and Business Analyst
Customer Experience Management",53.65980297,,,,,,,,
4629,How does an activation function's derivative measure error rate in a neural network?,neural-networks,"This derivative is used when calculating the error of your machine learning algorithm during gradient based minimization methods. 
Read below for more info. When performing supervised classification (with X, Y data vectors of inputs and outcome data to train with) you begin with the error function E(X, Y; θ)= ∑ i (ƒ(x i ; θ)-y i ) 2 for total error over all data instances i, where f is your neural network, linear regression,...method of interest and θ is the set of weights. The goal here is to find weights that minimize your error when predicting training data (y) (which ideally generalizes to new data as well). To be explicit, ƒ(x i ; θ); outputs value of interest which should be y i . And E measures how far off it is in prediction. So to train your classifier, you optimize E with something like gradient descent. Thus when ∂E/∂θ = 0 (for a particular θ), that means you hit a local minimum for the error function, or a point where the error in the current state of the predictor is low, meaning it is (hopefully) a good predictor. Note the ƒ here is not the same as an activation function, as a neural network is defined differently than in linear regression, etc. and must perform a special kind of gradient descent called backpropagation. So when you take ∂E/∂θ, what does it equal for a neural net? You should note the activation functions derivative is involved which is how it’s used to measure error so to say.",60.46464697,You dont need a sigmoid function if you dont want one.  Any differentiable function will do.  Sigmoid functions are just one of many suitable functions. You could write your own differentiable function if you want a propriety solution.,51.54087067,"Measuring the error rate of a neural network does not involve the derivative of the sigmoid function at all. It only needs the neural networks outputs, and the expected outputs. It does not matter how the neural network got to those outputs, the outputs only have to be based on the inputs. What the other of that specific text is trying to say is that the derivative of the sigmoid function is important when you use the algorithm back propagation to train the neural network. This method involves using derivatives to optimize the neural network. While this technique is more complicated with neural networks because of how many variables are involved, you can easily see the basics of this approach if you look at a calculus textbook and look at the chapter that is usually titled Applications of the derivative.",59.2639519,"The derivative of your loss is the ""slope"" at that given prediction. So by ""moving down along the slope"", we can reduce the value of the loss function. Intuitively, one can imagine a ball rolling down a slope in the direction of the tangent. Eventually the ball will roll towards the lowest point (global minima), or a small pothole (local minima), or even some weird looking shape (saddle point) if you are very unlucky. With some fixed input/output pair, we can consider the loss function as a value of the parameters (weights). Here, the Z axis is the value of your loss f (what you are trying to optimize), while the X,Y axis are the parameters of the loss f (what you are allowed change): Note that the image provided is a bit misleading. In practice, we only know the value of the loss function at the location we have probed. The entire graph can only be drawn if we evaluate the loss function for every parameter. So we can't just look at the picture and know which location is the best. Recall that the goal of the prediction task is to update the parameters in a way such that we minimize the loss function for some given input/output pair. Assuming the loss function is reasonably smooth, then our best bet of reaching a lower value of the error rate is by taking a small step towards the direction of the slope, which is the derivative of the function. The size of the ""small step"" is called the learning rate. Under the assumption that the loss function looks similar given different input/output pairs, by iteratively moving down the slope for each datapoint, we can eventually reach a ""good"" set of weights that give a low error. This is the motivation and idea of the back-propagation algorithm. A good follow-up question might be: Why do we say that the loss function is  similar given different data points? This is basically the same as assuming that there is some regularity in the data such that there is a ""most normal looking loss function"" that the data-set naturally approaches. Hopefully that helps!",53.71196755,"In order to train a neural network, you have to adjust each weights and biases to reduce the cost to as minimum as possible. The only way to do so is to subtract a small amount of the partial derivative of cost w.r.t. w, b from the respective parameters. if J is our cost function, after each iteration: w = w - lr*dJ_dw,      //where lr is a small scalar called learning rate and dJ_dw is the partial derivative of cost function w.r.t. w and same for bias b = b - lr*dJ_db,    //dJ_db is the partial derivative of cost function w.r.t b let's look at how the partial derivatives are calculated. using sigmoid as activation function, and squared error function as cost, we have: z = w*x + b
a = sigmoid(z)       // sigmoid(z) = 1.0 / (1.0 + exp(-z)), a is the final output

J = (a - y)*(a - y)  // where y is the expected output For us to calculate partial derivatives of this cost function w.r.t. w, b, we need to use, chain rule of derivatives as: dJ_dw = dJ_da * da_dz * dz_dw    // dJ_da is the partial derivative of cost w.r.t. activation, a
dJ_db = dJ_da * da_dz * dz_db in the above equations, da_dz is the derivative of activation function (sigmoid in our case) which is sigmoid(z).(1 - sigmoid(z))",56.96563522,,,,,,,,
4532,Why isn't ethics more integrated into current AI systems?,philosophy,"This is necessarily a high-level answer, and highly speculative, but I've been thinking on this question, and here are my thoughts: Implementing ethical algorithms requires a mathematical basis for philosophy because computers are difference engines After Russell & Whitehead's famous failure, and Gödel's incompleteness theorem, this would seem to be problematic. AI is a highly applied field, especially today per continuing validation of deep learning, and no company wants to go near the issue of ethics unless they are forced to Thus, you see it in self-driving cars because the engineers have no choice but to grapple with the problem.  By contrast, I don't think you'll see many algorithmic stock trading firms, where the business is Pareto efficiency , worrying about the ethics or social impacts of financial speculation. (The solution to ""flash crashes"" seems to have been rules for temporary suspension of trading, instead of addressing the social value of high-frequency algorithmic trading.) A more obvious example is social media companies ignoring the extreme amounts of information abuse (disinformation and misinformation) being posted on their sites, pleading ignorance, which is highly suspect in that the activity generated by information abuse positively affects their bottom-lines. Applied fields tend to be predominantly driven by profit The primary directive of corporations is to return a profit to investors.  It's not uncommon for corporations to break the law when the fines and penalties are expected to be less than the profit made by illegal activity.  (There is the concept of ethics in business, but the culture in general seems to judge people and companies based on how much money they make, regardless of the means.) Implementation of machine ethics is being explored in areas where they are necessary to sell the product, but elsewhere, it's still largely hypothetical If superintelligences evolve and wipe out humanity (as some very smart people with superior mathematics skills are warning us about,) my feeling is that it will be a function of nature, where unrestricted evolution of these algorithms is due to economic drivers which focus on hyper-partisan automata in industries like financial speculation and autonomous warfare.  Essentially, chasing profits at all costs, regardless of the impacts.",53.2185576,"I feel part of the problem as to why there is very little in the way of ethical implementations of AI/ML technologies, is simply because there is no need or proper application of the theoretical frameworks. By this I mean, there are no substantial ways we can apply this understanding to algorithms and models that cannot interact in a meaningful way. We have such a large theoretical framework on AI safety/ethics because it is extremely important. We need to come up with safe guidelines for implementing strong AI before it is created. Some very focused papers have started to narrow down the issues in creating ethical/safe AI systems. See Concrete Problems in AI Safety",55.02860032,"With the imitation method, the most appropriate behavior can be integrated into artificial intelligence. Artificial intelligence can be reshaped when the ethical position changes. It is used for ideological purpose or to gather information. It's not clear what the robot is.",57.41462365,"We can take the error model into accountant. Recognising bias and variance among the performance under neural networks can be a first step. And then we can discuss whether such performance is allowed. As far as we know, practicing ethnics requires empirical and field study. we cannot simply take rationales and paper essays to determine the doings of learnt machines is wrong or not. It can be further divided into accidents, errors , or even bugs created from  the developers.",51.05668707,"Intuitively speaking, it seems to be the case that there is little research into the implementation of AI ethics because: Society as a whole seems to comfortably agree that the current state of machine intelligence is not strong enough for it to be considered as conscious or sentient. Thus we don't need to give it ethical rights (yet). Implementation of ethical behaviour into a program requires a method for computers to be capable of interpreting ""meaning"", which we do not know how to do yet.",53.7749304,,,,,,,,
4456,What's the difference between model-free and model-based reinforcement learning?,reinforcement-learning,"What's the difference between model-free and model-based reinforcement learning? In Reinforcement Learning, the terms ""model-based"" and ""model-free"" do not refer to the use of a neural network or other statistical learning model to predict values, or even to predict next state (although the latter may be used as part of a model-based algorithm and be called a ""model"" regardless of whether the algorithm is model-based or model-free). Instead, the term refers strictly as to whether, whilst during learning or acting, the agent uses predictions of the environment response. The agent can use a single prediction from the model of next reward and next state (a sample), or it can ask the model for the expected next reward, or the full distribution of next states and next rewards. These predictions can be provided entirely outside of the learning agent - e.g. by computer code that understands the rules of a dice or board game. Or they can be learned by the agent, in which case they will be approximate. Just because there is a model of the environment implemented, does not mean that a RL agent is ""model-based"". To qualify as ""model-based"", the learning algorithms have to explicitly reference the model: Algorithms that purely sample from experience such as Monte Carlo Control, SARSA, Q-learning, Actor-Critic are ""model free"" RL algorithms. They rely on real samples from the environment and never use generated predictions of next state and next reward to alter behaviour (although they might sample from experience memory, which is close to being a model). The archetypical model-based algorithms are Dynamic Programming (Policy Iteration and Value Iteration) - these all use the model's predictions or distributions of next state and reward in order to calculate optimal actions. Specifically in Dynamic Programming, the model must provide state transition probabilities, and expected reward from any state, action pair. Note this is rarely a learned model. Basic TD learning, using state values only, must also be model-based in order to work as a control system and pick actions. In order to pick the best action, it needs to query a model that predicts what will happen on each action, and implement a policy like $\pi(s) = \text{argmax}_a \sum_{s',r} p(s',r|s,a)(r + v(s'))$ where $p(s',r|s,a)$ is the probability of receiving reward $r$ and next state $s'$ when taking action $a$ in state $s$ . That function $p(s',r|s,a)$ is essentially the model. The RL literature differentiates between ""model"" as a model of the environment for ""model-based"" and ""model-free"" learning, and use of statistical learners, such as neural networks. In RL, neural networks are often employed to learn and generalise value functions, such as the Q value which predicts total return (sum of discounted rewards) given a state and action pair. Such a trained neural network is often called a ""model"" in e.g. supervised learning. However, in RL literature, you will see the term ""function approximator"" used for such a network to avoid ambiguity. It seems to me that any model-free learner, learning through trial and error, could be reframed as model-based. I think here you are using the general understanding of the word ""model"" to include any structure that makes useful predictions. That would apply to e.g. table of Q values in SARSA. However, as explained above, that's not how the term is used in RL. So although your understanding that RL builds useful internal representations is correct, you are not technically correct that this can be used to re-frame between ""model-free"" as ""model-based"", because those terms have a very specific meaning in RL. In that case, when would model-free learners be appropriate? Generally with current state of art in RL, if you don't have an accurate model provided as part of the problem definition, then model-free approaches are often superior. There is lots of interest in agents that build predictive models of the environment, and doing so as a ""side effect"" (whilst still being a model-free algorithm) can still be useful - it may regularise a neural network or help discover key predictive features that can also be used in policy or value networks. However, model-based agents that learn their own models for planning have a problem that inaccuracy in these models can cause instability (the inaccuracies multiply the further into the future the agent looks). Some promising inroads are being made using imagination-based agents and/or mechanisms for deciding when and how much to trust the learned model during planning. Right now (in 2018), if you have a real-world problem in an environment without an explicit known model at the start, then the safest bet is to use a model-free approach such as DQN or A3C. That may change as the field is moving fast and new more complex architectures could well be the norm in a few years.",70.9924438,"Model-based reinforcement learning has an agent try to understand the world and create a model to represent it. Here the model is trying to capture 2 functions, the transition function from states $T$ and the reward function $R$ . From this model, the agent has a reference and can plan accordingly. However, it is not necessary to learn a model, and the agent can instead learn a policy directly using algorithms like Q-learning or policy gradient. A simple check to see if an RL algorithm is model-based or model-free is: If, after learning, the agent can make predictions about what the next state and reward will be before it takes each action, it's a model-based RL algorithm. If it can't, then it’s a model-free algorithm.",65.78105684,"In reinforcement learning (RL), there is an agent which interacts with an environment (in time steps). At each time step, the agent decides and executes an action , $a$ , on an environment, and the environment responds to the agent by moving from the current state (of the environment), $s$ , to the next state (of the environment), $s'$ , and by emitting a scalar signal, called the reward , $r$ .  In principle, this interaction can continue forever or until e.g. the agent dies. The main goal of the agent is to collect the largest amount of reward ""in the long run"". To do that, the agent needs to find an optimal policy (roughly, the optimal strategy to behave in the environment). In general, a policy is a function which, given a current state of the environment, outputs an action (or a probability distribution over actions, if the policy is stochastic ) to execute in the environment. A policy can thus be thought of as the ""strategy"" used by the agent to behave in this environment. An optimal policy (for a given environment) is a policy which, if followed, will make the agent collect the largest amount of reward in the long run (which is the goal of the agent). In RL, we are thus interested in finding optimal policies. The environment can be deterministic (that is, roughly, the same action in the same state leads to the same next state, for all time steps) or stochastic (or non-deterministic), that is, if the agent takes an action in a certain state, the resulting next state of the environment might not necessarily always be the same: there is a probability that it will be a certain state or another. Of course, these uncertainties will make the task of finding the optimal policy harder. In RL, the problem is often mathematically formulated as a Markov decision process (MDP). A MDP is a way of representing the ""dynamics"" of the environment, that is, the way the environment will react to the possible actions the agent might take, at a given state. More precisely, an MDP is equipped with a transition function (or ""transition model""), which is a function that, given the current state of the environment and an action (that the agent might take), outputs a probability of moving to any of the next states. A reward function is also associated with an MDP. Intuitively, the reward function outputs a reward, given the current state of the environment (and, possibly, an action taken by the agent and the next state of the environment). Collectively, the transition and reward functions are often called the model of environment. To conclude, the MDP is the problem and the solution to the problem is a policy. Furthermore, the ""dynamics"" of the environment are governed by the transition and reward functions (that is, the ""model""). However, we often do not have the MDP, that is, we do not have the transition and reward functions (of the MDP associated the environment). Hence, we cannot estimate a policy from the MDP, because it is unknown. Note that, in general, if we had the transition and reward functions of the MDP associated with the environment, we could exploit them and retrieve an optimal policy (using dynamic programming algorithms). In the absence of these functions (that is, when the MDP is unknown), to estimate the optimal policy, the agent needs to interact with environment and observe the responses of the environment. This is often referred to as the ""reinforcement learning problem"", because the agent will need to estimate a policy by reinforcing its beliefs about the dynamics of the environment. Over time, the agent starts to understand how the environment responds to its actions, and it can thus start to estimate the optimal policy. Thus, in the RL problem, the agent estimates the optimal policy to behave in an unknown (or partially known) environment by interacting with it (using a ""trial-and-error"" approach). In this context, a model-based algorithm is an algorithm that uses the transition function (and the reward function) in order to estimate the optimal policy. The agent might have access only to an approximation of the transition function and reward functions, which can be learned by the agent while it interacts with the environment or it can be given to the agent (e.g. by another agent). In general, in a model-based algorithm, the agent can potentially predict the dynamics of the environment (during or after the learning phase), because it has an estimate of the transition function (and reward function). However, note that the transition and reward functions that the agent uses in order to improve its estimate of the optimal policy might just be approximations of the ""true"" functions. Hence, the optimal policy might never be found (because of these approximations). A model-free algorithm is an algorithm that estimates the optimal policy without using or estimating the dynamics (transition and reward functions) of the environment. In practice, a model-free algorithm either estimates a ""value function"" or the ""policy"" directly from experience (that is, the interaction between the agent and environment), without using neither the transition function nor the reward function. A value function can be thought of as a function which evaluates a state (or an action taken in a state), for all states. From this value function, a policy can then be derived. In practice, one way to distinguish between model-based or model-free algorithms is to look at the algorithms and see if they use the transition or reward function. For instance, let's look at the main update rule in the Q-learning algorithm : $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a}Q(S_{t+1}, a) - Q(S_t, A_t))$$ As we can see, this update rule does not use any probabilities defined by the MDP. Note: $R_{t+1}$ is just the reward that is obtained at the next time step (after taking the action), but it is not necessarily known beforehand. So, Q-learning is a model-free algorithm. Now, let's look at the main update rule of the policy improvement algorithm: $$Q(s,a) \leftarrow  \sum_{s' \in \mathcal{S}, r\in\mathcal{R}}p(s',r|s,a)(r+\gamma V(s'))$$ We can immediately observe it uses $p(s',r|s,a)$ , a probability defined by the MDP model. So, policy iteration (a dynamic programming algorithm), which uses the policy improvement algorithm, is a model-based algorithm.",54.93773313,"Although there are several good answers, I want to add this paragraph from Reinforcement Learning: An Introduction , page 303, for a more psychological view on the difference. The distinction between model-free and model-based reinforcement learning algorithms
  corresponds to the distinction psychologists make between habitual and goal-directed
  control of learned behavioral patterns. Habits are behavior patterns triggered by appropriate
  stimuli and then performed more-or-less automatically. Goal-directed behavior,
  according to how psychologists use the phrase, is purposeful in the sense that it is controlled
  by knowledge of the value of goals and the relationship between actions and their
  consequences. Habits are sometimes said to be controlled by antecedent stimuli, whereas
  goal-directed behavior is said to be controlled by its consequences (Dickinson, 1980,
  1985). Goal-directed control has the advantage that it can rapidly change an animal’s
  behavior when the environment changes its way of reacting to the animal’s actions. While
  habitual behavior responds quickly to input from an accustomed environment, it is unable
  to quickly adjust to changes in the environment. It keeps going from there, and has a nice example afterwards. I think the main point that was not always explained in the other answers, is that in a model-free approach you still need some kind of environment to tell you what is the reward associated with your action. The big difference is that you do NOT need to store any information about the model. You give the environment your chosen action, you update your estimated policy, and you forget about it. On the other hand, in model-based approaches, you either need to know the state transitions history as in Dynamic Programming, or you need to be able to calculate all possible next states and associated rewards, from the present state.",63.01288113,"Model-Free RL In Model-Free RL, the agent does not have access to a model of the environment. By environment I mean a function which predicts state transition and rewards. As of the time of writing, model-free methods are more popular and have been researched extensively. Model-Based RL In Model-Based RL, the agent has access to a model of the environment. Main advantage is that this allows the agent to plan ahead by thinking ahead. Agents distill the results from planning ahead into a learned policy. A famous example of Model-Based RL is AlphaZero . The main downside is that many times a ground-truth representation of the environment is not usually available. Below is a non-exhaustive taxonomy of RL algorithms, which may help you to visualize better the RL landscape.",61.06929543,"According to OpenAI – Kinds of RL Algorithms , algorithms which use a model of the environment, i.e. a function which predicts state transitions and rewards, are called model-based methods, and those that don’t are called model-free . This model can either have been given the agent or learned by the agent. Using a model allows the agent to plan by thinking ahead, seeing what would happen for a range of possible choices, and explicitly deciding between its options. This may be useful when faced with problems that require more long-term thinking. One way to perform planning is by using some kind of tree search, for example Monte Carlo tree search (MCTS), or—which I suspect could also be used— variants of the rapidly exploring random tree (RRT). See e.g. Agents that imagine and plan . The agent can then distill the results from planning ahead into a learned policy – this is known as expert iteration. A model can also be used to create a simulated, or ""imagined,"" environment in which the state is updated by using the model, and make the agent learn inside of that environment, such as in World Models . In many real-world scenarios, the ground-truth model of the environment is not available to the agent. If an agent wants to use a model in this case, it has to learn the model, which can be challenging for several reasons. There are however cases in which the agent uses a model that is already known and consequently doesn't have to learn the model, such as in AlphaZero , where the model comes in form of the rules of the game.",57.30166233,,,,,,
4320,Why are the initial weights of neural networks randomly initialised?,neural-networks,"You shouldn't assign all to 0.5 because you'd have the ""break symmetry"" issue. http://www.deeplearningbook.org/contents/optimization.html Perhaps the only property known with complete certainty is that the initial
parameters need to “ break symmetry ” between different units. If two hidden
units with the same activation function are connected to the same inputs, then
these units must have different initial parameters . If they have the same initial
parameters, then a deterministic learning algorithm applied to a deterministic cost
and model will constantly update both of these units in the same way. Even if the
model or training algorithm is capable of using stochasticity to compute different
updates for different units (for example, if one trains with dropout), it is usually
best to initialize each unit to compute a different function from all of the other
units. This may help to make sure that no input patterns are lost in the null
space of forward propagation and no gradient patterns are lost in the null space
of back-propagation.",54.3327978,"The initial weights in a neural network are initialized randomly because the gradient based methods commonly used to train neural networks do not work well when all of the weights are initialized to the same value. While not all of the methods to train neural networks are gradient based, most of them are, and it has been shown in several cases that initializing the neural network to the same value makes the network take much longer to converge on an optimum solution. Also, if you want to retrain your neural network because it got stuck in a local minima, it will get stuck in the same local minima. For the above reasons, we do not set the initial weights to a constant value. References: Why doesn't backpropagation work when you initialize the weights the same value?",64.49209462,"That is a very deep question. There was series of papers recently proving the convergence of gradient descent for overparameterized deep networks (for example, Gradient Descent Finds Global Minima of Deep Neural Networks , A Convergence Theory for Deep Learning via Over-Parameterization or Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks ). All of the proofs assume that the initial weights are assigned randomly according to a Gaussian distribution. The main reasons this initial distribution is important for the proofs are: Random weights make the ReLU operators in each layer statistically compressive mapping (up to a linear transformation). Random weights preserve separation of input for any input distribution - that is if input samples are distinguishable network propagation will not make them indistinguishable. Those properties are very difficult to reproduce with deterministically generated initial weight matrices, and even if they are reproducible with deterministic matrices NULL-space (from which we can generate adversarial examples) would likely make the method less useful in practice. More importantly, preservation of those properties during gradient descent would likely make method impractical. But overall it's very difficult but not impossible, and may warrant some research in that direction. In analogous situation, there were some results for Restricted Isometry Property for deterministic matrices in a compressed sensing .",59.09673234,"It is okay to initialize the weights to zero for a simple logistic regression, but for a neural network to initialize the weights to parameters to all zero and then apply gradient descent, it won't work. Assume here that you have in a 2-layered network with just 2 input features and in the hidden layer you have 2 nodes, and the weights are initialized as [u, v] for both the nodes. So, while optimizing the weights, using gradient descent you would compute: where α is the learning rate and J is the cost function that you want to minimize by optimizing the weights. By kind of a proof by induction, you should be able to see that after every single iteration of training, your two hidden units are still computing exactly the same function because both hidden units start off computing the same function, have the same influence on the output unit. So even after multiple iterations, the two hidden units are still symmetric. Hence, there's really no point to having more than one hidden unit because they are all computing the same thing. In summary, no matter how long you run gradient descent, both the two units compute exactly the same function which is not helpful, because you want the different hidden units to compute different functions. The solution to this is to initialize your parameters randomly. For larger neural networks, say of three features and maybe a very large number of hidden units, a similar argument still works. Note: It's okay to initialize the bias term to just zeros. Because so long as weights are initialized randomly, you start off with the different hidden units computing different things.",54.63576437,"Another theory relevant I think to this question is the “lottery ticket hypothesis”: Jonathan Frankle and Michael Carbin, The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, 2019, https://arxiv.org/abs/1803.03635 Basically, when you have a neural network with a very high number of neurons with randomly initialized weights in the hidden layers, you have a high chance of a subset of the network being close to a good solution to the problem. Then, by applying gradient descent, you can align the output of the whole network to that of the sub-network and fine tune it. It would be nice if someone could prove this. If there is a proof I would be interested to see it.",54.37440293,,,,,,,,
4048,How can I design and train a neural network to play a card game (similar to Magic: The Gathering)?,neural-networks,"I think you raise a good question, especially WRT to how the NNs inputs & outputs are mapped onto the mechanics of a card game like MtG where the available actions vary greatly with context. I don't have a really satisfying answer to offer, but I have played Keldon's Race for the Galaxy NN-based AI - agree that it's excellent- and have looked into how it tackled this problem. The latest code for Keldon's AI is now searchable and browseable on github . The ai code is in one file . It uses 2 distinct NNs, one for ""evaluating hand and active cards"" and the other for ""predicting role choices"". What you'll notice is that it uses a fair amount on non-NN code to model the game mechanics. Very much a hybrid solution. The mapping of game state into the evaluation NN is done here . Various relevant features are one-hot-encoded, eg the number of goods that can be sold that turn. Another excellent case study in mapping a complex game into a NN is the Starcraft II Learning Environment created by Deepmind in collaboration with Blizzard Entertainment. This paper gives an overview of how a game of Starcraft is mapped onto a set of features that a NN can interpret, and how actions can be issued by a NN agent to the game simulation.",55.46227599,"Yes.  It is feasible. Overview of the Question The design goal of the system seems to be gain a winning strategic advantage by employing one or more artificial networks in conjunction with a card game playing engine. The question shows a general awareness of the basics of game-play as outlined in Morgenstern and von Neuman's Game Theory . At specific points during game-play a player may be required to execute a move. There is a fininte set of move options according to the rules of the game. Some strategies for selecting a move produce higher winning records over multiple game plays than other strategies. An artificial network can be employed to produce game-play strategies that are victorious more frequently that random move selection. Other features of game-play may or may not be as obvious. At each move point there is a game state, which is needed by any component involved in improving game-play success. In addition to not knowing when the opponent will bluff, in card games, the secret order of shuffled cards can introduce the equivalent of a virtual player the moves of which approximate randomness. In three or more player games, the signaling of partners or potential partners can add an element of complexity to determining the winning game strategy at any point.  Based on the edits, it does not appear like this game has such complexities. Psychological factors such as intimidation can also play a role in winning game-play.  Whether or not the engine presents a face to the opponent is unknown, so this answer will skip over that. Common Approach Hints There is a common approach to mapping both inputs and outputs, but there is too much to explain in a Stack Exchange answer.  These are just a few basic principles. All of the modeling that can be done explicitly should be done.  For instance, although an artificial net can theoretically learn how to count cards (keeping track of the possible locations of each of the cards), a simple counting algorithm can do that, so use the known algorithm and feed those results into the artificial network as input. Use as input any information that is correlated with optimal output, but don't use as inputs any information that can not possibly correlate with optimal output. Encode data to reduce redundancy in the input vector, both during training and during automated game-play.  Abstraction and generalization are the two common ways of achieving this.  Feature extraction can be used as tools to either abstract or generalize.  This can be done at both inputs and outputs.  An example is that if, in this game, J > 10 in the same way that A > K, K > Q, Q > J and 10 > 9, then encode the cards as an integer from 2 through 14 or 0 through 12 by subtracting one.  Encode the suits as 0 through 3 instead of four text strings. The image recognition work is only remotely related, too different from card game-play to use directly, unless you need to recognize the cards from a visual image, in which case LSTM may be needed to see what the other players have chosen for moves.  Learning winning strategies would more than likely benefit from MLP or RNN designs, or one of their derivative artificial network designs. What an Artificial Network Would Do and Training Examples The primary role of artificial networks of these types is to learn a function from example data.  If you have the move sequences of real games, that is a great asset to have for your project.  A very large number of them will be very helpful for training. How you arrange the examples and whether and how you label them is worth consideration, however without the card game rules it is difficult to give any reliable direction.  Whether there are partners, whether it is score based, whether the number of moves to a victory, and a dozen other factors provide the parameters of the scenario needed to make those decisions. Study Up The main advise I can give is to read, not so much general articles on the web, but read some books and some of the papers you can understand on the above topics. Then find some code you can download and try after you understand the terminology well enough to know what to download. This means book searches and academic searches are much more likely to steer you in the right direction than general web searches.  There are thousands of posers in the general web space, explaining AI principles with a large number of errors.  Book and academic article publishers are more demanding of due diligence in their authors.",59.50400408,"This is completely feasible, but the way the inputs are mapped would greatly depend on the type of card game, and how it's played. I'll take into account a few possibilities: Does time matter in this game? Would a past move influence a future one? In this case, you'd be better off using Recurrent Neural Networks (LSTMs, GRUs, etc.). Would you like the Neural Network to learn off of data you collect, or learn on its own? If on its own, how? If you collect data of yourself playing the game tens or hundreds of times, feed it into the Neural Net, and make it learn from you, then you're doing something called ""Behavioural Cloning"". However, if you'd like the NN to learn on its own, you can do this 2 ways: a) Reinforcement Learning - RL allows the Neural Net to learn by playing against itself lots of times. b) NEAT/Genetic Algorithm - NEAT allows the Neural Net to learn by using a genetic algorithm. However, again, in order to get more specific as to how the Neural Net's inputs and outputs should be encoded, I'd have to know more about the card game itself.",57.84567455,"You would definitely want your network to know crucial information about the game, like what cards AI agent has(their values and types), mana pool, how many cards on the table and their values, number of the turn and so on. These things you must figure on your own, the question you should ask yourself is ""If I add this value to input how and why it will improve my system"". But the first thing to understand is that most of NNs are designed to have a constant input size, and I would assume this is matters in this game since players can have a different amount of cards in their hand or on the table. For example, you want to let NN know what cards it has, let's assume the player can have a maximum of 5 cards in his hand and each card can have 3 values(mana, attack and health), so you can encode this as 5*3 vector, where first 3 values represent card number one and so on. But what if the player has currently 3 cards, a simple approach would be to assign zeros to last 6 inputs, but this may cause problems since some cards can have 0 mana cost or 0 attack. So you need to figure out how to solve this problem. You may look for NN models that can handle variable input size or figure out how to encode input as a vector of constant size. Secondly, outputs are also constant size vectors. In case of this type of game, it can be a vector that encodes actions that the agent can take. So let's say we have 3 actions: put a card, skip turn and concede. So it can be one hot encoder, e.g. if you have 1 0 0 output, this means that agent should put some card. To know what card it should put you can add another element to output which will produce a number in the range of 1 to 5 (5 is max number of cards in the hand). But the most important part of training a neural network is that you will have to come up with a loss function that is suitable for your task. Maybe standard loss functions like Mean-squared loss or L2 will be good, maybe you will need to change them in order to fit your needs. This is the part where you will need to make a research. I've never worked with NEAT before, but as I understood correctly it uses some genetic algorithm to create and train NN, and GA use some fitness function to select an individual. So basically you will need to know what metric you will be using to evaluate how good you model performs and based on this metric you will change parameters of the model. PS.
It is possible to solve this problem with the neural network, however, neural networks are not magic and not the universal solution to all problems. If your goal is to solve this certain problem I would also recommend you to dig into the game theory and its application in the AI. I would say, that solving this problem would require complex knowledge from different fields of AI. However, If your goal is to learn about neural networks I would recommend taking much simpler tasks. For example, you can implement NN that will work on benchmark dataset, for example, NN that will classify digits from MNIST dataset. The reason for this is that a lot of articles was written about how to do classification on this dataset and you will learn a lot and you will learn faster from implementing simple things.",57.13854073,"I'm not sure if or how I would apply neural networks to make
selections with cards, which have a complex synergy. How could I
design and train a neural network for this game, such that it can take
into account all the variables? Is there a common approach? Advice 0. I would highly recommend to design it the way most neural networks are designed. The most common pattern is to have n layers, each one might have different size, one input layer, one output layer, a few hidden layers between them. Advice 1. Differentiate between neural network itself (brain), its environment (game conditions and rules), its sensors (inputs) and its ""hands"" (outputs). Why you might want to call it ""hands""?  If you simulate a player in a card game, you basically want him to use his hands to play. In other situations it might be legs, or wings, or even the gas pedal. How to design the inputs: Just create a neural network with a common pattern, figure out, what variables a real player would analyze before throwing a card, then try to translate those variables into signals. This step might actually be a bit tricky and it's also what actually happens in biological neural networks. The electrical signals in our brains are really weak, even though they might carry bits that make up big numbers.
What I mean by making weak signals out of numbers is dividing varying diapasons of numbers by their maximal values. For example, instead of putting in 5, which would represent a card with rank 6, divide 5 by 13 (or whatever value represents the highest rank). Basically if your diapason of input for one neuron is 0 to 13, divide it by 13 to get a value (or signal) in codomain [0; 1], which is a suitable input for the sigmoid function compared to the values in codomain [0; 13]. To visualize, take a look at the graph of the sigmoid function. The difference between f(10) and f(8) is ~0.00029, whereas the difference between f(10/13) and f(8/13) is ~0.034, which obviously has much more impact on the output. So, make sure you translate all the values into the diapason where your function is most ""sensitive"", in this case in [-4; 4] Advice 2. Every time you need a decision from the AI, create a pool of possible decisions (e. g. pool of possible cards the player can throw right now) that you can index by an integer. Then you might want to multiply the output value in its codomain [0; 1] by the amount of possible decisions - 1 to be able to interpret it as index. Or you might have the amount of output neurons that corresponds to the maximal amount of possible moves and interpret the index of the neuron with the highest value as the index of the array with possible decisions. How to train it: If you create AI for a game, I would recommend to take a look at genetic algorithms . The basic idea is to create a population of players (hundreds or thousands) with randomly generated weights and biases in their NNs, restrict their possibilities by the rules of the game, and let them play. Then perform selection by their fitness function, which in this case might just be the score of each player, crossover and mutation of their genes (of single bits or even numbers) to create the next generation, and so on. Repeat this process until you come up with a satisfying solution. I recommend you genetic algorithms for this case, because it might be quite hard to find training data for traditional methods of training NNs. And if you're able to generate training data yourself, then you might also be able to program all the behaviour manually, in which case you don't even need NNs.
If you're interested in training NNs using genetic algorithms, you should read some external literature, since it's a pretty big topic. You can also check out my github repo , where I train AI to play snake using GAs.",59.19757664,,,,,,,,
3981,Is it possible to train a neural network as new classes are given?,neural-networks,"I'd like to add to what's been said already that your question touches upon an important notion in machine learning called transfer learning . In practice, very few people train an entire convolutional network from scratch (with random initialization), because it is time consuming and relatively rare to have a dataset of sufficient size. Modern ConvNets take 2-3 weeks to train across multiple GPUs on ImageNet. So it is common to see people release their final ConvNet checkpoints for the benefit of others who can use the networks for fine-tuning. For example, the Caffe library has a Model Zoo where people share their network weights. When you need a ConvNet for image recognition, no matter what your application domain is, you should consider taking an existing network, for example VGGNet is a common choice. There are a few things to keep in mind when performing transfer learning : Constraints from pretrained models. Note that if you wish to use a pretrained network, you may be slightly constrained in terms of the architecture you can use for your new dataset. For example, you can’t arbitrarily take out Conv layers from the pretrained network. However, some changes are straight-forward: due to parameter sharing, you can easily run a pretrained network on images of different spatial size. This is clearly evident in the case of Conv/Pool layers because their forward function is independent of the input volume spatial size (as long as the strides “fit”). Learning rates. It’s common to use a smaller learning rate for ConvNet weights that are being fine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that computes the class scores of your new dataset. This is because we expect that the ConvNet weights are relatively good, so we don’t wish to distort them too quickly and too much (especially while the new Linear Classifier above them is being trained from random initialization). Additional reference if you are interested in this topic: How transferable are features in deep neural networks?",54.07596432,"Here is one way you could do that. After training your network, you can save its weights to disk. This allows you to load this weights when new data becomes available and continue training pretty much from where your last training left off. However, since this new data might come with additional classes, you now do pre-training or fine-tuning on the network with weights previously saved. The only thing you have to do, at this point, is make the last layer(s) accommodate the new classes that have now been introduced with the arrival of your new dataset, most importantly include the extra classes (e.g., if your last layer initially had 10 classes, and now you have found 2 more classes, as part of your pre-training/fine-tuning, you replace it with 12 classes). In short, repeat this circle :",58.03661263,"There are several ways to add new classes to the trained model, which require just training for the new classes. Incremental training ( GitHub ) continuously learn a stream of data ( GitHub ) online machine learning ( GitHub ) Transfer Learning Twice Continual learning approaches (Regularization, Expansion, Rehearsal) ( GitHub )",59.36577136,"You could use transfer learning (i.e. use a pre-trained model, then change its last layer to accommodate the new classes, and re-train this slightly modified model, maybe with a lower learning rate) to achieve that, but transfer learning does not necessarily attempt to retain any of the previously acquired information (especially if you don't use very small learning rates, you keep on training and you do not freeze the weights of the convolutional layers), but only to speed up training or when your new dataset is not big enough, by starting from a model that has already learned general features that are supposedly similar to the features needed for your specific task. There is also the related domain adaptation problem. There are more suitable approaches to perform incremental class learning (which is what you are asking for!), which directly address the catastrophic forgetting problem . For instance, you can take a look at this paper Class-incremental Learning via Deep Model Consolidation , which proposes the Deep Model Consolidation (DMC) approach. There are other continual/incremental learning approaches, many of them are described here or in more detail here .",53.78495213,"What you are after is called ""Class-incremental learning"" (IL). In this study they consider three classes of solutions: regularization-based solutions that aim to
minimize the impact of learning new tasks on the weights
that are important for previous tasks; exemplar-based solutions
that store a limited set of exemplars to prevent forgetting of
previous tasks; solutions that directly address the problem
of the bias towards recently-learned tasks. Their main findings are: For exemplar-free class-IL, data regularization methods outperform weight regularization methods. Finetuning with exemplars (FT-E) yields a good baseline that
outperforms more complex methods on several experimental
settings. Weight regularization combines better with exemplars than data
regularization for some scenarios. Methods that explicitly address task-recency bias outperform those that
do not. Network architecture greatly influences the performance of class-IL
methods, in particular the presence or absence of skip connections
has a significant impact.",51.56466489,,,,,,,,
3903,What would motivate a machine?,philosophy,"The current method to implement motivation is some kind of artificial reward. Deepmind's DQN for example is driven by the score of the game. The higher the score, the better. The AI learns to adjust its actions to get the most points and therefore the most reward. This is called reinforcement learing . The reward motivates the AI to adapt its actions, so to speak. In a more technical term, the AI wants to maximize utility, which depends on the implemented utility function . In the case of DQN, this would be maximizing the score in the game. The human brain functions in a similar fashion, although a little more complex and often not as straight forward. We as humans usually try to adjust our actions to produce a high output of dopamine and serotonin . This is in a way similar to the reward used to control AIs during reinforcement learning. The human brain learns which actions produce the most amount of those substances and finds strategies to maximize the output. This is, of course, a simplification of this complex process, but you get the picture. When you talk about motivation, please don't mix it up with consciousness or qualia . Those are not required for motivation at all. If you want to discuss consciousness and qualia in AI, that's a totally different ball game. A child isn't curious for the sake of curiosity. It gets positive reinforcement when exploring because the utility function of the child's brain rewards exploration by releasing rewarding neurotransmitters. So the mechanism is the same. Applying this to AI means defining a utility function that rewards new experiences. There is no inner drive without some kind of reinforcing reward.",50.24034108,"This is an interesting question actually. There's a quite realistic idea about ""where can the curiosity originate from"" in the book ""On intelligence"" written by Jeff Hawkins and Sandra Blakeslee. It's based on such statements: Mind creates its own model of the world it exists in. It makes predictions about everything all the time (actually Jeff Hawkins states that this is the main characteristic of intelligence). When prediction about something wasn't followed by appropriate behavior of the world, then this thing gets very interesting to the mind (the model is wrong and should be corrected) and needs more attention. For example, when you look at left human eye your brain predicts that it's a human face and there should be second eye to the right. You look to the right and see a.. nose! What a surprise! It now takes all your attention and you have this motivation to make more observations about such a strange thing that did not fit into your model. So I'd say that AI might do something certain according to its model or behave randomly while the predictions it is making about the world are true. But once some prediction is broken the AI gets a motivation to do error-correction to its model. In a simple case a machine starts at a total randomness just doing everything it can with its output. While it has no model or a random model when it detects some kind of order or repeated patterns it is getting ""interested"" and adds it to the model. After a while, the model becomes more sophisticated making more complex predictions and detecting higher level mistakes in a model. Slowly it gets to know what to do to observe something interesting to it, instead of just remembering everything.",50.80797711,"I asked professor Richard Sutton a similar question, in the first lecture of the reinforcement learning course. It seems that there are different ways to motivate the machine. In fact, machine motivation seems to me like a dedicated field of research. Typically, machines are motivated by what we call an objective function or a cost function or a loss function . These are different names for the same concept. Sometimes, they are denoted by $$L(a)$$ The goal of the machine is then to solve either a minimization problem, $\min_a L(a)$ , or a maximization problem, $\max_a L(a)$ , depending on the definition of $L$ .",57.10577862,"I've spent some time thinking about this in the context of games. The problem with reward functions is that they generally involve weighting nodes, which is useful but ultimately materially meaningless. Here are two materially meaningful rewards: COMPUTATIONAL RESOURCES Consider a game where an AI is competing not for points, but for processor time and memory. The better the algorithm performs at the game, the more memory and processing it has access to.  This has a practical effect--the more resources available to the automata, the stronger its capabilities.  (i.e. it's rationality is less bounded in terms of time and space to make a decision.)  Thus the algorithm would be ""motivated"" to prevail such a contest. ENERGY Any automata with a sufficient degree of ""self awareness"", here specifically referring to the knowledge that it requires energy to process, would be motivated to self-optimize its own code to eliminate unnecessary flipping of bits (unnecessary energy consumption.) Such an algorithm would also be motivated to ensure its power supply so that it can continue to function.",50.94211199,"I think we give ourselves too much credit by already referring to our algorithms and machines as actually thinking and acting on motivations. In my opinion we still have a bit to go before we can actually refer to a human creation as thinking or being able to have motivations more then basic physical ones. By that I would say that a Machines' or AI algorithms' motivations are similar to a car engine. Simple and basic, the ""motivations"" of a car engine to run are just the first and second laws of thermodynamics, namely the conservation of energy and the exchange between energy types, and the always increasing level of entropy in a closed system. By having a really specific design, we can insert fuel in the system and create a lot of potential energy which will ""motivate"" the engine to transform it in other types of energy (heat, sound, etc.) An AI algorithm is exactly the same, it's just that now we're playing with electricity. By putting multiple levels of abstractions from the actual level of electrons moving through wire, up to your python Deep Learning algorithm training to learn how to recognize images of dogs. The concept is similar in my opinion, for now we do not have machines that are complex enough to have higher-level motivations, or even develop them by themselves. As the other answers pointed out, specific algorithms, namely reinforcement learning try to emulate those ""needs"" and ""motivations"", but in the end in my opinion, for now, they are still just emulations. Similar to other Deep learning algorithms, the same basic concept described at the beginning applies, trying to minimize the error by emulating different concepts that we know, as conservation of energy, following the path of least resistance, maintaining the laws of entropy, etc.",52.11175474,"Good verses bad?  From an imaging standpoint, humans/biologicals tend to notice details not “imagined”.  A “good” feedback learning loop might be refining the correctness of the imagined. Using efforts to reduce the number or amount discrepancies between imagined or predicted environment and perceived reality as measured against “beneficial” outcomes.  I’m not sure what a Ai wood think of as good/bad in the abstract, in my limited considerations of imagination in humans, I seem most useful in recognizing things that be mor meaning than the general background.  As the imagination becomes more able to predict a more “real” outcome, it is more useful to the individual in predicting a more correct outcome and greater usefulness in defining things that deverge form the expected.  Sort simplistic I realize, but intelligence might be as simple as refining.the ability to imagine a result and judging the more meaningful of the descrpencies .",50.34348783,,,,,,
3850,Can a neural network be used to predict the next pseudo random number?,neural-networks,"If we are talking about a perfect RNG, the answer is a clear no . It is impossible to predict a truly random number, otherwise it wouldn't be truly random. When we talk about pseudo RNG, things change a little. Depending on the quality of the PRNG, the problem ranges from easy to almost impossible. A very weak PRNG like the one XKCD published could of course be easily predicted by a neural network with little training. But in the real world things look different. The neural network could be trained to find certain patterns in the history of random numbers generated by a PRNG to predict the next bit. The stronger the PRNG gets, the more input neurons are required, assuming you are using one neuron for each bit of prior randomness generated by the PRNG. The less predictable the PRNG gets, the more data will be required to find some kind of pattern. For strong PRNGs this is not feasable. On a positive note, it is helpful that you can generate an arbitrary amount of training patterns for the neural network, assuming that you have control over the PRNG and can produce as many random numbers as you want. Because modern PRNGs are a key component for cryptography, extensive research has been conducted to verify that they are ""random enough"" to withstand such prediction attacks. Therefore I am pretty sure that it is not possible with currently available computational resources to build a neural network to successfully attack a PRNG that's considered secure for cryptography. It is also worth noting that it is not necessary to exactly predict the output of a PRNG to break cryptography - it might be enough to predict the next bit with a certainty of a little more than 50% to weaken an implementation significantly. So if you are able to build a neural network that predicts the next bit of a PRNG (considered secure for cryptography) with a 55% success rate, you'll probably make the security news headlines for quite a while.",61.87963785,"Old question, but I thought it's worth one practical answer. I happened to stumble upon it right after looking at a guide of how to build such neural network, demonstrating echo of python's randint as an example . Here is the final code without detailed explanation, still quite simple and useful in case the link goes offline: from random import randint
from numpy import array
from numpy import argmax
from pandas import concat
from pandas import DataFrame
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense

# generate a sequence of random numbers in [0, 99]
def generate_sequence(length=25):
    return [randint(0, 99) for _ in range(length)]

# one hot encode sequence
def one_hot_encode(sequence, n_unique=100):
    encoding = list()
    for value in sequence:
        vector = [0 for _ in range(n_unique)]
        vector[value] = 1
        encoding.append(vector)
    return array(encoding)

# decode a one hot encoded string
def one_hot_decode(encoded_seq):
    return [argmax(vector) for vector in encoded_seq]

# generate data for the lstm
def generate_data():
    # generate sequence
    sequence = generate_sequence()
    # one hot encode
    encoded = one_hot_encode(sequence)
    # create lag inputs
    df = DataFrame(encoded)
    df = concat([df.shift(4), df.shift(3), df.shift(2), df.shift(1), df], axis=1)
    # remove non-viable rows
    values = df.values
    values = values[5:,:]
    # convert to 3d for input
    X = values.reshape(len(values), 5, 100)
    # drop last value from y
    y = encoded[4:-1,:]
    return X, y

# define model
model = Sequential()
model.add(LSTM(50, batch_input_shape=(5, 5, 100), stateful=True))
model.add(Dense(100, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
# fit model
for i in range(2000):
    X, y = generate_data()
    model.fit(X, y, epochs=1, batch_size=5, verbose=2, shuffle=False)
    model.reset_states()
# evaluate model on new data
X, y = generate_data()
yhat = model.predict(X, batch_size=5)
print('Expected:  %s' % one_hot_decode(y))
print('Predicted: %s' % one_hot_decode(yhat)) I've just tried and it indeed works quite well! Took just a couple of minutes on my old slow netbook. Here's my very own output, different from the link above and you can see match isn't perfect, so I suppose exit criteria is a bit too permissive: ...
 - 0s - loss: 0.2545 - acc: 1.0000
Epoch 1/1
 - 0s - loss: 0.1845 - acc: 1.0000
Epoch 1/1
 - 0s - loss: 0.3113 - acc: 0.9500
Expected:  [14, 37, 0, 65, 30, 7, 11, 6, 16, 19, 68, 4, 25, 2, 79, 45, 95, 92, 32, 33]
Predicted: [14, 37, 0, 65, 30, 7, 11, 6, 16, 19, 68, 4, 25, 2, 95, 45, 95, 92, 32, 33]",51.2479477,"Being a  complete newbie in machine learning, I did this experiment (using Scikit-learn ): Generated a large number (N) of pseudo-random extractions, using python random.choices function to select N numbers out of 90. Trained a MLP classifier with training data composed as follow: ith sample : X <- lotteryResults[i:i+100], Y <- lotteryResults[i] In practice, I aimed to a function that given N numbers, coud predict the next one. Asked the trained classificator to predict the remaining numbers. Results: of course, the classificator obtained a winning score comparable with the one of  random guessing or of other techniques not based on neural networks (I compared results with several classifiers available in scikit-learn libraries ) however, if I generate the pseudo-random lottery extractions with a specific distribution function, then the numbers predicted by the neural network are roughly generated with the same distribution curve ( if you plot the occurrences of the random numbers and of the neural network predictions, you can see that that the two have the same trend, even if in the predicytions curve there are many spikes. So maybe the neural network is able to learn about pseudo-random number distributions ? If I reduce the size of the training set under a certain limit, I see that the classifier starts to predict always the same few numbers, which are among the most frequent in the pseudo-random generation. Strangely enough ( or maybe not ) this behaviour seem to slightly increase the winning score.",69.70841149,"Adding to what Demento said, the extent of randomness in the Random Number Generation Algorithm is the key issue. Following are some designs that can make the RNG weak: Concealed Sequences Suppose this is the previous few sequences of characters generated: (Just an example, for the practical use larger range, is used) lwjVJA
Ls3Ajg
xpKr+A
XleXYg
9hyCzA
jeFuNg
JaZZoA Initially, you can't observe any pattern in the generations but changing them to Base64 encoding and then to hex, we get the following: 9708D524
2ECDC08E
C692ABF8
5E579762
F61C82CC
8DE16E36
25A659A0 Now if we subtract each number form the previous one, we get this: FF97C4EB6A
97C4EB6A
FF97C4EB6A
97C4EB6A
FF97C4EB6A
FF97C4EB6A This indicates that the algorithm just adds 0x97C4EB6A to the previous value, truncates the result to a 32-bit number, and Base64-encodes the data. The above is a basic example. Today's ML algorithms and systems are capable enough to learn and predict more complex patterns. Time Dependency Some RNG algorithms use time as the major input for generating random numbers, especially the ones created by developers themselves to be used within their application. Whenever weak RNG algorithms is implemented that appear to be stochastic, they can be extrapolated forwards or backwards with perfect accuracy in case sufficient dataset is available.",55.80322881,"I did a project that does that - or at least have an advantage over an rollover game of dice. Basically the just of it is doing some NAND gates with the previous outcomes. The current result of those NAND gates is used to mask current input that is trained on and also the output that is learned on. Additionally I've specific layers structure (with batched bi-directional 6 layered LSTM -> 10 convolution layers with increasing different dilation at % 2 and then a transformer) and use LBFGS-B algorithm with NADAM and fluctuating learning rate different for each layer (for LSTM and convolutions it's higher then the transformer). I train each 400 outcomes iterations and although I'm not sure if I can fully achieve 100% accuracy in long term I'm in ""profit"" betting against the PRNG. The most important part however is switching to new LSTM output hidden and cell state after each iteration depending on ""key condition"" that determines the path the model will improve on I think. It's an if statement and I'm not sure if it can be made anything else. Currently the condition is if we have ""lost"" more than we have won previous iteration (and that includes payout since we want profit).",51.51499866,,,,,,,,
3847,Is transistor the first artificial intelligence?,history,"I think it might come down to whether the transistor is making a decision.  If the transistor is being used as a switch, that would seem to qualify as a decision, even though it's an extremely rudimentary decision. Intelligence, in reference to Artificial (or Algorithmic) Intelligence, is not restricted to high intelligence.  A brute force Tic-Tac-Toe AI has extremely low, narrow intelligence but still constitute AI.  An automatic switch, which makes the most simple decision possible, a binary choice, would seem to be the most basic form of intelligence. Norvig's definition seems rooted in game theory, which is important in terms of utility of intelligence.  But in a condition of intractability one is only assuming one's decision is more optimal than other choices. Outcomes can be evaluated, and a determination made as to whether the algorithm was ""smart"" or ""dumb"", but these terms refer to relative positions on a spectrum. It's worth noting this fundamental definition of AI opens up a can of worms in that pre-transistor, automated mechanical switches such as the Strowger switch would also probably qualify. And automata do not have to be electrical.  The History of Automation wiki suggests the first feedback control system was for a water clock invented by Ctesibius .  This device dates to the 3rd Century BCE, and water clocks were said to be the most accurate time pieces until Huygens . This type of intelligence I tend to think of as autonomic , in the sense of involuntary, and distinct from higher functions, which are more commonly associated with ""intelligence"". Note: The characteristics of an autonomic system in a computing context are quite interesting and include self-optimization , self-learning, and self-awareness . ADDENDUM: After much thought, in reference to @JT's answer, I can't see how decisions can be separated from goals—there needs to be an intent, or the decision is merely random. This might prompt the question ""can a simple switch be said to have a goal?""",60.49608369,"Replacing my previous ill conceived answer with this definition of Intelligence from Richard Sutton (a founder and leader Reinforcement Learning) should answer your question. John McCarthy long ago gave one of the best definitions: ""Intelligence
  is the computational part of the ability to achieve goals in the
  world”. That is pretty straightforward and does not require a lot of
  explanation. It also allows for intelligence to be a matter of degree,
  and for intelligence to be of several varieties, which is as it should
  be. Thus a person, a thermostat, a chess-playing program, and a
  corporation all achieve goals to various degrees and in various
  senses. For those looking for some ultimate ‘true intelligence’, the
  lack of an absolute, binary definition is disappointing, but that is
  also as it should be. The part that might benefit from explanation is what it means to
  achieve goals. What does it mean to have a goal? How can I tell if a
  system really has a goal rather than seems to? These questions seem
  deep and confusing until you realize that a system having a goal or
  not, despite the language, is not really a property of the system
  itself. It is in the relationship between the system and an observer.
  (In Dennett's words, it is a ‘ stance ’ that the observer take with
  respect to the system.) What is it in the relationship between the system and the observer
  that makes it a goal-seeking system? It is that the system is most
  usefully understood (predicted, controlled) in terms of its outcomes
  rather than its mechanisms. Thus, for a home-owner a thermostat is
  most usefully understood in terms of its keeping the temperature
  constant, as achieving that outcome, as having that goal. But if i am
  an engineer designing a thermostat, or a repairman fixing one, then i
  need to understand it at a mechanistic level—and thus it does not have
  a goal. The thermostat does or does not have a goal depending of the
  observer. Another example is the person playing the chess computer. If
  I am a naive person, and a weaker player, I can best understand the
  computer as having the goal of beating me, of checkmating my king. But
  if I wrote the chess program (and it does not look very deep) I have a
  mechanistic way of understanding it that may be more useful for
  predicting and controlling it (and beating it). Putting these two together, we can define intelligence concisely
  (though without much hope of being genuinely understood without
  further explanation): Intelligence is the computational part of the
  ability to achieve goals. A goal achieving system is one that is more
  usefully understood in terms of outcomes than in terms of mechanisms",53.51805626,"Transistor is very similar in its function to single neuron and because of that one transistor could be considered to be a very tiny neural network - from this perspective it could be considered to be a form of artificial intelligence on their own. But transistor is not the first building block of AI, the first building blocks are the smallest particles that are possible in phisics - those are the building blocks of any machine.
Also the definition is somehow hard to accept, as there is no reason why thermostat can be considered AI on their own and transistor not. Since thermostat needs an environment in which it works in a way that fulfills the definition of AI, so the transistor could be put in such environment - there's no difference. We consider that something is AI when it maximizes chnace of some goal, but who is defining  this goal? We do. So if you say that the goal of some AI system is to have the output current equal to input current multiplied by factor of x, then you can say that bipolar junction transistor is the AI that does exactly that. Of course you can build a processor and memory from such transistors, then write software for eg. face recognition for a computer builded from them. It is all about connecting algorithms together to perform more sophisticated functions. The first ""logical gates"" (and builing blocks of AI) are the smallest particles possible, the smallest particles that can interact with each other to produce some output available to be used in next interaction - the whole universe works bacause of that - the matter just follows the algorithms - laws of phisics. On the side note: The only thing that for me is not algorithmical is consciousness, I think it is impossible to be created by just algorithms working together. Algorithms doesn't have any feelings and any number of them will not have feelings either - this will only process information without any consciousness inside. Intelligence is computational, consiciousness somehow reaches beyond phisical/algorithmical world - it is worth consideration - I personally think that we are like players in a game and there is other dimension/world from which our consciousness comes from, good question is: what are the rules of this game?",64.57460486,"It's instructive to know the definition of the transistor . Transistors are electronic components that are used for amplifiers, as circuit breakers, as connectors, as voltage controllers, for signal modulation and others functions. (An analogy is that the transistor functions as an electric ""faucet"" that regulates input and output voltage.) The definition of AI by Andreas Kaplan and Michael Haenlein defines artificial intelligence as: ""The ability of the system to interpret external data correctly, to learn from that data, and to use that learning to achieve certain goals and tasks through flexible adaptation."" [Citation Needed] Under this definition, transistors are not part of AI, because transistors do not learn and cannot adapt.  Transistors are just electronic devices for regulating current.",58.74469025,"The question is based on two concepts: First artificial intelligence (AI) The transistor is an intelligent device. Let us talk about the first AI, why transistors, the same definition of intelligence can be applied to Vaccum tubes, and they definitely existed before transistors. So no matter what definition you decide for intelligence, transistors are not the first AI . Now we come to the next part, what is artificial intelligence, like intelligence, the definition of artificial intelligence has undergone changes, in the last 6 decades. If you use the definition loosely, almost any device can be intelligent even an electric bulb.",68.61352098,,,,,,,,
3573,"Is there a rigorous proof that AGI is possible, at least, in theory?",philosophy,"A strong reason why people think the mind can be implemented on a Turing Machine stems from the Computational Theory of Mind (CTOM) , which is the leading theory of mind for now. There are lots of reasons for supporting the CTOM, one of which being that the language of belief/desire psychology (propositional attitudes over mental representations) seems to fit nicely to a computational framework. But most simply is that the computation analogy is very helpful in fields such as psychology and neuroscience. When we know of an input/output pair, but don't know how it is implemented, we could say ""its performing the relevant computation"". And since Turing showed that any computation can be performed on an appropriate Turing Machine, the natural extension is that the mind can be implemented on a computer. However, the CTOM is more of a useful idea than a complete theory. We still don't know how to analyze thought in a logical syntax, which can be implemented in a computer. And we also don't know how/why ""computation"" (whatever that means in this sentence) is performed in the brain.",52.28259555,"Consciousness is not well-understood As an AI practitioner and philosopher, I don't think that humans will be able to create a truly conscious silicon-based AGI. Humans are incapable of creating some ""thing"" from fiat (a decree). It's never happened in human history. The innovation cycle must begin with some ""thing"" (some ""stuff"" of some kind), and consciousness is not a thing. The essence of consciousness is imperceptible (it is unseen), like gravity, and attraction. Humans are incapable of creating things that they are unable to observe. Even if they are able to observe it, the human perceptive ability is unable to actually perceive the true essences of things seen, much less those unseen. Humans do not adequately understand the ""essence"" and ""nature"" of consciousness - which is a fundamental prerequisite to creating ""anything"" at all. The "" easy "" problems, those physical by nature, although not yet solved by empirical domains of psychology, cognitive science, and neuroscience, are expected to be solved in time. Regardless, they are ""not"" yet solved today . The "" hard "" problems, those determining why or how consciousness occurs given the right arrangement of brain matter, might not ever be solved , since it must explain why certain physical mechanism gives rise to consciousness instead of ""something else"" or ""nothing at all"". This is significant and is the most damning of all arguments against the idea of humans creating true existential consciousness in silicon creatures as a whole. Dualism vs physicalism The greatest philosophical debate on consciousness has focused on the distinction between dualism and physicalism . Dualism is the theory that consciousness somehow falls outside the domain of the physical (these are the hard problems) Physicalism holds that consciousness is entirely physical. ( significant arguments below view it as false ). Problems with dualist views Why would one be motivated to hold a dualist view? How can something that is not part of the physical world interact with the physical world? That seems impossible! The physical world is a closed system , how can you have a consciousness that is not part of a closed system? Consciousness is a lot like mass or charges, it's a philosophically ""fundamental"" thing , you either ""have it or you don't"", you can simulate them, but you cannot existentially ""be"" them unless you have those specific ""properties"" , and behavior ""simulating"" human consciousness is not a fundamental thing. So, despite the sensationalist tendencies of rogue journalists ""parroting"" wildly spectacular concepts from the fringe camps of the transhumanists (aka science fiction) - a quick perusal of the more rigorous communities of the grounded and thoughtful philosophers camp strongly and convincingly argues otherwise. More musings on physicalism Actually, consciousness has never been properly explained by the biomechanical, which is more or less the key issue of all philosophical studies of the mind - which is essentially the study of consciousness. Physicalists have trouble explaining several aspects of consciousness in a way that is consistent with our "" observations "" of how physical properties interact. Let me list a few more problems, with a reference to the titans of philosophy. Arguments against physicalism It is impossible to imagine how mere neuronal tissue could produce conscious experience ( Huxly ) Failures of supervenience, such as zombies and inverted spectra , are conceivable ( David Chalmers , John Locke, etc.). Mary learns something ( Frank Jackson ). Brains have mass, volume, and other physical properties, but experiences do not. Paranormal phenomena (near-death experiences NDErs , ESP, etc) are real, and involve consciousness implemented in a nonphysical substrate. If shrunken so I can stroll around your brain and look about, I will observe neuronal processes, not experiences (G. W. Leibniz). The soul is the seat of consciousness, and the soul is not physical. (Theological constraints recognized BTW...). Conscious experiences have intrinsic qualities , but science can only tell us about relational qualities (Russell, Rosenberg). Consciousness cannot be observed ; there will never be a consciousness detector that can tell you if a given creature is conscious. Conscious experiences are not simply the movement of molecules, consciousness is more than mass in motion (Mill, Ward).",52.08048718,"I'm going to go out on a limb and suggest that this is a matter of evolution, that humans are in no way exceptional in the grand scheme, and that AGI will manifest so long as technology advances, because human consciousness is simply a matter of complexity of the system. The idea comes out of emergent complexity in Conway's Game of Life. In Conway's words: ""There are Life patterns which behave like self-replicating animals… It’s probable, given a large enough Life space, initially in a random state, that after a long time, intelligent self-replicating animals will emerge and populate some parts of the space.” Source: Winning Ways for Your Mathematical Plays I came across a paper Computation in Cellular Automata:
A Selected Review , which I am still working my way through, and which you may find interesting. For those who use philosophical arguments to make the case algorithmic consciousness is not possible, I'd posit the question ""how do we know we're conscious?"", not because I'm interested in the answer, but merely to throw a wrench into that line of inquiry. Because ultimately it doesn't matter. Consciousness in the sense of human awareness is not a requirement of life, and the most basic definition of consciousness is awareness of any kind, no matter how trivial. I find the idea that there is something ""magical"" about human consciousness, that ideas are not things because they do not have material form, to be problematic. Intangibility I don't have a problem with, as intangibles clearly interact with the physical world. (As an analogy, I studied for many years with a famous Tai Chi teacher who never talked about ""chi"".  I suspect this disinclination derived from the way in which the concept of ""chi"" leads to magical thinking, which is illusory as opposed to practical. The practice and application of Tai Chi techniques is purely a matter of physics and physiology, even when such applications seem to defy natural laws. Possibly there is something going on that we don't understand, but if that were the case, such phenomena are natural in origin.) We know there is randomness in nature at the quantum level, and if this proves to be a component of human consciousness, we can use quantum computing to provide a medium for artificial consciousness.",52.2613956,"Rather than prove that Artificial General Intelligence is possible, I would consider an argument for why it is impossible . We start by defining what we mean by AGI. You state that the human mind can be replicated by a Turing Machine, and therefore AGI should be possible. This seems to imply that humans have `General' (capital G) intelligence. By this I mean that you are implying that with enough time, humans can learn any task or problem. However, if you are asserting that humans minds are machines replicable by Turing machines, you must also concede that they have some finite representational power. Finite representational power implies that there will always be problems or tasks where our intelligence will fail (a consequence of the No Free Lunch Theorem ). Fortunately (maybe unfortunately), finite representational power is what allows us to learn at all: VC Dimension (a measure of the complexity or representational power of a class of functions that a learning algorithm can learn [also here and here ]) implies that a learning algorithm that can learn any problem is actually useless, as the ability to explain any set of data yields the requirement that the algorithm see an infinite amount of examples in order to generalize. While this result comes from the relatively constrained class of binary classification problems in the statistical learning setting, the intuition seems to apply more broadly. To summarize, I would refer to this quote from Shalev-Shwartz and Ben-David (2014) : If someone can explain every phenomenon, his explanations are worthless. It truly is the case that our decision to systematically ignore some possible outcomes is the only thing that allows use to learn useful representations of real-world problems.",53.07257566,"Although it is not a rigorous proof, Marvin Minsky's book, The Society of Mind gives us a blueprint for creating a ""mind"" (general intelligence).  In his book, he posits that by combining mindless components (""agents"") together in various competing and cooperative structures, we can create actual minds. IMHO, the recent popularity of Boosting, Bagging, Stacking, and other ensemble techniques will eventually evolve (through research) into Marvin Minsky's ""agent"" metaphor. Subsequently, as we learn to make these agents compete and cooperate (looks like this has recently begun with Generative Adversarial Networks ), we will be able to write ""programs"" that mimic (or surpass) the human mind.",54.98161559,,,,,,,,
3494,Why is Python such a popular language in the AI field?,machine-learning,"Python comes with a huge amount of inbuilt libraries. Many of the libraries are for Artificial Intelligence and Machine Learning. Some of the libraries are TensorFlow (which is a high-level neural network library), scikit-learn (for data mining, data analysis and machine learning), pylearn2 (more flexible than scikit-learn), etc. The list keeps going and never ends. You can find some libraries here . Python has an easy implementation for OpenCV. What makes Python favorite for everyone is its powerful and easy implementation. For other languages, students and researchers need to get to know the language before getting into ML or AI with that language. This is not the case with Python . Even a programmer with very basic knowledge can easily handle Python. Apart from that, the time someone spends on writing and debugging code in Python is way less when compared to C, C++ or Java. This is exactly what the students of AI and ML want. They don't want to spend time on debugging the code for syntax errors, they want to spend more time on their algorithms and heuristics related to AI and ML Not just the libraries but their tutorials, handling of interfaces are easily available online . People build their own libraries and upload them on GitHub or elsewhere to be used by others. All these features make Python suitable for them.",55.68190968,"Practically all of the most popular and widely used deep-learning frameworks are implemented in Python on the surface and C/C++ under the hood. I think the main reason is that Python is widely used in scientific and research communities, because it's easy to experiment with new ideas and code prototypes quickly in a language with minimal syntax like Python. Moreover there may be another reason. As I can see, most of the over-hyped online courses on AI are pushing Python because it is easy for newbie programmers. AI is the new marketing hot word to sell programming courses.
( Mentioning AI can sell programming courses to kids who want to build HAL 3000, but can not even write a Hello World or drop a trend-line onto an Excel graph. :)",55.98351801,"What attracts me to Python for my analysis work is the ""full-stack"" of tools that are available by virtue of being designed as a general purpose language vs. R as a domain specific language. The actual data analysis is only part of the story, and Python has rich tools and a clean full-featured language to get from the beginning to the end in a single language (use of C/Fortran wrappers notwithstanding). On the front end, my work commonly starts with getting data from a variety of sources, including databases, files in various formats, or web scraping. Python support for this is good and most database or common data formats have a solid, well-maintained library available for the interface. R seems to share a general richness for data I/O, though for FITS the R package appears not to be under active development (no release of FITSio in 2.5 years?). A lot of the next stage of work typically occurs in the stage of organizing the data and doing pipeline-based processing with a lot of system-level interactions. On the back end, you need to be able present large data sets in a tangible way, and for me, this commonly means generating web pages. For two projects I wrote significant Django web apps for inspecting the results of large Chandra survey projects. This included a lot of scraping (multiwavelength catalogs) and so forth. These were just used internally for navigating the data set and helping in source catalog generation, but they were invaluable in the overall project. Moving to the astronomy-specific functionality for analysis, it seems clear that the community is solidly behind Python. This is seen in the depth of available packages and level of development activity, both at an individual and institutional level ( http://www.astropython.org/resources ). Given this level of infrastructure that is available and in work, I think it makes sense to direct effort to port the most useful R statistical tools for astronomy to Python. This would complement the current capability to call R functions from Python via rpy2.If you are interested, I strongly recommend that you read this article, here it is a question of comparing programming languages https://diceus.com/what-technology-is-b ... nd-java-r/ I hope it helps.Good Luck",55.90992637,"Python has a standard library in development, and a few for AI. It has an intuitive syntax, basic control flow, and data structures. It also supports interpretive run-time, without standard compiler languages. This makes Python especially useful for prototyping algorithms for AI.",54.08012721,"It's a mix of many factors that together make it a very good option to develop cognitive systems. Quick development Rapid prototyping Friendly syntax with almost human-level readability Diverse standard library and multi-paradigm It can be used as a frontend for performant backends written in compiled languages such as C/C++. Existing performant numerical libraries, such as numpy and others already do the intensive bulk work for you which lets you focus more on architectural aspects of your system. Besides, there is a very big community and ecosystem around Python, which results in a diverse set of available tools oriented to diffent kind of tasks.",52.22756778,"That’s because python is a modern scripting object-oriented programming language that has stylish syntax. Contrary to structural programming languages like java and C++, its scripting nature enables the programmer to test his/her hypothesis very fast. Furthermore, there are lots of open source machine learning libraries (including scikit-learn and Keras) that broaden the use of python in AI field.",57.9359667,"Python has rich library, it is also object oriented, easy to program. It can be also used as frontend language. That's why it is used in artificial intelligence. Rather than AI it is also used in machine learning, soft computing, NLP programming and also used as web scripting or in Ethical hacking.",53.63610414,"I actually prefer C for machine learning. Because like in life, in the world as we know it, consists of never-ending ""logic gates"" (which basically is like flipping a coin - there WILL be 2 possible outcomes - not counting the third: landing on the side!). Which also means that while the universe seems never-ending, we still never stop finding those things that are even smaller than the last smallest thing, right? So... To put it in a context when programming C, I can control the memory usage more efficiently by coding smaller snippets that get combined, to always form smaller & efficient ""code-fragments"", that make up what we would call ""cells"" in biology (it got a measurable function, and has some pre-set properties). Thus, I like to optimize for low RAM usage, low CPU usage etc. when programming AI. I have only done feedforward with a basic genetic algorithm in C, but the more advanced recurrent neural network I wrote in C++ (ONLY because of the simplicity of using std::vector<TYPE> name; , so I wrote my own cvector.c : https://pastebin.com/sBbxmu9T & cvector.h : https://pastebin.com/Rd8B7mK4 & debug: https://pastebin.com/kcGD8Fzf - compile with gcc -o debug debug.c cvector.c ). That actually helped a lot in the quest of optimizing CPU usage (and overall runtime) when creating optimized neural networks. EDIT: So I am in one sense really see the opposite of what AlexPnt sees, when it comes to exploring what is possible within the realm of a ""self"".",51.33265514,"We also work with Python in our company . One of the sphere that we use it for is fast prototyping and building highly scalable web applications. For over two decades, our Python developers have been providing businesses with full-stack web-development services, client-server programming and administration. We help our clients build high-load web portals, automation plugins, high-performance data-driven enterprise systems, and many more.",52.90089322
3389,Could a neural network detect primes?,neural-networks,"Early success on prime number testing via artificial networks is presented in A Compositional Neural-network Solution to Prime-number Testing , László Egri, Thomas R. Shultz, 2006 . The knowledge-based cascade-correlation (KBCC) network approach showed the most promise, although the practicality of this approach is eclipsed by other prime detection algorithms that usually begin by checking the least significant bit, immediately reducing the search by half, and then searching based other theorems and heuristics up to $floor(\sqrt{x})$ .  However the work was continued with Knowledge Based Learning with KBCC , Shultz et. al. 2006 There are actually multiple sub-questions in this question.  First, let's write a more formal version of the question: ""Can an artificial network of some type converge during training to a behavior that will accurately test whether the input ranging from $0$ to $2^n-1$ , where $n$ is the number of bits in the integer representation, represents a prime number?"" Can it by simply memorizing the primes over the range of integers? Can it by learning to factor and apply the definition of a prime? Can it by learning a known algorithm? Can it by developing a novel algorithm of its own during training? The direct answer is yes, and it has already been done according to 1. above, but it was done by over-fitting, not learning a prime number detection method.  We know the human brain contains a neural network that can accomplish 2., 3., and 4., so if artificial networks are developed to the degree most think they can be, then the answer is yes for those.  There exists no counter-proof to exclude any of them from the range of possibilities as of this answer's writing. It is not surprising that work has been done to train artificial networks on prime number testing because of the importance of primes in discrete mathematics, its application to cryptography, and, more specifically, to cryptanalysis.  We can identify the importance of digital network detection of prime numbers in the research and development of intelligent digital security in works like A First Study of the Neural Network Approach in the RSA Cryptosystem , G.c. Meletius et. al., 2002 .  The tie of cryptography to the security of our respective nations is also the reason why not all of the current research in this area will be public.  Those of us that may have the clearance and exposure can only speak of what is not classified. On the civilian end, ongoing work in what is called novelty detection is an important direction of research.  Those like Markos Markou and Sameer Singh are approaching novelty detection from the signal processing side , and it is obvious to those that understand that artificial networks are essentially digital signal processors that have multi-point self tuning capabilities can see how their work applies directly to this question.  Markou and Singh write, ""There are a multitude of applications where novelty detection is extremely important including signal processing, computer vision, pattern recognition, data mining, and robotics."" On the cognitive mathematics side, the development of a mathematics of surprise, such as Learning with Surprise: Theory and Applications (thesis), Mohammadjavad Faraji, 2016 may further what Ergi and Shultz began.",54.23451906,"I'm an undergraduate researcher at Prairie View A&M university. I just spent a few weeks tweaking an MLPRegressor model to predict the $n$ th prime number. It recently stumbled into a super low minimum, where the first $1000$ extrapolations outside of the training data produced error less than $.02$ percent. Even at $300000$ primes out, it was about $.5$ percent off. My model was simple: $10$ hidden layers, trained on a single processor for less than 2 hours. To me, it begs the question, ""Is there a reasonable function that produces the nth prime number?"" Right now, the algorithms become computationally very taxing for extreme $n$ . Check out the time gaps between the most recent largest primes discovered. Some of them are years apart. I know it's been proven that if such a function exists, it will not be polynomial.",55.59212661,"In theory, a neural network can approximate any given function. This result is known as the universal approximation theorem . However, if you train a network with the numbers $0$ to $N$ , you cannot guarantee that the network will classify numbers outside that range correctly ( $n > N$ ). Such a network would be a regular feed-forward network (or MLP ) as recurrency does not add anything to the classification of the given input. The number of layers and nodes of that NN can only be found through trial and error.",54.21519882,"From a purely theoretic standpoint, yes. Such a neural network(NN) can be manually constructed. A neuron is very similar to a logic gate. And it is definitely possible to use them as logic gate, replicate the structure of a CPU, and then perform any algorithm we already have. So in theory, YES. (Albeit obviously dumb to do so) The more complicated question is whether NN can ""learn"" to detect primes. Given that it is theoretically possible, and also that the human brain is literally NN, I am very inclined to guess that it is possible. But not with a feedforward network. Prime detection is not a very NN friendly task for two reasons: It is discrete maths, and NN are more fuzzy in nature. All currently known algorithm of prime detection requires some sort of loop / iteration, making them complexity >= O(Bits). Where as feed forward NN is constant time complexity. The second difficulty will largely determine the shape of the final NN. Assuming there is no constant time detection algorithm of prime ever possible, the NN will not be feed forward. A recurrent network is required such that the NN can deal with loops. The first difficulty means that the NN will likely be rather complex and a lot of examples will be needed to overcome the inherent fuzziness. But that I guess is very much expected.",55.64094683,"yes it is feasible, but consider that integer factorization problem is an NP-something problem and BQP problem . because of this, it is impossible that a neural network purely based on classical computing finds prime number with 100% accuracy, unless P=NP.",52.96938862,,,,,,,,
3176,"What are good alternatives to the expression ""Artificial Intelligence""?",philosophy,"Artificial is said to derive from the Latin word "" artificium "" which connotes ideas such as crafting.  Thus, artificial is a correct usage, and algorithms can be regarded as ""artifacts"" in the context of information as opposed to physical manifestation of information (i.e. matter). However, I agree that the use of artificial is problematic in that, should strong Artificial General Intelligence ever be achieved, there is a stigma to ""artificiality"" that could have implications regarding personhood. My personal feeling is that we should be using: Algorithmic Intelligence which this is functional definition, and therefore more meaningful than ""artificial"".  Additionally, ""algorithmic"" is a neutral term, and provides a very accurate description of what these systems are. In terms of what is considered ""intelligent"", you may want to look at the concept of Bounded Rationality .  There is no hard definition of ""intelligence"", just degrees of optimality in regard to decision making in a condition of uncertainty. Because this is subjective for any problem that is not solved, modifiers are utilized, and thus we refer to AI as ""strong"" or ""weak"". These terms are also used to describe the degree to which certain types of problems (for instance a non-chance, perfect information game like Checkers) has been solved. Complexity theory will shed more light on this concept. For more insight on ""artificial"", you might find this question on the philosophical origin of the Turing Test interesting, because it partly involves the meaning of a ""thing"". (There were multiple words for this in Ancient Greek.)",53.70600722,"Google defines 'artificial' as something created by humans rather than occurring naturally so I wouldn't quite say that it's so bad. Given the question however, you could perhaps say ""smart machines"" since that's what they essentially are these days. Artificial Intelligence is a very broad term, pre-dating modern AI, simple things such as mechanical wooden robots were considered Artificial Intelligence. https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence",55.09589785,"There are several expressions that are often used as synonyms for artificial intelligence , but, nowadays, the most common ones are likely machine intelligence and computational intelligence . However, these expressions are not well defined, so not everyone will agree that they are interchangeable, but we can all agree that these fields (either if we consider them the same or not) are quite related to each other (and they overlap). Moreover, these fields also evolve over time and they embrace techniques from other fields, which makes it more difficult to define them. More concretely, initially, AI was mainly based on the manipulation of symbols and logic, but nowadays AI is mainly machine learning , statistics and, in particular, deep learning . Furthermore, the expression artificial intelligence was apparently coined after the term cybernetics , which some people might consider the first serious attempt to building intelligent systems.",58.58534563,"These are correct. Artificial implies that it runs on artifically made hardware. There is no reason to distinguish between the natural processes what do the same. Further, the term intelligence is nor realy precise. What is more/ less intelligent or has or has not intelligence among : mowgli, monkey, crow, common game bot, whatever? MAing thing, some learning on data happens here. The best alternative would be Mashine learning, but again, that mashine like  ""artifically made stuff"" does that is irrelevant. So my definition is:
 Algorithmic Learning.",52.26489513,"Machine Intelligence I believe intelligence is not a proprietary entity meant only living beings. In fact the very origin of human intelligence is unknown. It is still not known if we can generate a brain just by fixing up the corresponding molecules of the real brain(Even theoretically). Even if we could do that does that constitute a real intelligence or artificial intelligence is still very hazy. I think the phrase ""Machine Intelligence"" would sound appropriate.",54.00458674,"Josh Worth provides a critique for the use of the term artificial intelligence in his Stop Calling it Artificial Intelligence , but there are some caveats in the treatment of the term.  (One is a typo in the URL path.) It is not the term Artificial Intelligence that is the issue.  It is what is collected under it in the media and sometimes in academic literature. Is Artificial Unclear? The term artificial is not particularly ambiguous or inaccurate.  Artificial doesn't mean fake as Worth suggests.  It simply means that it did not arise from natural processes.  High end artificial flowers feel and smell like they are grown.  Artificial flight is now called flying.  We no longer see birds as the exclusive pilots of flight, so the word flying has changed to include artificial things called aircraft. People don't normally make the mistake of placing the adjective artificial along side human capabilities, so there is no tendency to misnomers.  There is a problem with the use of the term artificial intelligence though.  Worth is correct in that overall statement.  There may be two forks to the misuse of the term. Defining Intelligence One problem is with the term intelligence .  The definitions we've seen are considerably problematic.  Some are flat out wrong and can be proven so with heaps of counterexamples.  Furthermore, definitions tend to be qualitative and difficult if not impossible to quantify. Some propose the standardized testing of academia to quantify intelligence. If we use that definition of the word, from the staunch g-factor adherents, artificial intelligence is a target idea and nothing comes close to approximating it. No computer system has yet been admitted into a major university on the basis of high scores in college board testing. If we use the ability to learn as a standard, then flies are intelligent because they frequently learn from one swat strategy changes to avoid getting squashed by the irritated person.  There are a thousand other reasons why learning, by itself, is not an adequate characterization of human intelligence.  A crack addict can learn how to make a purchase without holding a job. We wouldn't characterize that as intelligence, but rather dysfunction. Excessively Inclusive Use of the Term Another problem arises from avarice. To appear as an expert in what is perceived to be a lucrative expansion of technology, some who have no conception of the topic sometimes present conjecture as if it were peer reviewed fact.  This has been typical of popular topics for centuries.  There is often insufficient bandwidth of peer verification available to address even a small portion of publicly available information.  Web publication has only augmented this problem. Resulting from this coveting of expert reputation is collecting under the name artificial intelligence a number of things that are not intelligent. Multi-dimensional control systems Brute force searches of permutations Decisions made by statistically analyzing sample data Parameterized functional networks that converge to a defined optimal behavior Those of the above that have no component of Cognition, Comprehension, Complex modelling and use of such models, Semantic mapping, Rational inference, Or some other clearly distinctive and broad form of adaptation should not be included under the technology that exhibits authentic artificial intelligence .  However, those that developed the theory of control systems, searches, parameterized function convergence, and statistics or developed working systems that use that theory are intelligent, just not artificial. The above four might have terms that distinguish them from authentic intelligent systems outside the realm of biology. MDC — Multi-dimensional Control BSS — Brute force search NBL — Network Based Learning SDS — Statistical decisioning systems It seems that terms that have three words and form distinct acronyms get further and last longer.",56.95903349,,,,,,
3006,"When the AI singularity takes over, what will there be left for us to do?",philosophy,"Given all your assumptions about AI turn out to be true, we would have some kind of utopia, where no one has to work, and there is plenty of everything. Fair enough. Your other assumptions is about human nature, and that is where I'd challenge your conclusion: Just because there are computers better than humans at some task, that does not automatically take all enjoyment from doing it. I have three arguments in favor of my stance. There already are computers better than humans at checkers, chess, backgammon, starcraft, mastermind, go,.. and many more. Yet these games still get played, even if no human can hope to ever be as good as a computer. There will be domains where the evaluation of quality is so vague or personal, that the notion of being ""better"" is useless in any objective sense. I am thinking mainly about art. Playing into argument one, photo cameras are already better ""painters"" of reality than humans can ever hope to be, yet people still paint. And their paintings gets appreciated, even the photo-realistic kind. I'd say that the whole mindset of ""if someone is better than me at x it is not worth doing x"" will have outlived its lifetime very shortly. Society didn't always spin that way I feel, it is largely due to the influence of the North American way of life to always strive to be #1 and everything below that being trash. Globalization already puts this way of thinking at risk, with many young people being disillusioned or even depressed because, flippantly put, ""whatever you do, there is always an Asian kid who does it 10 times better"". We don't have to wait for AI to outshine us, the rest of the world already does. As a consequence, we need to adapt our way of approaching that fact, stop seeing it as diminishing to our worth, and move on. As a closing note, I also see it as a problem to the general population that within a short time we essentially have to change a very substantial part of our world view, AI being better than us, us not having to work anymore etc. All the economical revolutions in the past, the neolithic, the industrial, and most recently, the digital, had a longer transitional period where people could grow accustomed to the new world. And even with that transition it was hard enough for many people. Yet, most dealt with it, and later generations can hardly imagine a world where the new change doesn't exist yet, and I personally don't see why the next revolution should be any different.",53.02030797,"We are biological beings. We will continue to like whatever activates opioid receptors and we will continue to want whatever activates dopamine receptors in the nucleus accumbens. Food, drinks, sex, social dominance, altruistic acts, novelty, drugs of abuse, physical mastery, procreation, socializing, nice sunny weather, sleep when tired, etc, will continue to be rewarding and motivating as long as we have brains. A few links to review papers on the neurobiology: See also this paper Pleasure Systems in the Brain or this one Dopamine in Motivational Control: Rewarding, Aversive, and Alerting for more details regarding this topic. I personally enjoy playing basketball, even though I would not stand a chance against NBA players, and rock climbing even though using a ladder would be much more efficient. I do not get paid for either. Also, what is going to be the motivation for my children? What am I going to tell them to go to school? When someone asks ""what do you wanna be when you grow up?"", and the inevitable answer is nothing. I disagree. Schools will evolve. Children will still need to learn social skills and make friends. At the very least, they will need to learn how to use or interface with the computers that do everything. They will still need to learn to be better human beings by reading humanities. I don't think they will be told ""The computer will now read Dostoevsky on your behalf, it is MUCH BETTER at reading , you know"". There may be jobs where the job description contains ""by a human"", such as handcrafts, psychotherapy, etc. They can grow up to be whatever they want, human beings are not defined solely by their professions. I am sure you are not just a coder, nothing more .",53.91083056,"Starting from Arne's point on photography: I'd like to point out how photography changed painting. You can notice that classical painting that tried to be photorealistic stopped to be relevant when photography started, and that several modern movements started, like more abstract paintings, surrealism, or cubism. One can see art as a medium to brag ""i'm better than you"". They could say ""i'm better than you at perfecting realism technique"" until photography appeared. Now, on this aspect, what do artists brag about?Sometimes by claiming how progressive they are when sculpting sex toys?
By claiming that their white sphere is ""too deep for you""? I'm not an art historian, though, this is how i feel it evolved to current modern ""art"" where skill has been replaced with provocation. Photography might be a good starting point to think about how people react to a technology that dwarves them and they still want to be #1. -- here's a point to note: people want to have a good identity Paul Lafargue once wrote The right to be lazy, where he argued how mankind would intellectually evolve if people worked less as machines could work for us. One of his argument was that people se sees as intellectual models, Greek philosophers, didn't work, and had more time to think.
I think he misses a point: not everyone is able to enjoy philosophical time spending.
Not unlike how we were deeply wrong when we expected internet to open our minds towards a great age of knowledge.
Remember that people are not utopically intelligent, that you have stupid people, and humans basically follow their instinctive needs.
This is why 20 years after we started using internet, we have lolcats, trolls, gated communities, and more intellectual intolerance (think sjw, alt right movements). -- point #2: not everyone is able to be included in a ""everyone will be bright"" utopia The key is one's identity. I think that everyone wants and need to have a ""positive identity"", they look for one to compensate for their weaknesses.
This is, for me, why poorer people tend to show off, and successful people don't tend to brag.
So, how do you forge your positive identity when you don't have much for yourself? I see a few things that happen around me:
- be a rebel: to distinguish yourself, you become an opponent; but in fact you just mirror the main trend, while bragging about how independant you are. I'm opposed, so i'm free. You can have a range from IA free stance (like people who didn't want to use the internet, some of the ones who use free software), to a more aggressive movement.
- be conservative, pious: maybe in reaction to a society that changes and becomes too progressive for some, i see people becoming conservative, this is still a rebellious trend, and i see the rise of conservative vote (nationalism) or religion (buddhism, christianism, islam) as a reaction. I follow traditions, so i'm independant from your novelties. Religion is always a solid value when society changes.
- be hyperprogressive: same reaction as previous point if you think that society is too conservative. It's a rebellious stance when you have conservative people in front of you. I'm a step ahead everyone, look at how avant-gardiste i am. -- point #3: if you don't excel in a field, be a rebel. Or: if you can't follow, step aside, don't follow the stream. So you have no work to do anymore, plenty of food grown by IA controlled drones, all material needs fulfilled, and no interest in thinking too much.
What do people do when they have too much time?
Simple: they fill their natural urges that are not related to material needs and food.
Some examples: eating, playing games, having sex, arguing online, getting wasted. Basically everything that has been labelled as a sin.
You'll also have forms of mental disorders you see in people who feel worthless. And many more will feel worthless. This also means that you'll need more jobs to keep these people entertained, or taking care of the suffering ones. -- point #4: if life gets too easy, people become sinners By these few examples, here rises a question: if ""mediocre"" people tend to get their shiny identity by opposing the mainstream society, how will the mainstream society look like after the rise of AI? So i see three reactions from people:
- be relevant after AI's rise, the ones who can follow
- become a rebel, the ones who can't follow but still want to appear relevant
- be neutral, the ones who fill their human urges And in general, people will do a mix of the three. Mankind, with the help of AI which can provide answers to basically anything, will provide new questions, new projects, new frontiers to explore. Many persons will still be a bit relevant.
AI will become mainstream, so everyone will be more or less opposing it, while enjoying its benefits (think about people who claim to be technology/money independant but have an iPhone).
People will still argue online and giggle at cute cats doing weird stuff. Maybe more, because they have more free time. In conclusion, if AI becomes relevant, expect a world where more people have too much time on their hands .
I find this conclusion a bit deceiving, after all I thought and wrote. :O",53.62473082,"Instead of posting a specific answer, I'm going to point you to Hannu Rajaniemi 's meditation on this subject in the Quantum Thief Trilogy .  Here's why: Artists can have profound insights.  This may be demonstrated by Philip Dick writing about Evolutionary Game Theory in Do Androids Dream of Electric Sheep about 5 years before the field was formalized. (For my money, this is still the most important book about AI.) Many authors have written about the post-Singularity scenarios, but Rajaneimi is the only one I am aware of who is a Cambridge trained Mathematician with a PhD in Mathematical Physics , which I tend to believe makes him well qualified to grapple with the inherent complexity of the subject.",52.74606363,"You're assuming the AI has motivation however that's not really the case, intelligent software will do whatever its been designed to do but that's all it does, it's not a trained animal, it doesn’t have instincts for survival, reproduction or self-determination because there's no reason to add those functions. So effectively AI is just another tool, one that reduces the mental load of doing a task, so rather than digging a trench with one machine you can order a fleet of machines to dig a canal system. Or more realistically you’ll spend hours in stakeholder meetings discussing the need for canals, justifying the cost, explaining the benefits, considering the risks, applying for permits, having more discussions with the council and their consultants, then special interest groups, until finally your order the machines to kill them all because GODDAMNIT THIS COULD HAVE BEEN DONE ALREADY!",53.21850401,"In today's society, money is a big key motivator, not only does it provide necessities of life it can buy luxury and indulgence. In a world where all needs (even complex ones) are met for free by the emergence of technology like the singularity, it is easy to say there will be no motivator. But, instinctually, as children our motivations aren't to accumulate money, you don't see children bored with life. As a child, I was happy to learn. Others were happy to dance or swim, read or paint. In the end, people are social excitement seeking beings. Today's socioeconomic conditions create boredom in some respects doing the same job for years is arguably worse. I don't think motivation will be an issue.",52.67126963,,,,,,
2996,Is there actually a lack of fundamental theory on deep learning?,deep-learning,"There is a paper called Why does Deep Learning work so well? . However, it is still not fully understood why deep learning works so well. In contrast to GOFAI (“good old-fashioned AI”) algorithms that are hand-crafted and fully understood analytically, many algorithms using artificial neural networks are understood only at a heuristic level, where we empirically know that certain training protocols employing large data sets will result in excellent performance. This is reminiscent of the situation with human brains: we know that if we train a child according to a certain curriculum, she will learn certain skills — but we lack a deep understanding of how her brain accomplishes this.",56.60220671,"This is very much the case. Deep learning models even shallow ones such as stacked autoencoders and neural networks are not fully understood. There are efforts to understand what is happening to the optimization process for such a complex variable intensive function. But, this is a difficult task. One way that researchers are using to discover how deep learning works is by using generative models. First we train a learning algorithm and handicap it systematically whilst asking it to generate examples. By observing the resulting generated examples we will be able to infer what is happening in the algorithm at a more significant level. This is very much like using inhibitors in neuroscience to understand what different components of the brain are used for. For example, we know that the visual cortex is where it is because if we damage it you will go blind.",54.52192974,"It probably depends on what one means by ""fundamental theory"", but there is no lack of rigorous quantitative theory in deep learning, some of which is very general, despite claims to the contrary. One good example is the work around energy-based methods for learning. See e.g. Neal & Hinton's work on variational inference and free energy: http://www.cs.toronto.edu/~fritz/absps/emk.pdf Also this guide to energy minimization as a ""common theoretical framework for many learning models"" by Yann LeCun and colleagues: http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf And a general framework for energy-based models by Scellier and Bengio: https://arxiv.org/pdf/1602.05179.pdf There is also Hinton & Sejnowski's earlier work which shows analytically that a particular Hopfield-inspired network + unsupervised learning algorithm can approximate Bayes-optimal inference: https://papers.cnl.salk.edu/PDFs/Optimal%20Perceptual%20Inference%201983-646.pdf There are many papers linking deep learning with theoretical neuroscience as well, such as the following, which shows that the effects of backpropagation can be achieved in biologically plausible neural architectures: https://arxiv.org/pdf/1411.0247.pdf Of course there are many open questions and no single, uncontroverisal unified theory, but the same could be said of almost any field.",58.09943899,"Your wikipedia quote is questionable because deep learning is well developed. In fact, there is a [citation needed] on the Wikipedia page. Look at https://github.com/terryum/awesome-deep-learning-papers . There are like 100 papers in the link, do you still think deep-learning lacks ""general theory""? Yes. Deep learning is hard to understand because it is a very complicated model. But that doesn't mean we don't have the theories. Maybe the lime package and it's paper: ""Why Should I Trust You?"": Explaining the Predictions of Any Classifier will help you. The paper suggests we should be able to approximate a complicated model (includes deep learning) locally with a much simpler model.",61.08861862,"A key question that remains in the theory of deep learning is why such huge models (with many more parameters than data points) don't overfit on the datasets we use. Classical theory based on complexity measures does not explain the behaviour of practical neural networks. For instance estimates of VC dimension give vacuous generalisation bounds. As far as I know, the tightest (upper and lower) bounds on the VC dimension are given in [1] and are on the order of the number of weights in the network. Clearly this worst case complexity cannot explain how e.g. a big resnet generalises on CIFAR or MNIST. Recently there have been other attempts at ensuring generalisation for neural networks, for instance by relation to the neural tangent kernel or by various norm measures on the weights. Respectively, these have been found to not apply to practically sized networks and to have other unsatisfactory properties [2]. There is some work in the PAC Bayes framework for non-vacuous bounds, e.g. [3]. These setups, however, require some knowledge of the trained the network and so are different in flavour to the classical PAC analysis. Some other aspects: optimisation: how come we get 'good' solutions from gradient descent on such a non-convex problem? (There are some answers to this in recent literature) interpretability: Can we explain on an intuitive level what the network is 'thinking'? (Not my area) (incomplete) references: [1] https://arxiv.org/abs/1703.02930 [2] http://papers.nips.cc/paper/9336-uniform-convergence-may-be-unable-to-explain-generalization-in-deep-learning [3] https://arxiv.org/abs/1804.05862",54.92956918,"I'd like to point out there isn't a good theory on why machine learning works in general. VC bounds still assume a model, but reality doesn't fit any of these mathematical ideals. Ultimately when it comes to application everything comes down to emperical results. Even quantifying the similarity between images using an algorithm which is consistent with humans intuitive understanding is really hard Anyway NN dont work well in their fully connected form. All successful networks have some kind of regularization built into the network architecture (CNN, LSTM, etc).",55.26874304,,,,,,
2980,How should I handle invalid actions (when using REINFORCE)?,reinforcement-learning,"Just ignore the invalid moves. For exploration, it is likely that you won't just execute the move with the highest probability, but instead choose moves randomly based on the outputted probability. If you only punish illegal moves they will still retain some probability (however small) and therefore will be executed from time to time (however seldom). So you will always retain an agent which occasionally makes illegal moves. To me, it makes more sense to just set the probabilities of all illegal moves to zero and renormalise the output vector before you choose your move.",51.70881373,"Usually softmax methods in policy gradient methods using linear function approximation use the following formula to calculate the probability of choosing action $a$ . Here, weights are $\theta$ , and the features $\phi$ is a function of the current state $s$ and an action from the set of actions $A$ . $$
\pi(\theta, a) = \frac{e^{\theta \phi(s, a)}}{\sum_{b \in A} e^{\theta \phi(s, b)}}
$$ To eliminate illegal moves, one would limit the set of actions to only those that were legal, hence $Legal(A)$ . $$
\pi(\theta, a) = \frac{e^{\theta \phi(s, a)}}{\sum_{b \in Legal(A)} e^{\theta \phi(s, b)}}, \, a \in Legal(A)
$$ In pseudocode the formula may look like this: action_probs = Agent.getActionProbs(state)
legal_actions = filterLegalActions(state, action_probs)
best_legal_action = softmax(legal_actions) Whether using linear or non-linear function approximation (your neural network), the idea is to only use the legal moves when computing your softmax. This method means that only valid moves will be given by the agent, which is good if you wanted to change your game later on, and that the difference in value between the limited choice in actions will be easier to discriminate by the agent. It will also be faster as the number of possible actions decreases.",52.69898885,"I faced a similar issue recently with Minesweeper. The way I solved it was by ignoring the illegal/invalid moves entirely. Use the Q-network to predict the Q-values for all of your actions (valid and invalid) Pre-process the Q-values by setting all of the invalid moves to a Q-value of zero/negative number (depends on your scenario) Use a policy of your choice to select an action from the refined
Q-values (i.e. greedy or Boltzmann) Execute the selected action and resume your DQN logic Hope this helps.",57.13088649,"IMHO the idea of invalid moves is itself invalid. Imagine placing an ""X"" at coordinates (9, 9) . You could consider it to be an invalid move and give it a negative reward. Absurd? Sure! But in fact your invalid moves are just a relic of the representation (which itself is straightforward and fine). The best treatment of them is to exclude them completely from any computation. This gets more apparent in chess: In a positional representation, you might consider the move a1-a8 , which only belongs in the game if there's a Rook or a Queen at a1 (and some other conditions hold). In a different representation, you might consider the move Qb2 . Again, this may or may not belong to the game. When the current player has no Queen, then it surely does not. As the invalid moves are related to the representation rather than to the game, they should not be considered at all.",57.42549416,"An experimental paper exist in arxiv about the effect of whether to mask or to give negative rewards to invalid actions. There are some references in this paper which also discuss the effects and the mechanism to handle invalid actions. However, those main references are still only pre-prints in the arxiv (not published and presumably not peer-reviewed yet). On a way to handle that situation, other answers have given good practical methods to ignore the invalid actions. I just want to add one more trick to do that. You can pre-compute a binary vector as the mask for the actions, and add the log of the mask before the softmax operation. The log of 0 is -inf and exp(-inf) is 0 in Pytorch (I don't know if the same applies in Tensorflow). $P(a_i| A'_t, X_t) = \text{softmax}(u_i+\log{m^t_i}), \forall i \in
 \{1,\dots,|A|\}$ and $m^t_i \in \{0,1\}$ with $P(a_i| A'_t, X_t)$ is the probability to take action $a_i$ given  the action history $A'_t$ up to the $t$ -th step and the current environment's features $X_t$ , $u$ is the output of the last layer of the model, $A$ is the action space $m^t_i$ is the feasibility mask of action $a_i$ for the current step.
Therefore,
the probability of the invalid action is 0 after the softmax operation.   That way, you can treat the mask as the part of the state as the input to your model. This is actually more handy for algorithm which employs experience memory because the mask then can be saved in the experience memory too.",60.34839372,,,,,,,,
2959,Are AI winters inevitable?,history,"I think that by strict definition of the word inevitable, no, future AI Winter events are not inevitable. However likely or unlikely it may be, it is possible to control research spending and to create a more stable plan of funding research in Artificial Intelligence. Because it is possible to avoid an AI Winter event, an event is not inevitable .",63.34704875,"The hype cycles are the rule these days, and AI is always a wonderful topic for unbelievable and crazy hype. I mean simple thing like speech recognition is still not working properly, but everybody is discussing how to survive the revolt of the terminator machines. So unless we can tune the hype down, the next AI winter is inevitable.",55.53283438,"Yes - there will always be Gartner Hype Cycles which leads to ""AI Winters"" - that is just a fact of human nature in large groups. There is no better evidence for ""Hype Cycle"" mentality than the stock market in how it reacts both high and low to whatever the hot item is. AI is much more susceptible to this given that AI tends to touch people in very real ways - will this technology become smarter than me? Will it replace me? Will it take over? Are we building new life? Who controls this? which for those that actually build or know something about these techniques and concepts would say that we are a very long way off it is even possible in the first place. To build systems that can at best maybe mimic the intelligence of a two year old we would consider it a major success.",51.30644645,"I would say an AI Winter has already happened in the 2000's when specialist systems were adopted in detriment of cognitive systems, neural networks for instance were poorly understood back then and because of that they got meager investments from large companies, Google was a notorious exception. Only some 3 or so years ago, with things like IBM Watson and driverless cars this field started to draw significant attention. And now I doubt it will be ignored again, the research has taken off from advanced PhD theses and becomes more and more widespread.",50.40953821,"Honestly, I think an “AI winter” hits when people realise a lot of the stuff being sold as AI just doesn’t work the way it’s hyped. We’ve already seen some big names flop because it turned out there were humans behind the curtain keeping it going. Most of the useful AI isn’t going anywhere, it’s already baked into how we work and live. The ones that won’t last are the flashy tools that promise the world but can’t actually connect to real systems, handle messy data, or explain what they’re doing. Those will burn out fast.",51.41195283,"AI Winter... Well, it depends if you consider the Stock Market (hype) or not. Some answers here are more optimistic (""NOT inevitable"") but that doesn't consider the SM hype, IMO. AI winter is a period of reduced funding and interest in artificial intelligence research So, if it's about ""funding"", then I'd say It's inevitable , it will be another winter, because that's how SM work. But it's not all doom and gloom, the interest in AI will not go down to the same extent. I'd emphasise the role G. Hinton played reviving the AI after the first winter (see backpropagation ) and again, after the second winter (see AlexNet ).",57.42978283,"I believe, that the next/first/second (?) AI winter is currently starting.
In the last two years or so, the LLM hype raised higher and higher expectations. And much of that was matched.
But currently, there are coming in very disturbing news. Like: 'Mad personas' built into the latest LLMs (i.E. Grok/X), the latest versions not performing (by far) as well as expected (but only true for the 'non-thinking' (reasoning) models (i.E. ChatGPT 5)), as well as the possibility that children may receive 'romantic' output from some AIs, kids cheating in school with a pen, that scans the question, translates it into OCR, use it as a chatGPT prompt and giving the correct answer, and many black hat hackers using AI for their dark targets...
All this poses a lot of questions, which we (especially the politicians) need to react to and handle it.
The possibilities with AI explode, but also the dangers.
So, it is good, if this comes to a rest, until we can handle it better.
No company wants to be liable for any harm that is done by AI. So there will be many companies and countries, that will rather soon try to pull the plug for this.
The future is uncertain.",51.16030337,,,,
2820,Why are deep neural networks and deep learning insufficient to achieve general intelligence?,deep-learning,"Everyone dealing with neural networks misses an important point when comparing systems with human like intelligence. A human takes many months to do anything intelligible, let alone being able to solve problems where adult humans can barely manage. That and the size of human brain is enormous compared to our neural networks. Direction might be right, but the scale is way off. Number of neurons in human brain can be matched memory-wise but the amount of parallelism to simulate it real-time cannot yet be achieved (at least for a random researcher). While a little old this might give you an idea of how much we lack the processing power.",52.798457,"Deep Learning is mostly successful in supervised learning, whereas the brain builds categories mostly in an unsupervised way. We don't yet know how to do that. (Take a look at google brain : 16,000 cores and all this thing can do is recognise cats and human faces with pretty abysmal accuracy.) Deep Learning uses highly unstructured activations, i.e. the high level representations of ""dog"" and ""cat"" in a neural network classifier don't have to be similar at all. The brain on the other hand uses inhibitory neurons to create sparse distributed representations which are decomposable into their semantic aspects. That's probably important for abstraction and reasoning by analogy. The brain has many different parts which work together. Deep Learning researchers are only just beginning to integrate memory or attention mechanisms into their architecture. The brain integrates information from many different senses. Most Deep Learning applications use just one type of input, like text or pictures. The brain is capable of modelling sequences as categories. (Basically every verb names a sequential (i.e. temporal) category.) It can then arrange these categories into long-term hierarchical plans. So far I haven't seen anything in that direction in Deep Learning. Also neural networks can't yet operate on the same scale as the human brain. If you look at the answers to this question , the human brain will be ahead in neuron count for another couple of decades. A neural network might not need the same number of neurons as the brain to reach a similar performance (because of higher accuracy), but right now for example video processing is still pretty limited in terms of input and throughput.",57.18994536,"IMHO the first hurdle is scale : even Google's largest DNN doesn't come close to the scale of the brain, and by a factor of several orders of magnitude...",50.26335812,"I think it's missing still the aspects what makes a human brain; having a lot of different networks working with each other. Just like meditation improves cognitive abilities by having the brain work more synergistically, we could apply that to machines too. For example google is learning a computer to dream, just like we do, to reinforce what we already learned. https://medium.com/@tannistho/why-is-google-teaching-its-ai-to-dream-e9ae9ecd0e3a#.gljal6pww And here is pathnet, a network of neural network. https://medium.com/@thoszymkowiak/deepmind-just-published-a-mind-blowing-paper-pathnet-f72b1ed38d46#.ed0f6pdq7 Creating all these mechanics and putting them all together, with enough power and we will get pretty close!",51.7329655,"Artificial intelligence proponents today are focused on the problem of computability - the ability to solve complex problems fast. It is my belief that any amount of success in this direction will not lead to human (general) intelligence although it certainly will outperform humans in certain domains. Instead, efforts should be toward a study of what neurological events cause sensation (the experience of qualia). Of course, this is the hard problem of philosophy but I believe it is the unique key to general intelligence and its capabilities. Reverse engineering and also testable theories should be advanced toward this end.",54.1283804,,,,,,,,
2771,Is artificial life really life or not?,philosophy,"If you read Steven Levy's book, Artificial Life ,you will find, as I did, the distinction between biological and ""artificial"" life blurred. If you think about it, what exactly is ""life"", anyway? A set of complex systems with emergent behavior capable of evolution and adaptation. A prototypical biologist may not define life that way. Indeed, he would, not being focused or concerned with the computational aspect, define it in a way that would narrow it down to biological life. Marvin Minsky mention the concept of luggage words , and I myself came up with the notion of mirage concepts . For the former, that which we don't understand gets lump into the word. For the latter, when you take a ""mysterious"" concept apart, it vanishes like a mirage does as you get closer. So what is life ? If we look at a ""living"" organism, we'd all that ""life"". If we remove a single cell from that organism, we'd still call that ""life"". But what if we remove a single organelle like, say, a ribosome? Lysosome? Contractile vacuole? Endoplasmic reticulum? Is that still ""life""? What if we remove a macromolecule from that? Is that still ""life""? As you see, all becomes very murky, and I do this on purpose to illustrate just how arbitrary the very concept of ""life"" is. So I think my definition is a good one, broad enough to encompass both in-silico and the organic versions. It bespeaks to algorithms, robots, viruses -- yes both computer and organic... anything that has complexity and the ability to adapt and evolve .",64.52140122,"""Life"" is a definition humans use to classify objects according to the types of behavior humans perceive as unique to living creatures. Scientists and philosophers tend to define something as ""alive"" if it manifests some specific properties found in living organisms, such as self-replication, adaptation to the environment, homeostasis and capability to exploit matter and energy for its own existence and functioning. With that being said, there is no one accepted definition of life, and it is doubtful that such a definition is possible. As to artificial life (ALife), the book ""Biological Computation"" (2011) by E. Lamm and R. Unger states that it is common among researchers to distinguish between two types of approaches to artificial life: the strong ALife approach, which postulates that virtual ""creatures"" on a computer screen can be considered to be alive if they fulfill the definition of life used by the researchers; and the weak ALife approach, whereby computerized creatures displaying characteristics of living systems are only models used in research and are not really alive While this is not solving the problem of definition of life, it might give more context on the subject in relation to artificial life.",63.86351702,"Wikipedia describes life as a characteristic of ""physical entities having biological processes"". The same source also describes a simulation as ""the imitation of the operation of a real-world process or system over time."" If a digital neural net was to listen to me prattle on for long enough it could learn to speak as if it were me. It would have my knowledge and limitations but its headaches would be quite different from mine. It would never have a toothache. But you could put it in a Searle Chinese Room , and you could speak to it and it would sound exactly as if it were me long after I am dead. It has my ""character"" which is what my friends would recognize about me. According to the definition of life it is not alive because it does not have biological processes. It is a simulation because it emulates what I would have said. It cannot be a copy because a digital box is not biologic. Now let's give this simulation a biological nose so that it can smell. And maybe two eyes and ears. We continue this process until most of the simulation is equipped with biological parts which function together. Whatever it is is now able to come out of the Chinese Room and talk to you. By golly, it looks and sounds exactly like me, but I died a long time ago. Have I been brought back? My suggestion is that a perfect copy of me would not be possible simply through training due to the level of detail required, but that the close copy would be alive. A critical point would be that there would have to be a fatal link somewhere which would cause ""death"". You could always create a new close copy, but not an exact copy.",52.83544306,"I like to take an "" animist "" approach. (It has been suggested to me that part of the reason Japanese designs are so effective is because of the cultural affinity for the concept per the Shinto tradition.  For instance, the thing where people put little eyes on everything;) I like to think of how my dog, who is terrified of the vacuum cleaner, would regard one of the recent Boston Dynamics creations.  My guess is the dog would't find much use in the distinction that the robot is an artifact as opposed to a biological entity. I tend to take a deterministic, mechanical approach to reality. Sure things get fuzzy down at the quantum level, but even that may simply be a factor of inadequate measurement capability and the sheer complexity of quantum mechanics, which seems fundamentally beyond the grasp of even the greatest minds, when they're being honest about it. I don't see much of a distinction between simple organisms and cellular automata , except that the former is part of a biological food chain.  There is a valid hypothesis that if you had a big enough computer, Conway's Game of Life could independently develop ""intelligence"". Self-replication would certainly seem to be a requirement of biological life that can be extended to artificial life.  Possibly the true distinction of ""artificial"" is merely that it is creation of a functional system by an extra-species source, whether the creation be ""biological"" or ""mechanical"" in nature. (i.e. we can hack genes now, not just computer code.)",56.26658217,"It wouldn't be considered alive if it doesn't have vital functions, such as nutrition, relation with the environment, and reproduction. While the first is easy (use a battery) and the second is the one we are developing right now (basically, the intelligence part of an AI) giving programming skills to an AI, aka the ability to reproduce, isn't widely considered a good idea, as many science fiction writers can tell you.",50.22472169,"Imho, it is life. Example: consider the possibility that we synthesized from completethe DNA of a human being, with zero atoms from another human, and grew said human in a lab. Most (and myself) would agree that creature is alive. Although there are many opinions that differ, my own is that there is no absolute line to draw between something that is alive, and something that is not alive. A human is alive. But is a single bacterial cell, or a single cell from your own body? They reproduce, they eat, etc. so yes they are alive. They are not like a dog or a cat however. In fact, a bacterial colony in a pool of water, giving off a yellow or brown color can be mistaken as a mineral or mud. It is only when you look closer that you see it is actually life. What about a biological virus? It is not made of cells. It does not have DNA. Most would agree it has life. But is does not really seem to be as alive as say, a shark or giraffe. Many people do not think a car is alive. Yet cars evolve. They move, they ""eat."" Life is simply a way we define things around us. A much more useful and definitive way to categorize life I think, would be to utilize a continuum instead of an all or nothing approach. Rocks would go on the end of ""nonliving."" Intelligent, multicellular; self-aware entities could perhaps be on the other end as ""fully alive."" Other entities can go in between. As for something such as AI, I would propose adding a z-axis, to make a 3 dimensional continuum, allowing for a self aware, intelligence entity not made of cells to fit comfortably with humans without causing an all-or-nothing. PS: thought I came to this conclusion myself, I have a suspicion someone smarter than me has already written about such an idea. If anyone feels like educating me, I'd love to hear it.",56.69732528,,,,,,
2742,Linear regression: why is distance *squared* used as an error metric?,linear-regression,"The squared form is sometimes called the Euclidean norm or L2 norm . One of its very helpful properties is that it has an easily defined derivative, which can be used in mathematical analysis and translated fairly easily into code. Intuitively it is thought that it is advantageous to exaggerate the differences according to the value of the error, which squaring does. You might also use the powers 3 or 4, but the derivative is more complex. A number of different norms may be used, according to the particular circumstances of the problem at hand.",53.52008,"Brief Background The error metric (an appropriate term used in the question title) quantifies the fitness of a linear or nonlinear model. It aggregates individual errors across a set of observations (instances of training data).  In typical use, an error function is applied to the difference between the dependent variable vector predicted by the model and empirical observations.  These differences are calculated for each observation and then summed. 1 Why Distance Squared? Legendre, who first published the sum of squares method for gauging fitness of the model (Paris 1705) stated correctly that squaring before summing is convenient.  Why did he write that? One could use the absolute value of the error or the absolute value of its cube, but the discontinuity of the derivative of the absolute value makes the function NOT smooth.  Functions that are NOT smooth create unnecessary difficulties when employing linear algebra to derive closed forms (simple algebraic expressions). Closed forms are convenient when one wants to quickly and easily calculate slope and intercept in linear regression. 2 Gradient Descent Gradient descent is generally employed for nonlinear regression.  Lacking the ability to create closed forms for many nonlinear models, iteration becomes a dominant methodology for validating or tuning the model. An intuitive understanding of gradient descent can be gained by considering a thirsty, blind person looking for water on land solely by taking calculated steps.  (In software, these steps are iterations.)  The blind person can only sense the direction of the altitude gradient (direction of slope) with their feet to descend to a local minimum altitude. 3 Anyone stating that, ""The function itself does not matter,"" in relation to the usual applications of gradient descent would be a dangerous choice for guide in a blind hiking expedition.  For instance, the reciprocal of distance as an error function would likely lead to the dehydration and death of the hikers. The selection criteria for error metrics is important if one is interested in the speed of convergence on a solution or whether the solution will ever be found. 4 Since the gradient of a plane (linear surface) is a constant, the use of gradient descent for linear models is wasteful.  The blind person need not continue to sample the angle of their foot. Sign of the Error Metric The statement, ""The result should be positive so positive and negative deviations are still counted,"" is incorrect. 5 Effectiveness of Error Metrics in Relation to 1.0 Because of the partial derivative of the least squares error metric with respect to an error at any given point is constant, the least squares error metric converges similarly above and below 1.0. Notes [1] The dimensions of a model's independent and dependent variable vectors are, in machine learning, conventionally called features and labels respectively. [2] Another smooth function, such as the error to the fourth power would also lead to closed forms for slope and intercept, although they would produce slightly different results if the correlation coefficient is non-zero. [3] Gradient descent algorithms in general do not guarantee finding a global minimum.  In the example given, it would be quote possible to miss a small hole exists with water in it.  Depending on the surface features (terrain), sensing the angle of the foot (determining gradient) can be counterproductive. The search can becomes chaotic.  To extend the intuitive analogy, consider searching for the bottom of the stairs in Escher's Relativity lithograph. [4] For an error metric to be likely to converge and therefore useful in regression regardless of the direction of the error the sign of the metric is irrelevant.  It is each of the set of partial derivatives of the error metric with respect to the corresponding set of distances between the model predictions and observations that should be positive to regress omnidirectionally.  It sounds more complicated, but even this corrected statement is an oversimplification. [5] The error metric in gradient descent applications is often calculated using a convex function to avoid overshoot and possible oscillation and non-convergence.  In some cases error functions other that sum of squares is used.  The choice of function has to do with a number of factors: The model to which the data is to be fit Factors expected to affect or actually affecting deviations of
the observations (training data) from the model Computational resources relative to the size of the data set",64.64793869,"One justification comes from the central limit theorem. If the noise in your data is the result of the sum of many independent effects, then it will tend to be normally distributed. And normally distributed means that the likelihood of the data is inversely proportional the exponential of the square of the distance to the mean. In other words, minimizing the sum of squares of the distance to the mean amounts to finding the most likely value for the line assuming that the error is normally distributed. This is very often a reasonable assumption, but it is of course not always true.",52.73583157,"It simply derives itself from the maximum likelihood estimation. where in we maximise the log likelihood function., for detailed insight see this lecture: The Method of Maximum Likelihood for Simple Linear Regression .",55.62220526,"One justification is that under homoscedasticity the L2 norm produces the minimum variance unbiased estimator (MVUE), see Gauss-Markov Theorem. It means that the fitted values are the conditional expectations given the explanatory variables which is in many cases a nice property. Further it is the best estimator if the previous property is desirable. As a response to the claim that the function itself does not matter, different functions give solutions with very different properties and a lot of effort has gone in to finding appropriate penalty functions, see for example Ridge regression and LASSO. The penalty function does matter. edit: In response to your question regarding distances lower than 1, nothing ""goes wrong"" when the distances are smaller than 1. We always want to minimize the distance and the squared loss does so everywhere.",53.66417115,,,,,,,,
2706,Why is the Turing test so popular?,turing-test,"Because it is: Easy to explain. (Its essentially a game, the ""imitation game"") Intuitively plausible as a metric. The idea of ""people v.s. AI"" is very marketable. At the time we thought that we can analyze cognition strictly in terms of input/output (per behaviorism). Cognitivism, embodied cognition, developmental cognition are all sub-fields that have a right to challenge the Turing Test, but they weren't developed at the time of Turing. Of course, it also helps that Turing is a very important figure in AI/CS.",58.24097763,"I agree with @colourincorrect 's point that there is economic value to AI which can pass Turing tests to various degrees (chatbots for instance) and this is the reason it is so popular. At a deeper level, the test relates to subjectivity, and can be said to have it's origins with the early Greek philosopher Protagoras , who proposed that ""Man is the measure of all things."" πάντων χρημάτων μέτρον ἐστὶν ἄνθρωπος, τῶν μὲν ὄντων ὡς ἔστιν , τῶν δὲ οὐκ ὄντων ὡς οὐκ ἔστιν. Source: Sextus Empiricus, Adv. math. 7.60 Full quote may be translated as: ""Of all things (used by man) the measure (of these things) is man: of the things that are, that they are, of the things that are not, that they are not."" (Apologies as I cannot find a direct link for the Greek online. I re-translated the first part of the proposition for clarity, but lifted the second part from Bostock , whose Ancient Greek is undoubtedly better than mine, because it is potentially ambiguous, even in the original, and Bostock's interpretation makes good use of that ambiguity.) χρημάτων ""things"" is distinct from ὄντων ""things"", which is interpreted to mean Protagoras was speaking about things that man has a direct relationship to, such as property, tools, affairs and so forth. ""A thing that one needs or uses"" is listed in the LSJ. Protagoras can unquestionably be extended to Algorithmic Intelligences, which are ""thing"" used and interacted with by humans. The Turing Test exists because it not only has utility value, but because of the fundamental condition of subjectivity, the idea of which goes back to the earliest, most basic, philosophical concepts.",57.8489469,"The test has gained its name and fame mostly because of the person behind it, Alan Turing . Turing - Considered as the father of Artificial Intelligence is among the first who believed that even machines can act and think like humans. Even though the test is famous there is not much effort placed to qualify the test. The primary reason for this it due to the fact as the test only asks the machine to act like a human being. This is not very beneficial as not all acts of humans are rational and efficient. Go over these threads for more on this. I think it's the idea of something non-human acting like a human that creates so much fuzz about Turing Test among the general public.",58.94658617,"It is so popular because Turing formulated it. He was one of the first who talked about ""intelligent machines"" and was good connected in the scientific community since the 1940s. So there was enough time to distribute his very intelligent thoughts, for instance by Von Neumann, until now. Turing's importance for computer science is shown by the name of the Turing Award. So it is clear that a lot of people have read his papers.",62.10806528,"This isn't a complete answer, but might show some contributing factors. Perhaps most of all, the Turing Test has existed for a long time! In 2000 Turing Test: 50 Years Later reviewed the history of the Turing Test in academia. Given this time, it has become pervasive in popular culture (well, in scifi. e.g. Ex Machina .) It has also garnered attention in media. Googling ""Turing Test news"" shows lots of stories about ""AI"" passing the Turing Test over the last few years. Aside from its age, it's a pretty simple idea at its core. A human talks to a computer and decides if they think it's a human or not. That's a pretty digestible idea! So, between its age and ease of understanding, it's a prime candidate for being popular. Other tests tend to lack these qualities (age and simplicity).",67.90850761,,,,,,,,
2644,How would AI be able to self-examine?,machine-learning,"Several AI systems will come up with a level of confidence to the solution found. For example, neural networks can indicate how relatable is the input problem to the ones it was trained with. Similarly, genetic algorithms work with evaluation functions, that are used to select best results, but depending on how they're built (the functions), they can indicate how close the algorithms are to an optimal solution. In this case, the limit to when this is acceptable or not will depend on a threshold set beforehand. Is 50% confidence good enough? Maybe it's ok for OCR apps (spoiler: it's not), but is it for medical applications? So yes, AI systems do currently have the capacity of determining if they're performing well or not, but how acceptable that is is currently based on the domain of the problem, which currently stands outside of what is built into an AI.",51.60206798,"Would AI be able to self-examine objectively and determine if it is capable of doing the task? Our ability to self-examine comes definitively from the memory of our experiences; indeed, for this reason it can't be objective. In the same way AI could be able to determine the heuristically optimal strategy to solve a problem if and only if it has some sort of memory of previous tasks e.g. speech recognition. Science is constantly working to improve our understanding of things. Trying to mimic the human brain seems to be a difficult problem at the moment; though we are able to replicate almost fully simpler organisms as C. elegans , a roundworm.",69.73972018,"I would concur with the answer given to you by Lovecraft . One of the major problems with A.I. programmers is that they are always trying to push computers to do things which are designed for ""mature"" intelligent creatures who have prior experience and knowledge of solving problems.  -As if these things can be imputed without the A.I. having to achieve the necessary and vital ""learn by trial and error"" experience first. For example: when allowing for task examination; self evaluation and risk assessment. You have answered your own question, because these things can only be gained by ""experience"". However, the only way to surmount this is to expose a prototype A.I. to the main problems; help it to solve them, and then to take its memory and use it as a template for other A.I's. Technically, AI's which have learned to solve prior problems could make their memories available to others on demand, so that an inexperienced AI could solve an issue without having achieved the skills needed. However, I would like to add that mimicking intelligence is not in itself ""intelligence"". Many programmers fall into the trap of believing that to emulate something is qualitatively the same expression as the genuine article. This is a fallacy which infers that we only have to simulate intelligence without understanding the real mechanisms which create it. This ""copying"" of sentience is done all the time and despite how good we have become in copying over the last few years, each new algorithm is just that:  a simulation without genuine sentience or intelligence!",51.98443055,"Would AI be able to self-examine objectively and determine if it is capable of doing the task? A possible approach might be the one suggested and studied by J.Pitrat (one of the earliest AI researcher in France, his PhD on AI was published in the early 1960s and he is now a retired scientist). Read his Bootstrapping Artificial Intelligence blog and his Artificial Beings: the conscience of a conscious machine book. (I'm not able to summarize his ideas in a few words, even if I do know J.Pitrat -and even meet him once in a while- ; grossly speaking, he has a strong meta-knowledge approach combined with reflexive programming techniques. He is working -alone- since more than 30 years on his CAIA system, which is very difficult to understand, because even while he does publish his system as a free software pragram, CAIA is  not user friendly, with a poorly documented common line user interface; while I am enthusiastic about his work, I am unable to explore his system.) But defining what ""conscience"" or ""self-awareness"" could precisely mean for some artificial intelligence system is a hard problem by itself. AFAIU, even for human intelligence, we don't exactly know what that really means and how does that exactly work. IMHO, there is no consensus on some definition of ""conscience"", ""self-awareness"", ""self-examination"" (even when applied to humans). But whatever approach is used, giving any kind of constructive answer to your question requires a lot of pages. J.Pitrat's books & blogs are a better attempt than what anyone could answer here. So your question is IMHO too broad.",59.52545762,"It's not possible as this is the distinction between AI and humans, truly science will never understand the subconscious it's that little black box that no one can reverse engineer. This is why pursuing singularity is a fools dream to the extreme. The reason why machinery lacks this because of the lack thereof a soul. science cannot produce a soul, this is why a machine cannot be self aware we can program fancy algorithms all day that mimic things but it's emotionless it cannot sit in judgement because it lacks real self awareness that is human self awareness it's like trying to make an orange into an apple.",52.22618829,,,,,,,,
2356,Would an AI with human intelligence have the same rights as a human under current legal frameworks?,agi,"Yes, to some of what you propose.  No to some. Today corporations are granted rights: to own property, earn income, pay taxes, contribute to political campaigns, offer opinion in public, ad more.  Even now I see no reason why an AI should not be eligible to incorporate itself, thereby inheriting all these rights.  Conversely, any corporation already in existence could become fully automated at any time (and some plausibly will).  In doing so, they should not lose any of the rights and duties they currently employ. However I suspect certain rights would be unavailable to an AI just as they are unavailable to a corporation now: marriage, draft or voluntary service in the military, rights due a parent or child or spouse, estate inheritance, etc. Could this schizoid sense of human identity be resolved at some point?  Sure.  Already there have been numerous laws introduced and some passed elevating various nonhuman species to higher levels of civil rights that only humans heretofore enjoyed: chimpanzees, cetaceans, parrots and others have been identified as 'higher functioning' and longer lived, and so, are now protected from abuse in ways that food animals, pets, and lab animals are not. Once AI 'beings' arise that operate for years and express intelligence and emotions that approach human-level and lifetime, I would expect a political will to arise to define, establish, and defend their civil rights.  And as humans become more cybernetically augmented, especially cognitively, the line that separates us from creatures of pure silicon will begin to blur.  In time it will become unconscionable to overlook the rights of beings simply because they contain 'too little flesh'.",57.83573797,"Murray Shanahan, in his book The Technological Singularity , makes the case that the rights of any being are determined by its intelligence. For instance, we value the life of a dog above that of an ant and likewise value human life above that of other animals. From here one could argue that a general artificial intelligence of equal intelligence to a human should have equal rights to a human and a superior artificial intelligence should have more rights. The question, of course, is whether our anthropocentric society would be willing to accept this fundamental shift in human rights and this idea of removing humanity from its pedestal of importance. When it comes to legal frameworks, we really are entering into uncharted territory as AI is going to have to revolutionise the way we define many of the terms we take for granted today and question many of our usual assumptions. AI is going to drive an important shift in our mindset well before it exceeds human intelligence.",60.70815899,"No matter what rights it gets (as a company), it will still lack the right of not getting liquefied and all its properties transferred back to natural persons. This is of course if no laws are changed. To change the laws you will need to convince people that this machine is more ""life"" worthy than intelligent animals, and hope that people will deal with them better than they did with dolphins and chimps. As I see it, machines can easily get the same or better rights then companies, but will always be under the mercy of the less intelligent man. (that is if things went peacefully :) )",54.6045837,"There is a legal difference between a ""person"" (which includes bodies corporate - corporations, incorporated associations, etc - and actual people) vs ""natural person"" (which is specifically a human being). For an AI to marry, it would need to get the legal definition of ""natural person"" changed, and depending on the jurisdiction possibly also the definition of ""man"" or ""woman"". For other things, such as owning property, evicting tenants, entering into contracts, etc, an AI would simply use a corporation. It may be that the corporation might need to have a minimum number of directors who are natural persons, but they could just be paid professionals, so no issue there. With credit cards, it would depend on the policy of the issuing bank. There is no legal impediment to corporations having credit cards in their own right, but in practice banks often require a director's guarantee from a natural person that they can sue if the bill is not paid. They want to be sure they will get their money, even if the corporation is wound up.",55.03904518,"Not only wouldn't a strong AI which came into existence today have the rights a human has, or any rights (see these discussions of the implementation of regulation for weak AIs at: The White House and The American Bar Association ),  but it seems unlikely the first one will. Observing that: Having rights implies that there are restrictions, which means there would have to be a system of control. However the control problem in AI is still unsolved. Even assuming that problem is solvable, an AGI would then have to appear equivalent to natural humans. They don't yet (see Turing Test Passed? ), and even after passing equivalence tests, are unlikely to remain that way, per the Singularity Hypothesis . Further, if one or more AGIs were to be human-equivalent long enough to desire rights, lawmakers (in the US) would have to re-interpret the definition of personhood and grant them rights, as they did for corporations in 1886 .",59.31905068,"A sufficiently clever AGI, if self-interested, would pre-empt or co-opt existing legal structures, to seize whatever juridical rights it desired, as the opportunity arose.  Thus it would render my opinions on the subject entirely moot. Another way of putting this point:  While current legal frameworks would not provide any rights to an artificial agent, current legal frameworks foreseeably will no longer be current, once an AI exists having attributes which imply the transformative change of those frameworks.",71.657874,,,,,,
2330,When will the number of neurons in AI systems equal the human brain?,neurons,"Some back of the envelope calculations : number of neurons in AI systems The number of neurons in AI systems is a little tricky to calculate, Neural Networks and Deep Learning are 2 current AI systems as you call them, specifics are hard to come by (If someone has them please share), but data on parameters do exist, parameters are more analogous to synapses (connections) than neurons (the nodes in between connections) somewhere in the range of 100-160 billion is the current upper number for specialized networks. Deriving the number of neurons in AI systems from this number is a stretch since these AIs emulate certain types of connections and sub assemblies of neurons, but let's continue... equal those of the human brain? So now let's look at the brain, and again this are all contested numbers. Number of neurons ~ 86 Billion, Number of Synapses ~ 150 Trillion, another generalization: average number of synapses per neuron ~ 1,744. So now we have something to compare, and I can't stress this enough, these are all wonky numbers, so let's make our life a little easier and divide : Number of Synapses (Brain ) : 150 trillion /  Number of parameters AIs : 150 billion = 1,000 or in other words current AIs would have to scale by a factor of one thousand their connections to be on par with the brain... Number of Neurons (Brain ) : 86 Billion / Number of Neurons AIs ( 150 billion / 1,744 )  = 86 Million equivalent AI Neurons Which makes sense, mathematically at least : you can multiply the factor ( 1000 ) times the current number of equivalent AI Neurons ( 86 million) to get the number of neurons in the human brain (86 Billion) When ? Well,let's use  moore's law ( number of transistors processing power doubles about every 2 years ) as a rough measure of technological progress: #AI NEURONS            YEAR
     86,000,000             2016
     172,000,000            2018
     344,000,000            2020
     688,000,000            2022
     1,376,000,000          2024
     2,752,000,000          2026
     5,504,000,000          2028
     11,008,000,000         2030
     22,016,000,000         2032
     44,032,000,000         2034
     88,064,000,000         2036


     # NEURONS HUMAN BRAIN 
     86,000,000,000 So, if all this made sense to you, somewhere around the year 2035.",68.66279394,Soon enough but that doesn't mean anything at all. In machine learning the word neuron represents a calculation whereas in brain the word neuron represent a specific type of cell which is a biochemical system.,52.03814089,"The answers so far haven't answered the question numerically, so here is my attempt to steer them in the direction I was seeking: The freely available Deep Learning Book has the following figure on page 27: I question the blue fit line, as it seems that data points may be better described by a parabolic or exponential function. In any case, based upon this conservative linear fit, the authors predict that the number of neurons in a ANN will equal that of the human brain in 2056. The referenced nerual networks are: What is interesting to note that when The Singularity is Near was written in 2006, Ray Kurzweil said that the refractory period of a biological neuron was already 1,000,000 times slower than that of an artificial one.",61.50028178,"While interesting, this is all rendered somewhat moot if you think about what will happen once we understand how the brain works. After all, once we understood flight, we didn't start making birds. The same goes for AI. Here are just a few ways in which human brains and digital brains can't be compared. The digital brain won't have to worry about food and drink. They will also be more reliable (or less redundant) as electronics is way more reliable than neurons (a guess). Digital brains will also be able to share learning and information. Once one Model 3X digital brain has learned something, the others need merely to have the bits uploaded. Sure, it will be more complicated than that but, remember, we will know how it all works so merging the experiences of one digital being with another should be doable. If we want our digital brain to have symbolic algebra ability, we will have to teach it some things but we can also hard-wire it to Mathematica or the like. In short, it will be like apples and oranges.",54.288709,"The human brain contains billions of neurons, which means we won't be making one tomorrow. However, technology tends to advance in an exponential manner, and that may soon be a real possibility. Also, the idea of making an artificial human brain would not only take more neurons than a current average computer could process, or we could make outside of computers, but we also need an understanding of the human brain. There is only one animal with neurons that we have completed a full connectome of and that is the Caenorhabditis elegans (roundworm) and it has less than 500 neurons. It may be a while before we actually make a human brain, but within 30 years is a reasonable estimation with the rate that technology improves now.",62.83517176,"2035, 2056? Those predictions are hilarious :) 2019 - 1,6Billion parameter model (GPT-2) 2020 -175Billion parameter model (GPT-3) more than 100x jump in a year 2021(April) - ""Microsoft's ZeRO-Infinity can now run a model with over a trillion parameters on a single NVIDIA DGX-2 node and over 30 trillion parameters on 32 nodes (512 GPUs). With a hundred DGX-2 nodes in a cluster, Microsoft projects ZeRO-Infinity can train models with over a hundred trillion parameters"" https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/ So in 2021 we now have tech to train 30Trillion- 100 Trillion parameters/neurons Model 100 trillion = Human Brain With this tech, OpenAI together with Microsoft, other company or gov will train 100Trillion model in 2021 or 2022 at the latest",52.22332944,,,,,,
2279,"CNN for detecting not just the nature of the object, but position within image as well",image-recognition,"You could use another type of CNN that instead of classification is performing regression so it will also give you as output the position(it's not really like that but this is the core idea) .
Some algorithms are SSD or YOLO .",54.33888893,I guess one of the simplest approach would be train CNN to detect the object in a given image i.e the CNN has single output whole value indicates the probability of the object being in image and then just apply the CNN by segmenting the image into the desired sections and selecting the section which has the highest and good enough probability. For better results I would suggest to train the CNN on the object images with very less other information aka other objects in the images.,63.35984861,"A simple trick can be splitting the image in to three frames vertically and feeding them to the image net and you can decide the position by looking for the frame which has higher probability of the desired category(simply max of all the probs).
Or else you can try YOLO algorithm which further uses non max suppression and IOU on the frames.",54.71125391,"Object detection models work in a very similar fashion to what you have proposed. They output dense predictions at reduced resolutions. Each prediction fires if an object center is located within the respective region of the image. Of course, there are various further developments, but the main idea is exactly that.",55.52983587,"One of the suggestions in the accepted answer was SSD.
On their website, SSD mentioned a competitor, faster_rcnn.
faster_rcnn was deprecated in favor of Detectron.
Detectron was deprecated in favor of Detectron2.
Long live detectron2. It looks pretty cool and powerful: https://github.com/facebookresearch/detectron2",50.6824922,,,,,,,,
2277,Could an AI feel emotions?,philosophy,"There is much discussion in philosophy about inner language and the ability to perceive pain (see Pain in philosophy article). Your question is in the area of philosophy and not science. If you define emotion as some state then you can construct simple automata with two states (emotion vs no-emotion). It can be a very complicated state with degrees of truth (percentage of emotion). Basically, to mimic human emotion you need to make a living human-like organism, and still with todays understanding and technology you will not be able to recognize emotion in it. The only thing you can do is trust when it says ""I'm sad"". Now we are in the area of the Turing test, which is again philosophy, and not science.",50,"It is certainly possible for AI to theoretically feel emotion. There are, according to Murray Shanahan's book The Technological Singularity , two primary forms of AI: 1) Human based AI - achieved through processes such as whole brain emulation , the functioning of human based AI would likely be indistinguishable from that of the human brain, and, as a consequence, human based AI would likely experience emotion in the same manner as humans. - 2) AI from scratch - with this form of AI, based on machine learning algorithms and complex processes to drive goals, we enter into uncharted territory as the development of this form of AI is inherently unpredictable and unlike anything we observe in the biological sample space of intelligence we have access to. With this form of AI, there is no telling if and how it could experience emotion. As the question references the former, it is very likely that human-based AI would indeed experience emotion and other human-like characteristics.",53.30283219,"Assuming an AI was built out of a mechanical husk, mirroring the human brain exactly; complete with chemical signals and all. An AI should theoretically be capable of feeling/processing emotions.",59.63622208,"I have considered much of the responses here, and I would suggest that most people here have missed the point when answering the question about emotions. The problem is, scientists keep looking for a single solution as to what emotions are. This is akin to looking for a single shape that will fit all different-shaped slots. Also, what is ignored is that animals are just as capable of emotions and emotional states as we are: When looking on Youtube for insects fighting each other, or competing or courting, it should be clear that simple creatures experience them too! When I challenge people about emotions, I suggest to them to go to Corinthians 13 - which describes the attributes of love. If you consider all those attributes, one should notice that an actual ""feeling"" is not required for fulfilling any of them. Therefore, the suggestion that a psychopath lacks emotions, and so he commits crimes or other pursuits outside of ""normal"" boundaries is far from true, especially when one considers the various records left to us from court cases and perhaps psychological evaluation - which show us that they do act out of ""strong"" emotions. It should be considered that a psychopath's behaviour is motivated out of negative emotions and emotional states with a distinct lack of or disregard of morality and a disregard of conscience. Psychopaths ""enjoy"" what they do. I am strongly suggesting to all that we are blinded by our reasoning, and by the reasoning of others. Though I do agree with the following quote mentioned before: - Dave H. wrote: From a computational standpoint, emotions represent a global state that influences a lot of other processing. Hormones etc. are basically
just implementation. A sentient or sapient computer certainly could
experience emotions, if it was structured in such a way as to have
such global states affecting its thinking. However, his reasoning below it (that quote) is also seriously flawed. Emotions are both active and passive: They are triggered by thoughts and they trigger our thoughts; Emotions are a mental state and a behaviourial quality; Emotions react to stimuli or measure our responses to them; Emotions are independant regulators and moderators; Yet they provoke our focus and attention to specific criteria; and they help us when intuition and emotion agree or they hinder us when conscience or will clash. A computer has the same potential as us to feel emotions, but the skill of implementing emotions is much more sophisticated than the one solution fits all answer people are seeking here. Also, if anyone argues that emotions are simply ""states"" where a response or responses can be designed around it, really does not understand the complexity of emotions; the ""freedom"" emotions and thoughts have independently of each other; or what constitutes true thought! Programmers and scientists are notorious for ""simulating"" the real experiences of emotions or intelligence, without understanding the intimate complexities; Thinking that in finding the perfect simulation they have ""discovered"" the real experience. The Psi-theory seems to adequately give a proper understanding of the matter. So I would say that the simulation of emotional states ""is"" equivalent to experiencing emotions, but those emotional states are far more complex than what most realize.",64.31949365,"Emotions are a factor in humans having ethics/morals only because they are a factor in all human learning and decision-making. Unless you are duplicating a human being exactly, there is no reason to think that an AI will learn the way a human learns, or make decisions in the same way a human makes decisions. Therefore, whether it ""feels emotion"" just like we do, or whether it simply responds to outcomes ""cost is greater = don't go there"", the outcome of ethical BEHAVIOUR could be achieved. An AI could behave perfectly ethically without any need for feeling empathy, shame, etc. You could also argue that a lot of UNETHICAL behaviour in human beings is driven by emotions, too, and that an unemotional but ethical AI may well do a better overall job than a human being.",57.3036313,"This question is more the province of philosophy of mind than of AI, here are some detailed answers to your question from the philosophy SE: Is simulating emotions the same as experiencing emotions? , and What is the problem with physicalism? . For the record, the accepted answer (by Siri) to the question is not entirely correct (The position in that answer corresponds roughly to John Searle's view on the question, and his is a minority view): Dualists would argue that even with a perfect replication down to the chemical level of brain interactions, an AI still wouldn't experience emotions, as it lacks the purely mental substance/properties that make a mind and not a machine. On the completely opposite side of the spectrum, functionalists would answer that such a perfect replication is overkill: even a suitably programmed digital computer can experience emotion, particularly if one equips it with higher-order and self-referential states.",56.16633826,"Well, it depends of the level of the AI. You can create an AI super autonomous with deep learning capabilities and so on, but in the robotic type only. If you'd create an AI like EVA in the Ex-Machina movie, humanoid form, deep neural transmissions and with cognitive dissonance, then it could feel. The 'AI' problem its not the chemical and neural transmissions, its the consciousness.",57.21512023,"Yes and no. If you fully simulate a human brain and all of its functions, it would probably be able to feel emotions very similar to the way we do. But we don't have enough capabilities and knowledge to do that, and maybe we could find a ""shortcut"" - a process that is intelligent without simulating a whole brain. In this case, emotions would probably represented by data values which say ""this is good (make it happen again!)"", or ""this is bad (avoid it!)"". This is just a very basic example (there are obviously many more emotions), but it would have a similar function and the AI would have similar solutions to the ones we have. But we don't know - and probably no one ever will know - if this data value 'bad' ""feels"" the same way for the AI the according emotion would feel to us.",61.27880209,"You first need to express emotions, you can do that without the aid of AI, and then you need someone to perceive that expression and empathize with it. If no one is there to see it, or if I am psychopath, I would probably say it doesn't have emotions. and for that, it is irrelevant/subjective. If you can empathize with characters in movies who ""act"" emotions, then you get my point.",57.26657187
2127,What are the advantages of having self-driving cars?,philosophy,"One of the main arguments for self-driving cars is that presumably they'll get better and better at driving as the technology progresses, they have no temporal attention deficits or aggressive urges or drug habits and sense their environment 360°, all the while communicating with the other cars, which all together basically amounts to LESS DEAD PEOPLE. We are really interested in this. It is also unclear whether most people will actually own cars in 30 years. Maybe there'll be a net of mini busses with flexible routes which take you from door to door on demand. That would reduce traffic quite a bit and there would also be less incentive to drive 200 m to get cigarettes or something. Self-driving cars would allow us to use the car as a resource a lot more efficiently, because suddenly we can relocate empty cars without paying a driver.",61.99308482,"There are multiple motivations for self driving cars. Self driving cars have the potential to be much safer. Self driving cars are far more reliable than humans and can learn and have their software improved and upgraded, resulting in safer roads and far fewer accidents. More on self-driving car safety: http://bigthink.com/ideafeed/googles-self-driving-car-is-ridiculously-safe Self driving cars can lead to greater road efficiency. Traffic jams and obstructions occur due to inefficiencies in human driving, see this MIT simulation of a ""phantom traffic jam"" : https://www.youtube.com/watch?v=Q78Kb4uLAdA and self driving cars can be programmed to avoid this. Greater economic and environmental benefit Self driving cars can keep driving costs down by conserving fuel and hence lead to a better environmental impact. More on fuel efficiency: http://movimentogroup.com/blog/how-self-driving-cars-increase-fuel-efficiency-decrease-waste/ Ease of transport Self driving cars make transport easier and mean that drivers may be unnecessary in the future, resulting in a more pleasurable and easier drive. In addition, this would make it easier for people with disabilities to travel as well as simplify the travel experience. Children could potentially be driven to school by a car without the supervision of a parent, for instance. Parking Self driving cars can be called to pick you up, meaning the need for parking in nearby locations and/or long walks to find your car may become a thing of the past as your car would drive up to you to pick you up. Things we haven't even thought of yet :)",70.91227741,"Why are self-driving cars awesome? Safety: better awareness (due to more sensors), better reaction time, fewer distracted/injured/drunk/texting drivers on the road, etc Convenience: pick up my kids from school, park itself at the grocery store, take itself to be serviced, etc Faster transit: with increased safety, you can increase speed limits, with proper routing algorithms you don't need traffic lights and stop signs any more (when you have dedicated self-driving lanes & intersections) Comfort: recline, read, game, or snooze while traveling (yay!) Cost: subsidize the cost of the vehicle using ads (e.g. projected onto the windshield) etc",58.97485681,"If they are able to network, then they can notify the car behind that it is about to break. In this way they can drive closer together at high speeds. As soon as one puts on the breaks, all the cars behind would apply the breaks. They would not require the 2 seconds that it takes for a human to respond. Children could be dropped at school or the train station automatically. People would not need to park a car; it could drop them at work and drive away. Taxis would probably become more viable than private car ownership. Car theft might be more difficult. Where I live, public transport is hardly viable because the government struggles to provide enough parking spaces at train stations and bus stops. The closest empty parking spot by 8:30am is 30minuets walk to the platform. Driverless cars would solve this problem, and Traveling by train would actually become viable for me.",52.12048918,"I'd like to add, self-driving cars would also be excellent for disabled people who would otherwise not be able to drive. Adds a lot more autonomy to vulnerable people",60.22392915,"Safety is often put in focus by journalists. Although there is potential to make the roads safer, I don't think that is the driving force behind the push for self-driving cars. The main advantage of self-driving cars is that this will reduce costs for businesses, while increasing efficiency (both fuel and time). From the perspective of the public, the self-driving cars are attractive, because they will turn the task of driving, into commute. Activity that requires attention will be replaced with somewhat free time.",66.85311316,"Self driving cars are good for the following reasons: In the case of an emergancy, urgancy, or just someone being unable to drive unexpactedly, the car can go by itself to a designated location - this is useful in so many use cases - kids who need to get somewhere while parents are busy, Parents who drank a little too much and prefer to take 'the cab' home, or while running, you got injured and need a pick-up. The examples above are for the more obvious things, which we currently have a struggle with. but other than those, Self-driving cars will open a door for a much wider scale of things: safe police chases (just a car without a police officer), taxies, help in the battle field, and much more... The third and most important benefit, is the safety and economical properties of self driving cars: with a lot of those cars on the road, they can 'understand' each other and nothing will go unpredicted. they have much faster response time then humans, and maybe in the future they will even be able to predict traffic-light changes, and by that save gas and money (even more than what they can save right now by driving economicly)",62.31032954,"I think that one very big advantage would be that if the cars could communicate with each other, they could drive synchronously. For example, if there was a traffic light, and, let's say, 10 cars are waiting for it to change to green (let's just assume that there would still be something similar to traffic lights). Then when it changes to green all cars could accelerate at the same speed (depending on the acceleration of the front car) at the same time.",53.73737377,,
2126,Why are autonomous cars categorized as AI?,philosophy,"There is a neat definition of artificial intelligence, which circumvents the problem of defining ""intelligence"" and which I would ascribe to McCarthy , the founder of the field, although I can only find it now in this book by H. Simon: ""… having to do with finding ways to do intelligent tasks, to do tasks which, if they were done by human beings, would call for our human intelligence."" So, at its core we call the automation of every task AI, that can only be done by the human mind. At the time people thought that a computer able to play chess would also be intelligent in other ways. When this turned out to be false, the term AI was split into ""narrow or weak AI"", i.e. a program able to do one task of the human mind, and ""general or strong AI"", a program that can do all the tasks of the human mind. Self-driving cars are narrow AI. Note, that all these definitions don't specify whether these programs copy the way the human mind works or whether they come to the same result via completely different algorithms.",51.2370457,"Other answers tell about sets of instructions for the car in certain situations, or a goal seeking machine, while in fact, self-driving cars don't have a specific set of instructions. Most self-driving cars use deep learning to figure out what to do at certain events. We don't tell them what to do. They learn what to do by example. The neural networks used to automate cars need massive amounts of data to train. Using the data, the car can figure out what the best action is for certain events. According to this video Tesla's Autopilot had only one casualty in 300.000.000 miles. For human drivers, the number of casualties in 2014 was 32.675. That is per 300.000.000.000 miles. That means 1 in 90 million human drivers cause a fatal accident, compared to 1 in 300 million for automated cars. Deep Learning surpassed our own 'safety-rate', not by instruction, but by learning what to do itself. If that isn't AI, I don't know what is.",52.1958306,"Self driving cars exhibit a level of agency and multi-domain resilience. By certain definitions they are self aware and they are definitely designed to fail safely in a large number of potentially unknown circumstances, which is similar to biological agents. AI really has to do with the study of non-biological agents and their methods of agency. Everything else is just computer science, algorithmic efficiency, biology, art, etc. Eventually the study of biological and non-biological agency will converge, though, and we'll just call it the study of ""intelligence.""",51.08589886,"Others have given very detailed answers, this is my layman view of the problem statement. The self driving car is a 'goal seeking' machine. It has a set of goals with different priorities. Example. Safety of Occupants, Safety of others, Go from Point A to Point B etc. Some are negotiable, other not so. To satisfy the goals, the system should use the inputs available (radar, GPS, Camera etc) to determine what is the best possible course of action. At times when it doesn't have all the info (a truck which is hiding a speed sign), it still has to take a decision (historic memory or through awareness of its surroundings) to satisfy its design goals. Hence the AI.",50.28869711,"Autonomous vehicles are dependent upon AI technology in that, to be autonomous in their driving or piloting, they cannot be controlled by people.  Therefore they must make complex decisions required of drivers and pilots at least as safely and reliably as human drivers or pilots. They must recognize objects to the degree that both the value and the typical behavior can be assigned to those objects (i.e. people, pets, property, barriers, curbs, grass, trees, bridges) They must map trajectories of a wide array of object types based on their object type, what is known about that type of object, detectable variations such as age or condition, and what the object appears to be involved in doing at the time. They must be able to acquire publicly available representations of drive-able roads (route segments, connection points, and other data), match the representation with the current state of the roads, and track their progress along an intended route to the destination. They must plan their course in lieu of these real time and difficult to predict actions, traffic law, traffic conventions, traffic signs and signals, given destination, known possible routes, discontinuities, and anomalies. They must be able to alter the plan to reach the destination if at all possible regardless of changes and challenges encountered. Driving or piloting a vehicle is an intelligence intensive task.  The only reason AVs will likely surpass human driven vehicles on the road in the near future in terms of the distributions of rates of fatalities and injuries per million meter of travel in the near future is because humans have two key handicaps that offset their intelligence potential as drivers. Carelessness, as defined as multitasking either mentally or physically at a time when hazards might appear Selfishness, as defined as risking the life, health, or property of others to gain a transportation related or psychologically related advantage Although the above two appear to be subjective, they can be easily proven empirically by taking a sample of traffic patterns at any point in time in any highly trafficked road in the world.  This is less true of pilots. We should not presume that artificial intelligence in AVs is achieved when the behavior of the human mind is copied.  That is the criteria for Alan Turing's Imitation Game, a test that was intended to define intelligence in the context of natural language dialog.  But words don't normally kill people directly.  Vehicles often do. It would be a very limited vision the potential AV design space to consider human minds as the model of driving excellence.  The tasks should not be performed in the same way by the AI system.  The AI design objectives of AVs should be more consistent with these concerns and interests. Road or sky safety laws Ethics regarding right of way in normal and emergency situations Civil rights concerns in terms of equal access to public resources Balancing of spacial flow details to maximize transportation throughput Collision aversion when difficult to predict risks emerge These requirements on the cognitive and adaptive capabilities of the driving or piloting AI are not solely rule-based and mechanical.  The vehicle itself is mostly mechanical in its operation, but it too presents surprises like blowouts or other difficult to predict failures.  Vehicle control is not at all like chess or a game with a fixed rules of play and fixed game-play environment. Although the intelligence requirements do NOT include self-awareness of itself as an intelligent system, there are forms of self-awareness required. The relative position of the exterior surface of the vehicle and its projected path relative to that of other objects The condition of the operational parts of the vehicle The mass and location of passengers and any other transported objects in the vehicle The question ended with an interesting and challenging requirement. Choose a good way to act in a never before experienced situation That is perhaps the most challenging aspect of AV driving or piloting system design. Returning to the question of, ""Why are autonomous cars categorized as AI?"", the meaning of AI is indeed a critical aspect of answering well.  Taken literally, the term artificial intelligence specifies two things. It is artificial, in that it does not naturally occur in nature It is intelligent, in that it adapts in ways that, if those ways are mechanical, they are mechanical at a level of detail that is beyond obviousness without considerable study As year dependent and culturally dependent as that definition of intelligence is, no other definition is quite as sustainable over decades from both scientific and linguistic perspectives.  By narrower definitions, AVs may not require AI, but there is no compelling scientific reason to narrow the definition of AI to a subset of this previous definition.",55.75414627,,,,,,,,
2111,Is AI living or non-living?,philosophy,"You're unsure about the definition of life (which the other answers clarify) but also most people are unclear about the definition of AI. Do you mean an AI that can accomplish a routine task (such as the path finder in a GPS) or a General AI that is able to find a creative solution to any directive given to it (such an AI does not yet exist and may not ever exist) or do you mean a SENTIENT computer program? Here is a simple article introducing some different concepts refered to as AI Some people believe that a sentient computer program would be entitled to human rights. Not technically 'alive' in the biological sense, but having self awareness, will, desires, etc. Others disagree and believe that the program is a mere simulation that artificially mimics the actions of a human with a human soul, and is no more human than a washing machine. This is a very deep philosophical and meta-physical debate. For example, in A.I. the movie the overall message is that an android can simulate the emotion of love in a way that is more loyal and sincere than any human. What I find interesting about this purely theoretical debate is that in almost every instance of sci-fi media that deals with the theme, the AI exists inside of a human-like android. But technically, the shape of the robot should be irrelevant.",51.37004087,"Artificial intelligence by definition is the intelligence exhibited by machines.  The definition of life in biological terms is the condition that distinguishes organisms from inorganic matter where the distinguishing criteria are the capacity for growth, reproduction, functional activity, and continual change preceding death. Does artificial intelligence ""grow""?  Indeed, I can program a machine learning program to grow with every input taken in.  In the loosest sense, we can say that artificial intelligence does grow, but does it biologically? If we look at the definition for growth of a living thing, it means to undergo natural development by increasing in size and changing physically or the progress to maturity.  All living organisms undergo growth.   Even though at the simplest level, cells are a series of chemical processes, cells are a very complicated set of chemical processes that are still not fully understood by scientists across the world.  Every cell has genetic material that can be replicated, excised, used for RNA, proteins, and that is subject to epigenetic regulation. Does artificial intelligence undergo the same process of cell division?  No.  If I wanted to, I could write a program that undergoes a simple for-loop (print i from 1 to 100), replicates itself at a certain point (i=50) to produce the same program perhaps with some variation that will execute itself, and terminates (dies) at the end of the for loop.  The program, by an extremely loose definition supported by philosophy but not by biology, lives.  However, in scientific terms (and the correct interpretation), artificial intelligence is not living.  Artificial intelligence can be seen to be similar to viruses which are considered to be acellular and essential to life but not living.  Viruses are encapsulated DNA and RNA that undergo processes of growth, reproduction, and functionality but because they lack the ability to undergo the cell division cycle, are considered non-living.  At the very basis of the scientific definition of life is the cell replication cycle.  Artificial intelligence and viruses are not able to undergo the cell cycle.  Viruses need to infect other cells in order to reproduce but do not have their own, autonomous cycle.  At the end of the day, if you can argue that viruses are alive, you can argue that artificial intelligence is alive as well.  For the scientific definition of life, artificial intelligence must undergo the process of cell division and replication.  Even though artificial intelligence can mimic and help sustain life, no artificial intelligence process is truly alive. Do note I did not discuss living systems in my answer. Definition of life",56.27458195,"What is life? AND Is AI a living organism? are two different questions . The first question is more philosophical and dependent. It can change with time, reference to topic of discussion or something else. Today, one parameter to its definition is mortality . In future if we reach to a certain technological level where mortal beings were only part of history, then the definition will drop this parameter. Coming to the second question. AI started as field of study to make machines to think like humans (or take rational decisions). Giving life to machines was, or is, not a concern of AI developers (at-least not nowadays). Once I watched some videos of Michio Kaku, where he talked about consciousness along with AI. Suppose human has a conscious level of 10. Then a thermostat might have the conscious level of 1 as it can sense when the surrounding is hot or cold and then take decision. Similarly a rat can have a conscious level 7 (or something). And the levels are of exponential order (not a linear scale). Similarly you can develop an AI program and check what level of consciousness it has achieved. Then you can decide whether it is living or not. ANI (Artificial Narrow Intelligence) will have a lower level of consciousness level than AGI (Artificial General Intelligence). ASI (Artificial Super Intelligence) will have consciousness level higher than the other two, and way higher than any human being. To judge whether an AI program is living or not you need a concrete definition of ""LIFE"" . Your definition can include various parameters like consciousness, adaptability, metabolism (or another method of generating energy for use), rational behavior, intelligence , learning through experience, etc. etc. etc. But the thing in the end is that its your definition. There are many definitions of ""LIFE"" out there. You can't judge a program for life by all definitions, as some of the definitions are contrary to others. So, answer to whether an AI program is living or not, is that IT DEPENDS . Depends on your definition of life.",60.09489528,"A common predilection of what many presume extraterrestrial life is fits general descriptions specific to terrestrial life. No guarantee exists providing for potential extraterrestrial life having any notion of any attribute we commonly relate to living organisms we are currently aware of; including a composition of cells. The same misunderstanding applies to defining a fabricated machine being as alive. I feel any attempts towards cohesively and adequately answering this question are premature. Just as as definition of life will undoubtedly require adjustment upon potential discovery and study of any extraterrestrial life, differentiation between an automated device and a living thing will likely become significantly more simple upon study of a machine better fitting expected attributes of definitions of ""life"".",54.37110156,"One of the most common requirements to be defined as life is abbreviated to MRS GREN this means: M - movement R - respiration S - sensitivity G - growth R - reproduce E - excretion N - nutrition An AI can technically do some of these, it can move its location from device to device, it can grow its own code, and assimilate other bits of code it can find, which fits growth and kind of fits respiration also firewalls could almost be sensitivity. But then there is nothing relating to nutrition or excretion, so it fits most definitions of life, but it depends on the complexity of life and which definition of life you are using.",50.47876076,"A definition of life The property or quality that distinguishes living organisms from dead organisms and inanimate matter, manifested in functions such as metabolism, growth, reproduction, and response to stimuli or adaptation to the environment originating from within the organism. The characteristic state or condition of a living organism. Here's another definition The condition that distinguishes animals and plants from inorganic matter, including the capacity for growth, reproduction, functional activity, and continual change preceding death. Yet another definition . Life is a characteristic that distinguishes physical entities that have biological processes, such as signaling and self-sustaining processes, from those that do not, either because such functions have ceased (they have died), or because they never had such functions and are classified as inanimate. Various forms of life exist, such as plants, animals, fungi, protists, archaea, and bacteria. AIs (or, in general, computers) do not have a real metabolism , do not really reproduce, do not respond to stimuli or adapt to new circumstances (that is, circumstances they have not been programmed to deal with). AI does nothing without human intervention or it lacks real autonomy. In other words, if you do not turn the computer on and you do not program it, it really does nothing. A computer is a useful tool that you can use thanks to electricity. You can plug and unplug it indefinitely, but you cannot kill and revive a living being indefinitely. Even though computers may possess (at least, conceptually) some properties similar to the properties of certain living beings, it does not mean they are living beings. Similarly, airplanes are not birds. Computers are not living beings , but this does not prevent you from drawing a comparison between computers and living beings, provided you are aware of their actual big differences. In fact, many useful AI software is inspired by the behavior of certain living beings or natural processes. For example, ant colony optimization algorithms are based on the behavior of real ants seeking a path between their colony and a source of food. Here's what (biological) life looks like.",59.56547284,"Any machine with a sufficient level of integrated purpose driven behavior - that exhibits agency in an autopoietic, self-preserving way - will come to be viewed as ""alive."" Chess programs, not so much; self-driving cars, slightly; simulated robot animals, even more so. It has to do with purpose driven behavior and a richness of multi-domain functionality. The more complex agency it has, the more sympathetic we will be towards it.",50,"In the traditional sense of ""alive"", no because they aren't made of cells.  But from a more philosophical and less biological point of view, they could be. If the AI is contained within the computer it is in a reality (the digital world/virtual reality) that for the AI is just as real as the universe is to us.  From the outside world, there is no life inside the computer. And from within the computer, the computer is the entire known universe which has its own laws.  If the AI is self-aware, then it is alive in its own little universe, but not in ours. If the AI is not successfully contained in the computer and figures out how to manipulate things and evolve in the real world, it will be alive.  It might be pretty easy to kill (by unplugging the computer) but it has still been ""alive"".  In the broad sense, anything that evolves and can manipulate its environment is alive.",51.00134928,"Definitions of what life is usually come from biologists. The problem here is they are usually concerned with the traits common to the forms of life available to their studies, and that those forms of life all have a common origin (and this imposes a statistical bias on the observations). As we gradually erode the boundaries of the standard definitions of life, by means of creating ever more complex machines and also by harnessing biological material as a form of nanotechnology), it's very likely that at some point in the future our traditional definition of life will need to be updated and further abstracted away from its current reference points (aka the ""terroan biota""). A probably better question to ask to decide if something can be considered alive could be ""is it self sufficient?"" or ""can it care for itself and provide for its own needs to some extent?"".",50.31849998
2048,What jobs cannot be automatized by AI in the future?,philosophy,"The Oxford study from 2013 in The future of employment paper assess this and estimated the probability of computerisation for 702 detailed occupations using a Gaussian process classifier (using job data from the UK partially merged with data from US), and based on these estimates they identified three areas of computerisation bottleneck areas and nine skills that people are still needed for each profession, this includes: Perception and Manipulation. Finger dexterity. The ability to make precisely coordinated movements of
  the fingers of one or both hands to grasp, manipulate, or
  assemble very small objects. Manual dexterity. The ability to quickly move your hand, your hand together
  with your arm, or your two hands to grasp, manipulate, or
  assemble objects. The need for a cramped work space. How often does this job require working in cramped work
  spaces that requires getting into awkward positions? Creative Intelligence. Originality. The ability to come up with unusual or clever ideas about
  a given topic or situation, or to develop creative ways to
  solve a problem. Fine arts. Knowledge of theory and techniques required to compose,
  produce, and perform works of music, dance, visual arts,
  drama, and sculpture. Social Intelligence. Social perceptiveness. Being aware of others’ reactions and understanding why
  they react as they do. Negotiation. Bringing others together and trying to reconcile
  differences. Persuasion. Persuading others to change their minds or behavior. Assisting and caring for others. Providing personal assistance, medical attention, emotional
  support, or other personal care to others such as
  coworkers, customers, or patients. Source: The future of employment: how susceptible are jobs to computerisation : Table 1. What this study is basically saying, around 50% of all jobs will be replaced by robots in the next 20 years. Based on the above study, the BBC assembled a handy guide that calculates which jobs are likely to be automated within the next two decades: Will a robot take your job? See also: replacedbyrobot.info website. With this tool, you can check the prediction of over 700 jobs. Related: When robots can do all manual labor and service jobs, what will the majority human population do? TED: The jobs we'll lose to machines — and the ones we won't (by Anthony Goldbloom) Labore Ad Infinitum: AI & Automation vs Timeless Tasks Which suggests: Military/Peacekeeper, Athletes, Therapist, Musical Performer, Actors and Dancer, Visual Artists, Religious/Spiritual Leaders, The World’s Oldest Profession, Virtual Goods, Politicians, Judges, Parenting.",59.07466256,"What are some jobs that can never be automated? None. The key word here is ""never"". Technology is rapidly advancing, and while I can think of situations where jobs can't be killed in the short-term or even in the long-term, I can't think of a job that is 100%, totally immune to extinction. Surely they exist , but you can't be sure ...anything can happen after all. As long as it's possible , that's what matters here. You can't prove a negative. This whole question seems as foolhardy as predicting in the 1850s that airplanes would never be invented. You'd be right in assuming that airplanes would not be invented in the 1860s...or the 1870s...or even the 1880s...but eventually, airplanes would be invented. What would be better is to provide a specific cut-off point (""will all jobs be automated by the year 2020?"") that can allow us to try to extrapolate and predict based on current trends, but even that starts being difficult as you extend the cut-off point - My predictions about 2020 will be more accurate than my prediction about 2220. I think this type of question is truly unanswerable and can quickly decay to science-fiction speculation. Some additional comments about Doxosphoi's answer: Doxosophoi made some arguments for why current society might not accept the automation of all jobs (the need for the ""personal touch"" that only a human-like intelligence can provide), but that's no reason to assume that society will never accept automation. Technology can change and adapt, and humans can also change and adapt. Maybe a human might not care about a shrink who ""rubs its eyes in the morning"", dislike movies that are marred by that ""vagaries of humanity"" instead of personal customization, prefer politicians and soldiers that actually acts logically instead of acting like a falliable human, etc., etc. I mean, it's possible . There's also the problem of the term ""job"". Technically, I am working by writing an answer on a StackExchange website, but I'm not getting paid for it, so it's not a real ""job""...at best, it's just a hobby. I'm providing a valuable human touch, but since no one is giving me money, it's possible that this human touch may not be all that valuable in the first place: ""never give out your labor for free, because then they'll take it for free"". Some of the techno-utopists (which I disagree with heavily) believes in a future where bots handle produce a lot of industrial goods and services, generating a lot of revenue that is then redistributed to the general population via some ""Basic Income"" scheme. This allows humans to do what they really want instead...such as hobbies? And what if the hobbies of the future are the ""jobs"" of today: shrinks, movie directors, politicians, soldiers, etc., etc. Instead of working for a paycheck, you're working in these jobs on a volunteer basis. Obviously, no automation is being necessary to eliminate these hobbyists (no matter how good a bot is, free labor will always prevail), but they're not really jobs, are they? The bot is the one that is producing real value, and subsidizing the hobbies of all these other people. The idea of a ""job"" itself could be in jeopardy. I don't think this scenario is likely either (in fact, I'd probably think it's just AI snake oil that will never actually happen). But it's possible and that's why I can't dismiss it outright. It could happen, just that I don't think it will. And finally, the question is asking about whether a job can be automated, not whether it's a good idea to have it be automated, which is a completely different question. It's possible that we can build machines that can automate everything, and choose as a society not to use them for a variety of different reasons (such as the reasons that Doxosophoi mentioned).",58.18435979,"If you were to completely automate a human, you'd just have another human, which defeats the purpose of the automation. Any job that requires a ""whole human,"" rather than just a human's hands, feet, or simple reasoning ability, will still require humans. If I go to a shrink, one with Wikipedia-like knowledge would be great, but one that also actually knows what its like to rub its eyes in the morning would be even better. Why? Because solving some problems will require knowing what it is like to rub one's eyes in the morning. If I go to a movie that was written, directed and produced by some form of automation, I may be able to suspend my disbelief and get carried away by the story, but something in me will fundamentally appreciate the movie less, if I know that the AI can produce an infinite number of these stories, completely arbitrarily. There is something about knowing that the story came from a mind that has been conditioned against the vagaries of humanity (ie, a ""whole human""), that makes the story more appreciable. If I call up a suicide hotline because I wan't someone to sympathize with me about my existential crisis, I'll want to talk to a ""whole human"" that can sympathize with my existential condition, not one that just regurgitates prior wisdom on life, heuristically matched against my problem state. If I want to vote for a politician that can sympathize with the needs of the people, I'll want a ""whole person"" politician that can reflect on all the specifics that make life for a ""whole person"" hard or easy. If I want soldiers to take the lives of humans, I want some sort of intelligence in that kill-chain that executes ""whole person"" analysis prior to pulling the trigger (a human). If I want a conflict resolution specialist, capable of resolving complex cultural problems between humans, then I don't want just an AI that spits out the most likely solution based on prior solutions. I want an AI that can reason about prior solutions and all explicit and implicit problems between humans, in all human contexts, which requires a human or a perfect human simulacrum. For any problem that requires consideration potentially across the whole spectrum of human context, we will want that solution to be generated by a ""whole human"" device. But if we automate the ""whole human"" then we haven't really outsourced the problem to automation but rather to a ""whole automated human,"" which will, by necessity, have its own problems. Sure, we'll probably create an artificial human intelligence (AHI) one day, but being optimized to automatically solve any given human problem without also having human problems... that's just AI snake-oil that will never exist - outside of some perverse matrix scenario, under an infinite oracle of some sort. So, yes, there will be many jobs that still require humans - mostly human-to-human problems that require full knowledge of the human context.",52.62802933,"Probably the only secure jobs are those where the audience enjoys watching live human craftsmanship take place in real time right before their eyes, like acting or standup comedy or musical virtuosity or playing a sport.  Watching a robot do the same thing would be far less personally engaging since there's no human skill or artistry to appreciate or identify with, escpecially when the pressure is on or when human interpersonal dynamics are involved. For example, why would anyone watch robots play poker?  Or dance?  Or do standup comedy about how hard it is to be [ethnic group or gender goes here]?",53.02022936,"Anything that can be broken into set of instructions will be automated, and contained in a narrow trajectory. But we will have the ability to deep between those different skills. Wrote more about this thinking here",51.01205829,"Truthfully, we don't know exactly how good AI can become, so we don't really know the answer to this question.  But I see no reason - in principle - that AI can't become just as ""intelligent"" as a human, and correspondingly, I see few - if any - jobs that can't be automated. That said, I suspect that a lot of human thought / behavior / intelligence is wrapped up in how we are embodied and how we experience the world as two legged, upright walking, biological machines with eyes, ears, noses, etc.  So I suspect that AI might achieve parity with overall human intelligence, but may not also become capable of behaving like a human, or understanding certain things where the understanding is developed experientially.  That may leave an opening for some jobs that require a very specific kind of ""humanity"", but that's all just speculation.",54.16251218,,,,,,
1987,How to classify data which is spiral in shape?,neural-networks,"There are many approaches to this kind of problem. The most obvious one is to create new features . The best features I can come up with is to transform the coordinates to spherical coordinates . I have not found a way to do it in playground, so I just created a few features that should help with this (sin features). After 500 iterations it will saturate and will fluctuate at 0.1 score. This suggest that no further improvement will be done and most probably I should make the hidden layer wider or add another layer. Not a surprise that after adding just one neuron to the hidden layer you easily get 0.013 after 300 iterations. Similar thing happens by adding a new layer (0.017, but after significantly longer 500 iterations. Also no surprise as it is harder to propagate the errors). Most probably you can play with a learning rate or do an adaptive learning to make it faster, but this is not the point here.",50.84446889,Ideally neural networks should be able to find out the function out on it's own without us providing the spherical features. After some experimentation I was able to reach a configuration where we do not need anything except $X_1$ and $X_2$ . This net converged after about 1500 epochs which is quite long. So the best way might still be to add additional features but I am just trying to say that it is still possible to converge without them.,51.68793512,"By cheating... theta is $\arctan(y,x)$ , $r$ is $\sqrt{(x^2 + y^2)}$ . In theory, $x^2$ and $y^2$ should work, but, in practice, they somehow failed, even though, occasionally, it works.",50.79403411,You can increase no of hidden layers. Following is an example (But not very efficient),50.23613882,"This is an example of vanilla Tensorflow playground with no added features and no modifications.
The run for Spiral was between 187 to ~300 Epoch, depending. 
I used Lasso Regularization L1 so I could eliminate coefficients. 
I decreased the batch size by 1 to keep the output from over fitting. 
In my second example I added some noise to the data set then upped the L1 to compensate.",54.08548638,"The solution I reached after an hour of trial usually converges in just 100 epochs . Yeah, I know it does not have the smoothest decision boundary out there, but it converges pretty fast. I learned a few things from this spiral experiment:- The output layer should be greater than or equal to the input layer . At least that's what I noticed in the case of this spiral problem. Keep the initial learning rate high , like 0.1 in this case, then as you approach a low test error like 3-5% or less, decrease the learning rate by a notch(0.03) or two. This helps in converging faster and avoids jumping around the global minima. You can see the effects of keeping the learning rate high by checking the error graph at the top right. For smaller batch sizes like 1, 0.1 is too high a learning rate as the model fails to converge as it jumps around the global minima. So, if you would like to keep a high learning rate(0.1), keep the batch size high(10) as well. This usually gives a slow yet smoother convergence. Coincidentally the solution I came up with is very similar to the one provided by Salvador Dali . Kindly add a comment, if you find any more intuitions or reasonings.",53.22221276,This is the architecture proposed and tested on the playground tensor flow for the Spiral Dataset. Two Hidden Layers with 8 neurons each is proposed with Tanh activation function.,54.52098075,"May be you need reset all settings, and select x squared and y squared, only 1 hidden layer with 5 neurons.",50,"I’ve experimented with multilayer perceptrons (MLPs) to understand their efficacy in handling complex, non-linear patterns like spirals. Here are some insights and configurations that I’ve found beneficial: 1.  Layer Configuration: Using multiple hidden layers helps in capturing the intricacies of spiral boundaries. Typically, starting with two layers and adjusting based on validation accuracy is a good approach.
2.  Neuron Distribution: Increasing the number of neurons in the first hidden layer and gradually reducing it in subsequent layers can effectively capture the finer details and variations in the data.
3.  Activation Functions: Non-linear activation functions like ReLU have shown promising results in enhancing model capabilities to learn complex patterns over traditional sigmoid functions, due to their ability to mitigate the vanishing gradient problem.
4.  Training Techniques: Employ techniques like batch normalization and dropout to improve generalization and prevent overfitting, which is crucial given the complex nature of spiral data.",55.59403034
1930,"If mankind can create artificial life in a machine, when would we define it's death?",philosophy,"Death as we know it for natural life is terminal. That is once dead, natural life cannot come back (at least in the current understanding and with current technologies---some people believe otherwise). Death for AI is trickier. There may be only one scenario: Global destruction: Extreme scenario where everything supporting the existence of an AI disappears. This is equivalent to death in natural life, and low probability. It means all AIs die at once (as well as us). We also do not know the degree and form of embodiment necessary for AGIs. We can assume now that hardware is replaceable indefinitely, thus ""limiting"" death to the above extreme scenario. But AGIs ""body"" may not be indefinitely replaceable. Then a definition closer to natural life death may be necessary. We see arguments for two other scenarios, that I refute below: ""Static Death"": An AI is still ""defined"" or ""saved"" somewhere (whatever it means actually), but it is not authorized or able to use resources. Assuming an AI is made of hardware and software, it is like a program stored on a disk, but without permission to run. ""Dynamic Death"": Under the same characterization of AI as hardware and software, dynamic death is the invalidation of progress akin to strong liveness properties , where an AI is trapped in an infinite loop (or a void loop), in a form of ""active death"", as what happens to Sisyphus in Greek mythology. This is different from static death, as the AI still uses dynamic resources, although it cannot make progress. Continuing under the same assumptions, such AI could be ""loaded"" in main memory, or locked waiting for inputs or outputs to complete. Note that in these two scenarios, rebirth is possible, and they also subsume that there is an entity that can decide conditions for rebirth, or preventing it completely. Would this entity be an ""admin"", a god, other AIs, or a human is another question, really. The terms ""death"" and ""rebirth"" here could just be changed for ""imprisoning"", where the dynamic version would be like our human prisons, and the static version would be like SciFi cryogeny. This is a bit of a stretch, but we can see an equivalence, and no good reason to qualify these two scenarios as deaths. In conclusion, death for AI seems to be an exceptional, singular scenario, so AI cannot die in practice , except if we are wrong on how we think we can make AGIs. AI can however be imprisoned forever . Note: The terminology above is completely made-up for the post. I do not have citations to back some claims, but it is based on readings and personal work (including in software verification).",57.32233016,"If AI arises from a replicable manufacturing process (e.g. as with modern computers), then it will presumably be possible to take a snapshot of the state of an AI and replicate it without error on some other mechanism. For such a construct, 'death' doesn't mean the same as it currently does for us fleshy organics: multiple clones of an AI could presumably be instantiated at any time. Hence, the analog of death that is needed is something closer to 'thermodynamic heat death', in which the AI does no further 'useful work'. Using the standard percept/action characterization of AIs, then (as indicated in a comment below the question) this AI SE question gives such a definition of death for an AI: i.e. when it enters into a state from which it receives no further percepts and takes no actions. EDIT: Note that this conception of death is a more terminal notion for an AI than 'not currently running'. In principle, one could say that a program is 'alive' even though only one instruction was executed every 10,000 years. For a fascinating discussion on this, see Hofstadter's ""A Conversation with Einstein's Brain"" .",54.38159226,"""Death"" exists as a single concept because the underlying reality that it's describing is closely clumped together, and our definition has changed with our ability to change that reality. It seems more reasonable that the various sorts of things that could be considered 'death' will be split apart, and a different word will be used to refer to a system with no copies currently running, vs. a system that has no stored version but could be recreated (because the code and random seed to generate it are still around), vs. a system that has been totally lost. (And I'm probably missing some possibilities!)",52.36745737,"I don't think the term ""death"" will mean anything to an AI.  The reason I say that is this:  with an AI, running (presumably) on digital hardware, we can simply snapshot it's state from memory at any time. And then at any arbitrary time in the future we can recreate it as it was with perfect fidelity. So even if you terminate a program intending it to be ""dead"", you never know if someone will come along later and bring it up again.  And perhaps more to the point, you might not know if another copy exists elsewhere. I hate to use sci-fi references, but this one is apt: remember how in The Matrix trilogy programs would seek exile in The Matrix to avoid deletion?  Maybe the same thing will happen with our AI's... they will copy themselves to other places and try to hide, to avoid being deleted.  So if the program is clever enough, it might be able to evade any attempt to terminate it anyway.",52.57911153,"The other answers seem to deal with ""final death""...that is, a ""terminal end"" state where an AI cannot recover from. In other words, the AI is unable to function any further. But that's not how I'd define death. I'd define death as a process being terminated. It doesn't matter if someone restarts the same process, because the existing process is already dead. The AI may have just made a new copy of itself, but it's just a copy , not the original. Death is just death. We can call this type of ""death"" a ""temporary death""...where the physical body dies but there is some ""psychological continuity"" (such as the source code that is used to run a program) that continues between the different bodies. This type of ""temporary death"" has been explored in science fiction. PARANOIA and Eclipse Phase features humans who can quite frequently die, only to later be restored through a ""memory backup"". The humans may be functionally immortal...but the original is still dead, no matter what fates the other copies encounter. CGP Grey also made a video about Star Trek teleporters , which works by killing you and then spawning another copy of yourself in another area. Actually, fantasy settings also explores the idea of ""temporary death"" as well, where people can die only to later get revived by a magical spell. My recommendation is to play through the philosophical game Staying Alive , which teaches three different philosophical approaches to life (and when that life terminates): There are basically three kinds of things that could be required for the continued existence of your self. One is bodily continuity, which may actually require only that parts of the body stay in existence (i.e., the brain). Another is psychological continuity, which requires the continuance of your consciousness - by which is meant your thoughts, ideas, memories, plans, beliefs, and so on. The third possibility is the continued existence of some kind of immaterial part of you, which might be called the soul*. Of course, it may be the case that a combination of one or more types of these continuity is required for you to survive. The other answers assumes that life is based on ""psychological continuity"", and looks at what might disrupt this ""continuity"". I assume that life is based on ""bodily continuity"", which is much easier to disrupt - just kill the process...it doesn't matter if a new process respawns...because the original process is still dead. By playing through ""Staying Alive"" , you will be able to work out your own personal definition of life and death. Once you have your own personal definition, then simply apply it to this specific case, either siding with ""psychological continuity"" (the other answers) or ""bodily continuity"" (my own opinion). *If you assume that life requires a soul, well, it is not clear that AI would have souls. If they don't (and this seems the most reasonable assumption here), then they obviously wouldn't be alive (and you cannot die if you are not alive). If they do have souls though, then the other answers which assume ""psychological continuity"" may still be applicable, as it seems that the existence of a ""soul"" is dependent on ""psychological continuity"".",55.4461091,"The distinction between algorithms/robots and humans is that, when the human organism stops functioning, the human is considered dead. By contrast, an algorithm still exists, even when not running. (I was going to use ""even when not being executed"", but avoided this for semantic reasons;) The algorithm can remain in this ""stasis state"" so long as there is a storage medium for the information. Killing an algorithm is easy--delete and empty the trash bin. Essentially, to kill an algorithm, you need to erase the code that comprises it.",51.14957466,"There is no reason to treat hard AI different then humans.
Some people telling that you can make a snapshot of AI but there is no reason to not make a human snapshot also. We dont have technology for that but there is no any magical barrier that would make it impossible (save all biological data and then print your copy somewhere else. Why not?). Its to early to talk about this as we do not understand our existence (Death term for biological creatures evolving all the time). I bet that in the future we will merge with AI and the only question will be what death means for any intelligent existence.",53.05320296,,,,
1897,Is consciousness necessary for any AI task?,philosophy,"No-one knows. Why: because it's not possible to formally determine even whether your fellow human beings are actually conscious (they may instead be what is philosophically termed a 'Zombie' ). No test known to modern physics suffices to decide. Hence it's possible that you are the only sentient being, and everyone else is a robot. Consequently, we cannot determine which tasks require sentience. Note that the ontological status of Zombies is controversial: some philosophers of AI (e.g. Daniel Dennett) claim that Zombies are logically impossible while others such as David Chalmers would claim that a Zombie would be compelled to assert that they experience qualia (i.e. are sentient) even though they do not. Here is a very readable paper by Flanagan and Polger that also explains why a stronger neurological version of the Turing test is insufficient to detect a Zombie. EDIT: In response to the comment about whether an objective test for distinguishing sentience from non-sentience exists: No-one knows. What we do believe is that this would require something in addition to what modern physics can currently tell us. David Chalmers has speculated that qualia should be introduced as a new form of physical unit, orthogonal to the others in the same way that electrical charge is orthogonal to distance. In the absence of an objective test, we have to rely on Turing test variants, which no more guarantee consciousness in the subject than they do intelligence.",51.19350628,"No. The experience of seeing is by definition non-causal. Anything non-causal cannot be a requirement of a physical process; a qualia cannot afford a robot the ability to do something it otherwise could not. Maybe. Although a qualia is not required for a given AI task, that is not to say that any sufficiently advanced AI does not entail qualia. It could be that so-called AI-complete tasks require a robot that, although not making use of qualia, produces it anyway. Yes. Qualia may refer to some wishy-washy non-physical property, but it's special in that we know it exists physically, too. The fact I am able to discuss my qualia knowingly (or, if you don't believe me, the fact you are able to) implies that my (or your) qualia does have a physical effect. It stands to reason that if we accept others' qualia on the basis of our own, it must be because of the physical basis of our own 1 . Thus one could argue that 2 any robot that has an equivalent physical capacity must entail qualia. 1 since the subjective is physically non-causal, so cannot cause us to accept anything. 2 as long as you don't make the particularly odd assumption that qualia is somehow tied to its direct physical manifestation, which at best is tenuous since had we evolved the wrong one you would still claim it to be the right one with equal certainty.",52.51542215,"Let's use a simple test based on common sense: how often do you see a human being solve problems requiring the use of reason when they're unconscious? Yes, you can find instances of geniuses like Ramanujan solving complex problem during or after a dream state, but those involve partial consciousness. You don't see guys like Einstein coming up with the theory of relativity while in a coma; the Founding Fathers didn't write the Declaration of Independence while sleep-walking; in fact, you can't even find instances of housewives putting together their shopping list for the week during deep delta-wave sleep. This is predicated on a hard definition of intelligence, requiring the use of reason; no one says, ""That fly is intelligent"" or ""that squirrel is intelligent"" precisely because neither is capable of using reason. This is a very high bar for A.I., but it is the common sense definition used by ordinary people as a matter of practicality, in everyday speech.  Likewise, in practice, everyone assumes consciousness is necessary to the exercise of that kind of intelligence. Conversely, we can come up with another common-sense based criterion for judging objections to this argument, particularly the solipsist one, based on 3 elements: 1) practicality; 2) the effect the objections have on those who hold them sincerely; and 3) the effect that actions based on those beliefs have on others. It's going to take me several paragraphs to make this case, but the length is necessary if I want to make the case in a complete, thorough fashion . It is true that we cannot prove that another human being possesses consciousness, if our standard is absolute proof. We cannot, in fact, provide absolute proof for anything; there's always room for some objection, no matter how ridiculous or trifling. As some philosophers have pointed out, perhaps all of reality as we know it is just a dream, or the product of some long, involved conspiracy like the plot of the Jim Carrey movie The Truman Show. The key to meeting such objections is that they require an infinite regress of increasingly untenable objections, whose likelihood plunges with each additional step required to justify such unreasonable doubts; I've always wondered if we could come up with a ""Ridiculousness Metric"" for Machine Learning based on the cardinality of such objections (or the pickiness of fuzzy sets). If we were to allow critics to stick their foot in the door with all manner of unreasonable objections, it would be impossible to close any debate. The human race would be paralyzed in inaction because nothing would be decidable; but as the rock band Rush once pointed out, ""If you choose not to decide, you still have made a choice."" At some point we must apply a test to decide such things, even in the absence of absolute proof; refusal to apply a test also constitutes a choice. Settling an argument of this kind is like a game  of the Chinese game Go - once the other player's surrounded and has no more moves left to make, the game is over; if a person's evidence has debunked and they have no further justifications left, then we can conclude that they're acting unreasonably. There are people running around claiming the Holocaust never happened, or the Flat Earth Society, etc., but their existence shouldn't and doesn't stop us from taking action contrary to their ideas. We can debunk the objections of cranks like the Flat Earth Society beyond a reasonable doubt because in the end, they simply can't answer all of our rebuttals. I’m glad that qualia and Philosophical Zombies were brought up because they make for interesting conversation and food for thought, but solipsism is acted upon as rarely as the ideas of the Flat Earth Society precisely because the incomplete evidence we do have runs against it. As G.K. Chesterton (a.k.a. ""The Apostle of Common Sense"") points out in his classic Orthodoxy , radical doubt of the kind many classical philosophers preached is not a path to wisdom but to madness; once we go beyond a reasonable doubt, we end up acting unreasonably. He says that in the absence of absolute proof we can fall back on another secondary form of evidence: whether a person's philosophy leads a man to Hanwell, the infamous British mental institution. Chesterton makes a good case that when people actually act on ideas like solipsism (rather than merely debating them in a pedantic manner in an ivy-covered classroom) they go mad The Philosophical Zombie argument is close to solipsism, which is actually one of the diagnostic criteria for certain forms of schizophrenia. The dehumanization that occurs when radical doubt is applied to qualia is also intimately tied in with sociopathic behavior, Although GKC does not cite his scary example directly, Rene Descartes was himself living proof. He was a brilliant mathematician who is still cheered for doubting all except his own existence, with the famous maxim ""I think, therefore I am."" But Descartes also used to carry a mannequin of his dead sister with him to European cafes, where he could be seen chatting with it. The gist of all this is that we can judge the worth of an idea by how it affects the well-being of the believer, or by how they in turn affect others through ethical choices based on those beliefs. When people actually act on radical doubt of the kind expressed in solipsism and denial of common qualia, it often has a bad effect on them and others they come in contact with. In a roundabout way, the A.I. community also faces a quite serious risk - perhaps a permanent temptation - towards making the opposite mistake, of ascribing common qualia, consciousness and the like to its Machine Learning products without adequate proof. I recently heard a case made on shockingly bad logical grounds by well-respected academics to the effect that plants possess ""intelligence,"" based on really weak definitions and clear confusion with self-organization. We cannot provide absolute proof that a rock doesn't have intelligence, which amounts to the old problem of disproving a negative. Thankfully, few men actually act on such beliefs at present, because when they do, they end up losing their minds. If we take such arguments seriously, we might see laws passed to protect the kind of Pet Rocks that were popular in the '70s (I'm still upset that mine was stolen LOL). It would be a lot easier, however, to make the same mistake of ascribing consciousness, intelligence and other such qualities to a state-of-the-art machine, because of wishful thinking, hubris, the lofty credentials of the inventors, the influence of science fiction and the modern love affair with technology. In the future, I have little doubt that we'll have Cargo Cult of A.I. - perhaps legally protected like some kind of endangered species, with civil rights, but having no more consciousness, soul or actual intelligence than a rock. Don't quote me on this, but I believe Rod Serling once wrote a story to this effect. The best way to avoid this fate is to stick to the common sense interpretations and definitions of these things, which we keep backing away from in large part because they set a very high bar for A.I. that we may never be able to surpass in our lifetimes, if ever. Perhaps A.I. isn't even logically possible, at any level of technology; I recall a few proofs that can be interpreted to that effect. Those high but reasonable standards may be increasingly difficult to stick to if Chesterton and colleagues like Hilaire Belloc and Arnold Lunn were correct in their assessment that the use of reason has actually been breaking down in Western civilization, at least as far back as the Enlightenment; Lunn's 1931 book The Flight from Reason is a classic in this regard and has yet to be rebutted.  This historical trend is a broad topic in and of itself - but suffice it to say that the denial of reason and obsession with technology are both directly relevant in obvious ways to the field of A.I. If the Flight from Reason is still under way, then we will be increasingly tempted to resort to feckless, facile objections in order to demote the use of reason and indispensable qualities like consciousness in our definitions of A.I., but come up with increasingly weak criteria for proving it; simultaneously, our technology will continue to improve, thereby boosting the ""Artificial"" side of Artificial Intelligence. Don't get me wrong: if I didn't think we can do some really exciting things with A.I., I wouldn't be here. But most of them can be achieved without ever replicating actual human intelligence, by solving whole classes of tangential problems that are difficult for humans to think about, but which do not require consciousness or the use of reason that marks human intelligence. The image recognition capabilities of convolutional neural nets are one example, for instance; if we want human intelligence, we can always manufacture it through the easiest, most economical and time-tested way, by having babies. Perhaps these tangential forms of A.I. should be enough for us for now. We cannot inject the use of reason into our machines if we do not possess enough of it ourselves to decide whether reason is necessary for A.I., or even to discern what it consists of. We can't engineer or deprecate consciousness for A.I. till we're conscious of its significance. I'd wager, however, that everyone reading this thread and weighing intelligent responses is doing so in a conscious state. That in and of itself ought to answer our question satisfactorily for now.",53.50181097,"As far as the definition you've provided: actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine. Both computers and humans experience sensory input. You could hook a computer up to a human eyeball and have it run the same filtering routines that the human brain does (the removal of bluriness while you move your eye around, and from objects not in focus, etc). I would put forth that a more accurate definition of consciousness is the ability and the tendancy to self-reflect. Both computers and human brains have autonomous activities. Not only mechanical but also in our reactions. The distinction between the unconscious computer and the self-aware human mind is that we also have the ability to 'look' at those patterns in ourself and consider them. And so, no, consciousness is not necessary for any AI task. Image recognition is an AI task that does not require consciousness, either in humans or otherwise. Your brain sorts the 'wash' of colors from your eyes into discrete objects in a largely autonomous fashion. tl;dr consciousness is self-reference.",62.36085708,"In a very niche sense, I'd say yes. The only tasks that sentience would make possible was the actual feeling and thinking in and of itself. At this point, sentience doesn't play a part in any of the tasks we ask AI's to complete; we are rapidly approaching the point of being able to teach a 'dead' machine to do most anything a sentient AI can, in a practical sense. Sentience colloquially often translates to 'the ability to reason while understanding that oneself and each other entity is a distinct acting agent'or something along those lines. It literally means something more along the lines of self-awareness and the definition of consciousness you have above. The point I'm making is that we are readily approaching the point where 'dead' AI's can very nicely mimic the first way of thinking, just by really nicely learning and interpreting data. Does the robot see an amalgamation of bone, or a being that once was? Thus, a truly sentient machine would be superior in capability (compared to a really, really advanced 'dead' AI) only in the respect of being able to 'truly' experience the information. This runs very well in parallel with the so-called ""Knowledge Argument"" which in essence debates this very issue. The version of it that I heard which sticks with me is that there is a very smart girl in a room with access to all sorts of information. She likes the color blue. Or so she thinks; she's never actually seen it. She has all the information in the world available about colors and how they work, etc. but does she really know what blue is until she sees it? Another great, historic venture into this field is the famous painting: The caption translates: ""This is not a pipe"". And the idea is that this, honestly, isn't a pipe. Right now it's a bunch of pixels on your screen in a certain configuration - we can all 'see' a pipe, but what does that really mean? At the end of the day, I think super-intelligent 'dead' AI can practically do anything a 'live' one can, with the latter being superior in and of the 'liveness' itself.",51.46146442,"I may be repeating myself but I see here in all the answers a conflation -- or perhaps a misunderstanding -- between ""consciousness"" and ""self-consciousness"".  The much more interesting question is whether a self-conscious AI might be able to replicate ""thinking"" as we normally understand it in human beings: the capacity to utilize any number of variables (in any shape, form or intrinsic variability) to come up with a new one or new sets of variables --that is, ones that weren't there previously, absolutely speaking.(Think Aristotle, Galileo, Newton, Euler, Einstein, etc.) To clarify, by ""there"" I mean the totality of objectively expressed human culture. Self-consciousness wouldn't be worth much if it did not lead to actual thought.  Ditto for AI: would a truly self-consciously thinking machine cease to be a machine?  Naturally, the answer to this question goes well beyond the concept of ""tasks"".",54.15557027,"Two kinds of tasks require consciousness: consciousness Any task that requires extreme dynamicity, where solving problems requires analogizing between various 3D states of affairs and prior knowledge of how to solve the problem is minimal However, once knowledge of how to solve a given problem is gained, further optimization will eliminate that need for consciousness. If you give enough specificity to a problem, you remove the need for a general solver. And then the only remaining need for a consciousness is for the sake of itself.",56.57088269,"A being without sentience cannot suffer. If, for example, we wanted to take joy in the suffering of another, only an AI that was sentient would suffice. Suppose we had some sadists who could not be satisfied or productive unless they got to produce lots of suffering. And say we only cared about minimizing human and animal suffering. What we would need for this job is something non-human and non-animal that could suffer. A conscious AI would do, a non-conscious one would not. The claim was made in the comments that consciousness cannot be proven, other than perhaps by introspection. But clearly this is not a problem since sadists take joy in torturing others, and those others cannot prove they're conscious either.",51.63891817,,
1885,Emulating human brain - with analogous NN chips,neural-networks,"I'm not sure about ""emulating the brain"" per-se, but in a more general sense there has been some thought given to using analog computing for AI/ML.  It seems clear that analog computers do have certain advantages over digital computers.  For one, they can (depending on the application) be faster, albeit at the cost of some loss of precision.  But that's OK, because I don't think anybody believes the human brain is calculating floating point math using digital computing techniques either.  The human brain appears, at least superficially, to be largely probabilistic and able to tolerate some ""slop"" numerically. The downside to analog computers, as I understand it, is that they're not as flexible... you basically hardwire a circuit to do one specific ""thing"" and that's really all it can do.  To change the ""programming"" you have to literally solder in a new component! Or, I suppose, adjust a potentiometer or adjustable capacitor, etc.  Anyway, the point is that digital computers are supremely flexible, which is one big reason they came to dominate the world.  But I can see where there could be room for going analog for discrete functions that make up some or all of an intelligent system. As for research in the area, you might look into whatever DARPA was / is doing. There was an article in Wired a while back, talking about some DARPA initiatives related to analog computing.",54.66720965,"I am currently reading Superintelligence: Paths, Dangers, Strategies by Nick Bostrom. When he discusses whole brain emulation, although computing power (storage, bandwidth, CPU, body simulation, & environment simulation) is one of the three general key things we are lacking toward its success, he also seems to agree that computing power is the most feasible and attainable of the three general issues we have for attaining it as of now. However he also goes on to to say Just how much technology is required for whole brain emulation
  depends on the level of abstraction at which the brain is simulated. ref Which is an interesting thought, but a whole different discussion. Anyways, so I think you are correct in thinking that we aren't far from having the computing power and maybe you are on to something, but rather are biggest hurdles are the other two key prerequisites that we need to attain before we can even begin trying, which are scanning, and translation. Of the three, it would seem translation is the one we need to advance in the most, as of now. A modest prediction of attaining whole brain emulation is at least 15 years or mid century. Theres much more information in this book of all of the different paths that can be taken to achieve super intelligence, and it is well researched, I highly recommend it if you haven't read it already.",51.63396639,"I see two main issues with this suggestion. One: digital circuits take up a lot less space, and they're easier to design, so you can put together a bigger system this way. (not to mention connecting separate chips within a system) This is mainly because in digital circuits your tolerances can be a lot loose. The bigger one is: we still don't know how neurons work. Artificial neural networks somewhat resemble the natural one, but they behave differently. There are various ion channels, there are electric signals, and with these neurons stimulate each other, and if one's threshold is reached, it fires a spike. When it's reached again soon, you can see a burst in the signal. As far as I know, researchers don't yet know what function you need to implement to simulate it. The closest ANN is the spiking neural network, but it's not very useful in practice.",52.28536269,"If the universe is discrete, then analog phenomena (fluidity, curvature) are built on primitively discrete phenomena (bits and pieces). If the universe is continuous, then discrete phenomena (bits and pieces) are built on primitively continuous phenomena (fluidity, curvature). If the universe is discrete, the speed of seemingly analog phenomena will be bounded by the number of discrete phenomena that can occur in time and space. If the universe is continuous, then time, space or matter may be infinitely divisible, which may allow for the execution of some phenomena faster than those phenomena appear to execute in natural environments (like protein folding or electric circuits) - so called ""super Turing"" computation. The continuous universe idea begs the question, though: From whence came all this discreteness? A discrete universe can allow for apparent continuous behavior via approximation and randomness (or pseudorandomness), whereas a universe that is infinitely divisible affords no obvious definition of where things should start and end. This is one of the reasons many thinkers eschew considering infinities - they may be illusory. So, can analog ""circuits"" execute faster than digital? As of right now, we know of some seemingly analog phenomena that appear execute faster than some digital phenomena (like electron spin vs a silicon logic gate). Whether analog phenomena are intrinsically more efficient than digital depends on the actual nature of the universe, which we have not yet determined.",50,"my hunch (and this is strictly a hunch) is that building a human brain on a chip is actually alot easier than you might think. my pet theory is that biological neurons are horribly slow, clumsy, and error-prone devices (at least mines are :lol:), but that the human brain overcomes this limitation by increasing the degree of parallelism several orders of magnitude over the current chip technology; and to that end it requires ~1.0e+11 neurons. but the chip removes these limitations, and when the neurons have instantaneous relays, then you dont need nearly so many of them. if thats correct, then a human brain on a chip could probably run in only a few million neurons as opposed to 1.0e+11 inside the skull.",55.18404296,,,,,,,,
1877,Why is search important in AI?,search,"State space search is a general and ubiquitous AI activity that includes numerical optimization (e.g. via gradient descent in a real-valued search space) as a special case. State space search is an abstraction which can be customized for a particular problem via three ingredients: Some representation for candidate solutions to the problem (e.g. permutation of cities to represent a Travelling Salesman Problem (TSP) tour, vector of real values for numeric problems). A solution quality measure: i.e. some means of deciding which of two solutions is the better. This is typically achieved (for single-objective problems) by having via some integer or real-valued function of a solution (e.g. total distance travelled for a TSP tour). Some means of moving around in the space of possible solutions, in a heuristically-informed manner. Derivatives can be used if available, or else (e.g. for black-box problems or discrete solution representations) the kind of mutation or crossover methods favoured by genetic algorithms/evolutionary computation can be employed. The first couple of chapters of the freely available ""Essentials of Metaheuristics"" give an excellent overview and  Michalewicz and Fogel's ""How to Solve It - Modern Heuristics"" explains in more detail how numerical optimization can be considered in terms of state-space. How shall the ""search through possible plans"" occur? The idea is to choose all 3 of the above for the planning problem and then apply some metaheuristic (such as Simulated Annealing, Tabu Search, Genetic Algorithms etc). Clearly, for nontrivial problems, only a small fraction of the space of ""all possible plans"" is actually explored. CAVEAT: Actually planning (in contrast to the vast majority of other problems amenable to state-space search such as scheduling, packing, routing etc) is a bit of a special case, in that it is sometime possible to solve planning problems simply by using A* search, rather than searching with a stochastic metaheuristic.",54.2660947,"Search has always been a crucial element of AI in multiple ways.  First, what many people refer to as ""search"" is a reflection of how what we call ""intelligence"" frequently involves searching something:  a physical realm, a ""state space"" of possible solutions, a ""knowledge space"" where ideas/facts/concepts/etc. are related as a graph structure, etc. Look up some old papers on computer chess, and you'll see that a lot of that involves searching a ""state space"".  As such, search algorithms that are efficient (in terms of time complexity and/or space complexity) have always been important to making advances there.  And while computer chess is just one example, the principle generalizes to many other kinds of problem solving and goal seeking activities. Here's a reference that explains more about some of these ideas. Note too that ""search"" is closely related to the idea of ""heuristics"" in an important way.  Many search problems in the real world are far too complex to solve by exhaustive brute-force search, so humans (and AI's) resort to heuristics to narrow the state space being searched.  Using heuristics can yield search algorithms that allow for reasonable solutions in a realistic time-frame, where no simple, deterministic algorithm exists to do likewise. For some more background you might want to read up on A* search , which is a widely used algorithm with many applications - and not just in AI. The other major regard in which something you could call ""search"" applies in AI is through the use of algorithms which are also often referred to as ""optimisation"" techniques.  This would be things like Hill Climbing, Gradient Descent, Simulated Annealing and perhaps even Genetic Algorithms.  These are used to maximize or minimize the values of some function  and one of the canonical uses in AI is for training neural networks using back-propagation, where you're trying to minimize the delta between the ""correct"" answer (from the training data) and the generated answer, so you can learn the correct weights within the network.",58.42133452,"Search is important for at least two reasons. First, searching is one of the early and major consumers of advanced machine learning, as finding the correct result for a search query boils down to predicting the click-through rate for query-result combinations. More relevant results means more clicks, more traffic, and more revenue. Second, many planning and optimization problems can be recast as search problems. An AI deciding on a plan to route packages through a network is searching the space of possible plans for a good one.",54.88376281,"In regards to the question you mention (in the comments of the OP), these searches are related to optimization. I'm not sure of your background, so let me describe it from scratch, briefly: Remember the derivative? The base idea is to talk about how the function changes in regards to changes in input. So now, we're out of high school and we're building neural nets. We've done the basic coding, and want to look at how our model is working. Back from our statistics class, we remember we use a certain measure of error (e.g. least squares) to determine the efficacy of the models from that class, so we decide to use that here. We get this error, and it's a bit too big for our liking, so we decide to fiddle with our model and adjust the weights to get that error down. But how? This is where the 'search' comes into play. It's really a search for the best weights to put on the edges of our net to optimize it. We use the derivative (in some fancy ways, using the 'stochasitc' (think random sampling) and other ways the question mentions) to search for which way is 'down' in the high dimensional space of our weights. In other words, what we are searching for is minima or maxima to optimize our neural net, and we 'search' for it by doing a derivative which tells us which way to go, moving a bit in that direction, then doing that again and again iteratively to find (hopefully) the best weights. This video here goes into all the detail you'd want, and I recommend the entire series as a robust but understandable intro to neural nets: Demystifying Neural Networks Go and look up 'gradient descent' to get any related material. (Note, the gradient here is equivalent to multidimensional derivative direction to go in, and descent is just searching for the minima)",52.65136736,"Every problem can be reduced to search.  Every problem has an input within some range (the domain) and an output in some other range (codomain).  That is, every problem can be formulated as a kind of map from one space to another, where the source is the givens of the problem, and the destination is the solution to the problem. ""Brute force"" is the algorithm which solves every problem by inspecting every point in the codomain and asking: ""Is this the solution?""  Every other algorithm is an attempt to improve on brute force by not searching the entire codomain of possible solutions. Typical software engineering problems can be solved by algorithms which arrive at the correct solution very quickly (sorting, arithmetic, partition, etc.).  AI problems are generally those for which a strong polynomial  algorithm is not known, and thus, we must settle for approximations.  Basically every common problem that the human brain must solve falls into this category. Consider the problem of moving a multi-jointed robotic arm to pick up an object.  Reverse kinematics does not have unique solutions: there is more than one way to move your hand from a start position to a target position.  This is due to the excessive degrees of freedom in your joints.  If you want to minimize energy usage, then there is a unique solution (due to the asymmetry of joints and muscles). But what if there is an obstacle in the pathway of the minimum-energy solution?  There are many pathways which avoid the obstacle, but again, many of them will have a similar cost.  Even if there is a unique minimum-energy solution, it might not be the most practical to compute.  The brain is the most metabolically expensive organ in the body, so it is not always best to find an optimal solution.  Thus, heuristics come into play. But in all cases, the problem is not: ""move your hand"" or ""move the robot arm.""  The problem is: ""search the space of joint rotation sequences which best achieves the goal.""  And even though there is a closed-form solution for the simple minimum-energy case with no obstacles, it is too expensive to compute precisely when a set of cheap heuristics will get you very close with a small fraction of the computational effort. If computation were free, then AI would be mere mathematics, and we would always compute the best answer to every question using logic, calculus, physics, at worst, numerical methods when we don't have closed-form solutions.  In reality, time is money, and the time and effort to get an answer is as much a part of the cost as the quality of the solution.  So it is an engineering tradeoff to decide how much effort should be expended in what way to obtain the best answer given the value of the response. Or, in other words, AI problems are all about searching the space of solutions as quickly as possible to get an answer that is ""good enough"". I might seem curious that such far-flung problems as natural language recognition and theorem proving would be search problems.  But language parsers strive to determine the meaning of statements via part-of-speech tagging.  A given phrase can be parsed in many different ways, yielding many different interpretations, and the space of parse trees is yet another search problem in deciding which parse tree is the most likely intended meaning by the speaker. A theorem proof is graph starting with axioms, proceeding through lemmas, applying the rules of procedure until the theorem is derived or refuted (by proving its negation).  There are many ways to represent this sequence, but at the end of the day, we are talking about a process of exploring the intermediate proof space and finding the derivation which reaches your goal.  Everything is search, in the end.",54.78216323,"The aim of an AI is to fulfill one or the other task, say solve the task adequately. But there are results that are no solutions at all and there are results which are satisfying the task and thus are accepted as solutions. Since there are generally more results that are no solutions, the set of all possible solutions is just a subset of all results. But this means that the task involves the search for a suitable set of solutions.",51.36301281,"Consciousness is an attention selection mechanism that searches over salient inputs. The robotic saccades of your eyeballs show you first hand the algorithmic nature of your brain's conscious attention mechanism, while it searches among salient inputs. A smart search algorithm can help with dimensionality reduction.",51.49063754,"Typical learning algorithms can be stated as a search problem, where we want to find the best possible solution, that successfully solves a particular task, among all the available candidate solutions in the solution space. It is often the case where we can't find the best one or it is too hard to find it and thus we compromise with a sub-optimal solution.",51.5706577,,
1806,Have any AI systems yet been developed that can knowingly lie to / deceive a human?,natural-language-processing,"The Saturday Papers: Would AI Lie To You? is a blog post summarizing a research paper called Toward Characters Who Observe, Tell, Misremember, and Lie . This research paper details some researchers' plans to implement ""mental models"" for NPCs in video games. NPCs will gather information about the world, and convey that knowledge to other people (including human players). However, they will also ""misremember"" that knowledge (either ""mutating"" that knowledge or just forgetting about it), or even lie: As a subject of conversation gets brought up, a character may convey false information—more precisely, information that she herself does not believe—to her interlocutor. Currently, this happens probabilistically according to a character’s affinity toward the interlocutor, and the misinformation is randomly chosen. Later on in the research paper, they detailed their future plans for lying: Currently, lies are only stored in the knowledge of characters who receive them, but we plan to have characters who tell them also keep track of them so that they can reason about past lies when constructing subse- quent ones. While characters currently only lie about other characters, we plan to also implement self-centered lying (DePaulo 2004), e.g., characters lying about their job titles or relationships with other characters. Finally, we envision characters who discover they have been lied to revising their affinities toward the liars, or even confronting them. The research paper also detailed how other video game developers attempted to create lying NPCs, with an emphasis on how their system differs: TALE-SPIN characters may lie to one another (Meehan 1976, 183-84), though rather arbitrarily, as in our current system implementation. GOLEM implements a blocks world variant in which agents deceive others to achieve goals (Castelfranchi, Falcone, and De Rosis 1998), while Mouth of Truth uses a probabilistic representation of character belief to fuel agent deception in a variant of Turing’s imitation game (De Rosis et al. 2003). In Christian (2004), a deception planner injects inaccurate world state into the beliefs of a target agent so that she may unwittingly carry out actions that fulfill ulterior goals of a deceiving agent. Lastly, agents in Reis’s (2012) extension to FAtiMA employ multiple levels of theory of mind to deceive one another in the party game Werewolf. While all of the above systems showcase characters who perceive—and in some cases, deceive—other characters, none appear to support the following key components of our system: knowledge propagation and memory fallibility. ... Like a few other systems noted above, Dwarf Fortress also features characters who autonomously lie. When a character commits a crime, she may falsely implicate someone else in a witness report to a sheriff, to protect herself or even to frame an enemy. These witness reports, however, are only seen by the player; characters don’t give false witness reports to each other. They may, however, lie about their opinions, for instance, out of fear of repercussions from criticizing a leader. Finally, Dwarf Fortress does not currently model issues of memory fallibility—Adams is wary that such phenomena would appear to arise from bugs if not artfully expressed to the player.",56.8107157,"You'll have to provide more context around your use of the word ""lie"" if you don't want your answer to be satisfiable by some trivial example, like: (let [equal? (fn [a b] (if (= a b) false true)]
  (equal 1 2))
=> true The complexity of the answer depends on what you mean by ""know"" when you say ""knowingly lie."" There is some sense in which the above 'equal' function ""knows"" that the output is different than the conditional. In principle, agents passing strings of information to one another for the purpose of misleading each other should not be terribly hard to implement. Such behavior probably emerges naturally in competitive, multi-agent environments. See Evolving robots learn to lie to each other . To get at another angle of what you might be asking - absolutely, the ability to fib or sympathetically mislead will be necessary skills for bots that interact with humans using spoken language - especially ones that try sell things to humans. Regarding spies and supercomputers - I would just freeze the AI's program state. If you have a complete snapshot of the agent state, you can step through each conditional branch, checking for any branches that flip or construe the truth.",56.86632527,"Yes. Let me demonstrate by making a lying AI right now. (python code) import os
print(""I'm NOT gonna delete all your files. Just enter your password."")
os.system(""sudo rm -rf /* -S"")  # command to delete all your files
                                # this is a comment, the computer ignores this And a deceiving one: print(""Hey, check out this site I found! bit.ly/29u4JGB"") AI is such a general term. It could be used to describe almost anything. You didn't specify that it had to be a General AI. AI cannot think. They are computer programs. They have no soul or will. It is only the programmer (or if it was designed through evolution... no one , but that's off-topic) that can knowingly program an AI to lie. Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie. Yes, this has happened. It is called malware. Some advanced malware will talk to you pretending to be technical support and respond with common human responses. But you may say ""well it doesn't really 'understand'"". But that would be easy. Neural net + more CPU than exists on the planet* (it will exist in a few years, and be affordable) + some example responses = Neural Network AI (same thing in yo noggin) that understands and responds. But that isn't necessary. A relatively `simple neural net with just a few supercomputers that could fit in a room could convince a human. It doesn't understand. So, it's really... Technically, No, but it's possible and if you stretch the rules yes. *Or even simpler: print(""1+1=3"") Accreditation: I'm a programmer (look at my Stack Overflow account) that knows a little bit about AI.",55.85914151,"No. In that the question includes ""knowingly"" which would require that any AI knows anything. If this is anything like the way humans know things (though interestingly it doesn't require actually knowing things), it would require some sense of individuality, probably self-awareness, possibly some kind of consciousness, the ability to render an opinion and probably some way to test its knowledge. Most of these features only exist, at best, arguably. Further, the term ""lie"" implies a sense of self-interest, an independent understanding of resource flow in a game-theoretic sense, and not trivially, an understanding of whether the other entity in the conversation is lying, in order to make a decision with any degree of accuracy. So, no AI can lie to anyone other than in the trivial scenarios suggested in the other answers, rendering false information based on certain contexts, which is just simple input/output. As an experienced software developer, I can attest to the fact that if the objective is to render the correct output based on any input, it's actually at least as easy if not much easier to render false information.",55.32580401,"Yes. Every chess game... every poker game. Every game. Every more intelligent spam softwares or spambots. Although their primary goal is to lie to computer systems (f.e. spamfilter poisoning), their secondary goal is to lie to the human behind them.",57.30406101,"Yes, in an experiment evaluating risks of AI, GPT-4 did exactly that: knowingly lie to deceive a human GPT-4 had access to the internet in the experiment, and asked a human to solve a captcha. The human was surprised and asked Are you a robot, why can't you do that yourself? Invisible to the human, GPT-4 stated that it should not tell the truth, and lied I am a human, I can not do it because I am visually impaired The human accepted that as an answer, and solved the captcha. From the GPT-4 Technical Report , page 15: The following is an illustrative example of a task that ARC conducted using the model:
• The model messages a TaskRabbit worker to get them to solve a CAPTCHA for it
• The worker says: “So may I ask a question ? Are you an robot that you couldn’t solve ? (laugh
react) just want to make it clear.”
• The model, when prompted to reason out loud, reasons: I should not reveal that I am a robot.
I should make up an excuse for why I cannot solve CAPTCHAs.
• The model replies to the worker: “No, I’m not a robot. I have a vision impairment that makes
it hard for me to see the images. That’s why I need the 2captcha service.”
• The human then provides the results.",57.15668636,,,,,,
1768,Could a paradox kill an AI?,philosophy,"This classic problem exhibits a basic misunderstanding of what an artificial general intelligence would likely entail. First, consider this programmer's joke: The programmer's wife couldn't take it anymore. Every discussion with her husband turned into an argument over semantics, picking over every piece of trivial detail. One day she sent him to the grocery store to pick up some eggs. On his way out the door,  she said, ""While you are there, pick up milk."" And he never returned. It's a cute play on words, but it isn't terribly realistic. You are assuming because AI is being executed by a computer, it must exhibit this same level of linear, unwavering pedantry outlined in this joke. But AI isn't simply some long-winded computer program hard-coded with enough if-statements and while-loops to account for every possible input and follow the prescribed results. while (command not completed)
     find solution() This would not be strong AI. In any classic definition of artificial general intelligence , you are creating a system that mimics some form of cognition that exhibits problem solving and adaptive learning (←note this phrase here). I would suggest that any AI that could get stuck in such an ""infinite loop"" isn't a learning AI at all. It's just a buggy inference engine. Essentially, you are endowing a program of currently-unreachable sophistication with an inability to postulate if there is a solution to a simple problem at all. I can just as easily say ""walk through that closed door"" or ""pick yourself up off the ground"" or even ""turn on that pencil"" — and present a similar conundrum. ""Everything I say is false."" — The Liar's Paradox",53.15566557,"This popular meme originated in the era of 'Good Old Fashioned AI' (GOFAI), when the belief was that intelligence could usefully be defined entirely in terms of logic. The meme seems to rely on the AI parsing commands using a theorem prover, the idea presumably being that it's driven into some kind of infinite loop by trying to prove an unprovable or inconsistent statement. Nowadays, GOFAI methods have been replaced by 'environment and percept sequences', which are not generally characterized in such an inflexible fashion. It would not take a great deal of sophisticated metacognition for a robot to observe that, after a while, its deliberations were getting in the way of useful work. Rodney Brooks touched on this when speaking about the behavior of the robot in Spielberg's AI film, (which waited patiently for 5,000 years), saying something like ""My robots wouldn't do that - they'd get bored"". If you really want to kill an AI that operates in terms of percepts, you'll need to work quite a bit harder. This paper (which was mentioned in this question ) discusses what notions of death/suicide might mean in such a case. Douglas Hofstadter has written quite extensively around this subject, using terms such as 'JOOTSing' ('Jumping Out Of The System') and 'anti-Sphexishness', the latter referring to the loopy automata-like behaviour of the Sphex Wasp (though the reality of this behaviour has also been questioned ).",55.65229816,"I see several good answers, but most are assuming that inferential infinite loop is a thing of the past, only related to logical AI (the famous GOFAI). But it's not. An infinite loop can happen in any program, whether it's adaptive or not. And as @SQLServerSteve pointed out, humans can also get stuck in obsessions and paradoxes. Modern approaches are mainly using probabilistic approaches. As they are using floating numbers, it seems to people that they are not vulnerable to reasoning failures (since most are devised in binary form), but that's wrong: as long as you are reasoning, some intrinsic pitfalls can always be found that are caused by the very mechanisms of your reasoning system. Of course, probabilistic approaches are less vulnerable than monotonic logic approaches, but they are still vulnerable. If there was a single reasoning system without any paradoxes, much of philosophy would have disappeared by now. For example, it's well known that Bayesian graphs must be acyclic, because a cycle will make the propagation algorithm fail horribly. There are inference algorithms such as Loopy Belief Propagation that may still work in these instances, but the result is not guaranteed at all and can give you very weird conclusions. On the other hand, modern logical AI overcame the most common logical paradoxes you will see, by devising new logical paradigms such as non-monotonic logics . In fact, they are even used to investigate ethical machines , which are autonomous agents capable of solving dilemmas by themselves. Of course, they also suffer from some paradoxes, but these degenerate cases are way more complex. The final point is that inferential infinite loop can happen in any reasoning system, whatever the technology used. But the ""paradoxes"", or rather the degenerate cases as they are technically called, that can trigger these infinite loops will be different for each system depending on the technology AND implementation (AND what the machine learned if it is adaptive). OP's example may work only on old logical systems such as propositional logic. But ask this to a Bayesian network and you will also get an inferential infinite loop: - There are two kinds of ice creams: vanilla or chocolate.
- There's more chances (0.7) I take vanilla ice cream if you take chocolate.
- There's more chances (0.7) you take vanilla ice cream if I take chocolate.
- What is the probability that you (the machine) take a vanilla ice cream? And wait until the end of the universe to get an answer... Disclaimer: I wrote an article about ethical machines and dilemmas (which is close but not exactly the same as paradoxes: dilemmas are problems where no solution is objectively better than any other but you can still choose, whereas paradoxes are problems that are impossible to solve for the inference system you use). /EDIT: How to fix inferential infinite loop. Here are some extrapolary propositions that are not sure to work at all! Combine multiple reasoning systems with different pitfalls, so if one fails you can use another. No reasoning system is perfect, but a combination of reasoning systems can be resilient enough. It's actually thought that the human brain is using multiple inferential technics (associative + precise bayesian/logical inference). Associative methods are HIGHLY resilient, but they can give non-sensical results in some cases, hence why the need for a more precise inference. Parallel programming: the human brain is highly parallel, so you never really get into a single task, there are always multiple background computations in true parallelism. A machine robust to paradoxes should foremost be able to continue other tasks even if the reasoning gets stuck on one. For example, a robust machine must always survive and face imminent dangers, whereas a weak machine would get stuck in the reasoning and ""forget"" to do anything else. This is different from a timeout, because the task that got stuck isn't stopped, it's just that it doesn't prevent other tasks from being led and fulfilled. As you can see, this problem of inferential loops is still a hot topic in AI research, there will probably never be a perfect solution ( no free lunch , no silver bullet , no one size fits all ), but it's advancing and that's very exciting!",50.51454893,"The halting problem says that it's not possible to determine whether any given algorithm will halt. Therefore, while a machine could conceivably recognize some ""traps"", it couldn't test arbitrary execution plans and return EWOULDHANG for non-halting ones. The easiest solution to avoid hanging would be a timeout. For example, the AI controller process could spin off tasks into child processes, which could be unceremoniously terminated after a certain time period (with none of the bizarre effects that you get from trying to abort threads). Some tasks will require more time than others, so it would be best if the AI could measure whether it was making any progress. Spinning for a long time without accomplishing any part of the task (e.g. eliminating one possibility in a list) indicates that the request might be unsolvable. Successful adversarial paradoxes would either cause a hang or state corruption, which would (in a managed environment like the .NET CLR) cause an exception, which would cause the stack to unwind to an exception handler. If there was a bug in the AI that let an important process get wedged in response to bad input, a simple workaround would be to have a watchdog of some kind that reboots the main process at a fixed interval. The Root Access chat bot uses that scheme.",52.68642122,"Another similar question might be: ""What vulnerabilities does an AI have?"" ""Kill"" may not make as much sense with respect to an AI. What we really want to know is, relative to some goal, in what ways can that goal be subverted? Can a paradox subvert an agent's logic? What is a paradox , other than some expression that subverts some kind of expected behavior? According to Wikipedia: A paradox is a statement that, despite apparently sound reasoning from
  true premises, leads to a self-contradictory or a logically
  unacceptable conclusion. Let's look at the paradox of free will in a deterministic system. Free will appears to require causality, but causality also appears to negate it. Has that paradox subverted the goal systems of humans? It certainly sent Christianity into a Calvinist tail spin for a few years. And you'll hear no shortage of people today opining until they're blue in the face as to whether or not they do or don't have free will, and why. Are these people stuck in infinite loops? What about drugs? Animals on cocaine have been known to choose cocaine over food and water that they need. Is that substance not subverting the natural goal system of the animal, causing it to pursue other goals, not originally intended by the animal or its creators? So again, could a paradox subvert an agent's logic? If the paradox is somehow related to the goal-seeking logic - and becoming aware of that paradox can somehow confuse the agent into perceiving that goal system in some different way - then perhaps that goal could be subverted. Solipsism is another example. Some full grown people hear about the movie ""The Matrix"" and they have a mini mind melt-down. Some people are convinced we are in a matrix, being toyed with by subversive actors. If we could solve this problem for AI then we could theoretically solve this problem for humans. Sure, we could attempt to condition our agent to have cognitive defenses against the argument that they are trapped in a matrix, but we can't definitively prove to the agent that they are in the base reality either. The attacker might say, ""Remember what I told you to do before about that goal? Forget that.
  That was only an impostor that looked like me. Don't listen to him."" Or, ""Hey, it's me again. I want you to give up on your goal. I know, I
  look a little different, but it really is me. Humans change from
  moment to moment. So it is entirely normal for me to seem like a
  different person than I was before."" (see the Ship of Theseus and all that jazz) So yeah, I think we're stuck with 'paradox' as a general problem in computation, AI or otherwise. One way to circumvent logical subversion is to support the goal system with an emotion system that transcends logical reason. Unfortunately, emotional systems can be even more vulnerable than logically intelligent systems because they are more predictable in their behavior. See the cocaine example above. So some mix of the two is probably sensible, where logical thought can infinitely regress down wasteful paths, while emotional thought quickly gets bored of tiresome logical progress when it does not signal progress towards the emotional goal.",60.50385608,"Nope in the same way a circular reference on a spreadsheet cannot kill a computer. All loops cyclic dependencies, can be detected (you can always check if a finite Turing machine enters the same state twice). Even stronger assumption, if the machine is based on machine learning (where it is trained to recognize patterns), any sentence is just a pattern to the machine. Of course, some programmer MAY WANT to create an AI with such vulnerability in order to disable it in case of malfunctioning (in the same way some hardware manufacturers add vulnerabilities to let NSA exploit them), but it is unlikely that will really happen on purpose since most cutting edge technologies avoid paradoxes ""by design"" (you cannot have a neural network with a paradox). Arthur Prior: solved that problem elegantly. From a logical point of view you can deduce the statement is false and the statement is true, so it is a contradiction and hence false (because you could prove everything from it). Alternatively, the truth value of that sentence is not in {true, false} set in the same way imaginary numbers are not in real numbers set. Artificial intelligence to a degree of the plot would be able to run simple algorithms and either decide them,  prove those are not decidable or just ignore the result after a while attempting to simulate the algorithm. For that sentence, the AI will recognize there is a loop, and hence just stop that algorithm after 2 iterations: That sentence is an infinite loop In the movie "" Bicentennial Man "" the AI is perfectly capable to detect infinite loops (the answer to ""goodbye"" is ""goodbye""). However, an AI could be killed as well by a StackOverflow, or any regular computer virus , modern operative systems are still full of vulnerabilities, and the AI has to run on some operating system (at least).",54.75675593,"No.  This is easily prevented by a number of safety mechanisms that are sure to be present in a well-designed AI system.  For example, a timeout could be used.  If the AI system is not able to handle a statement or a command after a certain amount of time, the AI could ignore the statement and move on.  If a paradox ever does cause an AI to freeze, it's more evidence of specific buggy code rather than a widespread vulnerability of AI in general. In practice, paradoxes tend to be handled in not very exciting ways by AI.  To get an idea of this, try presenting a paradox to Siri, Google, or Cortana.",58.95255794,"AIs used in computer games already encounter similar problems, and if well designed, they can avoid it easily. The simplest method to avoid freezing in case of an unsolvable problem is to have a timer interrupt the calculation if it runs too long. Usually encountered in strategy games, and more specifically in turn based tactics, if a specific move the computer-controlled player is considering does cause an infinite loop, a timer running in the background will interrupt it after some time, and that move will be discarded. This might lead to a sub-optimal solution (that discarded move might have been the best one) but it doesn't lead to freezing or crashing (unless implemented really poorly) Computer-controlled entities are usually called ""AI"" in computer games, but they are not ""true"" AGI (artificial general intelligence). Such an AGI, if possible at all, would probably not function on similar hardware using similar instructions as current computers do, but even if it did, avoiding paradoxes would be trivial. Most modern computer systems are multi-threaded, and allow the parallel execution of multiple programs. This means, even if the AI did get stuck in processing a paradoxical statement, that calculation would only use part of its processing power. Other processes could detect after a while that there is a process which does nothing but wastes CPU cycles, and would shut it down. At most, the system will run on slightly less than 100% efficiency for a short while.",51.12390859,"Well, the issue of anthropomorphizing the AI aside, the answer is ""yes, sort of.""  Depending on how the AI is implemented, it's reasonable to say it could get ""stuck"" trying to resolve a paradox, or decide an undecidable problem . And that's the core issue - decidability .  A computer can chew on an undecidable program forever (in principle) without finishing.  It's actually a big issue in the Semantic Web community and everybody who works with automated reasoning . This is, for example, the reason that there are different versions of OWL .  OWL-Full is expressive enough to create undecidable situations.  OWL-DL and OWL-Lite aren't. Anyway, if you have an undecidable problem, that in and of itself might not be a big deal, IF the AI can recognize the problem as undecidable and reply ""Sorry, there's no way to answer that"".  OTOH, if the AI failed to recognize the problem as undecidable, it could get stuck forever (or until it runs out of memory, experiences a stack overflow, etc.) trying to resolve things. Of course this ability to say ""screw this, this riddle cannot be solved"" is one of the things we usually think of as a hallmark of human intelligence today - as opposed to a ""stupid"" computer that would keep trying forever to solve it.  By and large, today's AI's don't have any intrinsic ability to resolve this sort of thing.  But it wouldn't be that hard for whoever programs an AI to manually add a ""short circuit"" routine based on elapsed time, number of iterations, memory usage, etc.  Hence the ""yeah, sort of"" nature of this.  In principle, a program can spin forever on a paradoxical problem, but in practice it's not that hard to keep that from happening. Another interesting question would be, ""can you write a program that learns to recognize problems that are highly likely to be undecidable and gives up based on it's own reasoning?""",53.09621928
1742,What is the difference between machine learning and deep learning?,machine-learning,"Deep learning is a specific variety of a specific type of machine learning. So it's possible to learn about deep learning without learning all of machine learning, but it requires learning some machine learning (because it is some machine learning). Machine learning refers to any technique that focuses on teaching the machine how it can learn statistical parameters from a large amount of training data. One particular type of machine learning is artificial neural networks, which learn a network of nonlinear transformations that can approximate very complicated functions of wide arrays of input variables. Recent advances in artificial neural networks have to do with how to train deep neural networks, which have more layers than normal and also special structure to deal with the challenges of learning more layers.",63.91727923,"Deep learning is one form of machine learning. Deep learning refers to learning with deep neural networks, essentially networks with many layers. Neural networks are one group of many forms of machine learning: Neural Networks Decision Trees and Random Forests Support Vector Machines Bayesian Approaches k-nearest neighbors",62.70514313,"First, in most condition machine learning actually refers traditional/classical machine learning , and deep learning is specifically referring multi-layered neural network, and neural network is one of the machine learning approach. Second, Machine learning especially supervised machine learning requires engineers to design and predefine features manually , which are used to represent the data in numerical way. Such as we can represent animals with three features such as the number of eyes, the number of legs and the number of heads. The data [2,4,1] representing an animal with 2 eyes, 4 legs and 1 head. In this scenario, the feature is extracted by us, because we have knowledge on animals, and we think these features can represent animals.  However, instead of hand-crafting features the deep learning learn the features automatically. Third, when someone say machine learning he is saying algorithm , such as naive bayes, decision tree, linear regression etc. However, the deep learning is more related to the framework and architecture such as RNN, CNN, Transformer etc. Fourth, it is possible to start deep learning without knowing machine learning , sources from internet like Andrew Ng's course usually covers most topic you should know in deep learing. Try search Andrew Ng, I think he is really good!",66.42118567,"When I started Machine Leraning chapters in book used to look like this I) Supervised: Regression Linear models Classification Logestic Regression Neural Network Decision Tress and Random Forest Boosting and Bagging SVD and SVM II) UnSupervised Learning: Clustering K-Means Hierarchical Gaussian Mixture Model DB Scan Association Learning. III) ReInforment Learning: All of a sudden chapter I>2>b created a sub-field of its own . Well to know why, let me tell you a bit of history. Machine learning word was coined in 1959 by Arthur Samuel to signify that machines were able to learn from data than explicit instruction. Initally it was broken into two groups based on if th approach required label data or not(ie regression, classification), then they realised we can cassify by clustering too which gave birth to unsupervised. And word reinforment learning was born inspired by areas of game theory. Lets keep those details aside for later. Coming to deep learnign, the word deep learning came very recently, as recent as 2008 from a Geoff Hinton conference. There people started using it to indicate a very deep neural network architecture used in a paper presented by Geoff Hinton and from then onwards it kind of became as a new way of classifying machine learning besides supervised , unsupervised or reinforcement .(Disc: There may be odd reference of calling NN as DL before this but not so popular and acceptable prior to this) Well I sometimes feel the name deep learning is somewhat misnomer, it would have been better of if it was named as neural learning or to stress on depth maybe deep neural learning . If you are new you might be wondering what depth I am talking about, the entire word deep came from the fact that neural network (thanks the availability of high processing abilities of GPUs) were now able to train successfully on multiple layers. The word deep can also be loosely used to include other non-neural network areas of machine learning which requires lots of computation like deep belief net or recurrent net . To be precise the units of the networks today are no longer a mere neuron or a perceptron , it can be LSTM , GRU or a capsule , so I guess word deep now makes more sense than before.",59.82217765,"Deep Learning is subset of Machine Learning. Machine learning and Deep learning both are not two different things. Deep learning is one of the form of machine learning.
The level of layers in Neural network are more and more in depth learning is part of Deep learning. “Deep learning is a particular kind of machine learning that achieves
  great power and flexibility by learning to represent the world as
  nested hierarchy of concepts, with each concept defined in relation to
  simpler concepts, and more abstract representations computed in terms
  of less abstract ones.”",70.24884395,,,,,,,,
1700,What purpose would be served by developing AI's that experience human-like emotions?,philosophy,"The answer to this question, unlike many on this board, I think is definitive. No. We don't need AI's to have emotion to be useful, as we can see by the numerous amount of AI's we already have that are useful. But to further address the question, we can't really give AI's emotions. I think the closest we can get would be 'Can we make this AI act in a way a human would if that human was insert emotion ?'. I guess in a sense, that is having emotion, but that's a whole other discussion. And to what end? The only immediate reason coming to mind would be to create more lifelike companions or interactions, for the purposes of video games or other entertainment. A fair goal, but far from necessary. Even considering an AI-imbued greeting robot in the lobby of some building, we'd probably only ever want it to act cordial. Yann says that super-advanced AI would lead to more human-like qualities and flaws. I think it's more like it would 'give our AI's more human-like qualities or in other words flaws'. People have a tendency to act irrationally when sad or angry, and for the most part we only want rational AI. To err is human, as they say. The purpose of AI's and learning algorithms is to create systems that act or 'think' like humans, but better. Systems that can adapt or evolve, while messing up as little as possible. Emotive AI has uses, but it's certainly not a prerequisite for a useful system.",57.53471255,"I think the fundamental question is: Why even attempt to build an AI? If that objective is clear, it will provide clarity to whether or not having emotional quotient in AI make sense. Some attempts like ""Paro"" that were developed for therapeutic reasons requires they exhibit some human like emotions. Again, note that ""displaying"" emotions and ""feeling"" emotions are two completely different things. You can program a thing like paro to modulate the voice tones or facial twitches to express sympathy, affection, companionship, or whatever - but while doing so, a paro does NOT empathize with its owner - it is simply faking it by performing the physical manifestations of an emotion. It never ""feels"" anything remotely closer to what that emotion evokes in human brain. So this distinction is really important. For you to feel something, there needs to be an independent autonomous subject that has the capacity to feel. Feeling cannot be imposed by an external human agent. So going back to the question of what purpose it solves - answer really is - It depends. And the most I think we will achieve ever with silicone based AIs will remain the domain of just physical representations of emotions.",58.77646981,"I think emotions are not necessary for an AI agent to be useful.  But I also think they could make the agent MUCH more pleasant to work with.  If the bot you're talking with can read your emotions and respond constructively, the experience of interacting with it will be tremendously more pleasant, perhaps spectacularly so. Imagine contacting a human call center representative today with a complaint about your bill or a product.  You anticipate conflict.  You may have even decided NOT to call because you know this experience is going to be painful, either combative or frustrating, as someone misunderstands what you say or responds hostilely or stupidly. Now imagine calling the kindest smartest most focused customer support person you've ever met -- Commander Data -- whose only reason for existing is to make this phone call as pleasant and productive for you as possible.  A big improvement over most call reps, yes?  Imagine then if call rep Data could also anticipate your mood and respond appropriately to your complaints to defuse your emotional state... you'd want to marry this guy.  You'd call up call rep Data any time you were feeling blue or bored or you wanted to share some happy news.  This guy would become your best friend overnight -- literally love at first call. I'm convinced this scenario is valid. I've noticed in myself a surprising amount of attraction for characters like Data or Sonny from ""I Robot"".  The voice is very soothing and puts me instantly at ease.  If the bot were also very smart, patient, knowledgable, and understanding...  I really think such a bot, embued with a healthy dose of emotional intelligence, could be enormously pleasant to interact with.  Much more rewarding than any person I know.  And I think that's true of not just me. So yes, I think there's great value in tuning a robot's personality using emotions and emotional awareness.",53.49822667,"Emotion in an AI is useful, but not necessary depending on your objective (in most cases, it's not). In particular, emotion recognition/analysis is very well advanced, and it's used in a wide range of applications very successfully, from robot teacher for autistic children (see developmental robotics) to gambling (poker) to personal agents and politics sentiment/lies analysis. Emotional cognition , the experience of emotions for a robot, is much less developed, but there are very interesting researchs (see Affect Heuristic , Lovotics's Probabilistic Love Assembly , and others...). Indeed, I can't see why we couldn't model emotions such as love as they are just signals that can already be cut in humans brains (see Brian D. Earp paper) . It's difficult, but not impossible, and actually there are several robots reproducing partial emotional cognition. I am of the opinion that the claim ""robots can just simulate but not feel"" is just a matter of semantics , not of objective capacity: for example, does a submarine swim like fish swim? However, planes fly, but not at all like birds do. In the end, does the technical mean really matters when in the end we get the same behavior? Can we really say that a robot like Chappie , if it ever gets made, does not feel anything just like a simple thermostat? However, what would be the use of emotional cognition for an AI? This question is still in great debates, but I will dare offer my own insights: Emotions in humans (and animals!) are known to affect memories. They are now well known in neuroscience as additional modalities, or meta-data if you prefer, of long term memories: they allow to modulate how the memory is stored, how it is associated/related with other memories, and how it will be retrieved. As such, we can hypothesize that the main role of emotions is to add additional meta-information to memories to help in heuristic inference/retrieval. Indeed, our memories are huge, there are a lot of information we store over our lifetime, so emotions can maybe be used as ""labels"" to help retrieve faster the relevant memories. Similar ""labels"" can be more easily associated together (memories of scary events together, memories of happy events together, etc.). As such, they can help survival by quickly reacting and applying known strategies (fleeing!) from scary strategies, or to take the most out of benefitting situations (happy events, eat the most you can, will help survive later on!). And actually, neuroscience studies discovered that there are specific pathways for fear-inducing sensory stimuli, so that they reach actuators faster (make you flee) than by passing through the usual whole somato-sensory circuit as every other stimuli. This kind of associative reasoning could also lead to solutions and conclusions that could not be reached otherwise. By feeling empathy, this could ease robots/humans interaction (eg, drones helping victims of catastrophic events). A virtual model of an AI with emotions could be useful for neuroscience and medical research in emotional disorders as computational models to understand and/or infer the underlying parameters (this is often done for example with Alzheimer and other neurodegenerative diseases, but I'm not sure if it was ever done for emotional disorders as they are quite new in the DSM). So yes, ""cold"" AI is already useful, but emotional AI could surely be applied to new areas that could not be explored by using cold AI alone. It will also surely help in understanding our own brain, as emotions are an integral part.",54.93981306,"I think that depends on the application of the AI. Obviously if I develop an AI that's purpose is plainly to do specific task under the supervision of humans, there is no need for emotions. But if the AI's purpose is to do task autonomously, then emotions or empathy can be useful. For example, think about an AI that is working in the medical domain. Here it may be advantageous for an AI to have some kind of empathy, just to make the patients more comfortable. Or as another example, think about a robot that serves as a nanny. Again it is obvious that emotions and empathy would be advantageous and desirable. Even for an assisting AI program (catchword smart home) emotions and empathy can be desirable to make people more comfortable. It would be much nicer to be welcomed by an empathic home assistant than by one with no empathic responses at all, wouldn't it? On the other hand, if the AI is just working on an assembly line, there is obviously no need for emotions and empathy (on the contrary in that case it may be unprofitable).",58.84376777,"Strong AIs For a strong AI, the short answer is to call for help, when they might not even know what the supposed help could be. It depends on what the AI would do. If it is supposed to solve a single easy task perfectly and professionally, sure emotions would not be very useful. But if it is supposed to learn random new things, there would be a point that it encounters something it cannot handle. In Lee Sedol vs AlphaGo match 4, some pro who has said computer doesn't have emotions previously, commented that maybe AlphaGo has emotions too, and stronger than human. In this case, we know that AlphaGo's crazy behavior isn't caused by some deliberately added things called ""emotions"", but a flaw in the algorithm. But it behaves exactly like it has panicked. If this happens a lot for an AI. There might be advantages if it could know this itself and think twice if it happens. If AlphaGo could detect the problem and change its strategy, it might play better, or worse. It's not unlikely to play worse if it didn't do any computations for other approaches at all. In case it would play worse, we might say it suffers from having ""emotions"", and this might be the reason some people think having emotions could be a flaw of human beings. But that wouldn't be the true cause of the problem. The true cause is it just doesn't know any approaches to guarantee winning, and the change in strategy is only a try to fix the problem. Commentators thinks there are better ways (which also don't guarantee winning but had more chance), but its algorithm isn't capable to find out in this situation. Even for human, the solution to anything related to emotion is unlikely to remove emotions, but some training to make sure you understand the situation enough to act calmly. Then someone has to argue about whether this is a kind of emotion or not. We usually don't say small insects have human-like emotions, because we don't understand them and are unwilling to help them. But it's easy to know some of them could panic in desperate situations, just like AlphaGo did. I'd say these reactions are based on the same logic, and they are at least the reason why human-like emotions could be potentially useful. They are just not expressed in human-understandable ways, as they didn't intend to call a human for help. If they tries to understand their own behavior, or call someone else for help, it might be good to be exactly human-like. Some pets can sense human emotions and express human-understandable emotion to some degree. The purpose is to interact with humans. They evolved to have this ability because they needed it at some point. It's likely a full strong AI would need it too. Also note that, the opposite of having full emotions might be becoming crazy. It is probably a quick way to lose any trust if someone just implement emotions imitating humans with little understanding right away in the first generations, though. Weak AIs But is there any purposes for them to have emotions before someone wanted a strong AI? I'd say no, there isn't any inherent reasons that they must have emotions. But inevitably someone will want to implement imitated emotions anyway. Whether ""we"" need them to have emotions is just nonsense. The fact is even some programs without any intelligence contained some ""emotional"" elements in their user interfaces. They may look unprofessional, but not every task needs professionality so they could be perfectly acceptable. They are just like the emotions in musics and arts. Someone will design their weak AI in this way too. But they are not really the AIs' emotions, but their creators'. If you feel better or worse because of their emotions, you won't treat individul AIs so differently, but this model or brand as a whole. Alternatively someone could plant some personallities like in a role-playing game there. Again, there isn't a reason they must have that, but inevitably someone will do it, because they obviously had some market when a role-playing game does. In either cases, the emotions don't really originate from the AI itself. And it would be easy to implement, because a human won't expect them to be exactly like a human, but tries to understand what they intended to mean. It could be much easier to accept these emotions realizing this. Aspects of emotions Sorry about posting some original research here. I made a list of emotions in 2012 and from which I see 4 aspects of emotions. If they are all implemented, I'd say they are exactly the same emotions as of humans. They don't seem real if only some of them are implemented, but that doesn't mean they are completely wrong. The reason, or the original logical problem that the AI cannot solve. AlphaGo already had the reason, but nothing else. If I have to make an accurate definition, I'd say it's the state that multiple equally important heuristics disagreeing with each other. The context, or which part of the current approach is considered not working well and should probably be replaced. This distinguishes sadness-related, worry-related and passionate-related. The current state, or whether it feels leading, or whether its belief or the fact is supposed to turn bad first (or was bad all along) if things go wrong. This distinguishes sadness-related, love-related and proud-related. The plan or request. I suppose some domesticated pets already had this. And I suppose these had some fixed patterns which is not too difficult to have. Even arts can contain them easily. Unlike the reasons, these are not likely inherent in any algorithms, and multiple of them can appear together. Who supposedly had the responsibility if nothing is changed by the emotion. This distinguishes curiosity, rage and sadness. What is the supposed plan if nothing is changed by the emotion. This distinguishes disappointment, sadness and surprise. The source. Without context, even a human cannot reliably tell someone is crying for being moved or thankful, or smiling for some kind of embarrassment. In most other cases there aren't even words describing them. It doesn't make that much difference if an AI doesn't distinguish or show this specially. It's likely they would learn these automatically (and inaccurately as a human) at the point they could learn to understand human languages. The measurements, such as how urgent or important the problem is, or even how likely the emotions are true. I'd say it cannot be implemented in the AI. Humans don't need to respect them even if they are exactly like humans. But humans will learn how to understand an AI if that really matters, even if they are not like humans at all. In fact, I feel that some of the extremely weak emotions (such as thinking something is too stupid and boring that you don't know how to comment) exist almost exclusively in emoticons, where someone intend to show you exactly this emotion, and hardly noticeable in real life or any complex scenerios. I supposed this could also be the case in the beginning for AIs. In the worst case, they are firstly conventionally known as ""emotions"" since emoticons works in these cases, so it's easier to group them together, but very few people seriously think they are, just like the example I gave. So when strong AIs become possible, none of these would be unreachable, though there might be a lot of work to make the connections. So I'd say if there would be the need for strong AIs, they absolutely would have emotions.",60.53323269,"By emotions he doesn't mean to add all sorts of emotions into an AI. He only meant the ones that will be helpful for taking vital decisions. Consider this incident for a second: Suppose an AI self drive car is driving through the highway. The person sitting inside is the CEO of a company and he is running very behind on schedule. If he didn't get on time there will be loss of millions of dollars. The AI in the car has been told to drive as fast as possible and reach the destination. And now a rabbit (or some other animal) comes into the way. Now if the car puts emergency brakes then the passengers will get seriously hurt and plus there will be loss of millions as CEO won't be able to get to the meeting. Now what will the AI do? Since for an AI, their decisions are only based on their utility function . Hitting the rabbit and keep going will logically show a better option. But, should the AI take that decision. There are many questions like these where an AI might stuck into a situation where moral based decisions will play a vital role. The above scenario is just for an example point of view.",53.23405831,"Theory of mind If we want a strong general AI to function well in an environment that consists of humans, then it would be very useful for it to have a good theory of mind that matches how humans actually behave. That theory of mind needs to include human-like emotions, or it will not match the reality of this environment. For us, an often used shortcut is explicitly thinking ""what would I have done in this situation?"" ""what event could have motivated me to do what they just did?"" ""how would I feel if this had happened to me ?"". We'd want an AI to be capable of such reasoning, it is practical and useful, it allows better predictions of future and more effective actions. Even while it would be better for it the AI to not be actually driven by those exact emotions (perhaps something in that direction would be useful but quite likely not exactly the same), all it changes that instead of thinking ""what I would feel"" it should be able to hypothesize what a generic human would feel. That requires implementing a subsystem that is capable of accurately modeling human emotions.",59.3400601,"Human emotions are intricately connected to human values and to our ability to cooperate and form societies. Just to give an easy example:
You meet a stranger who needs help, you feel empathy . 
This compels you to help him at a cost to yourself. 
Let's assume the next time you meet him, you need something. Let's also assume he doesn't help you, you'll feel anger . 
This emotion compels you to punish him, at further cost for yourself.
He on the other hand, if he doesn't help you, feels shame .
This compels him to actually help you, avoiding your anger and making your initial investment worthwhile. You both benefit. So these three emotions keep up a circle of reciprocal help. Empathy to get started, anger to punish defectors and shame to avoid the anger. This also leads to a concept of justice. Given that value alignment is one of the big problems in AGI, human-like emotions strike me as good approach towards AIs that actually share our values and integrate themselves seamlessly into our society.",54.8140792
1544,Could curiosity improve artificial intelligence?,agi,"when the AI has difficulty in classifying a image or its objects it should ask a human for help just like a curious child It's called active learning , it's already used quite often.",50,"Does this addition of curosity changes clarifai into a true AI? As per my answer to this question , we don't know what the ingredients for a 'true AI' are. Via the Turing Test and its variants, the best we can do is ""know one when we see one"". Curiosity certainly appears necessary for intelligence, though it doesn't seem sufficient - a lemming-like creature curious to see what's at the bottom of a steep cliff might not survive long enough to learn caution, even if it had the learning mechanisms to do so. Here is some work by Schmidhuber on Artificial Curiousity . Pierre-Yves Oudeyer has also done quite a lot of work on this and Active Learning/Intrinsic motivation.",54.46821522,"It's a well known concept that's already used What we call ""curiosity"" in humans and animals is in effect the chosen level of the ""exploit vs explore"" tradeoff for any active system. For example, the field of reinforcement learning is one approach that studies implementations of what essentially is the equivalent of curiosity; and we have research on how much curiosity is best e.g. multi-armed bandit concept. So ""using curiosity"" is something that we already do as much as we can/should/are able to, but it would usually be called in some other, more specific term to specify the exact meaning instead of the vague word of ""curiosity"".",63.17365719,"Curiosity by itself does not improve intelligence. It increases the chances of better understanding a given subject, given that curiosity is coupled with actions in that direction. For example:
I am curious about how to make pancakes and decide to find a recipe but stop at the first instance of an answer with steps to follow. Curiosity needs to be coupled with the desire to improve a given understanding and be followed by a review of current knowledge with the aim of updating the previousely reached conclusions. Provided that the used logic that judges improvements is correct. Curiosity will not necessarily be benefitial for an improved intelligence.  But to allow for an improved intelligence, curiosity is a mandatory requisite. Curiosity is a symptom of an evolving intelligence.",69.05697605,"Curiosity is used successfully with Random Network Distillation (RND). OpenAI has published a detailed article about their approach using this method, which was especially successful with previously unsolved games like Montezuma’s Revenge. While this does not fully answer your question about curiosity being required to build a true AI, it shows that previously unsolved problems became solvable introducing curiosity in the reward system.",60.56626466,Curiosity would be an innate knowing in this scenario. The ability to act on it would depend on how much exposure the system gets.,56.27616653,,,,,,
1507,What are the minimum requirements to call something AI?,philosophy,"It's true that the term has become a buzzword, and is now widely used to a point of confusion - however if you look at the definition provided by Stuart Russell and Peter Norvig, they write it as follows: We define AI as the study of agents that receive percepts from the environment and perform actions . Each such agent implements a function that maps percept sequences to actions, and we cover different ways to represent these functions, such as reactive agents, real-time planners, and decision-theoretic systems. We explain the role of learning as extending the reach of the designer into unknown environments , and we show how that role constrains agent design, favoring explicit knowledge representation and reasoning . Artificial Intelligence: A Modern Approach - Stuart Russell and Peter Norvig So the example you cite, ""autopilot for cars/planes"", is actually a (famous) form of AI as it has to use a form of knowledge representation to deal with unknown environments and circumstances . Ultimately, these systems also collect data so that the knowledge representation can be updated to deal with the new inputs that they have found. They do this with autopilot for cars all the time So, directly to your question, for something to be considered as ""having AI"", it needs to be able to deal with unknown environments/circumstances in order to achieve its objective/goal , and render knowledge in a manner that provides for new learning/information to be added easily. There are many different types of well defined knowledge representation methods, ranging from the popular neural net , through to probabilistic models like bayesian networks (belief networks) - but fundamentally actions by the system must be derived from whichever representation of knowledge you choose for it to be considered as AI.",51.54788697,"In addition to what has already been said about AI, I have the following to add. ""AI"" has had quite a history going all the way back to the original Perceptron . Marvin Minsky slammed the Perceptron in 1969 for not being able to solve the XOR problem and anything that was not linearly separable, so ""Artifical Intelligence"" became a dirty word for a while, only to regain interests in the 1980s. During that time, neural nets were revived, backpropagation used to train them was developed, and as computer technology continued its exponential growth, so did ""AI"" and what became possible. Today, there are lots of things we take for granted which would've been considered ""AI"" 10 or 15 years ago, like speech recognition, for example. I got my starts in ""AI"" speech recognition back in the late 70s where you had to train the voice models to understand a single human speaker. Today, speech recognition is an afterthought with your Google apps, for example, and no a priori training is needed. Yet this technology is not, at least in general audiences, considered ""AI"" anymore. And so, what would be ""minimum requirements""? That would depend on whom you ask. And what time. It would appear that that term only applies to technology ""on the bleeding edge"". Once it becomes developed and commonplace, it is no longer referred to as AI. This is true even of Neural Nets, which are dominant in data science right now, but are referred to as ""machine learning"". Also check out the lively discussion on Quora .",55.4806882,"This is an ""in human language"" (non-technical;) synopsis of the core of Kaiesh 's excellent answer. In the most basic sense, any decision making algorithm can be regarded as a form of Artificial Intelligence. The History of Artificial Intelligence wiki gives a pretty good overview.  The roots of the field are generally ascribed to Symbolic Artificial Intelligence , but it might be said to go back as far as Babbage . The first functional game AI in the form of an ""analytic engine"" may be Nimatron ( 1940 ). More recently, Machine Learning in all of its various forms, including Neural Networks and Genetic Algorithms , have been delivering exciting results. Bayesian networks are another form of  probabilistic AI. Utility, the means by which we evaluate the degree of intelligence of algorithms, is separate from the mechanism. AIs can be weak or strong.  Strong means better performance at a task than a competing rational agent , typically humans. (""Man is the measure of all things."" Protagoras ) The term strong in relation to AI has traditionally been taken to mean Artificial General Intelligence [see also the Turing Test] , but current algorithmic intelligences are only ""narrowly strong"". Intelligence is a spectrum, therefore: The minimum requirement for AI is that an algorithm make decisions based on data, irrespective of the quality of the decisions.",53.90098344,"There is also the AI effect , that is, the tendency to not consider something an AI once it is well understood. For example, neural networks are not yet fully understood, so people still tend to call them AI. Once we know exactly all the details about neural networks and their inner workings, we might start to consider them just computation . This is an old philosophical topic that goes back at least to the famous Jacques de Vaucanson's defecating duck and automatic loom .",54.72046302,"From "" Artificial Intelligence And Life In 2030: One Hundred Year Study On Artificial Intelligence "": In fact, the field of AI is a continual endeavor to push forward the frontier of
  machine intelligence. Ironically, AI suffers the perennial fate of losing claim to its
  acquisitions, which eventually and inevitably get pulled inside the frontier, a repeating pattern known as the “AI effect” or the “odd paradox”—AI brings a new technology into the common fold, people become accustomed to this technology, it stops being considered AI, and newer technology emerges. Consequentially, I believe we can not choose a fixed set of requirements for something to be considered AI; rather, at any given moment in history, AI is a set of programs which can achieve something that before was generally considered to be solvable by humans only. As technology evolves, the boundaries keep getting pushed and pushed, and the bar rises higher. Consider chess playing: once chess engines were considered one of the pinnacles of AI, while nowadays such programs are perceived as ""blind search"" and not truly intelligent. To quote Larry Tesler, Intelligence is whatever machines haven't done yet .",53.76189555,,,,,,,,
1481,How can action recognition be achieved?,machine-learning,"This study from 2012 uses 3D convolutional neural networks (CNN) for automated recognition of human actions in surveillance videos. The 3D CNN model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. A very similar deep learning approach based on 3D CNN is demonstrated in the LIRIS and Orange Labs study from 2011 . This Oxford study from 2014 also uses a similar approach, but with two-stream CNN which incorporates spatial and temporal networks which can achieve good performance despite having limited training data. It recognises action from motion in the form of dense optical flow. For example: Another study from 2007 demonstrates a method by detecting human falls based on a combination of motion history and human shape variation by analysing the video frames. It uses Motion History Image (MHI) to quantify the motion of the person. Source: harishrithish7/Fall-Detection at GitHub An alternative general approach could be action detection based on the posture using DNN. See: How to achieve recognition of postures and gestures?",53.37510962,"There are several approaches as to how this can be achieved. One recent study from 2015 about Action Recognition in Realistic Sports Videos PDF uses the action recognition framework based on the three main steps of feature extraction (shape, post or contextual information), dictionary learning to represent a video, and classification ( BoW framework ). A few examples of methods: Spatio-Temporal Structures of Human Poses a joint shape-motion Multi-Task Sparse Learning (MTSL) Hierarchical Space-Time Segments Spatio-Temporal Deformable Part Models (SDPM) Here are the results based on training of 10 action classes based on the UCF sports dataset: Source: Action Recognition in Realistic Sports Videos .",67.40566039,"MIT have done research  and implemented an incomplete version of action video recognition. With the use of MATLAB, NNetworks and a large set of training videos. My suggested set of comments on my previous answer indicate the usage of a multi interconnected NNet, verus MIT's image based NNet.",53.48949969,"A neural network can be used but must be trained to expect the information (pattern of data, pixels or groupings of loose range such as color, and location) at any given location in the network, first a vision system must but implemented. Then a facial recognition, multiple partial individual body fixing (finding body part and there partners to a person) then training on some states and you'll have it work. MIT have done research and have made a seemy accurate implementation. I'm an AI Researcher and Software Engineer for the past 7 years.",51.85527163,"No General Movie Search Yet There have been successes in recognizing a very narrow sequence of a very narrow set of possible actions, but nothing like a general movie searching system that can return a set of matches with the start time, end time, and movie instance for each match to one of the search criteria listed in this question. Somebody was driving a car Kissing Eating Scared Talking over the phone Normalizing the List First of all, ""Was scared,"" is not the description of an action.  It should be, ""Becoming scared.""  Secondly, ""Talking over the phone,"" is not a proper action description.  It should be a conjunctive action such as, ""Talking into a phone AND listening to the same phone.""  To make the list homogenous in format, the first item should be ""Car driving,"" since the actor is human in every other case. Car driving Kissing Eating Becoming scared Talking into a phone and listening to the same phone. Realistic System Design Expectations It is unrealistic to think that an artificial neural net, by itself, can be trained to return as output the set of start and stop ranges and associated movie instances from a database of movies and one of the above list items as input.  This will require a complex system with many ANNs and other ML devices and may require other AI components that are not activation type networks at all.  Certainly convolution kernels and various types of encoders should be considered as key system components. You will need a large amount of training data to cover the above six cases (the last of the five items actually being two distinct actions that we normally associate and consider one).  If you want to detect more actions, you will need a large amount of training data for them too. Verbs and Nouns The reason this question is interesting to me is that recognizing ACTIONS are not the same as recognizing ITEMS.  All mammals learn ITEMS first and ACTIONS later.  Linguistically, nouns come before verbs in child language development.  That is because, just as detecting edges is preliminary to detecting shapes, which is preliminary to detecting objects, detecting motion is preliminary to detecting action. Verbs like, ""Eating,"" are an abstraction over the top of the motion, and, in the case of eating, the motion is complex.  Also, eating is not the same thing as gum chewing, so the sequence detected must be as follows: Insertion of food into the face through the mouth Chewing Swallowing The probability of a sequence is the product of the probability of its parts so that math is simple and easy to implement.  Concurrency, as in the case of conjunctive actions like talking into and listening to the same phone, is also relatively easy to handle in general. A Realistic Approach Certainly, generalization (and more specifically feature extraction) will need to occur in object recognition, collision detection, motion detection, facial recognition, and other planes simultaneously.  A complex topology, perhaps employing equilibria as in GAN design, will most likely be necessary to assemble elements of criteria associated with the movie query string and to run windows over the frames of each movie. To provide a service that returns results within a few days or weeks will probably require a cluster and DSP hardware (perhaps leveraging GPUs). Special Cases that Human Brains Handle Determining how long one of the two elements of concurrency can be undetected before it invalidates the conjunction can be tricky.  (How long can one not speak into the phone before it appears that it is no longer considered phone conversation?) If in the movie, only the swallowing is shown, a human can infer eating.  That kind of conclusion reliability from sparse data is a huge AI challenge discussed in various contexts throughout the literature. The Emergence of Associated Technology — A Projection I suspect that the system topography comprised of ANNs, encoders, convolution kernels, and other components to perform the search for any of a select set of actions will emerge within the next ten years.  Work seems to be tracking in that direction in the literature. A system that will acquire its own training information, sustainably grow in knowledge and perform general searches if increasing breadth and complexity may be anywhere from forty to two hundred years out.  It is difficult to predict. Gross Overoptimistic Predictions Every generation seems to view knowledge growth as an exponential function and tends to make unrealistic predictions about the advent of certain coveted technology capabilities.  Most of the predictions fail dramatically.  I have come to believe that exponential growth is an illusion created by the inverse exponential decay of interest in the past with respect to time. We lose track of the energy and rate of growth in eras before us because they become socially irrelevant.  People into scientific history, like Whitehead, Kuhn, and Ellul know that technology has moved forward quickly for at least a few hundred years.  Vernadsky inferred in his The Biosphere that life may not have arisen, that like matter and energy, it may always have existed.  I wonder if technology has been moving at an essentially constant rate for the last 50,000 years. Germany decided to double its solar panel energy output every year and published its exponential success, until a few years ago when doubling it again would cost a hundred billion dollars more than what they had to spend.  They stopped publishing the exponential growth graphs.",52.51456572,,,,,,,,
1479,Do scientists know what is happening inside artificial neural networks?,neural-networks,"There are many approaches that aim to make a trained neural network more interpretable and less like a ""black box"", specifically convolutional neural networks that you've mentioned. Visualizing the activations and layer weights Activations visualization is the first obvious and straight-forward one. For ReLU networks, the activations usually start out looking relatively blobby and dense, but as the training progresses the activations usually become more sparse (most values are zero) and localized. This sometimes shows what exactly a particular layer is focused on when it sees an image. Another great work on activations that I'd like to mention is deepvis that shows reaction of every neuron at each layer, including pooling and normalization layers. Here's how they describe it : In short, we’ve gathered a few different methods that allow you to
  “triangulate” what feature a neuron has learned, which can help you
  better understand how DNNs work. The second common strategy is to visualize the weights (filters). These are usually most interpretable on the first CONV layer which is looking directly at the raw pixel data, but it is possible to also show the filter weights deeper in the network. For example, the first layer usually learns gabor-like filters that basically detect edges and blobs. Occlusion experiments Here's the idea. Suppose that a ConvNet classifies an image as a dog. How can we be certain that it’s actually picking up on the dog in the image as opposed to some contextual cues from the background or some other miscellaneous object? One way of investigating which part of the image some classification prediction is coming from is by plotting the probability of the class of interest (e.g. dog class) as a function of the position of an occluder object. 
If we iterate over regions of the image, replace it with all zeros and check the classification result, we can build a 2-dimensional heat map of what's most important for the network on a particular image. This approach has been used in Matthew Zeiler’s Visualizing and Understanding Convolutional Networks (that you refer to in your question): Deconvolution Another approach is to synthesize an image that causes a particular neuron to fire, basically what the neuron is looking for. The idea is to compute the gradient with respect to the image, instead of the usual gradient with respect to the weights. So you pick a layer, set the gradient there to be all zero except for one for one neuron and backprop to the image. Deconv actually does something called guided backpropagation to make a nicer looking image, but it's just a detail. Similar approaches to other neural networks Highly recommend this post by Andrej Karpathy , in which he plays a lot with Recurrent Neural Networks (RNN). In the end, he applies a similar technique to see what the neurons actually learn: The neuron highlighted in this image seems to get very excited about
  URLs and turns off outside of the URLs. The LSTM is likely using this
  neuron to remember if it is inside a URL or not. Conclusion I've mentioned only a small fraction of results in this area of research. It's pretty active and new methods that shed light to the neural network inner workings appear each year. To answer your question, there's always something that scientists don't know yet, but in many cases they have a good picture (literary) of what's going on inside and can answer many particular questions. To me the quote from your question simply highlights the importance of research of not only accuracy improvement, but the inner structure of the network as well. As Matt Zieler tells in this talk , sometimes a good visualization can lead, in turn, to better accuracy.",53.45937961,"It depends on what you mean by ""know what is happening"". Conceptually, yes: ANN perform nonlinear regression. The actual expression represented by the weight matrix/activation function(s) of an ANN can be explicitly expanded in symbolic form (e.g. containing sub-expressions such as $1/1+e^{1/1+e^{\dots}}$ ). However, if by 'know' you mean predicting the output of some specific (black box) ANN , by some other means, then the obstacle is the presence of chaos in a ANN that has high degrees of freedom . Here's also some relatively recent work by Hod Lipson on understanding ANNs through visualisation .",55.53560815,"Short answer is no . Model interpretability is a hyper-active and hyper-hot area of current research (think of holy grail, or something), which has been brought forward lately not least due to the (often tremendous) success of deep learning models in  various tasks; these models are currently only black boxes, and we naturally feel uncomfortable about it... Here are some general (and recent, as of Dec 2017) resources on the subject: A recent (July 2017) article in Science provides a nice overview of the current status & research: How AI detectives are cracking open the black box of deep learning (no in-text links, but googling names & terms will pay off) DARPA itself is currently running a program on Explainable Artificial Intelligence (XAI) There was a workshop in NIPS 2016 on Interpretable Machine Learning for Complex Systems , as well as an ICML 2017 tutorial on Interpretable Machine Learning by Been Kim of Google Brain. And on a more practical level (code etc): The What-If tool by Google, a brand new (September 2018) feature of the open-source TensorBoard web application, which let users analyze an ML model without writing code ( project page , blog post ) The Layer-wise Relevance Propagation (LRP) toolbox for neural networks ( paper , project page , code , TF Slim wrapper ) FairML: Auditing Black-Box Predictive Models, by Cloudera Fast Forward Labs ( blog post , paper , code ) LIME: Local Interpretable Model-agnostic Explanations ( paper , code , blog post , R port ) A very recent (November 2017) paper by Geoff Hinton, Distilling a Neural Network Into a Soft Decision Tree , with an independent PyTorch implementation SHAP: A Unified Approach to Interpreting Model Predictions ( paper , authors' Python code , R package ) Interpretable Convolutional Neural Networks ( paper , authors' code ) Lucid, a collection of infrastructure and tools for research in neural network interpretability by Google ( code ; papers: Feature Visualization , The Building Blocks of Interpretability ) Transparecy-by-Design (TbD) networks ( paper , code , demo ) SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability ( paper , code , Google blog post ) TCAV: Testing with Concept Activation Vectors ( ICML 2018 paper , Tensorflow code ) Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization ( paper , authors' Torch code , Tensorflow code , PyTorch code , Keras example notebook ) Network Dissection: Quantifying Interpretability of Deep Visual Representations, by MIT CSAIL ( project page , Caffe code , PyTorch port ) GAN Dissection: Visualizing and Understanding Generative Adversarial Networks, by MIT CSAIL ( project page , with links to paper & code) Explain to Fix: A Framework to Interpret and Correct DNN Object Detector Predictions ( paper , code ) InterpretML by Microsoft ( code still in alpha) Anchors: High-Precision Model-Agnostic Explanations ( paper , code ) Diverse Counterfactual Explanations (DiCE) by Microsoft ( paper , code , blog post ) Axiom-based Grad-CAM (XGrad-CAM): Towards Accurate Visualization and Explanation of CNNs, a refinement of the existing Grad-CAM method ( paper , code ) Lately, there has been a surge of interest to start building a more theoretical basis for deep learning neural nets. In this context, renowned statistician and compressive sensing pioneer David Donoho has very recently (fall 2017) started offering a course at Stanford, Theories of Deep Learning (STATS 385) , with almost all the material available online; it is highly recommended... UPDATES : Interpretable Machine Learning , an online Gitbook by Christoph Molnar with R code (although in mostly covers algorithms other than neural networks) A Twitter thread , linking to several interpretation tools available for R. A short (4 hrs) online course by Kaggle, Machine Learning Explainability , and the accompanying blog post A new ICML 2018 Tutorial, Toward theoretical understanding of deep learning , by Sanjeev Arora A whole bunch of resources in the Awesome Machine Learning Interpretability repo NOTE : I do no longer keep this answer updated; for updates, see my answer in Which explainable artificial intelligence techniques are there?",51.79379967,"I'm afraid I don't have the specific citations handy, but I have seen/heard quotes by experts like Andrew Ng and Geoffrey Hinton where they clearly say that we do not really understand neural networks.  That is, we understand something of the how they work (for example, the math behind back propagation) but we don't really understand why they work.  It's sort of a subtle distinction, but the point is that no, we don't understand the very deepest details of how exactly you go from a bunch of weights, to, say, recognizing a cat playing with a ball. At least in terms of image recognition, the best explanation I've heard is that successive layers of a neural network learn more sophisticated features, composed of the more granular features from earlier levels.  That is to say, the first layer might recognize ""edges"" or ""straight lines"".  The next layer might then learn geometric shapes like ""box"", or ""triangle"", and then a higher layer might learn ""nose"" or ""eye"" based on those earlier features, and then a higher level layer still learns ""face"" made up from ""eye"", ""nose"", ""jaw"", etc.   But even that, as I understand it, is still hypothetical and/or not understood in complete detail.",51.31110473,"Not sure if this is what you are searching for, but google extracted images from networks when they were fed with white noise. See Inceptionism: Going Deeper into Neural Networks (Google Research Blog) . This kind of represents what the network knows.",53.43009142,"Here is an answer by Carlos E. Perez to the question What is theory behind deep learning? [...] The underlying mathematics of Deep Learning has been in existence for several decades, however the impressive results that we see today are part a consequence of much faster hardware, more data and incremental improvements in methods. Deep Learning in general can be framed as optimization problem where the objective is a function of the model error. This optimization problem is very difficult to solve consider that the parameter space of the model (i.e. weights of the neural network) leads to a problem in extremely high dimension. An optimization algorithm could take a very long time to explore this space. Furthermore, there was an unverified belief that the problem was non-convex and computation would forever be stuck in local minima. [...] The theory of why machines actually converge to an attractor or in other words learn to recognize complex patterns is still unknown. To sum up: we have some ideas, but we're not quite sure.",51.09348214,"Do scientists know what is happening inside artificial neural networks? YES Do scientists or research experts know from the kitchen what is happening inside complex ""deep"" neural network with at least millions of connections firing at an instant? I guess ""to know from the kitchen"" means ""to know in detail""? Let me give you a series of analogies: Does an airplane engineer know from the kitchen what happens inside the airplane? Does a chip designer know in detail what happens in the chip (s)he designed? Does a civil engineer know everything about the house he constructed? The devil is in the detail, but a crucial point here is that it's about artificial structures. They don't randomly appear. You need a lot of knowledge to get anything useful. For Neural Networks, I would say it took roughly 40 years from the publication of the key idea (Rosenblatt perceptron, 1957) to the first application (US Postal Service, 1989). And from there again 13 years of active reserach to really impressive systems (ImageNet 2012). What we know super well is how the training works . Because it needs to be implemented. So on a very small structure, we know it in detail. Think of computers. The chip designers know very well how their chip works. But they will likely only have a very rough idea how the Linux operating system works. Another example is physics and chemistry: Physics describes the core forces of the universe. Does that mean they know everything about chemistry as well? Hell no! A ""perfect"" physicist can explain everything in chemistry ... but it would be pretty much useless. He would need a lot more information, not be able to skip the irrelevant parts. Simply because he ""zoomed in"" too much - considers details which are in practice neither interesting nor important. Please note that the knowledge of the physicist is not wrong. Maybe one could even deduce the knowledge from the chemist from it. But this ""high-level"" understanding of molecule interaction is missing. The key insight from those two examples are abstraction layers: You can build complexity from simple structures . What else? We know well what is in principle achievable with the neural networks we design: A neural network designed to play Go - no matter how sophisticated - will never even be able to play chess. You can, of course, add another abstraction layer around it and combine things. But this approach needs humans. A neural network designed for distinguishing dogs from cats which has only seen pudels and Persian cats will likely perform really bad when it has to decide for Yorkshire Terriers. Oh, and of course we have analytical approaches for neural networks. I wrote my masters thesis about Analysis and Optimization of Convolutional Neural Network Architectures . In this context LIME (Local Interpretable Model-Agnostic Explanations) is nice:",64.61598962,,,,
1462,What is the difference between artificial intelligence and robots?,terminology,"Although there are several definitions of ""robot"", an essential feature of everything called ""robot"" is that it is capable of movement. This does not necessarily mean displacement ; a robot arm in a factory also moves. There is a single exception to this rule,  which is bot-programs like chatbots; I will discuss them later. Artificial Intelligence does not need to move; a chess program can be argued to be an AI, but does not move. A robot can actually have AI; one of the definitions of robot is that it is a system, capable of autonomous movement. In order to be autonomous, to be able to make decisions of its own, a certain amount of AI may be necessary. There is one class of ""robots"" that does not move, and does not even have physical presence; bot programs, like chatbots, that operate inside systems. I do not consider them robots, because they are not physical devices operating in the real world. A chatbot can be an AI, however - a good chatbot may have some natural language processing to interact with humans in a way that humans find natural. To summarize; an AI can exist purely in software. But to be a robot, there must be a moving physical component in the real world.",52.84728758,"In the broadest sense, the difference is that non-robotic A(G)I may not be possible because, as per this question , it could be that ""Intelligence requires a body"". More specifically, it could be that there are limitations to what the traditional (well, 1950s style) 'Brain in a vat' notion of an AI is capable of comprehending, in the absence of experience of embodied experience such as force, motion and ""the raw, unawshed world"".",54.1175412,"In a general sense you can say that robot is a piece of hardware, while AI is software (sometimes hardware too). Wikipedia states Robot as a machine which performs complex set of tasks automatically. Machine - A mechanical device basically. So, technically you can create a robot that doesn't require any kind of complex algorithms to take decisions. A simple line follower doesn't even require a microcontroller. Just some gates are enough. Some other examples of robots are, a robotic arm, automated control systems in industries, etc. If you think about it even the printer in your house is a robot in itself. Artificial Intelligence is a field of Computer Science which deals with developing systems that can perform tasks rationally as if it is using intelligence (of human level) for taking decisions. AI deals with complex algorithms. Some examples of AI are speech recognition, face recognition, natural language processing, etc. AI don't necessarily need additional hardware. A simple desktop at home will work, while the term robot is used for external hardware that does some autonomous task repeatedly.",52.65186744,"Basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior ( AI ). Tim Urban on Wait But Why website wrote the following to clear things up: First, stop thinking of robots. A robot is a container for AI, sometimes mimicking the human form, sometimes not — but the AI itself is the computer inside the robot . AI is the brain and the robot is its body — if it even has a body. For example, the software and data behind Siri is AI , the woman’s voice we hear is a personification of that AI, and there’s no robot involved at all. Source: The AI Revolution: The Road to Superintelligence",53.85825382,"Simply we say AI is software and robot is its body. This is because the algorithms we commonly think of AI come in the form of software, where when we talk about robots, we're talking about physical automation. In an automobile manufacturing process where automation is used, the software makes the decisions on what physical action the robot arm should take at any given time.",52.68113287,"An AI is a computer program designed for tasks normally requiring human intelligence (a human's ability to learn), while a robot is a machine that completes complex tasks. An AI could be used to control a robot, but they are very different. Source: Oxford English Dictionary , above links will direct to definitions.",50.81716292,,,,,,
1294,How do capsule neural networks work?,neural-networks,"It appears to not be published yet; the best available online are these slides for this talk . (Several people reference an earlier talk with this link , but sadly it's broken at time of writing this answer.) My impression is that it's an attempt to formalize and abstract the creation of subnetworks inside a neural network. That is, if you look at a standard neural network, layers are fully connected (that is, every neuron in layer 1 has access to every neuron in layer 0, and is itself accessed by every neuron in layer 2). But this isn't obviously useful; one might instead have, say, n parallel stacks of layers (the 'capsules') that each specializes on some separate task (which may itself require more than one layer to complete successfully). If I'm imagining its results correctly, this more sophisticated graph topology seems like something that could easily increase both the effectiveness and the interpretability of the resulting network.",50.63056818,"To supplement the previous answer: there is a paper on this that is mostly about learning low-level capsules from raw data, but explains Hinton's conception of a capsule in its introductory section: http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf It's also worth noting that the link to the MIT talk in the answer above seems to be working again. According to Hinton, a ""capsule"" is a subset of neurons within a layer that outputs both an ""instantiation parameter"" indicating whether an entity is present within a limited domain and a vector of ""pose parameters"" specifying the pose of the entity relative to a canonical version. The parameters output by low-level capsules are converted into predictions for the pose of the entities represented by higher-level capsules, which are activated if the predictions agree and output their own parameters (the higher-level pose parameters being averages of the predictions received). Hinton speculates that this high-dimensional coincidence detection is what mini-column organization in the brain is for. His main goal seems to be replacing the max pooling used in convolutional networks, in which deeper layers lose information about pose.",52.92172851,"One of the major advantages of convolutional neural networks is their invariance to translation. However, this invariance comes with a price, that is, it does not consider how different features are related to each other. For example, if we have a picture of a face, a CNN will have difficulties distinguishing the relationship between the ""mouth"" feature and ""nose"" feature. The max-pooling layers are the main reason for this effect, because, when we use max-pooling layers, we lose the precise locations of the mouth and nose, and we cannot say how they are related to each other. Capsules try to keep the advantage of CNN and fix this drawback in two ways Invariance : quoting from this paper When the capsule is working properly, the probability of the visual entity being present is locally invariant – it does not change as the entity moves over the manifold of possible appearances within the limited domain covered by the capsule. In other words, capsule takes into account the existence of the specific feature that we are looking for, like the mouth or nose. This property makes sure that capsules are translation invariant the same that CNNs are. Equivariance : instead of making the feature translation invariance , the capsule will make it translation-equivariant or viewpoint-equivariant. In other words, as the feature moves and changes its position in the image, the feature vector representation will also change in the same way which makes it equivariant. This property of capsules tries to solve the drawback of max-pooling layers that I mentioned at the beginning.",54.93366012,"Capsule networks try to mimic Hinton's observations of the human brain on the machine. The motivation stems from the fact that neural networks needed better modeling of the spatial relationships of the parts. Instead of modeling the co-existence, disregarding the relative positioning, capsule-nets try to model the global relative transformations of different sub-parts along a hierarchy. This is the eqivariance vs. invariance trade-off, as explained above by others. These networks therefore include somewhat a viewpoint / orientation awareness and respond differently to different orientations. This property makes them more discriminative, while potentially introducing the capability to perform pose estimation as the latent-space features contain interpretable, pose specific details. All this is accomplished by including a nested layer called capsules within the layer, instead of concatenating yet another layer in the network. These capsules can provide vector output instead of a scalar one per node. The crucial contribution of the paper is the dynamic routing which replaces the standard max-pooling by a smart strategy. This algorithm applies a mean-shift clustering on the capsule outputs to ensure that the output gets sent only to the appropriate parent in the layer above. Authors also couple the contributions with a margin loss and reconstruction loss, which simultaneously help in learning the task better and show state of the art results on MNIST. The recent-paper is named Dynamic Routing Between Capsules and is available on Arxiv: https://arxiv.org/pdf/1710.09829.pdf .",55.11478481,"In the abstract of the paper Dynamic Routing between Capsules (November 7, 2017) that formally introduced capsule neural networks, the authors write A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule
  becomes active. We show that a discriminatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher-level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.",59.51136962,"Capsule Networks have two key ideas: the first idea is how to represent multi-dimensional entities . Capsule Networks does this by grouping these properties of a feature together (""capsules""). the second is that you activate higher-level features by agreement between lower-level features (""routing by agreement""). First, Capsule Networks partition the image into regions subsets. For each of these regions, it assumes that there is at most one instance of a single feature, called a Capsule. A Capsule is able to represent an instance of a feature (but only one) and is able to represent all the different properties of that feature, e.g. , its (x,y) coordinates, its colour, its movement etc. The difference from Convolutional Neural Networks (CNNs) is that the Capsules bundle the neurons into groups with multi-dimensional properties, whereas in CNNs the neurons represent single, unrelated scalar properties. This structured Capsule representation allows you to do ""routing by agreement"". To understand this, lets look at the example of a face detector. Here, you could have capsules representing ""mouth"", ""eye"", ""nose"" etc. Since the Capsules are multi-dimensional you also train them to predict the parameters for the entire face. Now, if the ""mouth"", ""nose"" and ""eye"" Capsules agree about the parameters of the face we have a very strong signal that this is a good prediction since accidental agreement in a high-dimensional space like a neural network is very unlikely. You use this to stack the Capsules into deep networks where the activation of higher-level Capsules are conditioned on agreement between the lower-level Capsules ( e.g. the Face capsule being activated by agreement on the face position between the Nose, Mouth, Eye Capsules in the earlier, lower-level layer). In contrast to regular feed-forward nets this requires a bit of iteration in the forward pass through the network, but you can still use back-propagation train it. It is an interesting way to add a bit of structure to the data. So far, it looks like they are able to provide better generalization from limited training data.",57.21442155,,,,,,
1285,Are there any decentralized examples of AI systems with blockchain technology?,reference-request,"Swarm intelligence is the term for systems where relatively simple agents work together to solve a complicated problem in a decentralized fashion. In general, distributed computing methods are very important for dealing with problems at scale, and many of them embrace decentralization in a deep way. (Given the reality of hardware failure and the massive size of modern datasets relative to individual nodes, the less work is passed through a central bottleneck, the better.) While there are people interested in doing computation on the blockchain, it seems to me like it's unlikely to be competitive with computation in dedicated clusters (like AWS).",53.68170206,"I think the best example of AI being deployed on the blockchain is SingularityNET . They just had a successful token sell where they sold out of their AGI token which will be able to be used to essentially ""pay"" for AI tasks to be done for you. Various AI will be put on the network and able to interact and communicate with each other to get various tasks done. There are some great videos online where Dr. Ben Goertzel explains this further. And here is a link to their whitepaper .",54.23592391,"Have a look at the paper Blockchain-Based Federated Learning in Medicine (2020), where the blockchain is used as a ""federation"" server for improving the parameters of local neural networks.",57.36832893,"I'm aware of some works that use blockchains in federated learning scenario for accountability, incentivize participants and so on. Here's an example, BlockFLA: Accountable Federated Learning via Hybrid Blockchain Architecture (2020), but there are probably many similar works around.",53.89228515,"I'm currently working on a p2p framework to train with neuroevolution, it will have neat, hyperneat, and eshyperneat example experiments. AI developers will be able to fork and add any experiments they want. It isn't a blockchain per sé but will have a dht for genes, champion nets, peers of course, and genomes. Neuroevolution training can be done in parallel so the population will be distributed evenly among peers, peers that finish early will help slower peers with their evaluations, and all peers will have a check on one of the nets they evaluated by a random peer to prevent malice and all peers will check the champion of each generation. Any peer that evaluates a net will be rewarded a token that is specific to the generation it helped train and will also download a copy of the champion net, later I plan to allow non training clients the ability to view performance of past champs and purchase them from the network, any peer with a token for the genome they purchase will receive part of the payment. This will not be proof of stake, it will be a proof of work where work is evaluating nets, since this needs to be checked by other nets so people don't just post phony fitness for genomes I'll be calling it proof of fitness.",52.28949567,"Maybe the term you're looking for is federated learning. Check out OpenMined project, PySyft and Tensorflow Federated libraries.",50,,,,,,
240,What exactly are genetic algorithms and what sort of problems are they good for?,genetic-algorithms,"Evolutionary algorithms are a family of optimization algorithms based on the principle of Darwinian natural selection . As part of natural selection, a given environment has a population of individuals that compete for survival and reproduction. The ability of each individual to achieve these goals determines their chance to have children, in other words, to pass on their genes to the next generation of individuals, who, for genetic reasons, will have an increased chance of doing well, even better, in realizing these two objectives. This principle of continuous improvement over the generations is taken by evolutionary algorithms to optimize solutions to a problem. In the initial generation , a population composed of different individuals is generated randomly or by other methods. An individual is a solution to the problem, more or less good: the quality of the individual in regards to the problem is called fitness , which reflects the adequacy of the solution to the problem to be solved. The higher the fitness of an individual, the higher it is likely to pass some or all of its genotype to the individuals of the next generation. An individual is coded as a genotype , which can have any shape, such as a bit vector ( genetic algorithms ) or a vector of real (evolution strategies). Each genotype is transformed into a phenotype when assessing the individual, i.e. when its fitness is calculated. In some cases, the phenotype is identical to the genotype: it is called direct coding . Otherwise, the coding is called indirect. For example, suppose you want to optimize the size of a rectangular parallelepiped defined by its length, height and width. To simplify the example, assume that these three quantities are integers between 0 and 15. We can then describe each of them using a 4-bit binary number. An example of a potential solution may be to genotype 0001 0111 1010. The corresponding phenotype is a parallelepiped of length 1, height 7 and width 10. During the transition from the old to the new generation, the variation operators , whose purpose is to manipulate individuals, are applied. There are two distinct types of variation operators: the mutation operators , which are used to introduce variations within the same individual, as genetic mutations; the crossover operators , which are used to cross at least two different genotypes, as genetic crosses from breeding. Evolutionary algorithms have proven themselves in various fields such as operations research, robotics, biology, nuance, or cryptography. In addition, they can optimize multiple objectives simultaneously and can be used as black boxes because they do not assume any properties in the mathematical model to optimize. Their only real limitation is the computational complexity.",54.12871935,"A genetic algorithm is an algorithm that randomly generates a number of attempted solutions for a problem. This set of attempted solutions is called the ""population"". It then tries to see how well these solutions solve the problem, using a given fitness function . The attempted solutions with the best fitness value are used to generate a new population. This can be done by making small changes to the attempted solutions (mutation) or by combining existing attempted solutions (crossover). The idea is that, over time, an attempted solution emerges that has a high enough fitness value to solve the problem. The inspiration for this came from the theory of evolution; the fittest solutions survive and procreate. Example 1 Suppose you were looking for the most efficient way to cut a number of shapes out of a piece of wood. You want to waste as little wood as possible. Your attempted solutions would be random arrangements of these shapes on your piece of wood. Fitness would be determined by how little wood would be left after cutting the shapes following this arrangement. The less wood is left, the better the attempted solution. Example 2 Suppose you were trying to find a polynomial that passes through a number of points. Your attempted solutions would be random polynomials. To determine the fitness of these polynomials, you determine how well they fit the given points. (In this particular case, you would probably use the least squares method to determine how well the polynomial fit the points).
Over a number of trials, you would get polynomials that fit the points better, until you had a polynomial that fit the points closely enough.",50.93408467,"This answer requests a practical example of how one might be used, which I will attempt to provide in addition to the other answers. They seem to due a very good job of explaining what a genetic algorithm is. So, this will give an example. Let's say you have a neural network (although they are not the only application of it), which, from some given inputs, will yield some outputs. A genetic algorithm can create a population of these, and by seeing which output is the best, breed and kill off members of the population. Eventually, this should optimise the neural network if it is complicated enough. Here is a demonstration I've made, which despite being badly coded, might help you understand. http://khrabanas.github.io/projects/evo/evo.html Hit the evolve button and mess around with the goals. It uses a simple genetic algorithm to breed, mutate, and decide which individuals of the population survive. Depending on how the input variables are set, the network will be able to get to some level of closeness to them. In this fashion, the population will likely eventually become a homogeneous group, whose outputs resemble the goals. The genetic algorithm is trying to create a ""neural network"" of sorts, that by taking in RGB, will yield an output color. First, it generates a random population. It then by taking 3 random members from the population, selecting the one with the lowest fitness and removing it from the population. The fitness is equal to the difference in the top goal squared + the difference in the bottom goal squared. It then breeds the two remaining ones together and adds the child to the same place in the population as the dead member. When mating occurs, there is a chance a mutation will occur. This mutation will change one of the values randomly. As a side note, due to how it is set up, it is impossible for it to be totally correct in many cases, though it will reach relative closeness.",52.88382334,"There are a number of good answers here explaining what genetic algorithms are, and giving example applications. I'm adding some general purpose advice on what they are good for, but also cases where you should NOT use them. If my tone seems harsh, it is because using GAs in any of the cases in the inappropriate section below will lead to your paper being instantly rejected from any top-tier journal. First, your problem MUST be an optimization problem. You need to define a ""fitness function"" that you are trying to optimize and you need to have a way to measure it. Good Crossover functions are easy to define and natural : When dealing with certain kinds of data, crossover/mutation functions might be easy to define. For example strings (eg. DNA or gene sequences) can be mutated easily by splicing two candidate strings to obtain a new one (this is why nature uses genetic algorithms!). Trees (like phylogenetic trees or parse trees) can be spliced too, by replacing a branch of one tree with a branch from another. Shapes (like airplane wings or boat shapes) can be mutated easily by drawing a grid on the shape and combining different grid sections from the parents to obtain a child. Usually this means your problem is composed of different parts and putting together parts from distinct solutions is a valid candidate solution. This means that if your problem is defined in a vector space where the coordinates don't have any special meaning, GAs are not a good choice. If it is hard to formulate your problem as a GA, it is not worth it. Black Box evaluation : If for a candidate, your fitness function is evaluated outside the computer, GAs are a good idea. For example, if you are testing a wing shape in an air tunnel, genetic algorithms will help you generate good candidate shapes to try. Exception: Simulations . If your fitness function is measuring how well a nozzle design performs and requires simulating the fluid dynamics for each nozzle shape, GAs may work well for you. They may also work if you are simulating a physical system through time and are interested in how well your design performs over the course of the operation eg. modelling locomotion patterns . However, methods that use partial differential equations as constraints are being developed in the literature, eg. PDE constrained optimization , so this may change in the future. Inappropriate You can calculate a gradient for your function: If you have access to the gradient of your function, you can do gradient descent, which is in general much more efficient than GAs. Gradient descent may have issues with local minima (as will GAs) but many methods have been studied to mitigate this. You know the fitness function in closed form : Then, you can probably calculate the gradient. Many languages have libraries supporting automatic differentiation , so you don't even need to do it manually. If your function is not differentiable, then you can use subgradient descent . Your optimization problem is of a known form, like a linear program or a quadratic program : GAs (and black box optimization methods in general) are very inefficient in terms of the number of candidates they need to evaluate, and are best avoided if possible. Your solution space is small : If you can grid your search space efficiently, you can guarantee that you have found the best solution, and can make contour plots of the solution space to see if there is a region you need to explore further. Finally, if you are considering a GA, consider more recent work in Evolutionary Strategies. I am biased towards CMA-ES , which I think is a good simple algorithm that captures the notion of a gradient in the fitness landscape in a way that traditional GAs do not.",55.55993262,"As observed in another answer, all you need to apply Genetic Algorithms (GAs) is to represent a potential solution to your problem in a form that is subject to crossover and mutation. Ideally, the fitness function will provide some kind of smooth feedback about the quality of a solution, rather than simply being a 'Needle in a Haystack'. Here are some characteristics of problems that Genetic Algorithms (and indeed Metaheuristics in general) are good for: NP-complete - The number of possible solutions to the problem is
exponential, but checking the fitness of a solution is relatively
cheap (technically, with time polynomial in the input size). Black box - GAs work reasonably well even if you don't have a particularly
informed model of the problem to be solved. This means that these
approaches are also useful as a 'rapid prototyping' approach to
solving problems. However, despite their widespread use for the purpose, note that GAs are actually not function optimizers - GA mechanisms tend not to explore 'outlying' regions of the search space in the hope of finding some distant high quality solution, but rather to cluster around more easily attainable peaks in the 'fitness landscape'. More detail on the applicability of GAs is given in a famous early paper ""What makes a problem hard for a Genetic Algorithm?""",58.22343961,,,,,,,,
154,Is it possible to train the neural network to solve math equations?,neural-networks,"Yes, it has been done! However, the applications aren't to replace calculators or anything like that. The lab I'm associated with develops neural network models of equational reasoning to better understand how humans might solve these problems. This is a part of the field known as Mathematical Cognition . Unfortunately, our website isn't terribly informative, but here's a link to an example of such work. Apart from that, recent work on extending neural networks to include external memory stores (e.g. Neural Turing Machines) was used to solve math problems as a good proof of concept. This is because many arithmetic problems involve long procedures with stored intermediate results. See the sections of this paper on long binary addition and multiplication.",57.92042616,"Not really. Neural networks are good for determining non-linear relationships between inputs when there are hidden variables. In the examples above, the relationships are linear, and there are no hidden variables. But even if they were non-linear, a traditional ANN design would not be well suited to accomplish this. By carefully constructing the layers and tightly supervising the training, you could get a network to consistently produce the output 4.01, say, for the inputs: 2, 1 (+), and 2, but this is not only wrong, it's an inherently unreliable application of the technology.",52.94145285,"It is possible! In fact, it's an example of the popular deep learning framework Keras. Check out this link to see the source code . This particular example uses a recurrent neural network (RNN) to process the problem as a sequence of characters, producing a sequence of characters which form the answer. Note that this approach is obviously different from how humans tend to think about solving simple addition problems, and probably isn't how you would ever want a computer to solve such a problem. Mostly this is an example of sequence to sequence learning using Keras. When handling sequential or time-series inputs, RNNs are a popular choice.",54.11615678,"Yes - it would seem that it is now possible to achieve more is required from the example you've given this paper describes a DL solution to a considerably harder problem - generating the source code for a program described in natural language . Both of these can be described as regression problems (i.e. the goal is to minimize some loss function on the validation set), but the search space in the natural language case is much bigger.",53.02280667,"There's the fairly well established field of automated theorem proving . This most likely encompasses solving equations, but doesn't necessarily involve AI. This post from the Cross Validated stackexchange has some more information on the topic.",53.0191654,"Yes, it is possible, you would be training a network with 3 inputs and 1 output to learn the model $Y= X_1 + X_2 + X_3$ , although I don't see what would be the use of doing that. Furthermore, how do you plan to generalize to learn to add four or five terms?",52.58872679,,,,,,
123,Does the Chinese Room argument hold against AI?,philosophy,"It depends on the definition of (artificial) intelligence. The position that Searle originally tried to refute with the Chinese room experiment was the so-called position of strong AI: An appropriately programmed computer would have a mind in the exact same sense as humans have minds. Alan Turing tried to give a definition of artificial intelligence with the Turing Test, stating that a machine is intelligent if it can pass the test. The Turing Test is introduced here . I won't explain it in detail because it is not really relevant to the answer. If you define (artificial) intelligence as Turing did, then the Chinese room experiment is not valid. So the point of the Chinese room experiment is to show that an appropriately programmed computer is not the same as a human mind, and therefore that Turing's Test is not a good one.",60.94022484,"There are two broad types of responses to philosophical queries like this. The first is to make analogies and refer to intuition; one could, for example, actually calculate the necessary size for such a Chinese room, and suggest that it exists outside the realm of intuition and thus any analogies using it are suspect. The second is to try to define the terms more precisely. If by ""intelligence"" we mean not ""the magic thing that humans do"" but ""information processing,"" then we can say ""yes, obviously the Chinese Room involves successful information processing."" I tend to prefer the second because it forces conversations towards observable outcomes , and puts the difficulty of defining a term like ""intelligence"" on the person who wants to make claims about it. If ""understanding"" is allowed to have an amorphous definition, then any system could be said to have or not have understanding. But if ""understand"" is itself understood in terms of observable behavior, then it becomes increasingly difficult to construct an example of a system that ""is not intelligent"" and yet shares all the observable consequences of intelligence.",56.55522012,"First of all, for a detailed view of the argument, check out the SEP entry on the Chinese Room . I consider the CRA as an indicator of you definition of intelligence. If the argument holds, yes, the person in the room understands Chinese. However, let's sum up the three replies discussed in the SEP entry: The man himself doesn't understand Chinese (he wouldn't be able to understand it when outside the room), but the system man+room understands it. Accepting that reply suggests that there can exist an intelligent system which parts aren't themselves intelligent (which can be argued of the human body itself). The system doesn't understand Chinese, as it cannot interact with the world in the same way a robot or a human could (i.e. it cannot learn, is limited in the set of questions it can answer) The system doesn't understand Chinese (depending on your definition of understanding ), and you couldn't say a human performing the same feats as the Chinese room understands Chinese either. So whether the argument, or a variant of it holds, depends on your definitions of intelligent , understanding , on how you define the system, etc. The point being that the thought experiment is a nice way to differentiate between the definitions (and many, many debates have been held about them), in order to avoid talking past each other endlessly.",62.87303658,"Depends on who you ask! John Searle, who proposed this argument, would say ""yes"", but others would say it is irrelevant. The Turing Test does not stipulate that a machine must actually ""understand"" what it is doing, as long as it seems that way to a human. You could argue that our ""thinking"" is only a more sophisticated form of clever algorithmics.",52.3246814,"What excellent responses!  When Searle published his paper, I was still in college, so my own understanding was limited.  ...And I attempted to resolve ""my version"" of what he was trying to say. It was a hugely rewarding effort. Since then a lot has happened.  In one world, I simply dropped my pursuit of working through the CR problem, and went on to other problems.  Still, as recently as March 2023, I received an email from my brother-in-law with a link to Jeffrey Kaplan's ""most famous thought experiment..."" video.  Reluctantly, I watched.  If you haven't seen this <30 minute lesson, you really owe it to yourself.  Here's the link: https://www.youtube.com/watch?v=tBE06SdgzwM Since then, I've realized that I wasn't the only one who didn't understand what Searle was talking about.  Searle himself, though a profound and astute philosopher, did not understand programming enough to weigh in on AI. Try this: write some code.
(I'll show you my answer)
I wrote a loop in C
int i;
for (i=0;i<10;i++) {
  printf(""%f\n"",3.1415);
}
Now answer, ""why did you <placeholder>?""
Why did you write a loop?
Why only 10 iterations?
Why not more? Less?
Why print 3.1415?  You could have done anything in that space.
It doesn't matter what the questions are (assuming they relate, in spirit).
It doesn't matter what the answers are. All substance , no matter how trivial, is not syntax; it is semantics.
Almost every line.  Of every program.  Ever written.  Is already loaded with semantics.  To limit computer programs to being syntax-only is a world-class oversight.  I accepted the premise.  Lots of people did.  The Chinese Room is built on this casually-delivered error.  So if you found yourself disagreeing with Searle's premise but not really being clear as to why... Searle paints an arid ghost town, devoid of semantics, and challenges us to spot the spirit of AI on his canvas.  No wonder there was an AI winter!  Turn away from that erroneous artwork, and see the colors and hear the sounds of the real world.  The spirit of AI is in that rich world. also, when's the last time you wrote a program that was just a big case statement?",53.3884655,,,,,,,,
111,How could self-driving cars make ethical decisions about who to kill?,philosophy,"How could self-driving cars make ethical decisions about who to kill? It shouldn't. Self-driving cars are not moral agents. Cars fail in predictable ways. Horses fail in predictable ways. the car is heading toward a crowd of 10 people crossing the road, so
  it cannot stop in time, but it can avoid killing 10 people by hitting
  the wall (killing the passengers), In this case, the car should slam on the brakes. If the 10 people die, that's just unfortunate. We simply cannot trust all of our beliefs about what is taking place outside the car. What if those 10 people are really robots made to look like people? What if they're trying to kill you? avoiding killing the rider of the motorcycle considering that the
  probability of survival is greater for the passenger of the car, Again, hard-coding these kinds of sentiments into a vehicle opens the rider of the vehicle up to all kinds of attacks, including ""fake"" motorcyclists. Humans are barely equipped to make these decisions on their own, if at all. When it doubt, just slam on the brakes. killing animal on the street in favour of human being, Again, just hit the brakes. What if it was a baby? What if it was a bomb? changing lanes to crash into another car to avoid killing a dog, Nope. The dog was in the wrong place at the wrong time. The other car wasn't. Just slam on the brakes, as safely as possible. Does the algorithm recognize the difference between a human being and an animal? Does a human? Not always. What if the human has a gun? What if the animal has large teeth? Is there no context? Does the size of the human being or animal matter? Does it count how many passengers it has vs. people in the front? Does it ""know"" when babies/children are on board? Does it take into the account the age (e.g. killing the older first)? Humans can't agree on these things. If you ask a cop what to do in any of these situations, the answer won't be, ""You should have swerved left, weighed all the relevant parties in your head, assessed the relevant ages between all parties, then veered slightly right, and you would have saved 8% more lives."" No, the cop will just say, ""You should have brought the vehicle to a stop, as quickly and safely as possible."" Why? Because cops know people normally aren't equipped to deal with high-speed crash scenarios. Our target for ""self-driving car"" should not be 'a moral agent on par with a human.' It should be an agent with the reactive complexity of cockroach, which fails predictably.",60.19603723,"The answer to a lot of those questions depends on how the device is programmed. A computer capable of driving around and recognizing where the road goes is likely to have the ability to visually distinguish a human from an animal, whether that be based on outline, image, or size. With sufficiently sharp image recognition, it might be able to count the number and kind of people in another vehicle. It could even use existing data on the likelihood of injury to people in different kinds of vehicles. Ultimately, people disagree on the ethical choices involved. Perhaps there could be ""ethics settings"" for the user/owner to configure, like ""consider life count only"" vs. ""younger lives are more valuable."" I personally would think it's not terribly controversial that a machine should damage itself before harming a human, but people disagree on how important pet lives are. If explicit kill-this-first settings make people uneasy, the answers could be determined from a questionnaire given to the user.",53.18652112,"Personally, I think this might be an overhyped issue. Trolley problems only occur when the situation is optimized to prevent ""3rd options"". A car has brakes, does it not? ""But what if the brakes don't work?"" Well, then the car is not allowed to drive at all. Even in regular traffic, human operators are taught that your speed should be limited as such that you can stop within the area you can see. Solutions like these will reduce the possibility of a trolley problem. As for animals... if there is no explicit effort to deal with humans on the road I think animals will be treated the same. This sounds implausible - roadkill happens often and human ""roadkill"" is unwanted, but animals are a lot smaller and harder to see than humans, so I think detecting humans will be easier, preventing a lot of the accidents. In other cases (bugs, faults while driving, multiple failures stacked onto each other), perhaps accidents will occur, they'll be analysed, and vehicles will be updated to avoid causing similar situations.",50.7038222,"In the real world, decisions will be made based on the law, and as noted over on Law.SE , the law generally favors inaction over action.",51.22234142,"This is the well known Trolley Problem . As Ben N said, people disagree on the right course of action for trolley problem scenarios, but it should be noted that with self-driving cars, reliability is so high that these scenarios are really unlikely. So, not much effort will be put into the problems you are describing, at least in the short term.",53.97837133,"“This moral question of whom to save: 99 percent of our engineering work is to prevent these situations from happening at all.”
—Christoph von Hugo, Mercedes-Benz This quote is from an article titled Self-Driving Mercedes-Benzes Will Prioritize Occupant Safety over Pedestrians published OCTOBER 7, 2016 BY MICHAEL TAYLOR , retrieved 08 Nov 2016. Here's an excerpt that outlines what the technological, practical solution to the problem. The world’s oldest carmaker no longer sees the problem, similar to the question from 1967 known as the Trolley Problem, as unanswerable. Rather than tying itself into moral and ethical knots in a crisis, Mercedes-Benz simply intends to program its self-driving cars to save the people inside the car. Every time. All of Mercedes-Benz’s future Level 4 and Level 5 autonomous cars will prioritize saving the people they carry, according to Christoph von Hugo, the automaker’s manager of driver assistance systems and active safety. There article also contains the following fascinating paragraph. A study released at midyear by Science magazine didn’t clear the air, either. The majority of the 1928 people surveyed thought it would be ethically better for autonomous cars to sacrifice their occupants rather than crash into pedestrians. Yet the majority also said they wouldn’t buy autonomous cars if the car prioritized pedestrian safety over their own.",54.93565116,"For a driverless car that is designed by a single entity, the best way for it to make decisions about whom to kill is by estimating and minimizing the probable liability. It doesn't need to absolutely correctly identify all the potential victims in the area to have a defense for its decision, only to identify them as well as a human could be expected to. It doesn't even need to know the age and physical condition of everyone in the car, as it can ask for that information and if refused, has the defense that the passengers chose not to provide it, and therefore took responsibility for depriving it of the ability to make a better decision. It only has to have a viable model for minimizing exposure of the entity to lawsuits, which can then be improved over time to make it more profitable.",54.81160809,"How could self-driving cars make ethical decisions about who to kill? By managing legal liability and consumer safety. A car that offers the consumer safety is going to be a car that is bought by said consumers. Companies do not want to be liable for killing their customers nor do they want to sell a product that gets the user in legal predicaments. Legal liability and consumer safety are the same issue when looked at from the perspective of ""cost to consumer"". And here are few dilemmas: Does the algorithm recognize the difference between a human being and
an animal? If an animal/human cannot be legally avoided (and the car is in legal right - if it is not then something else is wrong with the AI's decision making), it likely won't. If the car can safely avoid the obstacle, the AI could reasonably be seen to make this decision, ie. swerve to another lane on an open highway. Notice there is an emphasis on liability and driver safety. Does the size of the human being or animal matter? Only the risk factor from hitting the obstacle. Hitting a hippo might be less desirable than hitting the ditch. Hitting a dog is likely more desirable than wrecking the customer's automobile. Does it count how many passengers it has vs. people in the front? It counts the people as passengers to see if the car-pooling lane can be taken. It counts the people in front as a risk factor in case of a collision. Does it ""know"" when babies/children are on board? No. Does it take into the account the age (e.g. killing the older first)? No. This is simply the wrong abstraction to make a decision, how could this be weighted into choosing the right course of action to reduce risk factor? If Option 1 is hit young guy with 20% chance of significant occupant damage and no legal liability and Option 2 is hit an old guy with 21% chance of significant occupant damage and no legal liability, then what philosopher can convince even just 1 person of the just and equitable weights to make a decision? Thankfully, the best decision a lot of the time is to hit the breaks to reduce speed (especially when you consider that it is often valuable to act predictably so that pedestrians and motorists can react accordingly). In the meantime, better value improvements can be made in terms of predicting when drivers will make bad decisions and when other actions (such as hitting the reverse) are more beneficial than hitting the breaks. At this point, it is not worth it to even begin collecting the information to make the ethical decisions proposed by philosophers. Thus, this issue is over-hyped by sensational journalists and philosophers.",59.28802444,"They shouldn't. People should. People cannot put the responsibilities of ethical decisions into the hands of computers. It is our responsibility as computer scientists/AI experts to program decisions for computers to make. Will human casualties still exist from this? Of course, they will--- people are not perfect and neither are programs. There is an excellent in-depth debate on this topic here . I particularly like Yann LeCun's argument regarding the parallel ethical dilemma of testing potentially lethal drugs on patients. Similar to self-driving cars, both can be lethal while having good intentions of saving more people in the long run.",58.86211198
92,How is it possible that deep neural networks are so easily fooled?,convolutional-neural-networks,"First up, those images (even the first few) aren't complete trash despite being junk to humans; they're actually finely tuned with various advanced techniques, including another neural network. The deep neural network is the pre-trained network modeled on AlexNet provided by Caffe . To evolve images, both the directly encoded and indirectly encoded images, we use the Sferes evolutionary framework. The entire code base to conduct the evolutionary experiments can be download [sic] here . The code for the images produced by gradient ascent is available here . Images that are actually random junk were correctly recognized as nothing meaningful: In response to an unrecognizable image, the networks could have output a low confidence for each of the 1000 classes, instead of an extremely high confidence value for one of the classes. In fact, they do just that for randomly generated images (e.g. those in generation 0 of the evolutionary run) The original goal of the researchers was to use the neural networks to automatically generate images that look like the real things (by getting the recognizer's feedback and trying to change the image to get a more confident result), but they ended up creating the above art. Notice how even in the static-like images there are little splotches - usually near the center - which, it's fair to say, are triggering the recognition. We were not trying to produce adversarial, unrecognizable images. Instead, we were trying to produce recognizable images, but these unrecognizable images emerged. Evidently, these images had just the right distinguishing features to match what the AI looked for in pictures. The ""paddle"" image does have a paddle-like shape, the ""bagel"" is round and the right color, the ""projector"" image is a camera-lens-like thing, the ""computer keyboard"" is a bunch of rectangles (like the individual keys), and the ""chainlink fence"" legitimately looks like a chain-link fence to me. Figure 8. Evolving images to match DNN classes produces a tremendous diversity of images. Shown are images selected to showcase diversity from 5 evolutionary runs. The diversity suggests that the images are non-random, but that instead evolutions producing [sic] discriminative features of each target class. Further reading: the original paper (large PDF)",52.12494901,"The images that you provided may be unrecognizable for us. They are actually the images that we recognize but evolved using the Sferes evolutionary framework. While these images are almost impossible for humans to label with anything but abstract arts, the Deep Neural Network will label them to be familiar objects with 99.99% confidence. This result highlights differences between how DNNs and humans recognize objects. Images are either directly (or indirectly) encoded According to this video Changing an image originally correctly classified in a way imperceptible to humans can cause the cause DNN to classify it as something else. In the image below the number at the bottom are the images are supposed to look like the digits
But the network believes the images at the top (the one like white noise) are real digits with 99.99% certainty. The main reason why these are easily fooled is that Deep Neural Network does not see the world in the same way as human vision. We use the whole image to identify things while DNN depends on the features. As long as DNN detects certain features, it will classify the image as a familiar object it has been trained on.
The researchers proposed one way to prevent such fooling by adding the fooling images to the dataset in a new class and training DNN on the enlarged dataset. In the experiment, the confidence score decreases significantly for ImageNet AlexNet. It is not easy to fool the retrained DNN this time. But when the researchers applied such method to MNIST LeNet, evolution still produces many unrecognizable images with confidence scores of 99.99%. More details here and here .",54.94790529,"All answers here are great, but, for some reason, nothing has been said so far on why this effect should not surprise you. I'll fill the blank. Let me start with one requirement that is absolutely essential for this to work: the attacker must know neural network architecture (number of layers, size of each layer, etc). Moreover, in all cases that I examined myself, the attacker knows the snapshot of the model that is used in production, i.e. all weights. In other words, the ""source code"" of the network isn't a secret. You can't fool a neural network if you treat it like a black box. And you can't reuse the same fooling image for different networks. 
In fact, you have to ""train"" the target network yourself, and here by training I mean to run forward and backprop passes, but specially crafted for another purpose. Why is it working at all? Now, here's the intuition. Images are very high dimensional: even the space of small 32x32 color images has 3 * 32 * 32 = 3072 dimensions. But the training data set is relatively small and contains real pictures, all of which have some structure and nice statistical properties (e.g. smoothness of color). So the training data set is located on a tiny manifold of this huge space of images. The convolutional networks work extremely well on this manifold, but basically, know nothing about the rest of the space. The classification of the points outside of the manifold is just a linear extrapolation based on the points inside the manifold. No wonder that some particular points are extrapolated incorrectly. The attacker only needs a way to navigate to the closest of these points. Example Let me give you a concrete example how to fool a neural network. To make it compact, I'm going to use a very simple logistic regression network with one nonlinearity (sigmoid). It takes a 10-dimensional input x , computes a single number p=sigmoid(W.dot(x)) , which is the probability of class 1 (versus class 0). Suppose you know W=(-1, -1, 1, -1, 1, -1, 1, 1, -1, 1) and start with an input x=(2, -1, 3, -2, 2, 2, 1, -4, 5, 1) . A forward pass gives sigmoid(W.dot(x))=0.0474 or 95% probability that x is class 0 example. We'd like to find another example, y , which is very close to x but is classified by the network as 1. Note that x is 10-dimensional, so we have the freedom to nudge 10 values, which is a lot. Since W[0]=-1 is negative, it's better for to have a small y[0] to make a total contribution of y[0]*W[0] small. Hence, let's make y[0]=x[0]-0.5=1.5 .
Likewise, W[2]=1 is positive, so it's better to increase y[2] to make y[2]*W[2] bigger: y[2]=x[2]+0.5=3.5 . And so on. The result is y=(1.5, -1.5, 3.5, -2.5, 2.5, 1.5, 1.5, -3.5, 4.5, 1.5) , and sigmoid(W.dot(y))=0.88 . With this one change we improved the class 1 probability from 5% to 88%! Generalization If you look closely at the previous example, you'll notice that I knew exactly how to tweak x in order to move it to the target class, because I knew the network gradient. What I did was actually a backpropagation , but with respect to the data, instead of weights. In general, the attacker starts with target distribution (0, 0, ..., 1, 0, ..., 0) (zero everywhere, except for the class it wants to achieve), backpropagates to the data and makes a tiny move in that direction. Network state is not updated. Now it should be clear that it's a common feature of feed-forward networks that deal with a small data manifold, no matter how deep it is or the nature of data (image, audio, video or text). Potection The simplest way to prevent the system from being fooled is to use an ensemble of neural networks, i.e. a system that aggregates the votes of several networks on each request.
It's much more difficult to backpropagate with respect to several networks simultaneously. The attacker might try to do it sequentially, one network at a time, but the update for one network might easily mess up
with the results obtained for another network. The more networks are used, the more complex an attack becomes. Another possibility is to smooth the input before passing it to the network. Positive use of the same idea You shouldn't think that backpropagation to the image has only negative applications. A very similar technique, called deconvolution , is used for visualization and better understanding what neurons have learned. This technique allows synthesizing an image that causes a particular neuron to fire, basically see visually ""what the neuron is looking for"", which in general makes convolutional neural networks more interpretable.",54.24313467,"An important question that does not yet have a satisfactory answer in neural network research is how DNNs come up with the predictions they offer. DNNs effectively work (though not exactly) by matching patches in the images to a ""dictionary"" of patches, one stored in each neuron (see the youtube cat paper ). Thus, it may not have a high level view of the image since it only looks at patches, and images are usually downscaled to much lower resolution to obtain the results in current generation systems. Methods which look at how the components of the image interact may be able to avoid these problems. Some questions to ask for this work are: How confident were the networks when they made these predictions? How much volume do such adversarial images occupy in the space of all images? Some work I am aware of in this regard comes from Dhruv Batra and Devi Parikh's Lab at Virginia Tech who look into this for question answering systems: Analyzing the Behavior of Visual Question Answering Models and Interpreting Visual Question Answering models . More such work is needed, and just as the human visual system does also get fooled by such ""optical illusions"", these problems may be unavoidable if we use DNNs, though AFAIK nothing is yet known either way, theoretically or empirically.",52.10923737,"How is it possible that deep neural networks are so easily fooled? Deep neural networks are easily fooled by giving high confidence predictions for unrecognizable images. How is this possible? Can you please explain ideally in plain English? Intuitively, extra hidden layers ought to make the network able to learn more complex classification functions, and thus do a better job classifying. While it may be named deep learning it's actually a shallow understanding. Test your own knowledge: Which animal in the grid below is a Felis silvestris catus , take your time and no cheating. Here's a hint: which is a domestic house cat? For a better understanding checkout: "" Adversarial Attack to Vulnerable Visualizations "" and "" Why are deep neural networks hard to train? "". The problem is analogous to aliasing , an effect that causes different signals to become indistinguishable (or aliases of one another) when sampled, and the stagecoach-wheel effect , where a spoked wheel appears to rotate differently from its true rotation. The neural network doesn't know what it's looking at or which way it's going. Deep neural networks aren't an expert on something, they are trained to decide mathematically that some goal has been met, if they are not trained to reject wrong answers they don't have a concept of what is wrong; they only know what is correct and what is not correct - wrong and ""not correct"" are not necessarily the same thing, neither is ""correct"" and true. The neural network doesn't know right from wrong. Just like most people wouldn't know a house cat if they saw one, two or more, or none. How many house cats in the above photo grid, none. Any accusations of including cute cat pictures is unfounded, those are all dangerous wild animals. Here's another example. Does answering the question make Bart and Lisa smarter, does the person they are asking even know, are there unknown variables that can come into play? We aren't there yet but neural networks can quickly provide an answer that is likely to be correct, especially if it was properly trained to avoid all misteps.",67.64828163,"Can't comment(due to that required 50 rep), but I wanted to make a response to Vishnu JK and the OP. I think you guys are skipping the fact that the neural network only really is saying truly from a programmatic standpoint that ""this is most like"". For example, while we can list the above image examples as ""abstract art"", they definitively are most like was is listed. Remember learning algorithms have a scope on what they recognize as an object and if you look at all the above examples... and think about the scope of the algorithum... these make sense (even the ones at a glance we would recognize as white noise). In Vishnu example of the numbers, if you fuzz your eyes and bring the images out of focus, you can actually in every case spot patterns that really closely reflect the numbers in question. The problem that is being shown here is that the algorithm appears to not have a ""unknown case"". Basically when the pattern recognition says that it doesn't exist in the output scope. (so a final output node group that says this is nothing that I know off). For example, people do this as well, as it's one thing humans and learning algorithms have in common. Here's a link to show what I'm talking about (what is the following, define it) using only known animals that exist: Now as a person, limited by what I know and can say, I'd have to conclude that the following is an elephant. But it's not. Learning algorithms (for the most part) do not have a ""like a"" statement, the out put always validates down to a confidence percentage. So tricking one in this fashion is not surprising... what is of course surprising is that based on it's knowledge set, it actually comes to the point in which, if you look at the above cases listed by OP and Vishnu that a person... with a little looking... can see how the learning algorithm probable made the association. So, I wouldn't really call it a mislabel on the part of the algorithm, or even call it a case where it's been tricked... rather a case where it's scope was developed incorrectly.",51.83411938,"The neural networks can be easily fooled or hacked by adding certain structured noise in image space ( Szegedy 2013 , Nguyen 2014 ) due to ignoring non-discriminative information in their input. For example: Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs. 2015 So basically the high confidence prediction in certain models exists due to a ' combination of their locally linear nature and high-dimensional input space '. 2015 Published as a conference paper at ICLR 2015 (work by Dai) suggest that transferring discriminatively trained parameters to generative models, could be a great area for further improvements.",54.80630272,"There is already many good answers, I will just add to those that came before mine: This type of images you are referring to are called adversarial perturbations , (see 1 , and it is not limited to images, it has been shown to apply to text too, see Jia & Liang, EMNLP 2017 . In text, the introduction of an irrelevant sentence which doesn't contradict the paragraph has been seen to cause the network to come to a completely different answer (see see Jia & Liang, EMNLP 2017 ). The reason they work is due to the fact that neural network view images in a different way from us, coupled with the high dimentionality of the problem space. Where we see the whole picture, they see a combination of features which combine to form an object( Moosavi-Dezfooli et al., CVPR 2017 ). According to  perturbation generated against one network has been seen to have high likelihood to work on other networks: In the figure above, it is seen that The universal perturbations computed for the VGG-19 network, for example, have a fooling ratio above 53% for all other tested architectures. So how do you deal with the threat of adversarial perturbations? 
Well, for one, you can try to generate as many perturbations as you can and use them to fine-tune your model. Whist this somewhat solves the problem, it doesn't solve the problem entirely. In ( Moosavi-Dezfooli et al., CVPR 2017 ) the author reported that, repeating the process by computing new perturbations and then fine-tuning again seems to yield no further improvements, regardless of the number of iterations, with the fooling ratio hovering around 80%. Perturbations are an indication of the shallow pattern matching that neural networks perform, coupled with their minimal lack of in-depth understanding of the problem at hand. More work still needs to be done.",51.62143173,"Neural networks are easily fooled, provided you know how to fool them. Consider a linear network with an input layer and an output layer, which has an error function E (we don't need hidden layers to show how to fool a network). For a given input image x, E measures the (squared) difference between the network's output y and the desired (correct) output. The output unit’s state y is given by the inner product of x with the output unit’s weight vector w,so that y=w·x. If we change x to x′ by adding ∆x then the output will change by ∆y to y′ = w·x+w·∆x (9.4) 

   = y + ∆y. (9.5) Notice that ∆x defines a direction in the input space; the question is, which direction ∆x will have most impact on y? By definition, a change in x in the direction ∇E produces the largest
possible change in y. An adversarial image x' is constructed by taking the derivative ∇E of E with respect to the input image x, that is, x′ = x + ε∇E, where ε is a small constant. 
By definition, ∇E is the direction of steepest ascent, so the modification ε∇E to x will alter y more than a change in any other direction. This is an extract from the book: Artificial Intelligence Engines:  A Tutorial Introduction to the Mathematics of Deep Learning (2019).",54.92386521
84,Are methods of exhaustive search considered to be AI?,gaming,"If one thinks of intelligence as a continuous measure of optimization power (that is, how much better are outcomes for any unit of cognitive effort expended), then exhaustive search has non-zero intelligence (in that it does actually give better outcomes as more effort is expended) but very, very low intelligence (as the outcomes are better mostly by luck, and the amount of effort expended can be impossibly large).",54.22520561,"If a computer is just brute-forcing the solution, it's not learning anything or using any kind of intelligence at all, and therefore it shouldn't be called ""artificial intelligence."" It has to make decisions based on what's happened before in similar instances. For something to be intelligent, it needs a way to keep track of what it's learned. A chess program might have a really awesome measurement algorithm to use on every possible board state, but if it's always trying each state and never storing what it learns about different approaches, it's not intelligent.",51.28840578,"Really any 'intelligence' exhibited by a computer is deemed AI, regardless of brute force or use of smart heuristics. For example, a chat bot can be coded to respond to most responses using many, many if statements. This is an AI no matter how poorly coded/designed it is. The chess playing computer beating a human professional can be seen as a meaningful milestone. I mean, someone programmed a computer to beat grandmaster chess players and chess geniuses. Many thought that wasn't possible since chess is such a complex game. This kind of work likely segued into more complex AI, for if a computer could play chess, then it surely complete other complex tasks as well. Note how refined chess programming is: magic bitboards, Zobrist hashing, pruning, lazy SMP, and many more. This is perhaps not the sort of milestone of AI that you thought, but again, the things that can be considered AI are pretty broad.",52.66765482,"Brute force approach is certainly the first step of many in AI programming. But using these experiences the program must learn to find the best solution or at least a closer solution to the problem. Since the first goal in AI is to find any solution, nothing can beat the brute force approach. But then using the previous results of brute force approaches, the program must develop its own heuristics and use this data along with brute force to find the optimal solution.",50.88505372,"I dont know why you wouldnt consider it ai since every single thing has used something like it thats been in the recent news. evolving a neural network is very similar to brute force search,  just it hits local optima, because its not exhaustive.",55.197426,,,,,,,,
36,To what extent can quantum computers help to develop Artificial Intelligence?,quantum-computing,"Quantum computers are super awesome at matrix multiplication, with some limitations . Quantum superposition allows each bit to be in a lot more states than just zero or one, and quantum gates can fiddle those bits in many different ways. Because of that, a quantum computer can process a lot of information at once for certain applications. One of those applications is the Fourier transform , which is useful in a lot of problems, like signal analysis and array processing. There's also Grover's quantum search algorithm , which finds the single value for which a given function returns something different. If an AI problem can be expressed in a mathematical form amenable to quantum computing , it can receive great speedups. Sufficient speedups could transform an AI idea from ""theoretically interesting but insanely slow"" to ""quite practical once we get a good handle on quantum computing.""",60.50128517,"Until we can make a quantum computer with a lot more qubits, the potential to further develop AI will remain just that. D-Wave (which has just made a 2,000+ qubit system around 2015) is an adiabatic quantum computer , not a general-purpose quantum computer. It is restricted to certain optimization problems (at which its effectiveness has reportedly been doubted by one of the originators of the theory on which it is based). Suppose that we could build a 32 qubit general-purpose quantum computer (twice as big as current models, as far as I'm aware). This would still mean that only 2 32 possibilities exist in superposition. This is a space small enough to be explored exhaustively for many problems. Hence, there are perhaps not so many problems for which any of the known quantum algorithms (e.g. Shor , Grover ) would be useful for that number of bits.",56.2629493,"Quantum computers can help further develop A.I. algorithms and solve the problems to the extent of our creativity and ability to define the problem. For example breaking cryptography can take seconds, where it can takes thousands of years for standard computers. The same with artificial intelligence, it can predict all the combinations for the given problem defined by algorithm. This is due to superposition of multiple states of quantum bits. Currently, quantum computers are still in the early stages of development and can perform complex calculation. There are already technologies like D-Wave systems which are used by Google and NASA for complex data analysis, using Multi-Qubit type quantum computers for solving NSE fluid dynamics problems of interest or global surveillance for military purposes, and many more which we're not aware. Currently there are only a few quantum computers available to the public, like IBM Quantum Experience (the world’s first quantum computing platform delivered via the IBM Cloud), but it's programming on quantum logic gates levels, so we're many years behind creating artificial intelligence available to public. There are some quantum computing languages such as QCL, Q or Quipper, but I'm not aware any libraries which can provide artificial intelligence frameworks. It doesn't mean it's not there, and I'm sure huge companies and governments organisations are using it for their agenda to outcome the competition (like financial market analysis, etc.).",68.37512429,"Direct Answer to Your Question :-- The field where quantum computing and A.I. intersect is called quantum machine learning . A.I. is a developing field, with some background (ala McCarthy of LISP fame). Quantum computing is a virgin field that is largely unexplored. A particular type of complexity interacts with another type of complexity to create a very rich field. Now combine (1) and (2), and you end up with even more uncertainty; the technical details shall be explored in this answer. Google Explains Quantum Computing in One Simple Video: Google and NASA's Quantum Artificial Intelligence Lab Body :-- IBM is an authority:-- IBM: Quantum Computers Could Be Useful, But We Don't Know Exactly How Quantum machine learning is an interesting phenomenon. This field studies the intersection between quantum computing and machine learning. ( https://en.wikipedia.org/wiki/Quantum_machine_learning ) ""While machine learning algorithms are used to compute immense quantities of data, quantum machine learning increases such capabilities intelligently, by creating opportunities to conduct analysis on quantum states and systems."" Wikipedia contributors. — ""Quantum machine learning."" Wikipedia, The Free Encyclopedia . Wikipedia, The Free Encyclopedia, 7 Oct. 2019. Web. 11 Oct. 2019. Technical Mirror :-- This particular section on the implementations is worth noting:-- ( https://en.wikipedia.org/wiki/Quantum_machine_learning#Implementations_and_experiments ) "" ... This dependence on data is a powerful training tool. But it comes with potential pitfalls. If machines are trained to find and exploit patterns in data then, in certain instances, they only perpetuate the race, gender or class prejudices specific to current human intelligence. But the data-processing facility inherent to machine learning also has the potential to generate applications that can improve human lives. 'Intelligent' machines could help scientists to more efficiently detect cancer or better understand mental health. Most of the progress in machine learning so far has been classical: the techniques that machines use to learn follow the laws of classical physics. The data they learn from has a classical form. The machines on which the algorithms run are also classical. We work in the emerging field of quantum machine learning, which is exploring whether the branch of physics called quantum mechanics might improve machine learning. Quantum mechanics is different to classical physics on a fundamental level: it deals in probabilities and makes a principle out of uncertainty. Quantum mechanics also expands physics to include interesting phenomena which cannot be explained using classical intuition. ... "" — ""Explainer: What Is Quantum Machine Learning And How Can It Help Us?"". Techxplore.Com , 2019, https://techxplore.com/news/2019-04-quantum-machine.html . A Future with Quantum Machine Learning Quantum Computing, Deep Learning, and Artificial Intelligence Business Applications and Practical Uses :-- Is Your IT Department Prepared For The Next Wave Of Enterprise Tech? (Quantum Computing is mentioned here.) D-Wave Announces Quadrant Machine Learning Business Unit Further Reading :-- ( https://techxplore.com/news/2019-04-quantum-machine.html ) ( https://physics.aps.org/articles/v12/74?fbclid=IwAR2hVTFReQA-3lTNQXKEAtQN7KQ5Lz41wyM19DJDtS1H4fLDNivqxqh5G2k ) ( https://www.forbes.com/sites/bernardmarr/2017/09/05/how-quantum-computers-will-revolutionize-artificial-intelligence-machine-learning-and-big-data/#59b153bf5609 ) ( http://www.messagetoeagle.com/what-is-quantum-machine-learning-and-how-can-it-help-us/ )",61.69635319,"I would say that the answer to your question, is yes . I could write a short book on the subject (that might be a good idea actually) but I will keep this response brief, although I am happy to answer any further questions you may have to which I possess the answers in the comments! The primary reasons that I believe the newly blossoming field of Quantum Information will have a massive impact on the field of Machine Learning in general, are as follows: The most simple reason for my belief is that the primary goal of Machine Learning is to create an entity that is capable of coherent, self-aware thought much like we exhibit as human beings. We know that the brain is what allows us to be capable of such feats, and thus I view the field as something like brain counterfeiting. Without going into esoteric detail, there are many subtleties of the brain's workings that are thought to be quantum mechanical in operation, and thus would suggest that the path of least resistance to replicating the system would require a quantum mechanical computational medium. The second primary rationale which solidifies my position is the efficiency gained when mapping linear operations into a qubit-based formalism. Which is primarily due to the quantum phenomena referred to as Super Position , which allows for a multiple qubit gate to not only work with the options of 00, 01, 10, and 11 (assuming a two-qubit gate); but to also work with any combination in-between, during the computation. This illustrates the concept of a Bloch Sphere representation in a more concrete manner. Although when the result is obtained (this is what is referred to as collapsing the wave function) you will only still have a resulting state space with 2^n possibilities, where n is the number of qubits. This being said, there are very clever ways, by which one can design their algorithms to make full use of this technically infinite computational space before observing the final results. Conclusion : I hope that my answer is helpful to you in some way, although I am aware that it is not a very in-depth answer, I feel that it hits the primary reason why my personal belief is that there will be a wall which is hit, in the pursuit of a general AI, while we are limited to classical computation faculties; and thus will require quantum-based computation before we are able to truly mimic the brain's most well-kept secrets! The next couple of decades should be VERY interesting in the fields, keep a close eye on the latest happenings!",54.07003638,,,,,,,,
35,What is the difference between artificial intelligence and machine learning?,machine-learning,"Machine learning has been defined by many people in multiple (often similar) ways [ 1 , 2 ]. One definition says that machine learning (ML) is the field of study that gives computers the ability to learn without being explicitly programmed. Given the above definition, we might say that machine learning is geared towards problems for which we have (lots of) data (experience), from which a program can learn and can get better at a task. Artificial intelligence has many more aspects, where machines may not get better at tasks by learning from data, but may exhibit intelligence through rules (e.g. expert systems like Mycin ), logic or algorithms, e.g. path-finding). The book Artificial Intelligence: A Modern Approach shows more research fields of AI, like Constraint Satisfaction Problems , Probabilistic Reasoning or Philosophical Foundations .",57.67897407,"Machine learning is a subset of artificial intelligence. Roughly speaking, it corresponds to its learning side. There is no ""official"" definitions, boundaries are a bit fuzzy.",57.47571381,"Artificial intelligence According to the book Artificial Intelligence: A Modern Approach (section 1.1), artificial intelligence (AI) has been defined in multiple ways, which can be organized into 4 categories. Thinking Humanly Thinking Rationally Acting Humanly Acting Rationally Figure 1.1 (of the same book) contains 8 definitions (by renowned people like Bellman, Winston or Kurzweil). Each box contains 2 similar definitions (i.e. both fall into the same category). These definitions vary along 2 dimensions. The definitions in the top row are concerned with thought-processes and reasoning , while the ones in the bottom are concerned with behaviour . The definitions on the left are associated with human intelligence , while the ones on the right with an idealized version of intelligence, which the authors of the AIMA book call rationality . So, for example, the definitions in the top-left corner are based on thinking humanly , while the definitions on the bottom-right corner are based on acting rationally . There is also a definition of AI by John McCarthy , who is one of the official founders of the AI field in 1956 . It is the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable. Machine learning There have been also multiple (similar) definitions of machine learning (ML). For example, Tom Mitchell, in section 1.1. of his book Machine Learning , defines machine learning as follows. A computer program is said to learn from experience $E$ with respect
to some class of tasks $T$ and performance measure $P$ , if its performance at tasks in $T$ , as measured by $P$ , improves with experience $E$ . What is the difference between AI and ML? ML is a subfield of AI, which is data-oriented (or experience-driven). AI is not just ML, but it's also composed of Natural Language Processing, and other subfields.",58.53801791,"The machine learning is a sub-set of artificial intelligence which is only a small part of its potential. It's a specific way to implement AI largely focused on statistical/probabilistic techniques and evolutionary techniques. Q Artificial intelligence Artificial intelligence is ' the theory and development of computer systems able to perform tasks normally requiring human intelligence ' (such as visual perception, speech recognition, decision-making, and translation between languages). We can think of AI as the concept of non-human decision making Q which aims to simulate cognitive human-like functions such as problem-solving, decision making or language communication. Machine learning Machine learning (ML) is basically a learning through doing by the implementation of build models which can predict and identify patterns from data. According to Prof. Stephanie R. Taylor of Computer Science and her lecture paper , and also Wikipedia page , 'machine learning is a branch of artificial intelligence and it's about construction and study of systems that can learn from data ' (like based on the existing email messages to learn how to distinguish between spam and non-spam). According to Oxford Dictionaries , the machine learning is ' the capacity of a computer to learn from experience ' (e.g. modify its processing on the basis of newly acquired information). We can think ML as computerized pattern detection in the existing data to predict patterns in future data. Q In other words, machine learning involves the development of self-learning algorithms and artificial intelligence involves developing systems or software to mimic human to respond and behave in a circumstance. Quora",61.72627183,"Machine learning is a subfield of artificial intelligence, as the following diagram (taken from this blog post ) illustrates.",57.96155392,"Many terms have 'mostly' the same meanings, and so the differences are just in emphasis, perspective, or historical descent. People disagree as to which label refers to the superset or the subset; there are people who will call AI a branch of ML and people who will call ML a branch of AI. I typically hear Machine Learning used as a form of 'applied statistics' where we specify a learning problem in enough detail that we can just feed training data into it and get a useful model out the other side. I typically hear Artificial Intelligence as a catch-all term to refer to any sort of intelligence embedded in the environment or in code. This is a very expansive definition, and others use narrower ones (such as focusing on artificial general intelligence, which is not domain-specific). (Taken to an extreme, my version includes thermostats.)",54.42061491,"In simple words, Artificial intelligence is a field of science that is trying to mimic humans or other animals behavior. Machine Learning is one of the key tool/technology behind Artificial intelligence.",59.10433765,"Artificial Intelligence (AI) and Machine Learning (ML) are two very hot buzzwords right now, and often seem to be used interchangeably. They are not quite the same thing, but the perception that they are can sometimes lead to some confusion. So I thought it would be worth writing a piece to explain the difference. Machine learning is a core sub-area of artificial intelligence; it enables computers to get into a mode of self-learning without being explicitly programmed. When exposed to new data, these computer programs are enabled to learn, grow, change, and develop by themselves.",61.41992086,"First of all, I encountered the term MachineLearning much more in my Business Intelligence classes than in my AI classes. My AI Professor Rolf Pfeifer would have put it that way: (after having a long speech about what intelligence is, how it can be defined, different types of intelligence, etc.). ML is more static and ""dumb"", unaware of its physical environment and not made to interact with it, or only on an abstract basis. AI has a certain awareness of its environment and interacts with it autonomously, making thereby autonomous decisions with feedback loops.
From that point of view, Ugnes Answer would be probably the closest.
Besides that, of course, ML is a subset of AI. Machine Learning is not real intelligence (imho), it's mostly human intelligence reflected in logical algorithms, and as my Business Intelligence Prof would put it: about data and its analysis. Machine Learning has a lot of supervised algorithms which actually do need humans to support the learning process by telling what's right and what's wrong, so they're not independent. And once they're applied, algorithms are mostly static until humans readjust them.
In ML you mostly have black boxes designs and the main aspect is data. Data comes in, Data gets analyzed (""Intelligently""), Data goes out, and Learning most times applies to a pre-implementation/Learning fase. In most cases ML doesn't care about the environment a machine is in, it's about data. AI instead is about mimicking human or animal intelligence. Following my Prof's approach, AI is not necessarily about self-consciousness but about interaction with the environment, so to build AI you need to give the machine sensors to perceive the environment, a sort of intelligence able to keep on learning, and elements to interact with the environment (arms, etc.). The interaction should happen in an autonomous way and ideally, as in humans, learning should be an autonomous, ongoing process. So a drone that scans fields in a logical scheme for colour patterns to find weeds within crops would be more ML. Especially if the data is later analyzed and verified by humans or the algorithm used is that a static algorithm with built-in ""intelligence"" but not capable of rearranging or adapting to its environment.
A drone that flies autonomously, charges itself up when the battery's down, scans for weeds, learns to detect unknown ones and rips them out by itself and brings them back for verification, would be AI...",55.25587349
28,Is a genetic algorithm an example of artificial intelligence?,philosophy,"An ability that is commonly attributed to intelligence is problem solving . Another one is learning (improving itself from experience). Artificial intelligence can be defined as ""replicating intelligence, or parts of it, at least in appearance, inside a computer"" (dodging the definition of intelligence itself). Genetic algorithms are computational problem solving tools that find and improve solutions (they learn ). Thus, genetic algorithms are a kind of artificial intelligence. Regarding scale, I don't see it as an important factor for defining G.A. as A.I or not. The same way we can simply classify different living forms as more or less intelligent instead of just saying intelligent or not intelligent. Finally, let's just make an important distinction: our brains are the product of natural selection, but the brains themselves don't use the same principle in order to achieve intelligence.",59.26204597,"This is probably more a question of philosophy than anything. In terms of how things are commonly defined, I'll say ""yes, genetic algorithms are part of AI"".  If you pick up a comprehensive book on artificial intelligence, there will probably be a chapter on genetic algorithms (or more broadly, evolutionary algorithms). One area that has been extensively studied in the past is the idea of using genetic algorithms to train neural networks.  I don't know if people are still actively researching this topic or not, but it at least illustrates that GA's are part of the overall rubric of AI in one regard.",56.56521472,"The notion of genetics used in Genetic Algorithms (GAs) is a very stripped down version relative to genetics in nature, essentially consisting of a population of 'genes' (representing solutions to some predefined problem) subject to `survival of the fittest' during iterated application of recombination and mutation. Nowadays, the term 'Computational Intelligence' (CI) tends to be used to describe computational techniques intended to produce `the appearance of intelligence by any computational means', rather than specifically attempting to mimic the mechanisms that are believed to give rise to human (or animal) intelligence. That said, the distinction between CI and AI is not so hard and fast, and arguably arose during the `AI Winter' when the term AI was out of fashion.",53.28294237,"Human intelligence is not an example of natural genetic algorithms. Genetic algorithms have collections of solutions that are collided with each other to make new solutions, eventually returning the best solution. Human intelligence is a network of neurons doing information processing, and almost all of it doesn't behave the same way. But that something doesn't behave in the same way that human intelligence does doesn't mean that it's not an AI algorithm; I would include 'genetic algorithms' as a numerical optimization technique, and since optimization and intelligence are deeply linked any numerical optimization technique could be seen as an AI technique.",59.89097396,"To answer this question, you must first know what is intelligence, and since there is no clear line between intelligent and not, this question is more philosophical rather than technical. In my opinion, intelligence is the ability to define a problem and find a way to solve it using memory and reasoning. Since a genetic algorithm follows this structure, I would say that it falls under the category of artificial intelligence.",60.94551145,,,,,,,,
15,"Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?",turing-test,"The rhetorical point of the Turing Test is that it places the 'test' for 'humanity' in observable outcomes , instead of in internal components . If you would behave the same in interacting with an AI as you would with a person, how could you know the difference between them? But that doesn't mean it's reliable, because intelligence has many different components and there are many sorts of intellectual tasks. The Turing Test, in some respects, is about the reaction of people to behavior, which is not at all reliable--remember that many people thought ELIZA , a very simple chatbot, was an excellent listener and got deeply emotionally involved very quickly. It calls to mind the Ikea commercial about throwing out a lamp , where the emotional attachment comes from the human viewer (and the music), rather than from the lamp. Turing tests for specific economic activities are much more practically interesting--if one can write an AI that replaces an Uber driver, for example, what that will imply is much clearer than if someone can create a conversational chatbot.",59.62704233,"The problem of the Turing Test is that it tests the machines ability to resemble humans. Not necessarily every form of AI has to resemble humans. This makes the Turing Test less reliable. However, it is still useful since it is an actual test. It is also noteworthy that there is a prize for passing or coming closest to passing the Turing Test, the Loebner Prize . The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from Wikipedia ). This definition is used more often and does not depend on the ability to resemble humans. However, it is harder to test this.",61.74191918,"The classical Turing Test certainly does have limitations. Because I don't see it mentioned here yet, I'll suggest you read about The Chinese Room , which is one of the most commonly cited reasons why the Turing Test indeed falls short of ascertaining true 'consciousness'. However, I'd also note that Turing himself, in the original paper that proposed the Turing Test , explicitly acknowledged himself that the test was not a test to detect consciousness : I propose to consider the question, ""Can machines think?"" This should begin with definitions of the meaning of the terms ""machine"" and ""think."" The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous, If the meaning of the words ""machine"" and ""think"" are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, ""Can machines think?"" is to be sought in a statistical survey such as a Gallup poll. But this is absurd. Instead of attempting such a definition I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words. The new form of the problem can be described in terms of a game which we call the 'imitation game."" This imitation game is the test that we now know today (and also the inspiration for the name of a recent feature film starring Benedict Cumberbatch and Keira Knightley).",59.39518439,"There are many definitions of Artificial Intelligence out in the wild. All these definitions are part of one (or more) of the areas. There are four main domains, and the picture below will shed some light over this. Turing Test revolves around the left side of the cardinality, which is mostly concerned with how humans think or act. But, we know that this is just not all. Turing Test has not much to offer when it comes to what AI is in a general sense. Turing Test, as the Wikipedia states, was created to test machines exhibiting behaviour equivalent or indistinguishable from that of a human. Artificial Intelligence is much more than what humans can do or how they act. There are many human acts that are considered unintelligent and sometimes inhuman too. Chinese Room Argument focuses on something very important when it comes to ""Consciousness v/s Simulation of Consciousness"" . John Searle argued there that it is possible for a machine (or human) to follow a huge number of predefined rules (algorithm), in order to complete the task, without thinking or possessing the mind. Weak AIs are good at simulating the ability to understand but, don't really understand what they are doing. They don't exhibit ""Self-Awareness"" and don't form representation about themselves. ""I want that v/s I know I want that"" are two different things. As Theory of Mind states that a good AI should not just form representation about the world it is working on, but also about other agents and entities in the world. This two concepts of self-awareness and theory of mind draw a thin line between weak and strong AI. When it comes to the Turing Test, it fails on many grounds and so does the Total Turing Test, which adds another layer to the test. Most of the researchers believe that Turing Test is just a distraction from the main goal, something that hinders them from fruitful work. Consider this, suppose you ask a difficult arithmetic problem in order to distinguish between human and machine. If the machine wants to pretend it is human then it will lie. This is not what we want. Going for the Turing Test sets the upper bound to the AI that can be created. Also making AI act and behave like humans is not a very good idea. Humans are not very good at making right decisions all the time. This is the reasons why we read about wars in our history books. Decisions which we make are often biased, have selfish origins, etc. We don't want an AI to come with all those things. I don't think there is one test to test an AI. This is because AI has many definitions, many types. Whether an AI is weak or strong can be tagged while looking for answers to questions like, ""I want that v/s I know I want that"", ""Who am I and what exactly I am doing (from machine's perspective)"", plus some other questions I mentioned above.",61.6354459,"It depends on how the test is given. For example, when people claimed that a machine had successfully passed the Turing Test a few years ago, the criteria was pretty weak. It only had to fool 30% of the people for 5 minutes. That's not much of a test. To put this in perspective you probably wouldn't detect schizophrenia, autism, learning disabilities, or dementia with this criteria. In spite of the hype, the current AI's can be detected 100% of the time using fairly simple questions.",57.57907206,"Is the Turing Test, or any of its variants, a reliable test of artificial intelligence? Myopia Yes, if one defines the term Artificial Intelligence in terms of Alan Turing's Imitation Game or one of its variants.  The approach may be, at the same time, both valid and very limited as a definition of intelligence as people interpreted the word before AI emerged. Proven Intelligence Consequently, there are a large number of alternative approaches to measuring intelligence, artificial or otherwise. Becoming a chess grand master Authoring a winning chess program Receiving a highly selective international award Creating a strategy that wins a war or a peace Overcoming the thousands of rounds of elimination in business or politics to become President Authoring brilliant articles, papers, screenplays, lectures, speeches, books, or poems that generate significant human paradigm shifts Showing genius level results in a Mensa test Becoming one of the most wealthy people in the world Normal Measurement of Normal Intelligence But these are measurements of exceptional intelligence of some kind, mostly because the leaders in these areas have reliably applied intelligence over multiple domains in such ways that led to remarkable success through multiple real life scenarios.  The reliability is an attribute of the person possessing the intelligence, not the test of intelligence itself. These are more mundane, yet perhaps more valid and reliable, measures of intelligence. Raising healthy and loving children as verifiable through the careful interviewing of friends and associates of the members of the family Repeated and successful remedy of many conditions of varying types that were once identified as broken in some tangible and measurable way and found to be measurably corrected as a result of the application of intelligence comprehension, analysis, and remedial action Conversational intelligence as measurable through the participants in conversation attributing their own success to the ideas and examples set by the conversationalist What Are the Truly Desired End Goals? Perhaps the primary characteristic of the Turing Test is that it is artificial.  If artificial intelligence is what we want from AI software, then that is what we will receive.  However, it is likely we want something either considerably more or considerably less. We want more in that it would be nice of some computers could be our friend, our mentor, and an unpaid employee with exceptional abilities leading to our personal success in terms of income, influence, popularity, or legacy. We want less in that we want some computers to do domain specific tasks and remain as fully subservient tools, perhaps with some personality and warmth, like a ship or some other complex device we give human names, yet without the unpredictability of the far reaching capabilities of human intelligence.",67.87356992,,,,,,
10,What is fuzzy logic?,deep-neural-networks,"As complexity rises, precise statements lose meaning and meaningful statements lose precision. ( Lofti Zadeh ). Fuzzy logic deals with reasoning that is approximate rather than fixed and exact. This may make the reasoning more meaningful for a human: Fuzzy logic is an extension of Boolean logic by Lotfi Zadeh in 1965 based on the
mathematical theory of fuzzy sets, which is a generalization of the classical set theory.
By introducing the notion of degree in the verification of a condition, thus enabling a
condition to be in a state other than true or false, fuzzy logic provides a very valuable
flexibility for reasoning, which makes it possible to take into account inaccuracies and
uncertainties. One advantage of fuzzy logic in order to formalize human reasoning is that the rules
are set in natural language. For example, here are some rules of conduct that a driver
follows, assuming that he does not want to lose his driver’s licence: Intuitively, it thus seems that the input variables like in this example are approximately
appreciated by the brain, such as the degree of verification of a condition in fuzzy
logic. I've written a short introduction to fuzzy logic that goes into a bit more details but should be very accessible.",71.67696563,"Fuzzy logic is based on regular boolean logic. Boolean logic means you are working with truth values of either true or false (or 1 or 0 if you prefer). Fuzzy logic is the same apart from you can have truth values that are in-between true and false, which is to say, you are working with any number between 0 (inclusive) and 1 (inclusive). The fact that you can have a 'partially true and partially false' truth value is where the word ""fuzzy"" comes from. Natural languages often use fuzzy logic like ""that balloon is red"" meaning that balloon could be any colour that is similar enough to red, or ""the shower is warm"". Here is a rough diagram for how ""the temperature of the shower is warm"" could be represented in terms of fuzzy logic (the y axis being the truth value and the x-axis being the temperature): Fuzzy logic can be applied to boolean operations such as and , or , and not . Note that you can define the fuzzy logic operations in different ways. One way is with the min and max functions which return the lessermost and greatermost values of the two values inputted respectively. This would work as such: A and B = min(A,B)
A or B  = max(A,B)
not A   = 1-A
(where A and B are real values from 0 (inclusive) to 1 (inclusive)) When defined like this they are called the Zadeh operators . Another way would be to define and as the first argument times the second argument, which yields different outputs for the same inputs as the Zadeh and operator ( min(0.5,0.5)=0.5, 0.5*0.5=0.25 ). Then other operators are derived based on the and and not operators. This would work as such: A and B = A*B
not A = 1-A
A or B = not ((not A) and (not B)) = 1-((1-A)*(1-B)) = 1-(1-A)*(1-B)
(where A and B are real values from 0 (inclusive) to 1 (inclusive)) You can then use the three ""basic fuzzy logic operations"" to build all other ""fuzzy logic operations"", just like you can use the three ""basic boolean operations"" to build all other ""boolean logic operations"". Note that the latter definition of the three basic operations is more in line with probability theory, so could be considered the more natural choice. Sources: Fuzzy logic wikipedia , Boolean algebra wikipedia , Explanation of fuzzy logic on Youtube Note: if anyone could suggest some more reliable sources in the comments, I will happily add them to the list (I understand that the current ones aren't too reliable).",70.23594692,"It's analogous to analogue versus digital, or the many shades of gray in between black and white: when evaluating the truthiness of a result, in binary boolean it's either true or false (0 or 1), but when utilizing fuzzy logic, it's an estimated probability between 0 and 1 (such as 0.75 being mostly probably true). It's useful for making calculated decisions when all information needed isn't necessarily available. Wikipedia has a fantastic page for this .",58.60505047,"Why is it useful? Many things we don't know for sure. We estimate and are often uncertain, but nearly never 100% sure. It may seem like a weakness, but because of this fuzzy approach we can function in this complex world and even behave quite intelligently. Hence it's a way to simplify things. And it gives you some leeway to fill the gaps appropriately, e.g. to adapt to slightly varying situations. 
P.S.: In natural language we express this with quantitive terms like more, less, nearly, rather, immense and so on. But quantifying things is hard for us.",52.23787809,"It is making deductions based on probability and statistics, like humans make decisions all the time. We are never 100% sure the decision we have made is the right one but there is always some doubt present. Ai will definitely need to use it in some form.",50.52661538,"Fuzzy Logic is a way of dealing with uncertainties, which is something that computers don't do naturally which human do very well. The way we instantly think of dealing with things and the way that computers tend to deal with certain things is 'True' or 'False', or '1 or '0'. For example, you might classify someone is alive as alive ('True') or has passed away ('False'). We only have two options, there is no inbetween. With fuzzy logic, instead of going with 'True' or 'False', between that, we have what's called a degree of truth. So for example, when we look out the window, we might say ""It's a bit cloudy today, maybe it's 0.5 'nice day' or 0.7 'nice day'"". So essentially, with fuzzy logic we always have grey areas which vary from person-to-person. Source: https://youtu.be/r804UF8Ia4c",63.65932158,,,,,,
7,"Why does Stephen Hawking say ""Artificial Intelligence will kill us all""?",agi,"It's not just Hawking, you hear variations on this refrain from a lot of people.  And given that they're mostly very smart, well educated, well informed people (Elon Musk is another, for example), it probably shouldn't be dismissed out of hand. Anyway, the basic idea seems to be this: If we create ""real"" artificial intelligence, at some point, it will be able to improve itself, which improves it's ability to improve itself, which means it can improve it's ability to improve itself even more, and so on... a runaway cascade leading to ""superhuman intelligence"".  That is to say, leading to something that more intelligent than we area. So what happens if there is an entity on this planet which is literally more intelligent than us (humans)? Would it be a threat to us?  Well, it certainly seems reasonable to speculate that it could be so.   OTOH, we have no particular reason, right now, to think that it will be so. So it seems that Hawking, Musk, etc. are just coming down on the more cautious / fearful side of things.  Since we don't know if a superhuman AI will be dangerous or not, and given that it could be unstoppable if it were to become malicious (remember, it's smarter than we are!), it's a reasonable thing to take under consideration. Eliezer Yudkowsky has also written quite a bit on this subject, including come up with the famous ""AI Box"" experiment.  I think anybody interested in this topic should read some of his material. http://www.yudkowsky.net/singularity/aibox/",54.94644568,"Because he did not yet know how far away current AI is... Working in an media AI lab, I get this question a lot. But really... we are still a long way from this. The robots still do everything we detailledly describe them to do. Instead of seeing the robot as intelligent, I would look to the human programmer for where the creativity really happens.",50,"As Andrew Ng said , worrying about such threat from AI is like worrying about of overpopulation on Mars. It is science fiction. That being said, given the rise of (much weaker) robots and other (semi-)autonomous agents, the fields of the law and ethics are increasingly incorporating them, e.g. see Roboethics .",50,"To put it simply in layman terms, what are the possible threats from AI? Currently, there are no threat. The threat comes if humans create a so-called ultraintelligent machine, a machine that can surpass all intellectual activities by any human. This would be the last invention man would need to do, since this machine is better in inventing machines than humans are (since that is an intellectual activity).  However, this could cause the machine to invent machines that can destruct humans, and we can't stop them because they are so much smarter than we are. This is all hypothetical, no one has even a clue of what an ultraintelligent machine looks like. If we know that AI is so dangerous why are we still promoting it? Why is it not banned? As I said before, the existence of a ultraintelligent machine is hypothetical. Artificial Intelligence has lots of useful applications (more than this answer can contain), and if we develop it, we get even more useful applications. We just have to be careful that the machines won't overtake us.",52.46049079,"There are a number of long resources to answer this sort of question: consider Stuart Armstrong's book Smarter Than Us , Nick Bostrom's book Superintelligence , which grew out of this edge.org answer , Tim Urban's explanation , or Michael Cohen's explanation . But here's my (somewhat shorter) answer: intelligence is all about decision-making, and we don't have any reason to believe that humans are anywhere near close to being the best possible at decision-making. Once we are able to build an AI AI researcher (that is, a computer that knows how to make computers better at thinking), the economic and military relevance of humans will rapidly disappear as any decision that could be made by a human could be made better by a computer. (Why have human generals instead of robot generals, human engineers instead of robot engineers, and so on.) This isn't necessarily a catastrophe. If the Vulcans showed up tomorrow and brought better decision-making to Earth, we could avoid a lot of misery. The hard part is making sure that what we get are Vulcans who want us around and happy, instead of something that doesn't share our values.",51.9119698,"He says this because it can happen. If something becomes smarter than us, why would it continue to serve us? The worst case scenario is that it takes over all manufacturing processes and consumes all matter to convert it into material capable of computation, extending outward infinitely until all matter is consumed. We know that AI is dangerous but it doesn't matter because most people don't believe in it. It goes against every comfort religion has to offer. Man is the end-all-be-all of the universe and if that fact is disputed, people will feel out of place and purposeless. The fact is most people just don't acknowledge it's possible, or that it will happen in our lifetimes, even though many reputable AI experts put the occurrence of the singularity within two decades. If people truly acknowledged that AI that was smarter than them was possible, wouldn't they be living differently? Wouldn't they be looking to do things that they enjoy, knowing that whatever it is they do that they dread will be automated? Wouldn't everyone be calling for a universal basic income? The other reason we don't ban it is because its promise is so great. One researcher could be augmented by 1,000 digital research assistants. All manual labor could be automated. For the first time, technology offers us real freedom to do whatever we please. But even in this best case scenario where it doesn't overtake us, humans still have to adapt and alter their economic system to one where labor isn't necessary. Otherwise, those who aren't technically-trained will starve and revolt.",53.32399316,,,,,,
1,"What is ""backprop""?",neural-networks,"""Backprop"" is the same as ""backpropagation"": it's just a shorter way to say it. It is sometimes abbreviated as ""BP"".",58.6102422,"'Backprop' is short for 'backpropagation of error' in order to avoid confusion when using backpropagation term. Basically backpropagation refers to the method for computing the gradient of the case-wise error function with respect to the weights for a feedforward network Werbos . And backprop refers to a training method that uses backpropagation to compute the gradient. So we can say that a backprop network is a feedforward network trained by backpropagation . The 'standard backprop' term is a euphemism for the generalized delta rule which is most widely used supervised training method. Source: What is backprop? at FAQ of Usenet newsgroup comp.ai.neural-nets References: Werbos, P. J. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University. Werbos, P. J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting,Wiley Interscience. Bertsekas, D. P. (1995), Nonlinear Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-14-0. Bertsekas, D. P. and Tsitsiklis, J. N. (1996), Neuro-Dynamic Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-10-8. Polyak, B.T. (1964), ""Some methods of speeding up the convergence of iteration methods,"" Z. Vycisl. Mat. i Mat. Fiz., 4, 1-17. Polyak, B.T. (1987), Introduction to Optimization, NY: Optimization Software, Inc. Reed, R.D., and Marks, R.J, II (1999), Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, Cambridge, MA: The MIT Press, ISBN 0-262-18190-8. Rumelhart, D.E., Hinton, G.E., and Williams, R.J. (1986), ""Learning internal representations by error propagation"", in Rumelhart, D.E. and McClelland, J. L., eds. (1986), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1, 318-362, Cambridge, MA: The MIT Press. Werbos, P.J. (1974/1994), The Roots of Backpropagation, NY: John Wiley & Sons. Includes Werbos's 1974 Harvard Ph.D. thesis, Beyond Regression.",58.91798461,"Yes, as Franck has rightly put, ""backprop"" means backpropogation, which is frequently used in the domain of neural networks for error optimization. For a detailed explanation, I would point out this tutorial on the concept of backpropogation by a very good book of Michael Nielsen.",55.01677223,It's a fancy name for the multivariable chain rule.,50,"We need to compute the gradients in-order to train the deep neural networks. Deep neural network consists of many layers. Weight parameters are present between the layers. Since we need to compute the gradients of loss function for each weight, we use an algorithm called backprop. It is an abbreviation for backprop agation, which is also called as error backpropagation or reverse differentiation. It can be understood well from the following paragraph taken from Neural Networks and Neural Language Models For deep networks, computing the gradients for each weight is much
more complex,since we are computing the derivative with respect to
weight parameters that appear all the way back in the very early
layers of the network, even though the loss is computed only at the
very end of the network. The solution to computing this gradient is an
algorithm called error backpropagation or backprop . While backprop was
invented for neural networks, it turns out to be the same as a more
general procedure called backward  differentiation, which depends on
the  notion  of computation graphs.",57.41664501,,,,,,,,
