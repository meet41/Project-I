{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a2369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c17611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (1.7.1)\n",
      "Requirement already satisfied: torch in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (2.8.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (4.55.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2025.7.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af96727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       question_text      topic\n",
      "0  organ choos concern eye stage three point desp...    quantum\n",
      "1  certainli turn marriag consid environ school p...     climat\n",
      "2                       marriag best film dream wish  psycholog\n",
      "3  guess soon hair compani letter fund mission me...    histori\n",
      "4  know say congress view wear amount money succe...     machin\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df = pd.read_csv(r\"E:\\Sem 7 Project\\Datasets\\Preprocessed Datasets\\output_part_1.csv\")\n",
    "df['question_text'] = df['question_text'].apply(ast.literal_eval).apply(lambda x: ' '.join(x))\n",
    "df['topic'] = df['topic'].apply(ast.literal_eval).apply(lambda x: x[0] if len(x) > 0 else '')\n",
    "print(df[['question_text', 'topic']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252d9e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               topic  label\n",
      "6           artifici      0\n",
      "28        blockchain      1\n",
      "10            career      2\n",
      "1             climat      3\n",
      "5         cybersecur      4\n",
      "39              data      5\n",
      "17             digit      6\n",
      "9               educ      7\n",
      "22  entrepreneurship      8\n",
      "7        environment      9\n",
      "69            health     10\n",
      "3            histori     11\n",
      "4             machin     12\n",
      "31            person     13\n",
      "2          psycholog     14\n",
      "0            quantum     15\n",
      "20             robot     16\n",
      "18          sociolog     17\n",
      "8              space     18\n",
      "14               web     19\n"
     ]
    }
   ],
   "source": [
    "# encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['topic'])\n",
    "num_labels = len(le.classes_)\n",
    "print(df[['topic', 'label']].drop_duplicates().sort_values('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b05118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 40\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['question_text'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "print(len(train_texts), len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c08cbf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Meet\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[  101, 17935,  3775,  ...,     0,     0,     0],\n",
      "        [  101,  3813,  2529,  ...,     0,     0,     0],\n",
      "        [  101,  2025,  2594,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2530,  3519,  ...,     0,     0,     0],\n",
      "        [  101,  2345,  5770,  ...,     0,     0,     0],\n",
      "        [  101,  3693,  3095,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    }
   ],
   "source": [
    "# tokenize text\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(texts, max_length=64):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize(train_texts)\n",
    "val_encodings = tokenize(val_texts)\n",
    "print(train_encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb48cfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 40\n"
     ]
    }
   ],
   "source": [
    "# datasets object\n",
    "import torch\n",
    "\n",
    "class BertDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = BertDataset(train_encodings, train_labels)\n",
    "val_dataset = BertDataset(val_encodings, val_labels)\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae86f2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (4.55.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\meet\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.3 MB 6.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.6/11.3 MB 3.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/11.3 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.4/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/11.3 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 5.0/11.3 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.8/11.3 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.8/11.3 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.6/11.3 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.1/11.3 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.9/11.3 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.7/11.3 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.5/11.3 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.3 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.55.2\n",
      "    Uninstalling transformers-4.55.2:\n",
      "      Successfully uninstalled transformers-4.55.2\n",
      "Successfully installed transformers-4.55.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b165222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Meet\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 06:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.904600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.810200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.830400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=2.9360512097676597, metrics={'train_runtime': 390.6977, 'train_samples_per_second': 1.229, 'train_steps_per_second': 0.154, 'total_flos': 15789214679040.0, 'train_loss': 2.9360512097676597, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a10e350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Meet\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 53:24, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.994600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.972500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.917400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.982300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.850300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.152400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.895600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.405600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.982800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.946400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.891700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.883500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.860900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=1.9493124421437582, metrics={'train_runtime': 3212.7479, 'train_samples_per_second': 0.747, 'train_steps_per_second': 0.093, 'total_flos': 78946073395200.0, 'train_loss': 1.9493124421437582, 'epoch': 15.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training - 2\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=15,  # Try 10–20\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c573748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Meet\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.983961582183838, 'eval_runtime': 12.5713, 'eval_samples_per_second': 3.182, 'eval_steps_per_second': 0.398, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# evaluation - 1\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af14e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Meet\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2296111583709717, 'eval_runtime': 23.7273, 'eval_samples_per_second': 1.686, 'eval_steps_per_second': 0.211, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "# evaluation - 2\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a80ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: marriag best film dream wish\n",
      "Predicted topic: person\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "            # approach - 1\n",
    "# test_texts = [\"example question text here\"]\n",
    "# test_encodings = tokenize(test_texts)\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**{k: v for k, v in test_encodings.items()})\n",
    "#     probs = torch.softmax(outputs.logits, dim=1)\n",
    "#     pred_label = probs.argmax(dim=1).item()\n",
    "#     pred_topic = le.inverse_transform([pred_label])[0]\n",
    "#     print(\"Predicted topic:\", pred_topic)\n",
    "\n",
    "\n",
    "            # approach - 2\n",
    "# Example: test_texts = [\"Your first question here\", \"Your second question here\"]\n",
    "# test_texts = [\"organ choos concern eye stage three point despit senior serv citizen idea u\"]\n",
    "test_texts = [\"marriag best film dream wish\"]\n",
    "# Tokenize input (make sure to use the same tokenizer and max_length as in training)\n",
    "test_encodings = tokenizer(\n",
    "    test_texts,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**{k: v for k, v in test_encodings.items()})\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    pred_labels = probs.argmax(dim=1).cpu().numpy()\n",
    "    pred_topics = le.inverse_transform(pred_labels)\n",
    "\n",
    "for question, topic in zip(test_texts, pred_topics):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted topic: {topic}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
